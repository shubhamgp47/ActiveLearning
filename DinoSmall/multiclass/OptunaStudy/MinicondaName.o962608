### Starting TaskPrologue of job 962608 on tg071 at Fri 27 Dec 2024 03:52:43 AM CET
Running on cores 6-7,14-15,22-23,30-31 with governor ondemand
Fri Dec 27 03:52:43 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:AF:00.0 Off |                    0 |
| N/A   30C    P0             25W /  250W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

/home/hpc/iwfa/iwfa044h/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).
  from pandas.core.computation.check import NUMEXPR_INSTALLED
  0%|          | 0/26880 [00:00<?, ?it/s] 25%|██▌       | 6767/26880 [00:00<00:00, 67663.45it/s] 54%|█████▎    | 14402/26880 [00:00<00:00, 70052.77it/s] 82%|████████▏ | 22047/26880 [00:00<00:00, 71854.47it/s]100%|██████████| 26880/26880 [00:00<00:00, 74086.05it/s]
  0%|          | 0/5760 [00:00<?, ?it/s]100%|██████████| 5760/5760 [00:00<00:00, 66973.99it/s]
  0%|          | 0/5760 [00:00<?, ?it/s]100%|██████████| 5760/5760 [00:00<00:00, 75006.65it/s]
[I 2024-12-27 03:53:39,934] Using an existing study with name 'optuna_study04multiclass' instead of creating a new one.
Best trial's number:  27
Best score: 0.7892361111111111
Best hyperparameters:
batch_size: 16
learning_rate: 0.00025
dropout: 0.2
optimizer: RMSprop
step_size: 14
gamma: 0.4
layer_freeze_upto: dino_model.blocks.3.ls2.gamma
Number of trials completed: 27
Number of pruned trials: 25
Total number of trails completed: 52
Number of trials to run: 148
==================== Training of trial number:53 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 14
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Epoch 1/100, Training Loss: 0.6154, Training F1-score: 0.7734, Validation Loss: 0.6498, Validation F1-score: 0.7672
Best F1-score till now on the current trail on Validation data: 0.7671874999999999
Epoch 2/100, Training Loss: 0.4509, Training F1-score: 0.8312, Validation Loss: 0.7359, Validation F1-score: 0.7417
Epoch 3/100, Training Loss: 0.3923, Training F1-score: 0.8533, Validation Loss: 0.7215, Validation F1-score: 0.7356
Epoch 4/100, Training Loss: 0.3697, Training F1-score: 0.8634, Validation Loss: 0.6821, Validation F1-score: 0.7592
Epoch 5/100, Training Loss: 0.3357, Training F1-score: 0.8763, Validation Loss: 0.9060, Validation F1-score: 0.7274
Epoch 6/100, Training Loss: 0.3165, Training F1-score: 0.8829, Validation Loss: 0.7558, Validation F1-score: 0.7410
Epoch 7/100, Training Loss: 0.3018, Training F1-score: 0.8875, Validation Loss: 0.7443, Validation F1-score: 0.7380
Epoch 8/100, Training Loss: 0.2926, Training F1-score: 0.8959, Validation Loss: 0.6911, Validation F1-score: 0.7561
Epoch 9/100, Training Loss: 0.2855, Training F1-score: 0.8954, Validation Loss: 0.8312, Validation F1-score: 0.7309
Epoch 10/100, Training Loss: 0.2801, Training F1-score: 0.8969, Validation Loss: 0.6955, Validation F1-score: 0.7627
Epoch 11/100, Training Loss: 0.2695, Training F1-score: 0.9007, Validation Loss: 0.7617, Validation F1-score: 0.7370
Epoch 12/100, Training Loss: 0.2631, Training F1-score: 0.9041, Validation Loss: 0.7662, Validation F1-score: 0.7422
Epoch 13/100, Training Loss: 0.2544, Training F1-score: 0.9067, Validation Loss: 0.8422, Validation F1-score: 0.7210
Epoch 14/100, Training Loss: 0.2506, Training F1-score: 0.9093, Validation Loss: 0.8731, Validation F1-score: 0.7392
Epoch 15/100, Training Loss: 0.2227, Training F1-score: 0.9200, Validation Loss: 0.8449, Validation F1-score: 0.7330
[I 2024-12-27 04:21:09,622] Trial 53 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.00025, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 14, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 16/100, Training Loss: 0.2150, Training F1-score: 0.9219, Validation Loss: 0.8296, Validation F1-score: 0.7391
Early stopping triggered after 16 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:54 ====================
Learning rate: 0.0000001000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.4.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 04:22:52,916] Trial 54 pruned. 
Epoch 1/100, Training Loss: 1.8825, Training F1-score: 0.4704, Validation Loss: 1.8611, Validation F1-score: 0.4479
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:55 ====================
Learning rate: 0.0075000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 15
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6663, Training F1-score: 0.7535, Validation Loss: 0.7489, Validation F1-score: 0.7429
Best F1-score till now on the current trail on Validation data: 0.7428819444444444
Epoch 2/100, Training Loss: 0.5260, Training F1-score: 0.8064, Validation Loss: 0.6825, Validation F1-score: 0.7642
Best F1-score till now on the current trail on Validation data: 0.7642361111111111
Epoch 3/100, Training Loss: 0.4581, Training F1-score: 0.8297, Validation Loss: 0.6990, Validation F1-score: 0.7410
Epoch 4/100, Training Loss: 0.4364, Training F1-score: 0.8388, Validation Loss: 0.6546, Validation F1-score: 0.7458
Epoch 5/100, Training Loss: 0.4042, Training F1-score: 0.8516, Validation Loss: 0.6475, Validation F1-score: 0.7505
Epoch 6/100, Training Loss: 0.3854, Training F1-score: 0.8589, Validation Loss: 0.6887, Validation F1-score: 0.7352
Epoch 7/100, Training Loss: 0.3704, Training F1-score: 0.8631, Validation Loss: 0.7509, Validation F1-score: 0.7413
Epoch 8/100, Training Loss: 0.3607, Training F1-score: 0.8676, Validation Loss: 0.7505, Validation F1-score: 0.7307
Epoch 9/100, Training Loss: 0.3578, Training F1-score: 0.8688, Validation Loss: 0.7596, Validation F1-score: 0.7111
Epoch 10/100, Training Loss: 0.3406, Training F1-score: 0.8750, Validation Loss: 0.8340, Validation F1-score: 0.7436
Epoch 11/100, Training Loss: 0.3373, Training F1-score: 0.8793, Validation Loss: 0.7377, Validation F1-score: 0.7297
Epoch 12/100, Training Loss: 0.3332, Training F1-score: 0.8797, Validation Loss: 0.9855, Validation F1-score: 0.7101
Epoch 13/100, Training Loss: 0.3241, Training F1-score: 0.8822, Validation Loss: 0.8426, Validation F1-score: 0.7345
Epoch 14/100, Training Loss: 0.3195, Training F1-score: 0.8852, Validation Loss: 0.7242, Validation F1-score: 0.7500
Epoch 15/100, Training Loss: 0.3174, Training F1-score: 0.8847, Validation Loss: 0.8614, Validation F1-score: 0.7323
Epoch 16/100, Training Loss: 0.2817, Training F1-score: 0.8997, Validation Loss: 0.7382, Validation F1-score: 0.7453
[I 2024-12-27 04:51:46,115] Trial 55 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.0075, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 15, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 17/100, Training Loss: 0.2756, Training F1-score: 0.9020, Validation Loss: 0.7423, Validation F1-score: 0.7507
Early stopping triggered after 17 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:56 ====================
Learning rate: 0.0001000000
Dropout: 0.32
Optimizer: Adam
Momentum term: 0
Step size: 18
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.7532, Training F1-score: 0.7290, Validation Loss: 0.6960, Validation F1-score: 0.7517
Best F1-score till now on the current trail on Validation data: 0.7517361111111112
Epoch 2/100, Training Loss: 0.5724, Training F1-score: 0.7846, Validation Loss: 0.7050, Validation F1-score: 0.7545
Best F1-score till now on the current trail on Validation data: 0.7545138888888889
Epoch 3/100, Training Loss: 0.5127, Training F1-score: 0.8062, Validation Loss: 0.7488, Validation F1-score: 0.7382
Epoch 4/100, Training Loss: 0.4806, Training F1-score: 0.8194, Validation Loss: 0.7418, Validation F1-score: 0.7370
Epoch 5/100, Training Loss: 0.4525, Training F1-score: 0.8299, Validation Loss: 0.7079, Validation F1-score: 0.7472
Epoch 6/100, Training Loss: 0.4307, Training F1-score: 0.8376, Validation Loss: 0.8230, Validation F1-score: 0.7273
Epoch 7/100, Training Loss: 0.4141, Training F1-score: 0.8451, Validation Loss: 0.7500, Validation F1-score: 0.7451
Epoch 8/100, Training Loss: 0.3983, Training F1-score: 0.8496, Validation Loss: 0.7332, Validation F1-score: 0.7453
Epoch 9/100, Training Loss: 0.3897, Training F1-score: 0.8542, Validation Loss: 0.7432, Validation F1-score: 0.7458
Epoch 10/100, Training Loss: 0.3755, Training F1-score: 0.8594, Validation Loss: 0.6659, Validation F1-score: 0.7535
Epoch 11/100, Training Loss: 0.3697, Training F1-score: 0.8621, Validation Loss: 0.7384, Validation F1-score: 0.7517
Epoch 12/100, Training Loss: 0.3640, Training F1-score: 0.8635, Validation Loss: 0.6824, Validation F1-score: 0.7509
Epoch 13/100, Training Loss: 0.3521, Training F1-score: 0.8688, Validation Loss: 0.7706, Validation F1-score: 0.7470
Epoch 14/100, Training Loss: 0.3482, Training F1-score: 0.8702, Validation Loss: 0.7357, Validation F1-score: 0.7417
Epoch 15/100, Training Loss: 0.3428, Training F1-score: 0.8713, Validation Loss: 0.7429, Validation F1-score: 0.7528
Epoch 16/100, Training Loss: 0.3362, Training F1-score: 0.8733, Validation Loss: 0.7053, Validation F1-score: 0.7648
Best F1-score till now on the current trail on Validation data: 0.7647569444444444
Epoch 17/100, Training Loss: 0.3296, Training F1-score: 0.8788, Validation Loss: 0.7355, Validation F1-score: 0.7530
Epoch 18/100, Training Loss: 0.3264, Training F1-score: 0.8790, Validation Loss: 0.7230, Validation F1-score: 0.7533
Epoch 19/100, Training Loss: 0.3039, Training F1-score: 0.8893, Validation Loss: 0.7334, Validation F1-score: 0.7519
Epoch 20/100, Training Loss: 0.3006, Training F1-score: 0.8876, Validation Loss: 0.7666, Validation F1-score: 0.7436
Epoch 21/100, Training Loss: 0.2961, Training F1-score: 0.8913, Validation Loss: 0.7177, Validation F1-score: 0.7589
Epoch 22/100, Training Loss: 0.2948, Training F1-score: 0.8915, Validation Loss: 0.7756, Validation F1-score: 0.7438
Epoch 23/100, Training Loss: 0.2938, Training F1-score: 0.8948, Validation Loss: 0.7473, Validation F1-score: 0.7497
Epoch 24/100, Training Loss: 0.2944, Training F1-score: 0.8928, Validation Loss: 0.7061, Validation F1-score: 0.7561
Epoch 25/100, Training Loss: 0.2865, Training F1-score: 0.8952, Validation Loss: 0.7670, Validation F1-score: 0.7470
Epoch 26/100, Training Loss: 0.2876, Training F1-score: 0.8939, Validation Loss: 0.7429, Validation F1-score: 0.7524
Epoch 27/100, Training Loss: 0.2850, Training F1-score: 0.8950, Validation Loss: 0.7812, Validation F1-score: 0.7486
Epoch 28/100, Training Loss: 0.2848, Training F1-score: 0.8953, Validation Loss: 0.7496, Validation F1-score: 0.7535
Epoch 29/100, Training Loss: 0.2780, Training F1-score: 0.8972, Validation Loss: 0.7680, Validation F1-score: 0.7528
Epoch 30/100, Training Loss: 0.2825, Training F1-score: 0.8955, Validation Loss: 0.7722, Validation F1-score: 0.7424
[I 2024-12-27 05:45:42,412] Trial 56 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.0001, 'dropout': 0.32, 'optimizer': 'Adam', 'step_size': 18, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 31/100, Training Loss: 0.2792, Training F1-score: 0.8989, Validation Loss: 0.7330, Validation F1-score: 0.7523
Early stopping triggered after 31 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:57 ====================
Learning rate: 0.0000075000
Dropout: 0.4
Optimizer: Adam
Momentum term: 0
Step size: 17
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 05:47:19,089] Trial 57 pruned. 
Epoch 1/100, Training Loss: 1.3508, Training F1-score: 0.5222, Validation Loss: 1.1560, Validation F1-score: 0.6193
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:58 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 15
Gamma: 0.2
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 05:49:04,526] Trial 58 pruned. 
Epoch 1/100, Training Loss: 0.5975, Training F1-score: 0.7767, Validation Loss: 0.7508, Validation F1-score: 0.7260
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:59 ====================
Learning rate: 0.0000007500
Dropout: 0.35
Optimizer: RMSprop
Momentum term: 0
Step size: 14
Gamma: 0.1
Batch size: 16
Number of frozen parameters: dino_model.blocks.0.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 05:50:43,315] Trial 59 pruned. 
Epoch 1/100, Training Loss: 1.7507, Training F1-score: 0.4541, Validation Loss: 1.5757, Validation F1-score: 0.4479
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:60 ====================
Learning rate: 0.0000750000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 18
Gamma: 0.3
Batch size: 8
Number of frozen parameters: dino_model.blocks.4.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6512, Training F1-score: 0.7628, Validation Loss: 0.7244, Validation F1-score: 0.7486
Best F1-score till now on the current trail on Validation data: 0.748611111111111
Epoch 2/100, Training Loss: 0.4956, Training F1-score: 0.8156, Validation Loss: 0.6375, Validation F1-score: 0.7785
Best F1-score till now on the current trail on Validation data: 0.7784722222222222
Epoch 3/100, Training Loss: 0.4404, Training F1-score: 0.8363, Validation Loss: 0.6884, Validation F1-score: 0.7688
Epoch 4/100, Training Loss: 0.4059, Training F1-score: 0.8506, Validation Loss: 0.7053, Validation F1-score: 0.7523
Epoch 5/100, Training Loss: 0.3752, Training F1-score: 0.8610, Validation Loss: 0.8028, Validation F1-score: 0.7318
Epoch 6/100, Training Loss: 0.3574, Training F1-score: 0.8696, Validation Loss: 0.8542, Validation F1-score: 0.7257
Epoch 7/100, Training Loss: 0.3440, Training F1-score: 0.8735, Validation Loss: 0.7537, Validation F1-score: 0.7359
Epoch 8/100, Training Loss: 0.3278, Training F1-score: 0.8818, Validation Loss: 0.7839, Validation F1-score: 0.7524
Epoch 9/100, Training Loss: 0.3175, Training F1-score: 0.8838, Validation Loss: 0.7608, Validation F1-score: 0.7545
Epoch 10/100, Training Loss: 0.3112, Training F1-score: 0.8848, Validation Loss: 0.7973, Validation F1-score: 0.7257
Epoch 11/100, Training Loss: 0.3003, Training F1-score: 0.8909, Validation Loss: 0.7420, Validation F1-score: 0.7469
Epoch 12/100, Training Loss: 0.2928, Training F1-score: 0.8930, Validation Loss: 0.6370, Validation F1-score: 0.7597
Epoch 13/100, Training Loss: 0.2861, Training F1-score: 0.8958, Validation Loss: 0.7356, Validation F1-score: 0.7547
Epoch 14/100, Training Loss: 0.2788, Training F1-score: 0.8992, Validation Loss: 0.7981, Validation F1-score: 0.7474
Epoch 15/100, Training Loss: 0.2709, Training F1-score: 0.9006, Validation Loss: 0.7515, Validation F1-score: 0.7391
Epoch 16/100, Training Loss: 0.2653, Training F1-score: 0.9039, Validation Loss: 0.7009, Validation F1-score: 0.7632
[I 2024-12-27 06:19:34,159] Trial 60 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 7.5e-05, 'dropout': 0.2, 'optimizer': 'RMSprop', 'step_size': 18, 'gamma': 0.3, 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 17/100, Training Loss: 0.2669, Training F1-score: 0.9033, Validation Loss: 0.7519, Validation F1-score: 0.7469
Early stopping triggered after 17 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:61 ====================
Learning rate: 0.0000025000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.2.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 06:21:15,574] Trial 61 pruned. 
Epoch 1/100, Training Loss: 1.5485, Training F1-score: 0.4462, Validation Loss: 1.3681, Validation F1-score: 0.4748
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:62 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 20
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.5998, Training F1-score: 0.7749, Validation Loss: 0.6765, Validation F1-score: 0.7521
Best F1-score till now on the current trail on Validation data: 0.7520833333333333
Epoch 2/100, Training Loss: 0.4479, Training F1-score: 0.8302, Validation Loss: 0.6775, Validation F1-score: 0.7500
Epoch 3/100, Training Loss: 0.3882, Training F1-score: 0.8563, Validation Loss: 0.7445, Validation F1-score: 0.7257
Epoch 4/100, Training Loss: 0.3504, Training F1-score: 0.8695, Validation Loss: 0.8375, Validation F1-score: 0.7241
Epoch 5/100, Training Loss: 0.3228, Training F1-score: 0.8810, Validation Loss: 0.6671, Validation F1-score: 0.7688
Best F1-score till now on the current trail on Validation data: 0.76875
Epoch 6/100, Training Loss: 0.3068, Training F1-score: 0.8901, Validation Loss: 0.7441, Validation F1-score: 0.7568
Epoch 7/100, Training Loss: 0.2893, Training F1-score: 0.8949, Validation Loss: 0.6603, Validation F1-score: 0.7545
Epoch 8/100, Training Loss: 0.2820, Training F1-score: 0.8990, Validation Loss: 0.7452, Validation F1-score: 0.7417
Epoch 9/100, Training Loss: 0.2700, Training F1-score: 0.9028, Validation Loss: 0.7095, Validation F1-score: 0.7590
Epoch 10/100, Training Loss: 0.2618, Training F1-score: 0.9055, Validation Loss: 0.7804, Validation F1-score: 0.7464
Epoch 11/100, Training Loss: 0.2567, Training F1-score: 0.9099, Validation Loss: 0.7593, Validation F1-score: 0.7401
Epoch 12/100, Training Loss: 0.2503, Training F1-score: 0.9120, Validation Loss: 0.6742, Validation F1-score: 0.7510
Epoch 13/100, Training Loss: 0.2453, Training F1-score: 0.9130, Validation Loss: 0.7585, Validation F1-score: 0.7464
Epoch 14/100, Training Loss: 0.2373, Training F1-score: 0.9145, Validation Loss: 0.8651, Validation F1-score: 0.7361
Epoch 15/100, Training Loss: 0.2320, Training F1-score: 0.9160, Validation Loss: 0.7958, Validation F1-score: 0.7517
Epoch 16/100, Training Loss: 0.2330, Training F1-score: 0.9156, Validation Loss: 0.8496, Validation F1-score: 0.7517
Epoch 17/100, Training Loss: 0.2252, Training F1-score: 0.9206, Validation Loss: 0.7774, Validation F1-score: 0.7352
Epoch 18/100, Training Loss: 0.2210, Training F1-score: 0.9201, Validation Loss: 0.8075, Validation F1-score: 0.7306
Epoch 19/100, Training Loss: 0.2201, Training F1-score: 0.9201, Validation Loss: 0.8642, Validation F1-score: 0.7198
[I 2024-12-27 06:54:40,489] Trial 62 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.001, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 20, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 20/100, Training Loss: 0.2172, Training F1-score: 0.9213, Validation Loss: 0.7356, Validation F1-score: 0.7613
Early stopping triggered after 20 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:63 ====================
Learning rate: 0.0050000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 18
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 06:56:16,190] Trial 63 pruned. 
Epoch 1/100, Training Loss: 0.6590, Training F1-score: 0.7538, Validation Loss: 0.7220, Validation F1-score: 0.7309
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:64 ====================
Learning rate: 0.0010000000
Dropout: 0.5
Optimizer: Adam
Momentum term: 0
Step size: 17
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 06:57:51,184] Trial 64 pruned. 
Epoch 1/100, Training Loss: 0.7333, Training F1-score: 0.7289, Validation Loss: 0.8049, Validation F1-score: 0.7108
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:65 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 13
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6055, Training F1-score: 0.7752, Validation Loss: 0.6420, Validation F1-score: 0.7700
Best F1-score till now on the current trail on Validation data: 0.7699652777777778
Epoch 2/100, Training Loss: 0.4548, Training F1-score: 0.8296, Validation Loss: 0.7784, Validation F1-score: 0.7146
Epoch 3/100, Training Loss: 0.3887, Training F1-score: 0.8540, Validation Loss: 0.7929, Validation F1-score: 0.7467
Epoch 4/100, Training Loss: 0.3476, Training F1-score: 0.8729, Validation Loss: 0.7377, Validation F1-score: 0.7229
Epoch 5/100, Training Loss: 0.3289, Training F1-score: 0.8779, Validation Loss: 0.7636, Validation F1-score: 0.7378
Epoch 6/100, Training Loss: 0.3074, Training F1-score: 0.8887, Validation Loss: 0.7805, Validation F1-score: 0.7208
Epoch 7/100, Training Loss: 0.2968, Training F1-score: 0.8906, Validation Loss: 0.7128, Validation F1-score: 0.7554
Epoch 8/100, Training Loss: 0.2813, Training F1-score: 0.8985, Validation Loss: 0.7770, Validation F1-score: 0.7399
Epoch 9/100, Training Loss: 0.2735, Training F1-score: 0.9017, Validation Loss: 0.7144, Validation F1-score: 0.7583
Epoch 10/100, Training Loss: 0.2617, Training F1-score: 0.9052, Validation Loss: 0.9424, Validation F1-score: 0.7462
Epoch 11/100, Training Loss: 0.2574, Training F1-score: 0.9071, Validation Loss: 0.7117, Validation F1-score: 0.7457
Epoch 12/100, Training Loss: 0.2507, Training F1-score: 0.9102, Validation Loss: 0.7772, Validation F1-score: 0.7434
Epoch 13/100, Training Loss: 0.2464, Training F1-score: 0.9110, Validation Loss: 0.9570, Validation F1-score: 0.7269
Epoch 14/100, Training Loss: 0.2109, Training F1-score: 0.9252, Validation Loss: 0.7833, Validation F1-score: 0.7519
Epoch 15/100, Training Loss: 0.2081, Training F1-score: 0.9253, Validation Loss: 0.8710, Validation F1-score: 0.7406
[I 2024-12-27 07:25:38,991] Trial 65 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.001, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 13, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 16/100, Training Loss: 0.2003, Training F1-score: 0.9283, Validation Loss: 0.8527, Validation F1-score: 0.7497
Early stopping triggered after 16 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:66 ====================
Learning rate: 0.0000100000
Dropout: 0.2
Optimizer: SGD
Momentum term: 0.8210578258856304
Step size: 18
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 07:27:30,520] Trial 66 pruned. 
Epoch 1/100, Training Loss: 1.5876, Training F1-score: 0.4461, Validation Loss: 1.4196, Validation F1-score: 0.4479
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:67 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 15
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.6.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 07:29:20,598] Trial 67 pruned. 
Epoch 1/100, Training Loss: 0.5976, Training F1-score: 0.7779, Validation Loss: 0.8358, Validation F1-score: 0.7148
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:68 ====================
Learning rate: 0.0002500000
Dropout: 0.25
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6422, Training F1-score: 0.7618, Validation Loss: 0.7150, Validation F1-score: 0.7484
Best F1-score till now on the current trail on Validation data: 0.7484375
Epoch 2/100, Training Loss: 0.4856, Training F1-score: 0.8153, Validation Loss: 0.7045, Validation F1-score: 0.7648
Best F1-score till now on the current trail on Validation data: 0.7647569444444444
Epoch 3/100, Training Loss: 0.4240, Training F1-score: 0.8387, Validation Loss: 0.6608, Validation F1-score: 0.7507
Epoch 4/100, Training Loss: 0.3954, Training F1-score: 0.8507, Validation Loss: 0.6684, Validation F1-score: 0.7550
Epoch 5/100, Training Loss: 0.3689, Training F1-score: 0.8610, Validation Loss: 0.7064, Validation F1-score: 0.7385
Epoch 6/100, Training Loss: 0.3436, Training F1-score: 0.8706, Validation Loss: 0.7191, Validation F1-score: 0.7562
Epoch 7/100, Training Loss: 0.3333, Training F1-score: 0.8772, Validation Loss: 0.7447, Validation F1-score: 0.7323
Epoch 8/100, Training Loss: 0.3249, Training F1-score: 0.8793, Validation Loss: 0.8131, Validation F1-score: 0.7295
Epoch 9/100, Training Loss: 0.3141, Training F1-score: 0.8826, Validation Loss: 0.7611, Validation F1-score: 0.7297
Epoch 10/100, Training Loss: 0.3074, Training F1-score: 0.8867, Validation Loss: 0.6787, Validation F1-score: 0.7410
Epoch 11/100, Training Loss: 0.2936, Training F1-score: 0.8918, Validation Loss: 0.7920, Validation F1-score: 0.7356
Epoch 12/100, Training Loss: 0.2857, Training F1-score: 0.8956, Validation Loss: 0.8965, Validation F1-score: 0.7241
Epoch 13/100, Training Loss: 0.2787, Training F1-score: 0.8976, Validation Loss: 0.7362, Validation F1-score: 0.7514
Epoch 14/100, Training Loss: 0.2764, Training F1-score: 0.8985, Validation Loss: 0.7664, Validation F1-score: 0.7415
Epoch 15/100, Training Loss: 0.2685, Training F1-score: 0.9023, Validation Loss: 0.8470, Validation F1-score: 0.7241
Epoch 16/100, Training Loss: 0.2663, Training F1-score: 0.9036, Validation Loss: 0.7261, Validation F1-score: 0.7372
[I 2024-12-27 08:00:21,116] Trial 68 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.00025, 'dropout': 0.25, 'optimizer': 'Adam', 'step_size': 16, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 17/100, Training Loss: 0.2400, Training F1-score: 0.9137, Validation Loss: 0.7471, Validation F1-score: 0.7543
Early stopping triggered after 17 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:69 ====================
Learning rate: 0.0000050000
Dropout: 0.3
Optimizer: RMSprop
Momentum term: 0
Step size: 17
Gamma: 0.1
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 08:02:13,870] Trial 69 pruned. 
Epoch 1/100, Training Loss: 1.3579, Training F1-score: 0.5081, Validation Loss: 1.2216, Validation F1-score: 0.6047
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:70 ====================
Learning rate: 0.0000005000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 18
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 08:04:00,798] Trial 70 pruned. 
Epoch 1/100, Training Loss: 1.8931, Training F1-score: 0.3771, Validation Loss: 1.6312, Validation F1-score: 0.4479
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:71 ====================
Learning rate: 0.0025000000
Dropout: 0.32
Optimizer: SGD
Momentum term: 0.8032641897310966
Step size: 19
Gamma: 0.2
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 08:05:54,662] Trial 71 pruned. 
Epoch 1/100, Training Loss: 0.7683, Training F1-score: 0.7170, Validation Loss: 0.7102, Validation F1-score: 0.7342
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:72 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 19
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 08:07:42,527] Trial 72 pruned. 
Epoch 1/100, Training Loss: 0.6124, Training F1-score: 0.7716, Validation Loss: 0.7244, Validation F1-score: 0.7396
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:73 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 08:09:33,731] Trial 73 pruned. 
Epoch 1/100, Training Loss: 0.6085, Training F1-score: 0.7728, Validation Loss: 0.8988, Validation F1-score: 0.7038
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:74 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 20
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 08:11:22,239] Trial 74 pruned. 
Epoch 1/100, Training Loss: 0.6081, Training F1-score: 0.7708, Validation Loss: 0.7016, Validation F1-score: 0.7203
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:75 ====================
Learning rate: 0.0002500000
Dropout: 0.28
Optimizer: Adam
Momentum term: 0
Step size: 19
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6406, Training F1-score: 0.7616, Validation Loss: 0.6739, Validation F1-score: 0.7665
Best F1-score till now on the current trail on Validation data: 0.7664930555555556
Epoch 2/100, Training Loss: 0.4947, Training F1-score: 0.8130, Validation Loss: 0.7186, Validation F1-score: 0.7408
Epoch 3/100, Training Loss: 0.4382, Training F1-score: 0.8339, Validation Loss: 0.7478, Validation F1-score: 0.7229
Epoch 4/100, Training Loss: 0.3984, Training F1-score: 0.8493, Validation Loss: 0.7595, Validation F1-score: 0.7271
Epoch 5/100, Training Loss: 0.3696, Training F1-score: 0.8638, Validation Loss: 0.7190, Validation F1-score: 0.7420
Epoch 6/100, Training Loss: 0.3510, Training F1-score: 0.8692, Validation Loss: 0.7057, Validation F1-score: 0.7507
Epoch 7/100, Training Loss: 0.3410, Training F1-score: 0.8736, Validation Loss: 0.7767, Validation F1-score: 0.7326
Epoch 8/100, Training Loss: 0.3284, Training F1-score: 0.8781, Validation Loss: 0.7327, Validation F1-score: 0.7309
Epoch 9/100, Training Loss: 0.3157, Training F1-score: 0.8850, Validation Loss: 0.8238, Validation F1-score: 0.7227
Epoch 10/100, Training Loss: 0.3059, Training F1-score: 0.8868, Validation Loss: 0.8448, Validation F1-score: 0.7224
Epoch 11/100, Training Loss: 0.2974, Training F1-score: 0.8897, Validation Loss: 0.7408, Validation F1-score: 0.7280
Epoch 12/100, Training Loss: 0.2938, Training F1-score: 0.8932, Validation Loss: 0.7403, Validation F1-score: 0.7431
Epoch 13/100, Training Loss: 0.2799, Training F1-score: 0.8975, Validation Loss: 0.8447, Validation F1-score: 0.7260
Epoch 14/100, Training Loss: 0.2786, Training F1-score: 0.8993, Validation Loss: 0.8798, Validation F1-score: 0.7198
Epoch 15/100, Training Loss: 0.2685, Training F1-score: 0.9014, Validation Loss: 0.8631, Validation F1-score: 0.7201
[I 2024-12-27 08:40:28,302] Trial 75 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.00025, 'dropout': 0.28, 'optimizer': 'Adam', 'step_size': 19, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 16/100, Training Loss: 0.2661, Training F1-score: 0.9025, Validation Loss: 0.7691, Validation F1-score: 0.7227
Early stopping triggered after 16 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:76 ====================
Learning rate: 0.0000002500
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 17
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 08:42:24,726] Trial 76 pruned. 
Epoch 1/100, Training Loss: 1.9179, Training F1-score: 0.2783, Validation Loss: 1.7825, Validation F1-score: 0.4144
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:77 ====================
Learning rate: 0.0007500000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 18
Gamma: 0.3
Batch size: 8
Number of frozen parameters: dino_model.blocks.0.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 08:44:12,904] Trial 77 pruned. 
Epoch 1/100, Training Loss: 0.6052, Training F1-score: 0.7731, Validation Loss: 0.6619, Validation F1-score: 0.7365
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:78 ====================
Learning rate: 0.0000010000
Dropout: 0.4
Optimizer: Adam
Momentum term: 0
Step size: 15
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.2.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 08:46:00,711] Trial 78 pruned. 
Epoch 1/100, Training Loss: 1.8030, Training F1-score: 0.4001, Validation Loss: 1.5851, Validation F1-score: 0.4479
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:79 ====================
Learning rate: 0.0010000000
Dropout: 0.35
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 08:47:48,798] Trial 79 pruned. 
Epoch 1/100, Training Loss: 0.6762, Training F1-score: 0.7491, Validation Loss: 0.7481, Validation F1-score: 0.7207
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:80 ====================
Learning rate: 0.0005000000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 14
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.4.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.5971, Training F1-score: 0.7764, Validation Loss: 0.6673, Validation F1-score: 0.7602
Best F1-score till now on the current trail on Validation data: 0.7602430555555556
Epoch 2/100, Training Loss: 0.4412, Training F1-score: 0.8334, Validation Loss: 0.7907, Validation F1-score: 0.7156
Epoch 3/100, Training Loss: 0.3882, Training F1-score: 0.8560, Validation Loss: 0.7574, Validation F1-score: 0.7340
Epoch 4/100, Training Loss: 0.3501, Training F1-score: 0.8682, Validation Loss: 0.8547, Validation F1-score: 0.7194
Epoch 5/100, Training Loss: 0.3244, Training F1-score: 0.8810, Validation Loss: 0.8536, Validation F1-score: 0.7099
Epoch 6/100, Training Loss: 0.3046, Training F1-score: 0.8878, Validation Loss: 0.7735, Validation F1-score: 0.7319
Epoch 7/100, Training Loss: 0.2901, Training F1-score: 0.8955, Validation Loss: 0.8334, Validation F1-score: 0.7354
Epoch 8/100, Training Loss: 0.2804, Training F1-score: 0.8977, Validation Loss: 0.7742, Validation F1-score: 0.7418
Epoch 9/100, Training Loss: 0.2675, Training F1-score: 0.9016, Validation Loss: 0.8369, Validation F1-score: 0.7424
Epoch 10/100, Training Loss: 0.2607, Training F1-score: 0.9053, Validation Loss: 0.7738, Validation F1-score: 0.7405
Epoch 11/100, Training Loss: 0.2562, Training F1-score: 0.9073, Validation Loss: 0.8379, Validation F1-score: 0.7219
Epoch 12/100, Training Loss: 0.2447, Training F1-score: 0.9136, Validation Loss: 0.8400, Validation F1-score: 0.7304
Epoch 13/100, Training Loss: 0.2420, Training F1-score: 0.9137, Validation Loss: 0.7275, Validation F1-score: 0.7490
Epoch 14/100, Training Loss: 0.2346, Training F1-score: 0.9160, Validation Loss: 0.7967, Validation F1-score: 0.7274
Epoch 15/100, Training Loss: 0.2048, Training F1-score: 0.9285, Validation Loss: 0.8628, Validation F1-score: 0.7443
[I 2024-12-27 09:16:25,239] Trial 80 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.0005, 'dropout': 0.2, 'optimizer': 'RMSprop', 'step_size': 14, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 16/100, Training Loss: 0.2005, Training F1-score: 0.9292, Validation Loss: 0.8557, Validation F1-score: 0.7432
Early stopping triggered after 16 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:81 ====================
Learning rate: 0.0000250000
Dropout: 0.5
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 09:18:09,271] Trial 81 pruned. 
Epoch 1/100, Training Loss: 1.1099, Training F1-score: 0.6094, Validation Loss: 0.8836, Validation F1-score: 0.6762
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:82 ====================
Learning rate: 0.0002500000
Dropout: 0.25
Optimizer: RMSprop
Momentum term: 0
Step size: 18
Gamma: 0.1
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 09:19:51,373] Trial 82 pruned. 
Epoch 1/100, Training Loss: 0.6102, Training F1-score: 0.7717, Validation Loss: 0.7200, Validation F1-score: 0.7406
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:83 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 19
Gamma: 0.1
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 09:21:42,996] Trial 83 pruned. 
Epoch 1/100, Training Loss: 0.5865, Training F1-score: 0.7831, Validation Loss: 0.7543, Validation F1-score: 0.7321
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:84 ====================
Learning rate: 0.0002500000
Dropout: 0.25
Optimizer: RMSprop
Momentum term: 0
Step size: 17
Gamma: 0.1
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6061, Training F1-score: 0.7735, Validation Loss: 0.6809, Validation F1-score: 0.7484
Best F1-score till now on the current trail on Validation data: 0.7484375
Epoch 2/100, Training Loss: 0.4751, Training F1-score: 0.8226, Validation Loss: 0.6649, Validation F1-score: 0.7530
Best F1-score till now on the current trail on Validation data: 0.7529513888888889
Epoch 3/100, Training Loss: 0.4172, Training F1-score: 0.8425, Validation Loss: 0.6987, Validation F1-score: 0.7505
Epoch 4/100, Training Loss: 0.3863, Training F1-score: 0.8553, Validation Loss: 0.7795, Validation F1-score: 0.7444
Epoch 5/100, Training Loss: 0.3588, Training F1-score: 0.8645, Validation Loss: 0.6278, Validation F1-score: 0.7703
Best F1-score till now on the current trail on Validation data: 0.7703125
Epoch 6/100, Training Loss: 0.3405, Training F1-score: 0.8747, Validation Loss: 0.7339, Validation F1-score: 0.7429
Epoch 7/100, Training Loss: 0.3260, Training F1-score: 0.8796, Validation Loss: 0.8024, Validation F1-score: 0.7365
Epoch 8/100, Training Loss: 0.3100, Training F1-score: 0.8863, Validation Loss: 0.6914, Validation F1-score: 0.7590
Epoch 9/100, Training Loss: 0.3022, Training F1-score: 0.8907, Validation Loss: 0.7431, Validation F1-score: 0.7425
Epoch 10/100, Training Loss: 0.2929, Training F1-score: 0.8946, Validation Loss: 0.7871, Validation F1-score: 0.7431
Epoch 11/100, Training Loss: 0.2810, Training F1-score: 0.8985, Validation Loss: 0.7981, Validation F1-score: 0.7302
Epoch 12/100, Training Loss: 0.2788, Training F1-score: 0.9007, Validation Loss: 0.7870, Validation F1-score: 0.7418
Epoch 13/100, Training Loss: 0.2742, Training F1-score: 0.9011, Validation Loss: 0.7907, Validation F1-score: 0.7516
Epoch 14/100, Training Loss: 0.2620, Training F1-score: 0.9052, Validation Loss: 0.7453, Validation F1-score: 0.7453
Epoch 15/100, Training Loss: 0.2603, Training F1-score: 0.9044, Validation Loss: 0.7971, Validation F1-score: 0.7436
Epoch 16/100, Training Loss: 0.2520, Training F1-score: 0.9089, Validation Loss: 0.7341, Validation F1-score: 0.7635
Epoch 17/100, Training Loss: 0.2516, Training F1-score: 0.9091, Validation Loss: 0.8368, Validation F1-score: 0.7450
Epoch 18/100, Training Loss: 0.2133, Training F1-score: 0.9245, Validation Loss: 0.7732, Validation F1-score: 0.7530
Epoch 19/100, Training Loss: 0.2068, Training F1-score: 0.9274, Validation Loss: 0.7651, Validation F1-score: 0.7526
[I 2024-12-27 09:53:25,984] Trial 84 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.00025, 'dropout': 0.25, 'optimizer': 'RMSprop', 'step_size': 17, 'gamma': 0.1, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 20/100, Training Loss: 0.2096, Training F1-score: 0.9268, Validation Loss: 0.7678, Validation F1-score: 0.7507
Early stopping triggered after 20 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:85 ====================
Learning rate: 0.0002500000
Dropout: 0.25
Optimizer: RMSprop
Momentum term: 0
Step size: 18
Gamma: 0.1
Batch size: 8
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6120, Training F1-score: 0.7718, Validation Loss: 0.6663, Validation F1-score: 0.7594
Best F1-score till now on the current trail on Validation data: 0.759375
Epoch 2/100, Training Loss: 0.4771, Training F1-score: 0.8206, Validation Loss: 0.6495, Validation F1-score: 0.7569
Epoch 3/100, Training Loss: 0.4169, Training F1-score: 0.8425, Validation Loss: 0.7040, Validation F1-score: 0.7648
Best F1-score till now on the current trail on Validation data: 0.7647569444444444
Epoch 4/100, Training Loss: 0.3864, Training F1-score: 0.8543, Validation Loss: 0.8069, Validation F1-score: 0.7260
Epoch 5/100, Training Loss: 0.3629, Training F1-score: 0.8660, Validation Loss: 0.6955, Validation F1-score: 0.7481
Epoch 6/100, Training Loss: 0.3381, Training F1-score: 0.8749, Validation Loss: 0.7101, Validation F1-score: 0.7354
Epoch 7/100, Training Loss: 0.3220, Training F1-score: 0.8818, Validation Loss: 0.6712, Validation F1-score: 0.7599
Epoch 8/100, Training Loss: 0.3138, Training F1-score: 0.8846, Validation Loss: 0.7739, Validation F1-score: 0.7479
Epoch 9/100, Training Loss: 0.3054, Training F1-score: 0.8865, Validation Loss: 0.6991, Validation F1-score: 0.7580
Epoch 10/100, Training Loss: 0.2894, Training F1-score: 0.8946, Validation Loss: 0.7403, Validation F1-score: 0.7571
Epoch 11/100, Training Loss: 0.2799, Training F1-score: 0.8951, Validation Loss: 0.7627, Validation F1-score: 0.7418
Epoch 12/100, Training Loss: 0.2755, Training F1-score: 0.8981, Validation Loss: 0.7099, Validation F1-score: 0.7385
Epoch 13/100, Training Loss: 0.2702, Training F1-score: 0.9014, Validation Loss: 0.7367, Validation F1-score: 0.7495
Epoch 14/100, Training Loss: 0.2672, Training F1-score: 0.9028, Validation Loss: 0.7462, Validation F1-score: 0.7493
Epoch 15/100, Training Loss: 0.2550, Training F1-score: 0.9080, Validation Loss: 0.8352, Validation F1-score: 0.7340
Epoch 16/100, Training Loss: 0.2544, Training F1-score: 0.9075, Validation Loss: 0.9166, Validation F1-score: 0.7149
Epoch 17/100, Training Loss: 0.2496, Training F1-score: 0.9097, Validation Loss: 0.7486, Validation F1-score: 0.7460
[I 2024-12-27 10:20:21,594] Trial 85 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.00025, 'dropout': 0.25, 'optimizer': 'RMSprop', 'step_size': 18, 'gamma': 0.1, 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 18/100, Training Loss: 0.2453, Training F1-score: 0.9116, Validation Loss: 0.7970, Validation F1-score: 0.7519
Early stopping triggered after 18 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:86 ====================
Learning rate: 0.0001000000
Dropout: 0.25
Optimizer: RMSprop
Momentum term: 0
Step size: 17
Gamma: 0.1
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6474, Training F1-score: 0.7636, Validation Loss: 0.6749, Validation F1-score: 0.7630
Best F1-score till now on the current trail on Validation data: 0.7630208333333334
Epoch 2/100, Training Loss: 0.5103, Training F1-score: 0.8090, Validation Loss: 0.7037, Validation F1-score: 0.7389
Epoch 3/100, Training Loss: 0.4565, Training F1-score: 0.8295, Validation Loss: 0.6760, Validation F1-score: 0.7464
Epoch 4/100, Training Loss: 0.4191, Training F1-score: 0.8422, Validation Loss: 0.7308, Validation F1-score: 0.7521
Epoch 5/100, Training Loss: 0.3944, Training F1-score: 0.8536, Validation Loss: 0.7536, Validation F1-score: 0.7389
Epoch 6/100, Training Loss: 0.3738, Training F1-score: 0.8596, Validation Loss: 0.7246, Validation F1-score: 0.7464
Epoch 7/100, Training Loss: 0.3562, Training F1-score: 0.8680, Validation Loss: 0.7694, Validation F1-score: 0.7366
Epoch 8/100, Training Loss: 0.3461, Training F1-score: 0.8724, Validation Loss: 0.7869, Validation F1-score: 0.7406
Epoch 9/100, Training Loss: 0.3369, Training F1-score: 0.8753, Validation Loss: 0.7120, Validation F1-score: 0.7562
Epoch 10/100, Training Loss: 0.3278, Training F1-score: 0.8789, Validation Loss: 0.7955, Validation F1-score: 0.7335
Epoch 11/100, Training Loss: 0.3183, Training F1-score: 0.8837, Validation Loss: 0.7659, Validation F1-score: 0.7377
Epoch 12/100, Training Loss: 0.3055, Training F1-score: 0.8862, Validation Loss: 0.9226, Validation F1-score: 0.7236
Epoch 13/100, Training Loss: 0.3059, Training F1-score: 0.8867, Validation Loss: 0.9168, Validation F1-score: 0.7373
Epoch 14/100, Training Loss: 0.2921, Training F1-score: 0.8908, Validation Loss: 0.7472, Validation F1-score: 0.7568
Epoch 15/100, Training Loss: 0.2884, Training F1-score: 0.8958, Validation Loss: 0.7077, Validation F1-score: 0.7564
[I 2024-12-27 10:44:05,356] Trial 86 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.0001, 'dropout': 0.25, 'optimizer': 'RMSprop', 'step_size': 17, 'gamma': 0.1, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 16/100, Training Loss: 0.2800, Training F1-score: 0.8961, Validation Loss: 0.7559, Validation F1-score: 0.7448
Early stopping triggered after 16 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:87 ====================
Learning rate: 0.0000001000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 20
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.6.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 10:45:29,969] Trial 87 pruned. 
Epoch 1/100, Training Loss: 1.9337, Training F1-score: 0.4361, Validation Loss: 1.9063, Validation F1-score: 0.4479
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:88 ====================
Learning rate: 0.0075000000
Dropout: 0.3
Optimizer: SGD
Momentum term: 0.9896105976793252
Step size: 13
Gamma: 0.2
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 10:46:58,074] Trial 88 pruned. 
Epoch 1/100, Training Loss: 0.8422, Training F1-score: 0.6941, Validation Loss: 0.7907, Validation F1-score: 0.7127
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:89 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 16
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.5913, Training F1-score: 0.7782, Validation Loss: 0.6747, Validation F1-score: 0.7595
Best F1-score till now on the current trail on Validation data: 0.7595486111111112
Epoch 2/100, Training Loss: 0.4524, Training F1-score: 0.8296, Validation Loss: 0.6380, Validation F1-score: 0.7583
Epoch 3/100, Training Loss: 0.3929, Training F1-score: 0.8522, Validation Loss: 0.8214, Validation F1-score: 0.7314
Epoch 4/100, Training Loss: 0.3579, Training F1-score: 0.8663, Validation Loss: 0.6809, Validation F1-score: 0.7578
Epoch 5/100, Training Loss: 0.3321, Training F1-score: 0.8754, Validation Loss: 0.6544, Validation F1-score: 0.7637
Best F1-score till now on the current trail on Validation data: 0.7637152777777778
Epoch 6/100, Training Loss: 0.3148, Training F1-score: 0.8857, Validation Loss: 0.7164, Validation F1-score: 0.7432
Epoch 7/100, Training Loss: 0.3022, Training F1-score: 0.8889, Validation Loss: 1.0142, Validation F1-score: 0.7111
Epoch 8/100, Training Loss: 0.2876, Training F1-score: 0.8943, Validation Loss: 0.7466, Validation F1-score: 0.7439
Epoch 9/100, Training Loss: 0.2761, Training F1-score: 0.9004, Validation Loss: 0.8082, Validation F1-score: 0.7415
Epoch 10/100, Training Loss: 0.2719, Training F1-score: 0.9012, Validation Loss: 0.7121, Validation F1-score: 0.7628
Epoch 11/100, Training Loss: 0.2598, Training F1-score: 0.9071, Validation Loss: 0.7090, Validation F1-score: 0.7443
Epoch 12/100, Training Loss: 0.2560, Training F1-score: 0.9071, Validation Loss: 0.7051, Validation F1-score: 0.7583
Epoch 13/100, Training Loss: 0.2489, Training F1-score: 0.9115, Validation Loss: 0.6638, Validation F1-score: 0.7667
Best F1-score till now on the current trail on Validation data: 0.7666666666666667
Epoch 14/100, Training Loss: 0.2410, Training F1-score: 0.9133, Validation Loss: 0.8858, Validation F1-score: 0.7318
Epoch 15/100, Training Loss: 0.2366, Training F1-score: 0.9154, Validation Loss: 0.8153, Validation F1-score: 0.7517
Epoch 16/100, Training Loss: 0.2322, Training F1-score: 0.9174, Validation Loss: 0.8282, Validation F1-score: 0.7384
Epoch 17/100, Training Loss: 0.2072, Training F1-score: 0.9281, Validation Loss: 0.7396, Validation F1-score: 0.7436
Epoch 18/100, Training Loss: 0.2035, Training F1-score: 0.9278, Validation Loss: 0.7558, Validation F1-score: 0.7491
Epoch 19/100, Training Loss: 0.2002, Training F1-score: 0.9308, Validation Loss: 0.7940, Validation F1-score: 0.7417
Epoch 20/100, Training Loss: 0.1962, Training F1-score: 0.9300, Validation Loss: 0.8086, Validation F1-score: 0.7340
Epoch 21/100, Training Loss: 0.1897, Training F1-score: 0.9328, Validation Loss: 0.7849, Validation F1-score: 0.7531
Epoch 22/100, Training Loss: 0.1900, Training F1-score: 0.9325, Validation Loss: 0.9038, Validation F1-score: 0.7375
Epoch 23/100, Training Loss: 0.1911, Training F1-score: 0.9315, Validation Loss: 0.8351, Validation F1-score: 0.7418
Epoch 24/100, Training Loss: 0.1897, Training F1-score: 0.9316, Validation Loss: 0.8499, Validation F1-score: 0.7352
Epoch 25/100, Training Loss: 0.1878, Training F1-score: 0.9345, Validation Loss: 0.8464, Validation F1-score: 0.7443
Epoch 26/100, Training Loss: 0.1862, Training F1-score: 0.9340, Validation Loss: 0.8116, Validation F1-score: 0.7396
Epoch 27/100, Training Loss: 0.1862, Training F1-score: 0.9344, Validation Loss: 0.8400, Validation F1-score: 0.7389
[I 2024-12-27 11:29:49,816] Trial 89 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.00025, 'dropout': 0.2, 'optimizer': 'RMSprop', 'step_size': 16, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 28/100, Training Loss: 0.1840, Training F1-score: 0.9346, Validation Loss: 0.8768, Validation F1-score: 0.7401
Early stopping triggered after 28 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:90 ====================
Learning rate: 0.0000007500
Dropout: 0.25
Optimizer: Adam
Momentum term: 0
Step size: 18
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 11:31:16,889] Trial 90 pruned. 
Epoch 1/100, Training Loss: 1.8133, Training F1-score: 0.4324, Validation Loss: 1.6183, Validation F1-score: 0.4479
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:91 ====================
Learning rate: 0.0000075000
Dropout: 0.28
Optimizer: RMSprop
Momentum term: 0
Step size: 15
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 11:32:50,181] Trial 91 pruned. 
Epoch 1/100, Training Loss: 1.1520, Training F1-score: 0.6059, Validation Loss: 0.9520, Validation F1-score: 0.6611
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:92 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 17
Gamma: 0.3
Batch size: 8
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 11:34:21,137] Trial 92 pruned. 
Epoch 1/100, Training Loss: 0.6063, Training F1-score: 0.7710, Validation Loss: 0.7717, Validation F1-score: 0.7361
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:93 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 17
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.6.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 11:35:52,675] Trial 93 pruned. 
Epoch 1/100, Training Loss: 0.6102, Training F1-score: 0.7725, Validation Loss: 0.8277, Validation F1-score: 0.6929
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:94 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 19
Gamma: 0.3
Batch size: 8
Number of frozen parameters: dino_model.blocks.6.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6080, Training F1-score: 0.7719, Validation Loss: 0.6732, Validation F1-score: 0.7531
Best F1-score till now on the current trail on Validation data: 0.753125
Epoch 2/100, Training Loss: 0.4542, Training F1-score: 0.8286, Validation Loss: 0.6521, Validation F1-score: 0.7406
Epoch 3/100, Training Loss: 0.3799, Training F1-score: 0.8571, Validation Loss: 0.6888, Validation F1-score: 0.7391
Epoch 4/100, Training Loss: 0.3457, Training F1-score: 0.8734, Validation Loss: 0.8787, Validation F1-score: 0.7382
Epoch 5/100, Training Loss: 0.3238, Training F1-score: 0.8841, Validation Loss: 0.7025, Validation F1-score: 0.7554
Best F1-score till now on the current trail on Validation data: 0.7553819444444444
Epoch 6/100, Training Loss: 0.3051, Training F1-score: 0.8897, Validation Loss: 0.7419, Validation F1-score: 0.7550
Epoch 7/100, Training Loss: 0.2920, Training F1-score: 0.8931, Validation Loss: 0.6848, Validation F1-score: 0.7472
Epoch 8/100, Training Loss: 0.2837, Training F1-score: 0.8987, Validation Loss: 0.7495, Validation F1-score: 0.7398
Epoch 9/100, Training Loss: 0.2746, Training F1-score: 0.9004, Validation Loss: 0.6747, Validation F1-score: 0.7552
Epoch 10/100, Training Loss: 0.2625, Training F1-score: 0.9049, Validation Loss: 0.7782, Validation F1-score: 0.7594
Best F1-score till now on the current trail on Validation data: 0.759375
Epoch 11/100, Training Loss: 0.2590, Training F1-score: 0.9085, Validation Loss: 0.6878, Validation F1-score: 0.7592
Epoch 12/100, Training Loss: 0.2561, Training F1-score: 0.9054, Validation Loss: 0.6983, Validation F1-score: 0.7465
Epoch 13/100, Training Loss: 0.2481, Training F1-score: 0.9099, Validation Loss: 0.8999, Validation F1-score: 0.7201
Epoch 14/100, Training Loss: 0.2416, Training F1-score: 0.9127, Validation Loss: 0.7355, Validation F1-score: 0.7564
Epoch 15/100, Training Loss: 0.2362, Training F1-score: 0.9159, Validation Loss: 0.7753, Validation F1-score: 0.7562
Epoch 16/100, Training Loss: 0.2354, Training F1-score: 0.9153, Validation Loss: 0.8556, Validation F1-score: 0.7438
Epoch 17/100, Training Loss: 0.2270, Training F1-score: 0.9191, Validation Loss: 0.7512, Validation F1-score: 0.7585
Epoch 18/100, Training Loss: 0.2241, Training F1-score: 0.9198, Validation Loss: 0.8292, Validation F1-score: 0.7509
Epoch 19/100, Training Loss: 0.2231, Training F1-score: 0.9194, Validation Loss: 0.8897, Validation F1-score: 0.7394
Epoch 20/100, Training Loss: 0.1846, Training F1-score: 0.9334, Validation Loss: 0.9441, Validation F1-score: 0.7444
Epoch 21/100, Training Loss: 0.1812, Training F1-score: 0.9337, Validation Loss: 0.8781, Validation F1-score: 0.7523
Epoch 22/100, Training Loss: 0.1778, Training F1-score: 0.9346, Validation Loss: 0.8683, Validation F1-score: 0.7547
Epoch 23/100, Training Loss: 0.1799, Training F1-score: 0.9353, Validation Loss: 0.9646, Validation F1-score: 0.7264
Epoch 24/100, Training Loss: 0.1787, Training F1-score: 0.9353, Validation Loss: 0.8462, Validation F1-score: 0.7516
[I 2024-12-27 12:12:57,417] Trial 94 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.001, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 19, 'gamma': 0.3, 'layer_freeze_upto': 'dino_model.blocks.6.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 25/100, Training Loss: 0.1752, Training F1-score: 0.9364, Validation Loss: 0.8705, Validation F1-score: 0.7491
Early stopping triggered after 25 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:95 ====================
Learning rate: 0.0000750000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.3
Batch size: 8
Number of frozen parameters: dino_model.blocks.6.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 12:14:28,682] Trial 95 pruned. 
Epoch 1/100, Training Loss: 0.6711, Training F1-score: 0.7525, Validation Loss: 0.7059, Validation F1-score: 0.7339
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:96 ====================
Learning rate: 0.0050000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 17
Gamma: 0.3
Batch size: 16
Number of frozen parameters: dino_model.blocks.6.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6534, Training F1-score: 0.7583, Validation Loss: 0.6921, Validation F1-score: 0.7530
Best F1-score till now on the current trail on Validation data: 0.7529513888888889
Epoch 2/100, Training Loss: 0.4991, Training F1-score: 0.8121, Validation Loss: 0.6788, Validation F1-score: 0.7446
Epoch 3/100, Training Loss: 0.4368, Training F1-score: 0.8351, Validation Loss: 0.7581, Validation F1-score: 0.7250
Epoch 4/100, Training Loss: 0.3891, Training F1-score: 0.8557, Validation Loss: 0.6854, Validation F1-score: 0.7415
Epoch 5/100, Training Loss: 0.3609, Training F1-score: 0.8661, Validation Loss: 0.6336, Validation F1-score: 0.7639
Best F1-score till now on the current trail on Validation data: 0.763888888888889
Epoch 6/100, Training Loss: 0.3492, Training F1-score: 0.8725, Validation Loss: 0.6688, Validation F1-score: 0.7391
Epoch 7/100, Training Loss: 0.3339, Training F1-score: 0.8789, Validation Loss: 0.8040, Validation F1-score: 0.7380
Epoch 8/100, Training Loss: 0.3200, Training F1-score: 0.8873, Validation Loss: 0.7035, Validation F1-score: 0.7444
Epoch 9/100, Training Loss: 0.3111, Training F1-score: 0.8879, Validation Loss: 0.8696, Validation F1-score: 0.7370
Epoch 10/100, Training Loss: 0.3098, Training F1-score: 0.8904, Validation Loss: 0.7217, Validation F1-score: 0.7509
Epoch 11/100, Training Loss: 0.3068, Training F1-score: 0.8909, Validation Loss: 0.6667, Validation F1-score: 0.7622
Epoch 12/100, Training Loss: 0.2953, Training F1-score: 0.8945, Validation Loss: 0.6907, Validation F1-score: 0.7660
Best F1-score till now on the current trail on Validation data: 0.7659722222222224
Epoch 13/100, Training Loss: 0.2851, Training F1-score: 0.8951, Validation Loss: 0.7286, Validation F1-score: 0.7599
Epoch 14/100, Training Loss: 0.2847, Training F1-score: 0.8976, Validation Loss: 0.7145, Validation F1-score: 0.7540
Epoch 15/100, Training Loss: 0.2817, Training F1-score: 0.8994, Validation Loss: 0.7101, Validation F1-score: 0.7453
Epoch 16/100, Training Loss: 0.2760, Training F1-score: 0.9032, Validation Loss: 0.6972, Validation F1-score: 0.7564
Epoch 17/100, Training Loss: 0.2703, Training F1-score: 0.9040, Validation Loss: 0.8056, Validation F1-score: 0.7476
Epoch 18/100, Training Loss: 0.2332, Training F1-score: 0.9176, Validation Loss: 0.7896, Validation F1-score: 0.7417
Epoch 19/100, Training Loss: 0.2271, Training F1-score: 0.9186, Validation Loss: 0.7246, Validation F1-score: 0.7578
Epoch 20/100, Training Loss: 0.2264, Training F1-score: 0.9194, Validation Loss: 0.7950, Validation F1-score: 0.7420
Epoch 21/100, Training Loss: 0.2232, Training F1-score: 0.9209, Validation Loss: 0.7743, Validation F1-score: 0.7498
Epoch 22/100, Training Loss: 0.2267, Training F1-score: 0.9180, Validation Loss: 0.7232, Validation F1-score: 0.7469
Epoch 23/100, Training Loss: 0.2212, Training F1-score: 0.9211, Validation Loss: 0.8622, Validation F1-score: 0.7387
Epoch 24/100, Training Loss: 0.2214, Training F1-score: 0.9215, Validation Loss: 0.7114, Validation F1-score: 0.7543
Epoch 25/100, Training Loss: 0.2207, Training F1-score: 0.9213, Validation Loss: 0.8276, Validation F1-score: 0.7354
Epoch 26/100, Training Loss: 0.2167, Training F1-score: 0.9231, Validation Loss: 0.7727, Validation F1-score: 0.7481
[I 2024-12-27 12:54:00,441] Trial 96 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.005, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 17, 'gamma': 0.3, 'layer_freeze_upto': 'dino_model.blocks.6.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 27/100, Training Loss: 0.2178, Training F1-score: 0.9214, Validation Loss: 0.8157, Validation F1-score: 0.7392
Early stopping triggered after 27 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:97 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 18
Gamma: 0.3
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 12:55:28,166] Trial 97 pruned. 
Epoch 1/100, Training Loss: 0.5949, Training F1-score: 0.7798, Validation Loss: 0.8865, Validation F1-score: 0.7155
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:98 ====================
Learning rate: 0.0000100000
Dropout: 0.32
Optimizer: RMSprop
Momentum term: 0
Step size: 15
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.0.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 12:56:55,795] Trial 98 pruned. 
Epoch 1/100, Training Loss: 1.1850, Training F1-score: 0.5915, Validation Loss: 0.9888, Validation F1-score: 0.6472
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:99 ====================
Learning rate: 0.0000025000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 19
Gamma: 0.1
Batch size: 8
Number of frozen parameters: dino_model.blocks.2.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 12:58:24,358] Trial 99 pruned. 
Epoch 1/100, Training Loss: 1.4070, Training F1-score: 0.4987, Validation Loss: 1.2410, Validation F1-score: 0.5712
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:100 ====================
Learning rate: 0.0010000000
Dropout: 0.4
Optimizer: RMSprop
Momentum term: 0
Step size: 16
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.4.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 12:59:54,899] Trial 100 pruned. 
Epoch 1/100, Training Loss: 0.6933, Training F1-score: 0.7424, Validation Loss: 0.7490, Validation F1-score: 0.7101
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:101 ====================
Learning rate: 0.0000500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 14
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 13:01:21,302] Trial 101 pruned. 
Epoch 1/100, Training Loss: 0.7984, Training F1-score: 0.7153, Validation Loss: 0.7622, Validation F1-score: 0.7085
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:102 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 15
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 13:02:51,619] Trial 102 pruned. 
Epoch 1/100, Training Loss: 0.6097, Training F1-score: 0.7760, Validation Loss: 0.7059, Validation F1-score: 0.7382
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:103 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 15
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 13:04:24,119] Trial 103 pruned. 
Epoch 1/100, Training Loss: 0.6105, Training F1-score: 0.7729, Validation Loss: 0.7596, Validation F1-score: 0.7281
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:104 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6140, Training F1-score: 0.7740, Validation Loss: 0.7258, Validation F1-score: 0.7533
Best F1-score till now on the current trail on Validation data: 0.7532986111111111
Epoch 2/100, Training Loss: 0.4491, Training F1-score: 0.8305, Validation Loss: 0.7407, Validation F1-score: 0.7380
Epoch 3/100, Training Loss: 0.3989, Training F1-score: 0.8509, Validation Loss: 0.6745, Validation F1-score: 0.7448
Epoch 4/100, Training Loss: 0.3648, Training F1-score: 0.8643, Validation Loss: 0.6770, Validation F1-score: 0.7660
Best F1-score till now on the current trail on Validation data: 0.7659722222222224
Epoch 5/100, Training Loss: 0.3399, Training F1-score: 0.8735, Validation Loss: 0.6501, Validation F1-score: 0.7656
Epoch 6/100, Training Loss: 0.3187, Training F1-score: 0.8812, Validation Loss: 0.7260, Validation F1-score: 0.7378
Epoch 7/100, Training Loss: 0.3081, Training F1-score: 0.8878, Validation Loss: 0.7887, Validation F1-score: 0.7429
Epoch 8/100, Training Loss: 0.2944, Training F1-score: 0.8932, Validation Loss: 0.7575, Validation F1-score: 0.7405
Epoch 9/100, Training Loss: 0.2869, Training F1-score: 0.8966, Validation Loss: 0.7892, Validation F1-score: 0.7233
Epoch 10/100, Training Loss: 0.2752, Training F1-score: 0.9012, Validation Loss: 0.7894, Validation F1-score: 0.7453
Epoch 11/100, Training Loss: 0.2675, Training F1-score: 0.9019, Validation Loss: 0.8368, Validation F1-score: 0.7384
Epoch 12/100, Training Loss: 0.2639, Training F1-score: 0.9028, Validation Loss: 0.7400, Validation F1-score: 0.7439
Epoch 13/100, Training Loss: 0.2567, Training F1-score: 0.9079, Validation Loss: 0.7609, Validation F1-score: 0.7422
Epoch 14/100, Training Loss: 0.2494, Training F1-score: 0.9120, Validation Loss: 0.7641, Validation F1-score: 0.7323
Epoch 15/100, Training Loss: 0.2466, Training F1-score: 0.9118, Validation Loss: 0.8117, Validation F1-score: 0.7425
Epoch 16/100, Training Loss: 0.2414, Training F1-score: 0.9132, Validation Loss: 0.7993, Validation F1-score: 0.7429
Epoch 17/100, Training Loss: 0.2144, Training F1-score: 0.9229, Validation Loss: 0.7690, Validation F1-score: 0.7425
Epoch 18/100, Training Loss: 0.2062, Training F1-score: 0.9260, Validation Loss: 0.8157, Validation F1-score: 0.7368
[I 2024-12-27 13:32:02,783] Trial 104 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.00025, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 16, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 19/100, Training Loss: 0.2083, Training F1-score: 0.9253, Validation Loss: 0.7601, Validation F1-score: 0.7460
Early stopping triggered after 19 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:105 ====================
Learning rate: 0.0000050000
Dropout: 0.35
Optimizer: Adam
Momentum term: 0
Step size: 17
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 13:33:37,274] Trial 105 pruned. 
Epoch 1/100, Training Loss: 1.4205, Training F1-score: 0.4763, Validation Loss: 1.2321, Validation F1-score: 0.5757
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:106 ====================
Learning rate: 0.0000005000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 14
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 13:35:07,289] Trial 106 pruned. 
Epoch 1/100, Training Loss: 1.9492, Training F1-score: 0.3036, Validation Loss: 1.7721, Validation F1-score: 0.4503
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:107 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 18
Gamma: 0.5
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.5917, Training F1-score: 0.7789, Validation Loss: 0.6462, Validation F1-score: 0.7594
Best F1-score till now on the current trail on Validation data: 0.759375
Epoch 2/100, Training Loss: 0.4441, Training F1-score: 0.8313, Validation Loss: 0.6455, Validation F1-score: 0.7670
Best F1-score till now on the current trail on Validation data: 0.7670138888888889
Epoch 3/100, Training Loss: 0.3953, Training F1-score: 0.8513, Validation Loss: 0.6424, Validation F1-score: 0.7679
Best F1-score till now on the current trail on Validation data: 0.7678819444444445
Epoch 4/100, Training Loss: 0.3580, Training F1-score: 0.8654, Validation Loss: 0.8057, Validation F1-score: 0.7349
Epoch 5/100, Training Loss: 0.3315, Training F1-score: 0.8749, Validation Loss: 0.7761, Validation F1-score: 0.7286
Epoch 6/100, Training Loss: 0.3166, Training F1-score: 0.8828, Validation Loss: 0.8287, Validation F1-score: 0.7278
Epoch 7/100, Training Loss: 0.3019, Training F1-score: 0.8909, Validation Loss: 0.7597, Validation F1-score: 0.7405
Epoch 8/100, Training Loss: 0.2847, Training F1-score: 0.8944, Validation Loss: 0.6893, Validation F1-score: 0.7564
Epoch 9/100, Training Loss: 0.2766, Training F1-score: 0.8988, Validation Loss: 0.6968, Validation F1-score: 0.7505
Epoch 10/100, Training Loss: 0.2674, Training F1-score: 0.9035, Validation Loss: 0.8009, Validation F1-score: 0.7325
Epoch 11/100, Training Loss: 0.2599, Training F1-score: 0.9055, Validation Loss: 0.7337, Validation F1-score: 0.7519
Epoch 12/100, Training Loss: 0.2561, Training F1-score: 0.9077, Validation Loss: 0.7550, Validation F1-score: 0.7509
Epoch 13/100, Training Loss: 0.2437, Training F1-score: 0.9113, Validation Loss: 0.8342, Validation F1-score: 0.7236
Epoch 14/100, Training Loss: 0.2439, Training F1-score: 0.9118, Validation Loss: 0.8858, Validation F1-score: 0.7472
Epoch 15/100, Training Loss: 0.2383, Training F1-score: 0.9139, Validation Loss: 0.8280, Validation F1-score: 0.7318
Epoch 16/100, Training Loss: 0.2316, Training F1-score: 0.9184, Validation Loss: 0.7719, Validation F1-score: 0.7545
Epoch 17/100, Training Loss: 0.2321, Training F1-score: 0.9161, Validation Loss: 0.7452, Validation F1-score: 0.7403
[I 2024-12-27 14:03:34,830] Trial 107 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.00025, 'dropout': 0.2, 'optimizer': 'RMSprop', 'step_size': 18, 'gamma': 0.5, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 18/100, Training Loss: 0.2253, Training F1-score: 0.9193, Validation Loss: 0.7226, Validation F1-score: 0.7372
Early stopping triggered after 18 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:108 ====================
Learning rate: 0.0010000000
Dropout: 0.5
Optimizer: SGD
Momentum term: 0.8581195706998593
Step size: 17
Gamma: 0.3
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 14:05:08,400] Trial 108 pruned. 
Epoch 1/100, Training Loss: 0.9185, Training F1-score: 0.6691, Validation Loss: 0.8602, Validation F1-score: 0.7061
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:109 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.6.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.5989, Training F1-score: 0.7770, Validation Loss: 0.6700, Validation F1-score: 0.7495
Best F1-score till now on the current trail on Validation data: 0.7494791666666667
Epoch 2/100, Training Loss: 0.4513, Training F1-score: 0.8308, Validation Loss: 0.6830, Validation F1-score: 0.7517
Best F1-score till now on the current trail on Validation data: 0.7517361111111112
Epoch 3/100, Training Loss: 0.3884, Training F1-score: 0.8573, Validation Loss: 0.7194, Validation F1-score: 0.7479
Epoch 4/100, Training Loss: 0.3555, Training F1-score: 0.8674, Validation Loss: 0.7283, Validation F1-score: 0.7458
Epoch 5/100, Training Loss: 0.3338, Training F1-score: 0.8781, Validation Loss: 0.7501, Validation F1-score: 0.7333
Epoch 6/100, Training Loss: 0.3136, Training F1-score: 0.8849, Validation Loss: 0.6937, Validation F1-score: 0.7427
Epoch 7/100, Training Loss: 0.3030, Training F1-score: 0.8876, Validation Loss: 0.7597, Validation F1-score: 0.7411
Epoch 8/100, Training Loss: 0.2878, Training F1-score: 0.8957, Validation Loss: 0.7346, Validation F1-score: 0.7420
Epoch 9/100, Training Loss: 0.2790, Training F1-score: 0.9014, Validation Loss: 0.7648, Validation F1-score: 0.7477
Epoch 10/100, Training Loss: 0.2709, Training F1-score: 0.9004, Validation Loss: 0.7511, Validation F1-score: 0.7436
Epoch 11/100, Training Loss: 0.2631, Training F1-score: 0.9050, Validation Loss: 0.8606, Validation F1-score: 0.7179
Epoch 12/100, Training Loss: 0.2519, Training F1-score: 0.9102, Validation Loss: 0.8279, Validation F1-score: 0.7345
Epoch 13/100, Training Loss: 0.2475, Training F1-score: 0.9099, Validation Loss: 0.7927, Validation F1-score: 0.7547
Best F1-score till now on the current trail on Validation data: 0.7546875
Epoch 14/100, Training Loss: 0.2399, Training F1-score: 0.9123, Validation Loss: 0.7755, Validation F1-score: 0.7413
Epoch 15/100, Training Loss: 0.2382, Training F1-score: 0.9127, Validation Loss: 0.8370, Validation F1-score: 0.7444
Epoch 16/100, Training Loss: 0.2341, Training F1-score: 0.9155, Validation Loss: 0.7706, Validation F1-score: 0.7502
Epoch 17/100, Training Loss: 0.2105, Training F1-score: 0.9243, Validation Loss: 0.8349, Validation F1-score: 0.7380
Epoch 18/100, Training Loss: 0.2061, Training F1-score: 0.9257, Validation Loss: 0.7512, Validation F1-score: 0.7418
Epoch 19/100, Training Loss: 0.1991, Training F1-score: 0.9279, Validation Loss: 0.7745, Validation F1-score: 0.7467
Epoch 20/100, Training Loss: 0.1977, Training F1-score: 0.9297, Validation Loss: 0.8032, Validation F1-score: 0.7517
Epoch 21/100, Training Loss: 0.1953, Training F1-score: 0.9312, Validation Loss: 0.8166, Validation F1-score: 0.7434
Epoch 22/100, Training Loss: 0.1924, Training F1-score: 0.9319, Validation Loss: 0.7488, Validation F1-score: 0.7571
Best F1-score till now on the current trail on Validation data: 0.7571180555555556
Epoch 23/100, Training Loss: 0.1945, Training F1-score: 0.9301, Validation Loss: 0.7384, Validation F1-score: 0.7576
Best F1-score till now on the current trail on Validation data: 0.7576388888888889
Epoch 24/100, Training Loss: 0.1937, Training F1-score: 0.9285, Validation Loss: 0.8221, Validation F1-score: 0.7403
Epoch 25/100, Training Loss: 0.1863, Training F1-score: 0.9333, Validation Loss: 0.8058, Validation F1-score: 0.7524
Epoch 26/100, Training Loss: 0.1878, Training F1-score: 0.9330, Validation Loss: 0.8333, Validation F1-score: 0.7385
Epoch 27/100, Training Loss: 0.1891, Training F1-score: 0.9324, Validation Loss: 0.8717, Validation F1-score: 0.7382
Epoch 28/100, Training Loss: 0.1820, Training F1-score: 0.9348, Validation Loss: 0.8708, Validation F1-score: 0.7382
Epoch 29/100, Training Loss: 0.1859, Training F1-score: 0.9330, Validation Loss: 0.9219, Validation F1-score: 0.7333
Epoch 30/100, Training Loss: 0.1820, Training F1-score: 0.9335, Validation Loss: 0.7964, Validation F1-score: 0.7509
Epoch 31/100, Training Loss: 0.1823, Training F1-score: 0.9323, Validation Loss: 0.9069, Validation F1-score: 0.7373
Epoch 32/100, Training Loss: 0.1819, Training F1-score: 0.9350, Validation Loss: 0.8350, Validation F1-score: 0.7427
Epoch 33/100, Training Loss: 0.1657, Training F1-score: 0.9402, Validation Loss: 0.8829, Validation F1-score: 0.7420
Epoch 34/100, Training Loss: 0.1685, Training F1-score: 0.9395, Validation Loss: 0.8521, Validation F1-score: 0.7405
Epoch 35/100, Training Loss: 0.1679, Training F1-score: 0.9406, Validation Loss: 0.9016, Validation F1-score: 0.7378
Epoch 36/100, Training Loss: 0.1625, Training F1-score: 0.9429, Validation Loss: 0.8527, Validation F1-score: 0.7438
Epoch 37/100, Training Loss: 0.1660, Training F1-score: 0.9401, Validation Loss: 0.8805, Validation F1-score: 0.7302
[I 2024-12-27 15:02:58,860] Trial 109 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.00025, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 16, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.6.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 38/100, Training Loss: 0.1636, Training F1-score: 0.9406, Validation Loss: 0.8753, Validation F1-score: 0.7377
Early stopping triggered after 38 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:110 ====================
Learning rate: 0.0025000000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 12
Gamma: 0.2
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6518, Training F1-score: 0.7582, Validation Loss: 0.6781, Validation F1-score: 0.7550
Best F1-score till now on the current trail on Validation data: 0.7550347222222222
Epoch 2/100, Training Loss: 0.4569, Training F1-score: 0.8280, Validation Loss: 0.6311, Validation F1-score: 0.7549
Epoch 3/100, Training Loss: 0.3940, Training F1-score: 0.8537, Validation Loss: 0.7938, Validation F1-score: 0.7319
Epoch 4/100, Training Loss: 0.3536, Training F1-score: 0.8728, Validation Loss: 0.7364, Validation F1-score: 0.7352
Epoch 5/100, Training Loss: 0.3312, Training F1-score: 0.8792, Validation Loss: 0.7288, Validation F1-score: 0.7332
Epoch 6/100, Training Loss: 0.3110, Training F1-score: 0.8863, Validation Loss: 0.7532, Validation F1-score: 0.7693
Best F1-score till now on the current trail on Validation data: 0.7692708333333333
Epoch 7/100, Training Loss: 0.2972, Training F1-score: 0.8917, Validation Loss: 0.7900, Validation F1-score: 0.7417
Epoch 8/100, Training Loss: 0.2816, Training F1-score: 0.8991, Validation Loss: 0.7058, Validation F1-score: 0.7354
Epoch 9/100, Training Loss: 0.2736, Training F1-score: 0.9022, Validation Loss: 0.9105, Validation F1-score: 0.7403
Epoch 10/100, Training Loss: 0.2695, Training F1-score: 0.9041, Validation Loss: 0.9288, Validation F1-score: 0.7377
Epoch 11/100, Training Loss: 0.2641, Training F1-score: 0.9055, Validation Loss: 0.8050, Validation F1-score: 0.7403
Epoch 12/100, Training Loss: 0.2553, Training F1-score: 0.9086, Validation Loss: 0.8071, Validation F1-score: 0.7382
Epoch 13/100, Training Loss: 0.2023, Training F1-score: 0.9291, Validation Loss: 0.7592, Validation F1-score: 0.7531
Epoch 14/100, Training Loss: 0.1945, Training F1-score: 0.9317, Validation Loss: 0.7837, Validation F1-score: 0.7469
Epoch 15/100, Training Loss: 0.1930, Training F1-score: 0.9326, Validation Loss: 0.8158, Validation F1-score: 0.7484
Epoch 16/100, Training Loss: 0.1909, Training F1-score: 0.9311, Validation Loss: 0.8737, Validation F1-score: 0.7389
Epoch 17/100, Training Loss: 0.1878, Training F1-score: 0.9335, Validation Loss: 0.8190, Validation F1-score: 0.7420
Epoch 18/100, Training Loss: 0.1892, Training F1-score: 0.9330, Validation Loss: 0.7512, Validation F1-score: 0.7517
Epoch 19/100, Training Loss: 0.1842, Training F1-score: 0.9351, Validation Loss: 0.8264, Validation F1-score: 0.7410
Epoch 20/100, Training Loss: 0.1834, Training F1-score: 0.9349, Validation Loss: 0.7726, Validation F1-score: 0.7559
[I 2024-12-27 15:33:42,937] Trial 110 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.0025, 'dropout': 0.2, 'optimizer': 'RMSprop', 'step_size': 12, 'gamma': 0.2, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 21/100, Training Loss: 0.1819, Training F1-score: 0.9359, Validation Loss: 0.8895, Validation F1-score: 0.7439
Early stopping triggered after 21 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:111 ====================
Learning rate: 0.0007500000
Dropout: 0.3
Optimizer: Adam
Momentum term: 0
Step size: 19
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.5.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6397, Training F1-score: 0.7619, Validation Loss: 0.6590, Validation F1-score: 0.7540
Best F1-score till now on the current trail on Validation data: 0.7539930555555555
Epoch 2/100, Training Loss: 0.4938, Training F1-score: 0.8108, Validation Loss: 0.7517, Validation F1-score: 0.7203
Epoch 3/100, Training Loss: 0.4302, Training F1-score: 0.8402, Validation Loss: 0.7422, Validation F1-score: 0.7403
Epoch 4/100, Training Loss: 0.3998, Training F1-score: 0.8509, Validation Loss: 0.8903, Validation F1-score: 0.7023
Epoch 5/100, Training Loss: 0.3671, Training F1-score: 0.8620, Validation Loss: 0.7022, Validation F1-score: 0.7500
Epoch 6/100, Training Loss: 0.3483, Training F1-score: 0.8715, Validation Loss: 0.6627, Validation F1-score: 0.7611
Best F1-score till now on the current trail on Validation data: 0.7611111111111111
Epoch 7/100, Training Loss: 0.3366, Training F1-score: 0.8753, Validation Loss: 0.7139, Validation F1-score: 0.7422
Epoch 8/100, Training Loss: 0.3206, Training F1-score: 0.8808, Validation Loss: 0.7793, Validation F1-score: 0.7399
Epoch 9/100, Training Loss: 0.3130, Training F1-score: 0.8841, Validation Loss: 0.7084, Validation F1-score: 0.7420
Epoch 10/100, Training Loss: 0.3051, Training F1-score: 0.8883, Validation Loss: 0.7591, Validation F1-score: 0.7318
Epoch 11/100, Training Loss: 0.2952, Training F1-score: 0.8901, Validation Loss: 0.7798, Validation F1-score: 0.7290
Epoch 12/100, Training Loss: 0.2926, Training F1-score: 0.8935, Validation Loss: 0.7803, Validation F1-score: 0.7405
Epoch 13/100, Training Loss: 0.2816, Training F1-score: 0.8968, Validation Loss: 0.7114, Validation F1-score: 0.7526
Epoch 14/100, Training Loss: 0.2822, Training F1-score: 0.8978, Validation Loss: 0.6993, Validation F1-score: 0.7356
Epoch 15/100, Training Loss: 0.2724, Training F1-score: 0.8998, Validation Loss: 0.8220, Validation F1-score: 0.7269
Epoch 16/100, Training Loss: 0.2704, Training F1-score: 0.9023, Validation Loss: 0.7501, Validation F1-score: 0.7514
Epoch 17/100, Training Loss: 0.2622, Training F1-score: 0.9061, Validation Loss: 0.7601, Validation F1-score: 0.7332
Epoch 18/100, Training Loss: 0.2633, Training F1-score: 0.9053, Validation Loss: 0.7721, Validation F1-score: 0.7458
Epoch 19/100, Training Loss: 0.2582, Training F1-score: 0.9045, Validation Loss: 0.7954, Validation F1-score: 0.7352
Epoch 20/100, Training Loss: 0.2284, Training F1-score: 0.9191, Validation Loss: 0.8220, Validation F1-score: 0.7398
[I 2024-12-27 16:18:34,340] Trial 111 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.00075, 'dropout': 0.3, 'optimizer': 'Adam', 'step_size': 19, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.5.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 21/100, Training Loss: 0.2217, Training F1-score: 0.9199, Validation Loss: 0.8485, Validation F1-score: 0.7292
Early stopping triggered after 21 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:112 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 14
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6106, Training F1-score: 0.7733, Validation Loss: 0.6973, Validation F1-score: 0.7488
Best F1-score till now on the current trail on Validation data: 0.7487847222222223
Epoch 2/100, Training Loss: 0.4478, Training F1-score: 0.8311, Validation Loss: 0.8495, Validation F1-score: 0.7252
Epoch 3/100, Training Loss: 0.3965, Training F1-score: 0.8514, Validation Loss: 0.6688, Validation F1-score: 0.7495
Best F1-score till now on the current trail on Validation data: 0.7494791666666667
Epoch 4/100, Training Loss: 0.3627, Training F1-score: 0.8656, Validation Loss: 0.8639, Validation F1-score: 0.7193
Epoch 5/100, Training Loss: 0.3438, Training F1-score: 0.8731, Validation Loss: 0.6813, Validation F1-score: 0.7646
Best F1-score till now on the current trail on Validation data: 0.7645833333333334
Epoch 6/100, Training Loss: 0.3247, Training F1-score: 0.8809, Validation Loss: 0.7609, Validation F1-score: 0.7450
Epoch 7/100, Training Loss: 0.3050, Training F1-score: 0.8885, Validation Loss: 0.7247, Validation F1-score: 0.7536
Epoch 8/100, Training Loss: 0.2958, Training F1-score: 0.8915, Validation Loss: 0.7539, Validation F1-score: 0.7444
Epoch 9/100, Training Loss: 0.2861, Training F1-score: 0.8945, Validation Loss: 0.6884, Validation F1-score: 0.7646
Epoch 10/100, Training Loss: 0.2732, Training F1-score: 0.9006, Validation Loss: 0.7442, Validation F1-score: 0.7523
Epoch 11/100, Training Loss: 0.2690, Training F1-score: 0.9016, Validation Loss: 0.7292, Validation F1-score: 0.7547
Epoch 12/100, Training Loss: 0.2618, Training F1-score: 0.9055, Validation Loss: 0.7855, Validation F1-score: 0.7450
Epoch 13/100, Training Loss: 0.2539, Training F1-score: 0.9078, Validation Loss: 0.7970, Validation F1-score: 0.7486
Epoch 14/100, Training Loss: 0.2522, Training F1-score: 0.9084, Validation Loss: 0.8214, Validation F1-score: 0.7266
Epoch 15/100, Training Loss: 0.2265, Training F1-score: 0.9185, Validation Loss: 0.8031, Validation F1-score: 0.7401
Epoch 16/100, Training Loss: 0.2172, Training F1-score: 0.9211, Validation Loss: 0.7944, Validation F1-score: 0.7396
Epoch 17/100, Training Loss: 0.2137, Training F1-score: 0.9227, Validation Loss: 0.8214, Validation F1-score: 0.7538
Epoch 18/100, Training Loss: 0.2125, Training F1-score: 0.9235, Validation Loss: 0.7504, Validation F1-score: 0.7528
Epoch 19/100, Training Loss: 0.2088, Training F1-score: 0.9244, Validation Loss: 0.8534, Validation F1-score: 0.7363
[I 2024-12-27 17:04:50,605] Trial 112 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.00025, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 14, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 20/100, Training Loss: 0.2085, Training F1-score: 0.9240, Validation Loss: 0.7196, Validation F1-score: 0.7554
Early stopping triggered after 20 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:113 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 13
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 17:07:34,419] Trial 113 pruned. 
Epoch 1/100, Training Loss: 0.6119, Training F1-score: 0.7751, Validation Loss: 0.6951, Validation F1-score: 0.7451
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:114 ====================
Learning rate: 0.0000002500
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 14
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 17:08:59,033] Trial 114 pruned. 
Epoch 1/100, Training Loss: 2.0628, Training F1-score: 0.1601, Validation Loss: 1.9781, Validation F1-score: 0.4174
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:115 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 15
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6174, Training F1-score: 0.7718, Validation Loss: 0.6750, Validation F1-score: 0.7552
Best F1-score till now on the current trail on Validation data: 0.7552083333333334
Epoch 2/100, Training Loss: 0.4519, Training F1-score: 0.8303, Validation Loss: 0.6965, Validation F1-score: 0.7359
Epoch 3/100, Training Loss: 0.4021, Training F1-score: 0.8509, Validation Loss: 0.7773, Validation F1-score: 0.7398
Epoch 4/100, Training Loss: 0.3637, Training F1-score: 0.8646, Validation Loss: 0.7340, Validation F1-score: 0.7326
Epoch 5/100, Training Loss: 0.3424, Training F1-score: 0.8727, Validation Loss: 0.7133, Validation F1-score: 0.7396
Epoch 6/100, Training Loss: 0.3221, Training F1-score: 0.8803, Validation Loss: 0.8011, Validation F1-score: 0.7312
Epoch 7/100, Training Loss: 0.3085, Training F1-score: 0.8870, Validation Loss: 0.7619, Validation F1-score: 0.7578
Best F1-score till now on the current trail on Validation data: 0.7578125
Epoch 8/100, Training Loss: 0.2977, Training F1-score: 0.8894, Validation Loss: 0.7692, Validation F1-score: 0.7516
Epoch 9/100, Training Loss: 0.2907, Training F1-score: 0.8923, Validation Loss: 0.8132, Validation F1-score: 0.7267
Epoch 10/100, Training Loss: 0.2748, Training F1-score: 0.8975, Validation Loss: 0.8681, Validation F1-score: 0.7382
Epoch 11/100, Training Loss: 0.2717, Training F1-score: 0.9009, Validation Loss: 0.7898, Validation F1-score: 0.7330
Epoch 12/100, Training Loss: 0.2640, Training F1-score: 0.9041, Validation Loss: 0.7259, Validation F1-score: 0.7373
Epoch 13/100, Training Loss: 0.2590, Training F1-score: 0.9065, Validation Loss: 0.7560, Validation F1-score: 0.7429
Epoch 14/100, Training Loss: 0.2533, Training F1-score: 0.9063, Validation Loss: 0.7016, Validation F1-score: 0.7580
Best F1-score till now on the current trail on Validation data: 0.757986111111111
Epoch 15/100, Training Loss: 0.2509, Training F1-score: 0.9098, Validation Loss: 0.7115, Validation F1-score: 0.7495
Epoch 16/100, Training Loss: 0.2206, Training F1-score: 0.9218, Validation Loss: 0.7156, Validation F1-score: 0.7542
Epoch 17/100, Training Loss: 0.2156, Training F1-score: 0.9219, Validation Loss: 0.8211, Validation F1-score: 0.7349
Epoch 18/100, Training Loss: 0.2123, Training F1-score: 0.9218, Validation Loss: 0.7454, Validation F1-score: 0.7528
Epoch 19/100, Training Loss: 0.2086, Training F1-score: 0.9247, Validation Loss: 0.7827, Validation F1-score: 0.7543
Epoch 20/100, Training Loss: 0.2105, Training F1-score: 0.9241, Validation Loss: 0.7889, Validation F1-score: 0.7417
Epoch 21/100, Training Loss: 0.2060, Training F1-score: 0.9270, Validation Loss: 0.7766, Validation F1-score: 0.7424
Epoch 22/100, Training Loss: 0.2061, Training F1-score: 0.9269, Validation Loss: 0.8463, Validation F1-score: 0.7330
Epoch 23/100, Training Loss: 0.2010, Training F1-score: 0.9284, Validation Loss: 0.8707, Validation F1-score: 0.7330
Epoch 24/100, Training Loss: 0.1994, Training F1-score: 0.9283, Validation Loss: 0.7644, Validation F1-score: 0.7453
Epoch 25/100, Training Loss: 0.2010, Training F1-score: 0.9280, Validation Loss: 0.7907, Validation F1-score: 0.7420
Epoch 26/100, Training Loss: 0.1991, Training F1-score: 0.9292, Validation Loss: 0.8002, Validation F1-score: 0.7488
Epoch 27/100, Training Loss: 0.1991, Training F1-score: 0.9290, Validation Loss: 0.7571, Validation F1-score: 0.7493
Epoch 28/100, Training Loss: 0.1916, Training F1-score: 0.9309, Validation Loss: 0.7929, Validation F1-score: 0.7552
[I 2024-12-27 18:11:09,889] Trial 115 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.00025, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 15, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 29/100, Training Loss: 0.1927, Training F1-score: 0.9293, Validation Loss: 0.7794, Validation F1-score: 0.7566
Early stopping triggered after 29 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:116 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 18:13:33,088] Trial 116 pruned. 
Epoch 1/100, Training Loss: 0.6104, Training F1-score: 0.7718, Validation Loss: 0.7188, Validation F1-score: 0.7387
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:117 ====================
Learning rate: 0.0005000000
Dropout: 0.25
Optimizer: RMSprop
Momentum term: 0
Step size: 18
Gamma: 0.1
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 18:15:48,493] Trial 117 pruned. 
Epoch 1/100, Training Loss: 0.6197, Training F1-score: 0.7691, Validation Loss: 0.8072, Validation F1-score: 0.7245
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:118 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 14
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6182, Training F1-score: 0.7723, Validation Loss: 0.6784, Validation F1-score: 0.7543
Best F1-score till now on the current trail on Validation data: 0.7543402777777778
Epoch 2/100, Training Loss: 0.4556, Training F1-score: 0.8265, Validation Loss: 0.6934, Validation F1-score: 0.7594
Best F1-score till now on the current trail on Validation data: 0.759375
Epoch 3/100, Training Loss: 0.3951, Training F1-score: 0.8536, Validation Loss: 0.6896, Validation F1-score: 0.7538
Epoch 4/100, Training Loss: 0.3643, Training F1-score: 0.8633, Validation Loss: 0.7050, Validation F1-score: 0.7507
Epoch 5/100, Training Loss: 0.3406, Training F1-score: 0.8718, Validation Loss: 0.7342, Validation F1-score: 0.7524
Epoch 6/100, Training Loss: 0.3234, Training F1-score: 0.8778, Validation Loss: 0.8435, Validation F1-score: 0.7302
Epoch 7/100, Training Loss: 0.3015, Training F1-score: 0.8881, Validation Loss: 0.8210, Validation F1-score: 0.7354
Epoch 8/100, Training Loss: 0.2987, Training F1-score: 0.8898, Validation Loss: 0.8064, Validation F1-score: 0.7278
Epoch 9/100, Training Loss: 0.2873, Training F1-score: 0.8948, Validation Loss: 0.7345, Validation F1-score: 0.7538
Epoch 10/100, Training Loss: 0.2740, Training F1-score: 0.8993, Validation Loss: 0.7547, Validation F1-score: 0.7474
Epoch 11/100, Training Loss: 0.2640, Training F1-score: 0.9044, Validation Loss: 0.7414, Validation F1-score: 0.7441
Epoch 12/100, Training Loss: 0.2632, Training F1-score: 0.9055, Validation Loss: 0.7515, Validation F1-score: 0.7434
Epoch 13/100, Training Loss: 0.2518, Training F1-score: 0.9081, Validation Loss: 0.7096, Validation F1-score: 0.7594
Epoch 14/100, Training Loss: 0.2542, Training F1-score: 0.9087, Validation Loss: 0.7875, Validation F1-score: 0.7422
Epoch 15/100, Training Loss: 0.2252, Training F1-score: 0.9182, Validation Loss: 0.7340, Validation F1-score: 0.7611
Best F1-score till now on the current trail on Validation data: 0.7611111111111111
Epoch 16/100, Training Loss: 0.2178, Training F1-score: 0.9212, Validation Loss: 0.7832, Validation F1-score: 0.7535
Epoch 17/100, Training Loss: 0.2139, Training F1-score: 0.9219, Validation Loss: 0.8485, Validation F1-score: 0.7391
Epoch 18/100, Training Loss: 0.2161, Training F1-score: 0.9224, Validation Loss: 0.7869, Validation F1-score: 0.7540
Epoch 19/100, Training Loss: 0.2126, Training F1-score: 0.9237, Validation Loss: 0.7914, Validation F1-score: 0.7438
Epoch 20/100, Training Loss: 0.2067, Training F1-score: 0.9260, Validation Loss: 0.8128, Validation F1-score: 0.7431
Epoch 21/100, Training Loss: 0.2080, Training F1-score: 0.9250, Validation Loss: 0.8913, Validation F1-score: 0.7372
Epoch 22/100, Training Loss: 0.2039, Training F1-score: 0.9283, Validation Loss: 0.8371, Validation F1-score: 0.7417
Epoch 23/100, Training Loss: 0.2038, Training F1-score: 0.9250, Validation Loss: 0.7879, Validation F1-score: 0.7543
Epoch 24/100, Training Loss: 0.1990, Training F1-score: 0.9269, Validation Loss: 0.8200, Validation F1-score: 0.7477
Epoch 25/100, Training Loss: 0.2031, Training F1-score: 0.9272, Validation Loss: 0.7923, Validation F1-score: 0.7464
Epoch 26/100, Training Loss: 0.1982, Training F1-score: 0.9273, Validation Loss: 0.8861, Validation F1-score: 0.7365
Epoch 27/100, Training Loss: 0.1980, Training F1-score: 0.9291, Validation Loss: 0.7550, Validation F1-score: 0.7599
Epoch 28/100, Training Loss: 0.1888, Training F1-score: 0.9304, Validation Loss: 0.8338, Validation F1-score: 0.7474
Epoch 29/100, Training Loss: 0.1841, Training F1-score: 0.9333, Validation Loss: 0.8330, Validation F1-score: 0.7425
[I 2024-12-27 19:17:10,916] Trial 118 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.00025, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 14, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 30/100, Training Loss: 0.1844, Training F1-score: 0.9330, Validation Loss: 0.8182, Validation F1-score: 0.7427
Early stopping triggered after 30 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:119 ====================
Learning rate: 0.0000250000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 15
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 19:19:58,310] Trial 119 pruned. 
Epoch 1/100, Training Loss: 0.8088, Training F1-score: 0.7172, Validation Loss: 0.8061, Validation F1-score: 0.7149
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:120 ====================
Learning rate: 0.0000010000
Dropout: 0.32
Optimizer: Adam
Momentum term: 0
Step size: 17
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.0.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 19:21:40,100] Trial 120 pruned. 
Epoch 1/100, Training Loss: 1.6825, Training F1-score: 0.4506, Validation Loss: 1.5157, Validation F1-score: 0.4479
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:121 ====================
Learning rate: 0.0001000000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 18
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6181, Training F1-score: 0.7712, Validation Loss: 0.7287, Validation F1-score: 0.7498
Best F1-score till now on the current trail on Validation data: 0.7498263888888889
Epoch 2/100, Training Loss: 0.4719, Training F1-score: 0.8223, Validation Loss: 0.6961, Validation F1-score: 0.7443
Epoch 3/100, Training Loss: 0.4184, Training F1-score: 0.8420, Validation Loss: 0.7653, Validation F1-score: 0.7349
Epoch 4/100, Training Loss: 0.3863, Training F1-score: 0.8547, Validation Loss: 0.7018, Validation F1-score: 0.7536
Best F1-score till now on the current trail on Validation data: 0.7536458333333333
Epoch 5/100, Training Loss: 0.3610, Training F1-score: 0.8656, Validation Loss: 0.7367, Validation F1-score: 0.7450
Epoch 6/100, Training Loss: 0.3452, Training F1-score: 0.8723, Validation Loss: 0.8128, Validation F1-score: 0.7352
Epoch 7/100, Training Loss: 0.3285, Training F1-score: 0.8792, Validation Loss: 0.7979, Validation F1-score: 0.7406
Epoch 8/100, Training Loss: 0.3159, Training F1-score: 0.8821, Validation Loss: 0.7244, Validation F1-score: 0.7602
Best F1-score till now on the current trail on Validation data: 0.7602430555555556
Epoch 9/100, Training Loss: 0.3059, Training F1-score: 0.8876, Validation Loss: 0.7678, Validation F1-score: 0.7479
Epoch 10/100, Training Loss: 0.2975, Training F1-score: 0.8912, Validation Loss: 0.7439, Validation F1-score: 0.7622
Best F1-score till now on the current trail on Validation data: 0.7621527777777778
Epoch 11/100, Training Loss: 0.2877, Training F1-score: 0.8952, Validation Loss: 0.8336, Validation F1-score: 0.7372
Epoch 12/100, Training Loss: 0.2799, Training F1-score: 0.8982, Validation Loss: 0.8181, Validation F1-score: 0.7477
Epoch 13/100, Training Loss: 0.2739, Training F1-score: 0.9001, Validation Loss: 0.8010, Validation F1-score: 0.7403
Epoch 14/100, Training Loss: 0.2668, Training F1-score: 0.9043, Validation Loss: 0.7143, Validation F1-score: 0.7488
Epoch 15/100, Training Loss: 0.2621, Training F1-score: 0.9056, Validation Loss: 0.7651, Validation F1-score: 0.7399
Epoch 16/100, Training Loss: 0.2545, Training F1-score: 0.9057, Validation Loss: 0.7122, Validation F1-score: 0.7615
Epoch 17/100, Training Loss: 0.2531, Training F1-score: 0.9094, Validation Loss: 0.8174, Validation F1-score: 0.7481
Epoch 18/100, Training Loss: 0.2508, Training F1-score: 0.9100, Validation Loss: 0.7439, Validation F1-score: 0.7460
Epoch 19/100, Training Loss: 0.2244, Training F1-score: 0.9190, Validation Loss: 0.7678, Validation F1-score: 0.7524
Epoch 20/100, Training Loss: 0.2206, Training F1-score: 0.9212, Validation Loss: 0.7642, Validation F1-score: 0.7481
Epoch 21/100, Training Loss: 0.2156, Training F1-score: 0.9221, Validation Loss: 0.7621, Validation F1-score: 0.7465
Epoch 22/100, Training Loss: 0.2190, Training F1-score: 0.9218, Validation Loss: 0.7646, Validation F1-score: 0.7545
Epoch 23/100, Training Loss: 0.2156, Training F1-score: 0.9234, Validation Loss: 0.8194, Validation F1-score: 0.7453
Epoch 24/100, Training Loss: 0.2189, Training F1-score: 0.9205, Validation Loss: 0.8473, Validation F1-score: 0.7307
[I 2024-12-27 20:13:36,741] Trial 121 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.0001, 'dropout': 0.2, 'optimizer': 'RMSprop', 'step_size': 18, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 25/100, Training Loss: 0.2101, Training F1-score: 0.9253, Validation Loss: 0.7814, Validation F1-score: 0.7470
Early stopping triggered after 25 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:122 ====================
Learning rate: 0.0075000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 15
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6641, Training F1-score: 0.7515, Validation Loss: 0.6963, Validation F1-score: 0.7569
Best F1-score till now on the current trail on Validation data: 0.7569444444444444
Epoch 2/100, Training Loss: 0.5206, Training F1-score: 0.8023, Validation Loss: 0.6855, Validation F1-score: 0.7413
Epoch 3/100, Training Loss: 0.4525, Training F1-score: 0.8301, Validation Loss: 0.7111, Validation F1-score: 0.7562
Epoch 4/100, Training Loss: 0.4245, Training F1-score: 0.8432, Validation Loss: 0.6327, Validation F1-score: 0.7597
Best F1-score till now on the current trail on Validation data: 0.7597222222222222
Epoch 5/100, Training Loss: 0.4026, Training F1-score: 0.8524, Validation Loss: 0.6562, Validation F1-score: 0.7543
Epoch 6/100, Training Loss: 0.3840, Training F1-score: 0.8577, Validation Loss: 0.6434, Validation F1-score: 0.7623
Best F1-score till now on the current trail on Validation data: 0.7623263888888888
Epoch 7/100, Training Loss: 0.3704, Training F1-score: 0.8619, Validation Loss: 0.6100, Validation F1-score: 0.7701
Best F1-score till now on the current trail on Validation data: 0.7701388888888889
Epoch 8/100, Training Loss: 0.3571, Training F1-score: 0.8669, Validation Loss: 0.7244, Validation F1-score: 0.7470
Epoch 9/100, Training Loss: 0.3574, Training F1-score: 0.8714, Validation Loss: 0.7331, Validation F1-score: 0.7384
Epoch 10/100, Training Loss: 0.3448, Training F1-score: 0.8727, Validation Loss: 0.6532, Validation F1-score: 0.7714
Best F1-score till now on the current trail on Validation data: 0.7713541666666667
Epoch 11/100, Training Loss: 0.3445, Training F1-score: 0.8753, Validation Loss: 0.6407, Validation F1-score: 0.7507
Epoch 12/100, Training Loss: 0.3290, Training F1-score: 0.8778, Validation Loss: 0.6081, Validation F1-score: 0.7762
Best F1-score till now on the current trail on Validation data: 0.7762152777777779
Epoch 13/100, Training Loss: 0.3277, Training F1-score: 0.8805, Validation Loss: 0.6644, Validation F1-score: 0.7790
Best F1-score till now on the current trail on Validation data: 0.7789930555555555
Epoch 14/100, Training Loss: 0.3250, Training F1-score: 0.8805, Validation Loss: 0.7500, Validation F1-score: 0.7366
Epoch 15/100, Training Loss: 0.3246, Training F1-score: 0.8813, Validation Loss: 0.7105, Validation F1-score: 0.7382
Epoch 16/100, Training Loss: 0.2841, Training F1-score: 0.8968, Validation Loss: 0.6547, Validation F1-score: 0.7566
Epoch 17/100, Training Loss: 0.2845, Training F1-score: 0.8980, Validation Loss: 0.6763, Validation F1-score: 0.7507
Epoch 18/100, Training Loss: 0.2765, Training F1-score: 0.8974, Validation Loss: 0.6444, Validation F1-score: 0.7616
Epoch 19/100, Training Loss: 0.2721, Training F1-score: 0.9004, Validation Loss: 0.7265, Validation F1-score: 0.7424
Epoch 20/100, Training Loss: 0.2726, Training F1-score: 0.8997, Validation Loss: 0.6585, Validation F1-score: 0.7684
Epoch 21/100, Training Loss: 0.2730, Training F1-score: 0.9004, Validation Loss: 0.6889, Validation F1-score: 0.7595
Epoch 22/100, Training Loss: 0.2739, Training F1-score: 0.8987, Validation Loss: 0.6444, Validation F1-score: 0.7625
Epoch 23/100, Training Loss: 0.2680, Training F1-score: 0.9027, Validation Loss: 0.6851, Validation F1-score: 0.7571
Epoch 24/100, Training Loss: 0.2677, Training F1-score: 0.9038, Validation Loss: 0.6848, Validation F1-score: 0.7569
Epoch 25/100, Training Loss: 0.2652, Training F1-score: 0.9040, Validation Loss: 0.6455, Validation F1-score: 0.7720
Epoch 26/100, Training Loss: 0.2631, Training F1-score: 0.9042, Validation Loss: 0.6674, Validation F1-score: 0.7642
Epoch 27/100, Training Loss: 0.2643, Training F1-score: 0.9051, Validation Loss: 0.6345, Validation F1-score: 0.7637
[I 2024-12-27 21:12:28,908] Trial 122 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.0075, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 15, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 28/100, Training Loss: 0.2625, Training F1-score: 0.9049, Validation Loss: 0.7031, Validation F1-score: 0.7523
Early stopping triggered after 28 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:123 ====================
Learning rate: 0.0075000000
Dropout: 0.28
Optimizer: Adam
Momentum term: 0
Step size: 15
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6859, Training F1-score: 0.7479, Validation Loss: 0.6331, Validation F1-score: 0.7736
Best F1-score till now on the current trail on Validation data: 0.7736111111111111
Epoch 2/100, Training Loss: 0.5382, Training F1-score: 0.7999, Validation Loss: 0.7162, Validation F1-score: 0.7233
Epoch 3/100, Training Loss: 0.4835, Training F1-score: 0.8175, Validation Loss: 0.6527, Validation F1-score: 0.7418
Epoch 4/100, Training Loss: 0.4500, Training F1-score: 0.8326, Validation Loss: 0.7611, Validation F1-score: 0.7227
Epoch 5/100, Training Loss: 0.4216, Training F1-score: 0.8453, Validation Loss: 0.6878, Validation F1-score: 0.7439
Epoch 6/100, Training Loss: 0.4088, Training F1-score: 0.8519, Validation Loss: 0.7376, Validation F1-score: 0.7361
Epoch 7/100, Training Loss: 0.3930, Training F1-score: 0.8572, Validation Loss: 0.7296, Validation F1-score: 0.7573
Epoch 8/100, Training Loss: 0.3808, Training F1-score: 0.8596, Validation Loss: 0.7150, Validation F1-score: 0.7337
Epoch 9/100, Training Loss: 0.3710, Training F1-score: 0.8652, Validation Loss: 0.6946, Validation F1-score: 0.7453
Epoch 10/100, Training Loss: 0.3648, Training F1-score: 0.8654, Validation Loss: 0.8136, Validation F1-score: 0.7510
Epoch 11/100, Training Loss: 0.3578, Training F1-score: 0.8712, Validation Loss: 0.8467, Validation F1-score: 0.7427
Epoch 12/100, Training Loss: 0.3543, Training F1-score: 0.8727, Validation Loss: 0.7355, Validation F1-score: 0.7319
Epoch 13/100, Training Loss: 0.3470, Training F1-score: 0.8759, Validation Loss: 0.7812, Validation F1-score: 0.7302
Epoch 14/100, Training Loss: 0.3437, Training F1-score: 0.8789, Validation Loss: 0.8139, Validation F1-score: 0.7344
Epoch 15/100, Training Loss: 0.3385, Training F1-score: 0.8774, Validation Loss: 0.8249, Validation F1-score: 0.7281
[I 2024-12-27 21:48:00,171] Trial 123 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.0075, 'dropout': 0.28, 'optimizer': 'Adam', 'step_size': 15, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 16/100, Training Loss: 0.2964, Training F1-score: 0.8934, Validation Loss: 0.7168, Validation F1-score: 0.7538
Early stopping triggered after 16 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:124 ====================
Learning rate: 0.0075000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 14
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 21:50:29,044] Trial 124 pruned. 
Epoch 1/100, Training Loss: 0.6640, Training F1-score: 0.7551, Validation Loss: 0.6770, Validation F1-score: 0.7458
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:125 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 21:52:42,358] Trial 125 pruned. 
Epoch 1/100, Training Loss: 0.6092, Training F1-score: 0.7742, Validation Loss: 0.8036, Validation F1-score: 0.7189
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:126 ====================
Learning rate: 0.0000075000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 15
Gamma: 0.3
Batch size: 16
Number of frozen parameters: dino_model.blocks.2.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 21:54:25,121] Trial 126 pruned. 
Epoch 1/100, Training Loss: 1.2529, Training F1-score: 0.5854, Validation Loss: 1.0121, Validation F1-score: 0.6486
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:127 ====================
Learning rate: 0.0000001000
Dropout: 0.25
Optimizer: RMSprop
Momentum term: 0
Step size: 18
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.4.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 21:56:53,066] Trial 127 pruned. 
Epoch 1/100, Training Loss: 2.0717, Training F1-score: 0.1231, Validation Loss: 1.9890, Validation F1-score: 0.1361
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:128 ====================
Learning rate: 0.0010000000
Dropout: 0.4
Optimizer: SGD
Momentum term: 0.8087300180898366
Step size: 16
Gamma: 0.1
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 21:58:35,530] Trial 128 pruned. 
Epoch 1/100, Training Loss: 0.9092, Training F1-score: 0.6747, Validation Loss: 0.8089, Validation F1-score: 0.7031
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:129 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 14
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.6.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 22:00:57,514] Trial 129 pruned. 
Epoch 1/100, Training Loss: 0.5967, Training F1-score: 0.7782, Validation Loss: 0.7266, Validation F1-score: 0.7247
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:130 ====================
Learning rate: 0.0000007500
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 13
Gamma: 0.2
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 22:02:27,722] Trial 130 pruned. 
Epoch 1/100, Training Loss: 1.9441, Training F1-score: 0.2703, Validation Loss: 1.7269, Validation F1-score: 0.4385
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:131 ====================
Learning rate: 0.0000750000
Dropout: 0.35
Optimizer: Adam
Momentum term: 0
Step size: 20
Gamma: 0.5
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 22:05:20,839] Trial 131 pruned. 
Epoch 1/100, Training Loss: 0.7946, Training F1-score: 0.7172, Validation Loss: 0.7592, Validation F1-score: 0.7297
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:132 ====================
Learning rate: 0.0001000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 18
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6846, Training F1-score: 0.7510, Validation Loss: 0.7041, Validation F1-score: 0.7589
Best F1-score till now on the current trail on Validation data: 0.7588541666666667
Epoch 2/100, Training Loss: 0.4978, Training F1-score: 0.8133, Validation Loss: 0.7162, Validation F1-score: 0.7488
Epoch 3/100, Training Loss: 0.4385, Training F1-score: 0.8376, Validation Loss: 0.7214, Validation F1-score: 0.7517
Epoch 4/100, Training Loss: 0.4062, Training F1-score: 0.8494, Validation Loss: 0.8722, Validation F1-score: 0.7193
Epoch 5/100, Training Loss: 0.3794, Training F1-score: 0.8586, Validation Loss: 0.8402, Validation F1-score: 0.7304
Epoch 6/100, Training Loss: 0.3571, Training F1-score: 0.8696, Validation Loss: 0.7863, Validation F1-score: 0.7424
Epoch 7/100, Training Loss: 0.3382, Training F1-score: 0.8739, Validation Loss: 0.7699, Validation F1-score: 0.7396
Epoch 8/100, Training Loss: 0.3307, Training F1-score: 0.8793, Validation Loss: 0.7284, Validation F1-score: 0.7559
Epoch 9/100, Training Loss: 0.3158, Training F1-score: 0.8846, Validation Loss: 0.7779, Validation F1-score: 0.7432
Epoch 10/100, Training Loss: 0.3133, Training F1-score: 0.8846, Validation Loss: 0.7084, Validation F1-score: 0.7512
Epoch 11/100, Training Loss: 0.2987, Training F1-score: 0.8912, Validation Loss: 0.7711, Validation F1-score: 0.7481
Epoch 12/100, Training Loss: 0.2943, Training F1-score: 0.8923, Validation Loss: 0.7412, Validation F1-score: 0.7465
Epoch 13/100, Training Loss: 0.2877, Training F1-score: 0.8944, Validation Loss: 0.7457, Validation F1-score: 0.7438
Epoch 14/100, Training Loss: 0.2853, Training F1-score: 0.8977, Validation Loss: 0.6980, Validation F1-score: 0.7632
Best F1-score till now on the current trail on Validation data: 0.7631944444444444
Epoch 15/100, Training Loss: 0.2704, Training F1-score: 0.9006, Validation Loss: 0.7055, Validation F1-score: 0.7564
Epoch 16/100, Training Loss: 0.2687, Training F1-score: 0.9022, Validation Loss: 0.7283, Validation F1-score: 0.7521
Epoch 17/100, Training Loss: 0.2637, Training F1-score: 0.9037, Validation Loss: 0.8495, Validation F1-score: 0.7382
Epoch 18/100, Training Loss: 0.2599, Training F1-score: 0.9050, Validation Loss: 0.7560, Validation F1-score: 0.7410
Epoch 19/100, Training Loss: 0.2407, Training F1-score: 0.9135, Validation Loss: 0.7877, Validation F1-score: 0.7396
Epoch 20/100, Training Loss: 0.2346, Training F1-score: 0.9164, Validation Loss: 0.7437, Validation F1-score: 0.7564
Epoch 21/100, Training Loss: 0.2353, Training F1-score: 0.9148, Validation Loss: 0.7927, Validation F1-score: 0.7474
Epoch 22/100, Training Loss: 0.2311, Training F1-score: 0.9173, Validation Loss: 0.7909, Validation F1-score: 0.7443
Epoch 23/100, Training Loss: 0.2288, Training F1-score: 0.9192, Validation Loss: 0.8062, Validation F1-score: 0.7424
Epoch 24/100, Training Loss: 0.2267, Training F1-score: 0.9190, Validation Loss: 0.7526, Validation F1-score: 0.7568
Epoch 25/100, Training Loss: 0.2278, Training F1-score: 0.9186, Validation Loss: 0.7371, Validation F1-score: 0.7589
Epoch 26/100, Training Loss: 0.2225, Training F1-score: 0.9206, Validation Loss: 0.8385, Validation F1-score: 0.7394
Epoch 27/100, Training Loss: 0.2238, Training F1-score: 0.9192, Validation Loss: 0.7642, Validation F1-score: 0.7516
Epoch 28/100, Training Loss: 0.2255, Training F1-score: 0.9210, Validation Loss: 0.8219, Validation F1-score: 0.7453
[I 2024-12-27 23:10:39,698] Trial 132 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.0001, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 18, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 29/100, Training Loss: 0.2192, Training F1-score: 0.9227, Validation Loss: 0.8139, Validation F1-score: 0.7392
Early stopping triggered after 29 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:133 ====================
Learning rate: 0.0001000000
Dropout: 0.32
Optimizer: Adam
Momentum term: 0
Step size: 17
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.7491, Training F1-score: 0.7310, Validation Loss: 0.7081, Validation F1-score: 0.7589
Best F1-score till now on the current trail on Validation data: 0.7588541666666667
Epoch 2/100, Training Loss: 0.5730, Training F1-score: 0.7842, Validation Loss: 0.7177, Validation F1-score: 0.7464
Epoch 3/100, Training Loss: 0.5230, Training F1-score: 0.8048, Validation Loss: 0.7190, Validation F1-score: 0.7498
Epoch 4/100, Training Loss: 0.4844, Training F1-score: 0.8208, Validation Loss: 0.6974, Validation F1-score: 0.7469
Epoch 5/100, Training Loss: 0.4581, Training F1-score: 0.8287, Validation Loss: 0.7017, Validation F1-score: 0.7609
Best F1-score till now on the current trail on Validation data: 0.7609374999999999
Epoch 6/100, Training Loss: 0.4338, Training F1-score: 0.8346, Validation Loss: 0.7362, Validation F1-score: 0.7401
Epoch 7/100, Training Loss: 0.4163, Training F1-score: 0.8436, Validation Loss: 0.7425, Validation F1-score: 0.7392
Epoch 8/100, Training Loss: 0.4054, Training F1-score: 0.8468, Validation Loss: 0.7300, Validation F1-score: 0.7436
Epoch 9/100, Training Loss: 0.3886, Training F1-score: 0.8535, Validation Loss: 0.7300, Validation F1-score: 0.7451
Epoch 10/100, Training Loss: 0.3761, Training F1-score: 0.8594, Validation Loss: 0.7465, Validation F1-score: 0.7373
Epoch 11/100, Training Loss: 0.3679, Training F1-score: 0.8612, Validation Loss: 0.7908, Validation F1-score: 0.7241
Epoch 12/100, Training Loss: 0.3603, Training F1-score: 0.8648, Validation Loss: 0.7153, Validation F1-score: 0.7495
Epoch 13/100, Training Loss: 0.3561, Training F1-score: 0.8667, Validation Loss: 0.7861, Validation F1-score: 0.7351
Epoch 14/100, Training Loss: 0.3472, Training F1-score: 0.8708, Validation Loss: 0.7523, Validation F1-score: 0.7424
Epoch 15/100, Training Loss: 0.3388, Training F1-score: 0.8719, Validation Loss: 0.8101, Validation F1-score: 0.7368
Epoch 16/100, Training Loss: 0.3356, Training F1-score: 0.8751, Validation Loss: 0.8066, Validation F1-score: 0.7312
Epoch 17/100, Training Loss: 0.3283, Training F1-score: 0.8763, Validation Loss: 0.7634, Validation F1-score: 0.7401
Epoch 18/100, Training Loss: 0.3111, Training F1-score: 0.8839, Validation Loss: 0.7739, Validation F1-score: 0.7392
Epoch 19/100, Training Loss: 0.3072, Training F1-score: 0.8875, Validation Loss: 0.7568, Validation F1-score: 0.7458
[I 2024-12-27 23:56:46,157] Trial 133 finished with value: 0.7892361111111111 and parameters: {'batch_size': 16, 'learning_rate': 0.0001, 'dropout': 0.32, 'optimizer': 'Adam', 'step_size': 17, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 20/100, Training Loss: 0.3004, Training F1-score: 0.8876, Validation Loss: 0.7901, Validation F1-score: 0.7342
Early stopping triggered after 20 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:134 ====================
Learning rate: 0.0050000000
Dropout: 0.32
Optimizer: Adam
Momentum term: 0
Step size: 19
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-27 23:59:58,572] Trial 134 pruned. 
Epoch 1/100, Training Loss: 0.6905, Training F1-score: 0.7428, Validation Loss: 0.6835, Validation F1-score: 0.7292
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:135 ====================
Learning rate: 0.0010000000
Dropout: 0.32
Optimizer: Adam
Momentum term: 0
Step size: 18
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 00:01:26,646] Trial 135 pruned. 
Epoch 1/100, Training Loss: 0.6470, Training F1-score: 0.7585, Validation Loss: 0.7390, Validation F1-score: 0.7153
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:136 ====================
Learning rate: 0.0000025000
Dropout: 0.32
Optimizer: Adam
Momentum term: 0
Step size: 17
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 00:04:37,645] Trial 136 pruned. 
Epoch 1/100, Training Loss: 1.4612, Training F1-score: 0.4569, Validation Loss: 1.2781, Validation F1-score: 0.5036
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:137 ====================
Learning rate: 0.0002500000
Dropout: 0.5
Optimizer: RMSprop
Momentum term: 0
Step size: 19
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 00:06:06,917] Trial 137 pruned. 
Epoch 1/100, Training Loss: 0.7465, Training F1-score: 0.7249, Validation Loss: 0.8265, Validation F1-score: 0.7175
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:138 ====================
Learning rate: 0.0001000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 17
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6416, Training F1-score: 0.7642, Validation Loss: 0.6518, Validation F1-score: 0.7576
Best F1-score till now on the current trail on Validation data: 0.7576388888888889
Epoch 2/100, Training Loss: 0.4789, Training F1-score: 0.8207, Validation Loss: 0.6153, Validation F1-score: 0.7686
Best F1-score till now on the current trail on Validation data: 0.7685763888888889
Epoch 3/100, Training Loss: 0.4225, Training F1-score: 0.8430, Validation Loss: 0.6713, Validation F1-score: 0.7557
Epoch 4/100, Training Loss: 0.3897, Training F1-score: 0.8560, Validation Loss: 0.7863, Validation F1-score: 0.7363
Epoch 5/100, Training Loss: 0.3634, Training F1-score: 0.8648, Validation Loss: 0.7446, Validation F1-score: 0.7424
Epoch 6/100, Training Loss: 0.3434, Training F1-score: 0.8716, Validation Loss: 0.7045, Validation F1-score: 0.7635
Epoch 7/100, Training Loss: 0.3314, Training F1-score: 0.8785, Validation Loss: 0.7230, Validation F1-score: 0.7401
Epoch 8/100, Training Loss: 0.3178, Training F1-score: 0.8813, Validation Loss: 0.6860, Validation F1-score: 0.7592
Epoch 9/100, Training Loss: 0.3107, Training F1-score: 0.8849, Validation Loss: 0.8113, Validation F1-score: 0.7259
Epoch 10/100, Training Loss: 0.2967, Training F1-score: 0.8914, Validation Loss: 0.7048, Validation F1-score: 0.7592
Epoch 11/100, Training Loss: 0.2867, Training F1-score: 0.8922, Validation Loss: 0.7503, Validation F1-score: 0.7538
Epoch 12/100, Training Loss: 0.2838, Training F1-score: 0.8969, Validation Loss: 0.7663, Validation F1-score: 0.7509
Epoch 13/100, Training Loss: 0.2712, Training F1-score: 0.9012, Validation Loss: 0.7200, Validation F1-score: 0.7573
Epoch 14/100, Training Loss: 0.2713, Training F1-score: 0.9019, Validation Loss: 0.7428, Validation F1-score: 0.7429
Epoch 15/100, Training Loss: 0.2596, Training F1-score: 0.9043, Validation Loss: 0.7817, Validation F1-score: 0.7477
Epoch 16/100, Training Loss: 0.2567, Training F1-score: 0.9063, Validation Loss: 0.7735, Validation F1-score: 0.7477
[I 2024-12-28 00:44:14,952] Trial 138 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 0.0001, 'dropout': 0.2, 'optimizer': 'Adam', 'step_size': 17, 'gamma': 0.4, 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 17/100, Training Loss: 0.2542, Training F1-score: 0.9078, Validation Loss: 0.8671, Validation F1-score: 0.7255
Early stopping triggered after 17 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:139 ====================
Learning rate: 0.0075000000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 16
Gamma: 0.3
Batch size: 16
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 00:47:21,388] Trial 139 pruned. 
Epoch 1/100, Training Loss: 0.8297, Training F1-score: 0.7437, Validation Loss: 0.8014, Validation F1-score: 0.6972
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:140 ====================
Learning rate: 0.0000100000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 10
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.6.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 00:48:47,755] Trial 140 pruned. 
Epoch 1/100, Training Loss: 1.1949, Training F1-score: 0.5968, Validation Loss: 0.9517, Validation F1-score: 0.6477
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:141 ====================
Learning rate: 0.0000500000
Dropout: 0.3
Optimizer: Adam
Momentum term: 0
Step size: 18
Gamma: 0.1
Batch size: 8
Number of frozen parameters: dino_model.blocks.5.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 00:51:59,927] Trial 141 pruned. 
Epoch 1/100, Training Loss: 0.7666, Training F1-score: 0.7276, Validation Loss: 0.7493, Validation F1-score: 0.7349
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:142 ====================
Learning rate: 0.0000750000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 18
Gamma: 0.3
Batch size: 8
Number of frozen parameters: dino_model.blocks.4.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6524, Training F1-score: 0.7630, Validation Loss: 0.6642, Validation F1-score: 0.7578
Best F1-score till now on the current trail on Validation data: 0.7578125
Epoch 2/100, Training Loss: 0.4996, Training F1-score: 0.8137, Validation Loss: 0.7690, Validation F1-score: 0.7384
Epoch 3/100, Training Loss: 0.4414, Training F1-score: 0.8355, Validation Loss: 0.6931, Validation F1-score: 0.7618
Best F1-score till now on the current trail on Validation data: 0.7618055555555555
Epoch 4/100, Training Loss: 0.4088, Training F1-score: 0.8492, Validation Loss: 0.7916, Validation F1-score: 0.7410
Epoch 5/100, Training Loss: 0.3791, Training F1-score: 0.8589, Validation Loss: 0.7279, Validation F1-score: 0.7524
Epoch 6/100, Training Loss: 0.3599, Training F1-score: 0.8669, Validation Loss: 0.8027, Validation F1-score: 0.7292
Epoch 7/100, Training Loss: 0.3431, Training F1-score: 0.8748, Validation Loss: 0.6952, Validation F1-score: 0.7608
Epoch 8/100, Training Loss: 0.3304, Training F1-score: 0.8797, Validation Loss: 0.7018, Validation F1-score: 0.7597
Epoch 9/100, Training Loss: 0.3232, Training F1-score: 0.8801, Validation Loss: 0.7140, Validation F1-score: 0.7634
Best F1-score till now on the current trail on Validation data: 0.7633680555555556
Epoch 10/100, Training Loss: 0.3120, Training F1-score: 0.8858, Validation Loss: 0.7233, Validation F1-score: 0.7547
Epoch 11/100, Training Loss: 0.2959, Training F1-score: 0.8904, Validation Loss: 0.8307, Validation F1-score: 0.7385
Epoch 12/100, Training Loss: 0.2934, Training F1-score: 0.8938, Validation Loss: 0.7094, Validation F1-score: 0.7627
Epoch 13/100, Training Loss: 0.2868, Training F1-score: 0.8938, Validation Loss: 0.7386, Validation F1-score: 0.7516
Epoch 14/100, Training Loss: 0.2818, Training F1-score: 0.8956, Validation Loss: 0.8289, Validation F1-score: 0.7359
Epoch 15/100, Training Loss: 0.2750, Training F1-score: 0.8983, Validation Loss: 0.8445, Validation F1-score: 0.7326
Epoch 16/100, Training Loss: 0.2685, Training F1-score: 0.9023, Validation Loss: 0.8343, Validation F1-score: 0.7427
Epoch 17/100, Training Loss: 0.2632, Training F1-score: 0.9049, Validation Loss: 0.7567, Validation F1-score: 0.7524
Epoch 18/100, Training Loss: 0.2622, Training F1-score: 0.9038, Validation Loss: 0.7061, Validation F1-score: 0.7583
Epoch 19/100, Training Loss: 0.2370, Training F1-score: 0.9164, Validation Loss: 0.7649, Validation F1-score: 0.7481
Epoch 20/100, Training Loss: 0.2334, Training F1-score: 0.9158, Validation Loss: 0.7739, Validation F1-score: 0.7354
Epoch 21/100, Training Loss: 0.2309, Training F1-score: 0.9160, Validation Loss: 0.7613, Validation F1-score: 0.7524
Epoch 22/100, Training Loss: 0.2289, Training F1-score: 0.9171, Validation Loss: 0.7871, Validation F1-score: 0.7474
Epoch 23/100, Training Loss: 0.2297, Training F1-score: 0.9187, Validation Loss: 0.7412, Validation F1-score: 0.7587
[I 2024-12-28 01:45:10,309] Trial 142 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 7.5e-05, 'dropout': 0.2, 'optimizer': 'RMSprop', 'step_size': 18, 'gamma': 0.3, 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 24/100, Training Loss: 0.2243, Training F1-score: 0.9191, Validation Loss: 0.7878, Validation F1-score: 0.7493
Early stopping triggered after 24 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:143 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 18
Gamma: 0.3
Batch size: 8
Number of frozen parameters: dino_model.blocks.4.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 01:48:13,992] Trial 143 pruned. 
Epoch 1/100, Training Loss: 0.5856, Training F1-score: 0.7808, Validation Loss: 0.9115, Validation F1-score: 0.7236
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:144 ====================
Learning rate: 0.0000750000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 17
Gamma: 0.3
Batch size: 8
Number of frozen parameters: dino_model.blocks.4.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6501, Training F1-score: 0.7616, Validation Loss: 0.6882, Validation F1-score: 0.7547
Best F1-score till now on the current trail on Validation data: 0.7546875
Epoch 2/100, Training Loss: 0.4976, Training F1-score: 0.8145, Validation Loss: 0.7752, Validation F1-score: 0.7352
Epoch 3/100, Training Loss: 0.4333, Training F1-score: 0.8370, Validation Loss: 0.6769, Validation F1-score: 0.7576
Best F1-score till now on the current trail on Validation data: 0.7576388888888889
Epoch 4/100, Training Loss: 0.4100, Training F1-score: 0.8461, Validation Loss: 0.6847, Validation F1-score: 0.7568
Epoch 5/100, Training Loss: 0.3769, Training F1-score: 0.8598, Validation Loss: 0.7625, Validation F1-score: 0.7363
Epoch 6/100, Training Loss: 0.3627, Training F1-score: 0.8659, Validation Loss: 0.7785, Validation F1-score: 0.7491
Epoch 7/100, Training Loss: 0.3463, Training F1-score: 0.8722, Validation Loss: 0.7522, Validation F1-score: 0.7479
Epoch 8/100, Training Loss: 0.3344, Training F1-score: 0.8780, Validation Loss: 0.8559, Validation F1-score: 0.7399
Epoch 9/100, Training Loss: 0.3191, Training F1-score: 0.8842, Validation Loss: 0.6960, Validation F1-score: 0.7583
Best F1-score till now on the current trail on Validation data: 0.7583333333333333
Epoch 10/100, Training Loss: 0.3090, Training F1-score: 0.8864, Validation Loss: 0.7676, Validation F1-score: 0.7342
Epoch 11/100, Training Loss: 0.3021, Training F1-score: 0.8904, Validation Loss: 0.6908, Validation F1-score: 0.7622
Best F1-score till now on the current trail on Validation data: 0.7621527777777778
Epoch 12/100, Training Loss: 0.2934, Training F1-score: 0.8917, Validation Loss: 0.9004, Validation F1-score: 0.7347
Epoch 13/100, Training Loss: 0.2850, Training F1-score: 0.8965, Validation Loss: 0.7906, Validation F1-score: 0.7344
Epoch 14/100, Training Loss: 0.2807, Training F1-score: 0.8992, Validation Loss: 0.7669, Validation F1-score: 0.7538
Epoch 15/100, Training Loss: 0.2770, Training F1-score: 0.8992, Validation Loss: 0.8062, Validation F1-score: 0.7347
Epoch 16/100, Training Loss: 0.2691, Training F1-score: 0.9034, Validation Loss: 0.7609, Validation F1-score: 0.7583
Epoch 17/100, Training Loss: 0.2658, Training F1-score: 0.9032, Validation Loss: 0.7655, Validation F1-score: 0.7573
Epoch 18/100, Training Loss: 0.2393, Training F1-score: 0.9135, Validation Loss: 0.7384, Validation F1-score: 0.7547
Epoch 19/100, Training Loss: 0.2394, Training F1-score: 0.9138, Validation Loss: 0.7843, Validation F1-score: 0.7488
Epoch 20/100, Training Loss: 0.2332, Training F1-score: 0.9164, Validation Loss: 0.7837, Validation F1-score: 0.7530
Epoch 21/100, Training Loss: 0.2328, Training F1-score: 0.9160, Validation Loss: 0.7179, Validation F1-score: 0.7594
Epoch 22/100, Training Loss: 0.2316, Training F1-score: 0.9160, Validation Loss: 0.7497, Validation F1-score: 0.7575
Epoch 23/100, Training Loss: 0.2282, Training F1-score: 0.9170, Validation Loss: 0.7278, Validation F1-score: 0.7538
Epoch 24/100, Training Loss: 0.2274, Training F1-score: 0.9182, Validation Loss: 0.7743, Validation F1-score: 0.7512
Epoch 25/100, Training Loss: 0.2262, Training F1-score: 0.9168, Validation Loss: 0.8505, Validation F1-score: 0.7349
[I 2024-12-28 02:44:19,572] Trial 144 finished with value: 0.7892361111111111 and parameters: {'batch_size': 8, 'learning_rate': 7.5e-05, 'dropout': 0.2, 'optimizer': 'RMSprop', 'step_size': 17, 'gamma': 0.3, 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma'}. Best is trial 27 with value: 0.7892361111111111.
Epoch 26/100, Training Loss: 0.2261, Training F1-score: 0.9169, Validation Loss: 0.7611, Validation F1-score: 0.7540
Early stopping triggered after 26 epochs.
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:145 ====================
Learning rate: 0.0010000000
Dropout: 0.25
Optimizer: RMSprop
Momentum term: 0
Step size: 19
Gamma: 0.3
Batch size: 8
Number of frozen parameters: dino_model.blocks.4.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 02:46:01,494] Trial 145 pruned. 
Epoch 1/100, Training Loss: 0.6331, Training F1-score: 0.7628, Validation Loss: 0.8530, Validation F1-score: 0.6859
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:146 ====================
Learning rate: 0.0000050000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 15
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 02:48:38,475] Trial 146 pruned. 
Epoch 1/100, Training Loss: 1.1932, Training F1-score: 0.5931, Validation Loss: 1.0106, Validation F1-score: 0.6448
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:147 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 17
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 02:50:45,909] Trial 147 pruned. 
Epoch 1/100, Training Loss: 0.5906, Training F1-score: 0.7799, Validation Loss: 0.7581, Validation F1-score: 0.7264
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:148 ====================
Learning rate: 0.0000750000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 18
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.1.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 02:52:15,375] Trial 148 pruned. 
Epoch 1/100, Training Loss: 0.6805, Training F1-score: 0.7551, Validation Loss: 0.7445, Validation F1-score: 0.7300
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:149 ====================
Learning rate: 0.0000005000
Dropout: 0.28
Optimizer: SGD
Momentum term: 0.8622637003807782
Step size: 19
Gamma: 0.3
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 02:54:50,715] Trial 149 pruned. 
Epoch 1/100, Training Loss: 2.0529, Training F1-score: 0.1995, Validation Loss: 2.0285, Validation F1-score: 0.3267
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:150 ====================
Learning rate: 0.0025000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.4.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 02:57:31,689] Trial 150 pruned. 
Epoch 1/100, Training Loss: 0.6367, Training F1-score: 0.7610, Validation Loss: 0.7213, Validation F1-score: 0.7481
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:151 ====================
Learning rate: 0.0002500000
Dropout: 0.2
Optimizer: RMSprop
Momentum term: 0
Step size: 14
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 03:00:09,325] Trial 151 pruned. 
Epoch 1/100, Training Loss: 0.5949, Training F1-score: 0.7783, Validation Loss: 0.8009, Validation F1-score: 0.7075
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:152 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 20
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 03:02:50,510] Trial 152 pruned. 
Epoch 1/100, Training Loss: 0.6045, Training F1-score: 0.7750, Validation Loss: 0.6875, Validation F1-score: 0.7290
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:153 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 20
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 03:05:25,723] Trial 153 pruned. 
Epoch 1/100, Training Loss: 0.6062, Training F1-score: 0.7716, Validation Loss: 0.7515, Validation F1-score: 0.7224
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:154 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 20
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.0.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 03:08:06,169] Trial 154 pruned. 
Epoch 1/100, Training Loss: 0.6065, Training F1-score: 0.7717, Validation Loss: 0.6880, Validation F1-score: 0.7377
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:155 ====================
Learning rate: 0.0000002500
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 19
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 03:09:36,281] Trial 155 pruned. 
Epoch 1/100, Training Loss: 1.9752, Training F1-score: 0.2725, Validation Loss: 1.8043, Validation F1-score: 0.4479
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:156 ====================
Learning rate: 0.0010000000
Dropout: 0.2
Optimizer: Adam
Momentum term: 0
Step size: 18
Gamma: 0.4
Batch size: 8
Number of frozen parameters: dino_model.blocks.2.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-28 03:12:09,383] Trial 156 pruned. 
Epoch 1/100, Training Loss: 0.6041, Training F1-score: 0.7730, Validation Loss: 0.7471, Validation F1-score: 0.7201
Best F1-score till now on Validation data: 0.7892361111111111
==================== Training of trial number:157 ====================
Learning rate: 0.0002500000
Dropout: 0.32
Optimizer: Adam
Momentum term: 0
Step size: 19
Gamma: 0.4
Batch size: 16
Number of frozen parameters: dino_model.blocks.3.ls2.gamma
Best F1-score on Validation data until now: 0.7892361111111111
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6765, Training F1-score: 0.7494, Validation Loss: 0.6588, Validation F1-score: 0.7516
Best F1-score till now on the current trail on Validation data: 0.7515625
Epoch 2/100, Training Loss: 0.5226, Training F1-score: 0.8033, Validation Loss: 0.7849, Validation F1-score: 0.7198
Epoch 3/100, Training Loss: 0.4673, Training F1-score: 0.8221, Validation Loss: 0.7971, Validation F1-score: 0.7260
Epoch 4/100, Training Loss: 0.4340, Training F1-score: 0.8365, Validation Loss: 0.7509, Validation F1-score: 0.7339
Epoch 5/100, Training Loss: 0.4010, Training F1-score: 0.8477, Validation Loss: 0.8174, Validation F1-score: 0.7286
Epoch 6/100, Training Loss: 0.3853, Training F1-score: 0.8580, Validation Loss: 0.7369, Validation F1-score: 0.7451
Epoch 7/100, Training Loss: 0.3743, Training F1-score: 0.8608, Validation Loss: 0.7796, Validation F1-score: 0.7312
Epoch 8/100, Training Loss: 0.3593, Training F1-score: 0.8658, Validation Loss: 0.8040, Validation F1-score: 0.7229
Epoch 9/100, Training Loss: 0.3430, Training F1-score: 0.8718, Validation Loss: 0.7743, Validation F1-score: 0.7201
Epoch 10/100, Training Loss: 0.3347, Training F1-score: 0.8769, Validation Loss: 0.7670, Validation F1-score: 0.7384
Epoch 11/100, Training Loss: 0.3295, Training F1-score: 0.8791, Validation Loss: 0.7424, Validation F1-score: 0.7514
Epoch 12/100, Training Loss: 0.3173, Training F1-score: 0.8824, Validation Loss: 0.8140, Validation F1-score: 0.7375
Epoch 13/100, Training Loss: 0.3122, Training F1-score: 0.8855, Validation Loss: 0.8473, Validation F1-score: 0.7189
Epoch 14/100, Training Loss: 0.3097, Training F1-score: 0.8854, Validation Loss: 0.7463, Validation F1-score: 0.7476
Epoch 15/100, Training Loss: 0.3031, Training F1-score: 0.8898, Validation Loss: 0.7312, Validation F1-score: 0.7582
Best F1-score till now on the current trail on Validation data: 0.7581597222222223
Epoch 16/100, Training Loss: 0.2954, Training F1-score: 0.8912, Validation Loss: 0.7440, Validation F1-score: 0.7486
Epoch 17/100, Training Loss: 0.2915, Training F1-score: 0.8938, Validation Loss: 0.8577, Validation F1-score: 0.7354
slurmstepd: error: *** JOB 962608 ON tg071 CANCELLED AT 2024-12-28T03:52:42 DUE TO TIME LIMIT ***
=== JOB_STATISTICS ===
=== current date     : Sat 28 Dec 2024 03:52:44 AM CET
= Job-ID             : 962608 on tinygpu
= Job-Name           : MinicondaName
= Job-Command        : /home/woody/iwfa/iwfa044h/runner4.sh
= Initial workdir    : /home/woody/iwfa/iwfa044h
= Queue/Partition    : v100
= Slurm account      : iwfa with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 1-00:00:20
= Total RAM usage    : 1.5 GiB of requested  GiB (%)   
= Node list          : tg071
= Subm/Elig/Start/End: 2024-12-27T02:35:31 / 2024-12-27T02:35:31 / 2024-12-27T03:52:22 / 2024-12-28T03:52:42
======================
=== Quota infos ======
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
Tesla V100-PCIE-32GB, 00000000:AF:00.0, 458963, 49 %, 15 %, 642 MiB, 86341711 ms
