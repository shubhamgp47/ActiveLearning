(26880, 5)
(5760, 5)
(5760, 5)
3
multiclass_train_df shape: (26880, 2)
multiclass_test_df shape: (5760, 2)
multiclass_val_df shape: (5760, 2)
Calculated train_size: 8
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.2500[0m        [32m2.0327[0m       [35m0.2233[0m      [31m0.2233[0m        [94m1.9289[0m     +  0.0000  11.2845
      2      0.2500        [32m1.9712[0m       0.1854      0.1854        1.9898        0.0000  10.5026
      3      [36m0.5000[0m        [32m1.7284[0m       [35m0.3267[0m      [31m0.3267[0m        [94m1.8632[0m     +  0.0000  10.6153
      4      [36m0.6250[0m        [32m1.6674[0m       0.2106      0.2106        1.9695        0.0000  10.3572
      5      0.6250        [32m1.5344[0m       [35m0.3321[0m      [31m0.3321[0m        1.8673        0.0000  10.6106
      6      [36m0.8750[0m        [32m1.3737[0m       [35m0.3359[0m      [31m0.3359[0m        1.8914        0.0000  10.7174
      7      0.7500        1.4372       0.3214      0.3214        1.9118        0.0000  10.8920
      8      0.8750        [32m1.3501[0m       0.3087      0.3087        1.9198        0.0000  10.9048
      9      [36m1.0000[0m        1.3720       0.3149      0.3149        1.9225        0.0000  10.9149
     10      0.8750        [32m1.3006[0m       0.3182      0.3182        1.9235        0.0000  10.5176
     11      0.8750        1.3092       0.3167      0.3167        1.9235        0.0000  10.7002
     12      1.0000        1.3720       0.3158      0.3158        1.9257        0.0000  10.8686
     13      0.7500        1.4509       0.3153      0.3153        1.9249        0.0000  10.5958
Stopping since valid_loss has not improved in the last 11 epochs.
Pre F1 micro score = 0.2668
Pre F1 macro score = 0.1538

Iteration:  1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.3333[0m        [32m1.7302[0m       [35m0.4519[0m      [31m0.4519[0m        [94m1.6527[0m     +  0.0000  10.6896
      2      0.3333        [32m1.6025[0m       [35m0.5210[0m      [31m0.5210[0m        [94m1.5947[0m     +  0.0000  10.7812
      3      [36m0.7083[0m        [32m1.4323[0m       0.5059      0.5059        1.6098        0.0000  10.7171
      4      [36m0.7500[0m        [32m1.3039[0m       0.5057      0.5057        [94m1.5654[0m     +  0.0000  10.9513
      5      [36m0.8333[0m        [32m1.2777[0m       [35m0.5724[0m      [31m0.5724[0m        [94m1.5382[0m     +  0.0000  10.7236
      6      [36m0.8750[0m        [32m1.2379[0m       [35m0.5769[0m      [31m0.5769[0m        [94m1.5356[0m     +  0.0000  10.8172
      7      [36m1.0000[0m        [32m1.0979[0m       [35m0.5887[0m      [31m0.5887[0m        [94m1.5339[0m     +  0.0000  10.8010
      8      0.9167        [32m1.0829[0m       0.5878      0.5878        1.5345        0.0000  10.9038
      9      0.9167        1.1515       [35m0.5960[0m      [31m0.5960[0m        [94m1.5285[0m     +  0.0000  10.5929
     10      0.8750        1.0886       0.5948      0.5948        1.5305        0.0000  10.8161
     11      0.9167        [32m1.0499[0m       0.5946      0.5946        1.5308        0.0000  10.9619
     12      0.8750        1.0838       0.5948      0.5948        1.5286        0.0000  10.8732
     13      0.9167        [32m1.0150[0m       0.5950      0.5950        [94m1.5279[0m     +  0.0000  10.9236
     14      0.9167        1.1356       0.5938      0.5938        1.5282        0.0000  10.8554
     15      0.9167        1.1099       0.5920      0.5920        1.5289        0.0000  10.7217
     16      1.0000        1.0808       0.5911      0.5911        1.5289        0.0000  10.7311
     17      0.9167        1.0853       0.5917      0.5917        1.5286        0.0000  10.7788
     18      0.9583        1.0715       0.5918      0.5918        1.5284        0.0000  10.8629
     19      0.9583        1.1019       0.5927      0.5927        1.5282        0.0000  10.6461
     20      0.9167        1.0604       0.5920      0.5920        1.5283        0.0000  11.0916
     21      0.9167        1.0617       0.5920      0.5920        1.5283        0.0000  11.0202
     22      1.0000        [32m1.0098[0m       0.5920      0.5920        1.5283        0.0000  10.7554
     23      0.9583        [32m1.0027[0m       0.5922      0.5922        1.5282        0.0000  10.7814
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24]
F1 Micro Score after query 1: 0.6131944444444445
F1 Macro Score after query 1: 0.24395162940823784
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_0.pt

Iteration:  2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.7321[0m        [32m1.3620[0m       [35m0.6191[0m      [31m0.6191[0m        [94m1.4655[0m     +  0.0000  10.9985
      2      [36m0.7500[0m        [32m1.1733[0m       [35m0.6316[0m      [31m0.6316[0m        [94m1.3944[0m     +  0.0000  10.9387
      3      [36m0.7857[0m        [32m1.0499[0m       [35m0.6413[0m      [31m0.6413[0m        [94m1.3649[0m     +  0.0000  11.0814
      4      [36m0.8750[0m        [32m0.9197[0m       [35m0.6438[0m      [31m0.6438[0m        [94m1.3168[0m     +  0.0000  11.0217
      5      [36m0.9107[0m        [32m0.8493[0m       [35m0.6446[0m      [31m0.6446[0m        [94m1.2861[0m     +  0.0000  11.2195
      6      0.9107        [32m0.8186[0m       [35m0.6599[0m      [31m0.6599[0m        [94m1.2742[0m     +  0.0000  10.7729
      7      0.9107        [32m0.7972[0m       [35m0.6634[0m      [31m0.6634[0m        [94m1.2694[0m     +  0.0000  10.9025
      8      [36m0.9286[0m        0.8194       [35m0.6635[0m      [31m0.6635[0m        [94m1.2671[0m     +  0.0000  10.9689
      9      [36m0.9464[0m        [32m0.7747[0m       [35m0.6653[0m      [31m0.6653[0m        [94m1.2624[0m     +  0.0000  10.9514
     10      [36m0.9643[0m        [32m0.7423[0m       [35m0.6667[0m      [31m0.6667[0m        1.2635        0.0000  11.0308
     11      0.9286        0.7580       0.6667      0.6667        1.2632        0.0000  10.8852
     12      0.9643        [32m0.7387[0m       0.6665      0.6665        [94m1.2618[0m     +  0.0000  11.0156
     13      0.9643        0.7446       0.6661      0.6661        [94m1.2609[0m     +  0.0000  10.9851
     14      0.9643        0.7510       0.6661      0.6661        1.2609        0.0000  10.7520
     15      0.9464        0.7549       0.6663      0.6663        [94m1.2608[0m     +  0.0000  10.8907
     16      0.9464        [32m0.7251[0m       0.6663      0.6663        [94m1.2607[0m     +  0.0000  10.9216
     17      0.9643        0.7270       0.6661      0.6661        1.2607        0.0000  10.8593
     18      0.9464        [32m0.7062[0m       0.6660      0.6660        [94m1.2605[0m     +  0.0000  10.9427
     19      [36m0.9821[0m        [32m0.6958[0m       0.6661      0.6661        [94m1.2605[0m     +  0.0000  10.8312
     20      0.9464        0.7207       0.6661      0.6661        [94m1.2605[0m     +  0.0000  11.0132
     21      0.9643        0.7165       0.6661      0.6661        [94m1.2604[0m     +  0.0000  11.1431
     22      0.9286        0.7580       0.6661      0.6661        [94m1.2604[0m     +  0.0000  11.2311
     23      0.9464        0.7583       0.6661      0.6661        [94m1.2604[0m     +  0.0000  11.1364
     24      0.9107        0.7902       0.6661      0.6661        [94m1.2603[0m     +  0.0000  11.0402
     25      0.9464        0.7239       0.6661      0.6661        [94m1.2603[0m     +  0.0000  11.0057
     26      0.9643        0.7532       0.6661      0.6661        [94m1.2603[0m     +  0.0000  10.9371
     27      0.9821        0.7216       0.6661      0.6661        [94m1.2603[0m     +  0.0000  11.1343
     28      0.9286        0.7707       0.6661      0.6661        [94m1.2603[0m     +  0.0000  11.0616
     29      0.9286        0.7494       0.6661      0.6661        [94m1.2603[0m     +  0.0000  10.7690
     30      0.9286        0.7488       0.6661      0.6661        [94m1.2603[0m     +  0.0000  11.0287
     31      0.9286        0.7793       0.6661      0.6661        [94m1.2603[0m     +  0.0000  11.0969
     32      0.9286        0.7597       0.6661      0.6661        [94m1.2603[0m     +  0.0000  10.9287
     33      0.9107        0.7529       0.6661      0.6661        1.2603        0.0000  11.3253
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24, 56]
F1 Micro Score after query 2: 0.6947916666666667
F1 Macro Score after query 2: 0.23101802800707172
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_1.pt

Iteration:  3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.6696[0m        [32m1.2709[0m       [35m0.6253[0m      [31m0.6253[0m        [94m1.2935[0m     +  0.0000  11.1881
      2      [36m0.6964[0m        [32m1.1256[0m       [35m0.7108[0m      [31m0.7108[0m        [94m1.1745[0m     +  0.0000  11.2835
      3      [36m0.7679[0m        [32m1.0114[0m       [35m0.7236[0m      [31m0.7236[0m        [94m1.1252[0m     +  0.0000  11.3596
      4      [36m0.8571[0m        [32m0.9284[0m       [35m0.7337[0m      [31m0.7337[0m        [94m1.1032[0m     +  0.0000  11.1015
      5      [36m0.9018[0m        [32m0.8131[0m       0.7328      0.7328        [94m1.0887[0m     +  0.0000  11.1597
      6      [36m0.9107[0m        [32m0.7811[0m       0.7238      0.7238        [94m1.0764[0m     +  0.0000  11.1560
      7      [36m0.9286[0m        [32m0.7750[0m       0.7201      0.7201        [94m1.0757[0m     +  0.0000  11.2182
      8      [36m0.9375[0m        [32m0.7343[0m       0.7179      0.7179        [94m1.0743[0m     +  0.0000  11.3242
      9      0.9375        0.7677       0.7163      0.7163        [94m1.0733[0m     +  0.0000  11.2368
     10      0.9286        0.7410       0.7160      0.7160        [94m1.0724[0m     +  0.0000  11.2507
     11      0.9375        0.7345       0.7146      0.7146        1.0724        0.0000  11.1417
     12      0.9107        [32m0.7001[0m       0.7141      0.7141        1.0724        0.0000  11.3298
     13      [36m0.9554[0m        0.7042       0.7142      0.7142        [94m1.0715[0m     +  0.0000  11.1041
     14      0.9375        0.7188       0.7142      0.7142        [94m1.0708[0m     +  0.0000  11.2791
     15      0.9286        0.7344       0.7135      0.7135        [94m1.0707[0m     +  0.0000  11.6717
     16      0.9196        0.7131       0.7135      0.7135        [94m1.0707[0m     +  0.0000  11.3292
     17      0.9554        0.7060       0.7139      0.7139        [94m1.0705[0m     +  0.0000  10.9109
     18      0.9286        0.7111       0.7137      0.7137        [94m1.0703[0m     +  0.0000  11.3709
     19      0.9375        0.7312       0.7135      0.7135        1.0704        0.0000  11.2616
     20      0.9196        [32m0.6994[0m       0.7128      0.7128        1.0706        0.0000  11.3734
     21      0.9375        0.7092       0.7127      0.7127        1.0706        0.0000  11.4525
     22      0.9554        [32m0.6982[0m       0.7125      0.7125        1.0705        0.0000  11.2229
     23      0.9554        0.7145       0.7125      0.7125        1.0705        0.0000  11.1548
     24      0.9554        0.7297       0.7125      0.7125        1.0705        0.0000  11.0608
     25      0.9375        0.7059       0.7123      0.7123        1.0705        0.0000  11.2162
     26      0.9554        0.7021       0.7123      0.7123        1.0705        0.0000  11.4524
     27      0.9464        [32m0.6943[0m       0.7125      0.7125        1.0705        0.0000  11.3021
     28      0.9375        0.7272       0.7125      0.7125        1.0705        0.0000  11.2447
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24, 56, 112]
F1 Micro Score after query 3: 0.7324652777777778
F1 Macro Score after query 3: 0.28266641296598966
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_2.pt

Iteration:  4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.6875[0m        [32m1.1111[0m       [35m0.5844[0m      [31m0.5844[0m        [94m1.1940[0m     +  0.0000  11.8519
      2      [36m0.8606[0m        [32m0.8722[0m       [35m0.6226[0m      [31m0.6226[0m        [94m1.1417[0m     +  0.0000  11.7587
      3      [36m0.8894[0m        [32m0.7826[0m       [35m0.6408[0m      [31m0.6408[0m        [94m1.1051[0m     +  0.0000  11.8221
      4      [36m0.9183[0m        [32m0.7020[0m       [35m0.6780[0m      [31m0.6780[0m        [94m1.0570[0m     +  0.0000  11.7767
      5      [36m0.9615[0m        [32m0.6269[0m       [35m0.7030[0m      [31m0.7030[0m        [94m1.0304[0m     +  0.0000  11.8226
      6      [36m0.9712[0m        [32m0.5870[0m       [35m0.7191[0m      [31m0.7191[0m        [94m1.0001[0m     +  0.0000  11.9643
      7      0.9615        0.5964       [35m0.7198[0m      [31m0.7198[0m        [94m0.9996[0m     +  0.0000  11.6800
      8      0.9712        [32m0.5605[0m       0.7189      0.7189        [94m0.9962[0m     +  0.0000  11.7756
      9      0.9712        [32m0.5463[0m       0.7189      0.7189        [94m0.9937[0m     +  0.0000  11.8861
     10      [36m0.9808[0m        [32m0.5189[0m       0.7194      0.7194        [94m0.9915[0m     +  0.0000  11.7428
     11      0.9712        0.5415       [35m0.7220[0m      [31m0.7220[0m        [94m0.9865[0m     +  0.0000  11.5110
     12      0.9712        0.5316       [35m0.7224[0m      [31m0.7224[0m        [94m0.9844[0m     +  0.0000  11.6347
     13      0.9712        0.5336       0.7220      0.7220        [94m0.9839[0m     +  0.0000  11.9961
     14      0.9760        [32m0.5095[0m       [35m0.7226[0m      [31m0.7226[0m        [94m0.9835[0m     +  0.0000  11.4442
     15      0.9760        0.5369       0.7220      0.7220        [94m0.9833[0m     +  0.0000  11.6845
     16      0.9712        0.5382       0.7214      0.7214        [94m0.9828[0m     +  0.0000  11.8523
     17      0.9760        0.5283       0.7217      0.7217        [94m0.9822[0m     +  0.0000  11.8060
     18      0.9663        0.5324       [35m0.7227[0m      [31m0.7227[0m        [94m0.9817[0m     +  0.0000  11.4169
     19      0.9760        0.5450       [35m0.7231[0m      [31m0.7231[0m        [94m0.9813[0m     +  0.0000  11.9156
     20      0.9760        [32m0.5064[0m       [35m0.7233[0m      [31m0.7233[0m        [94m0.9808[0m     +  0.0000  11.4007
     21      0.9663        0.5294       [35m0.7236[0m      [31m0.7236[0m        [94m0.9807[0m     +  0.0000  11.2585
     22      [36m0.9856[0m        0.5083       0.7236      0.7236        [94m0.9807[0m     +  0.0000  11.8526
     23      0.9712        0.5215       [35m0.7238[0m      [31m0.7238[0m        0.9807        0.0000  11.2273
     24      0.9712        0.5229       0.7238      0.7238        [94m0.9806[0m     +  0.0000  11.5242
     25      0.9760        0.5111       0.7238      0.7238        [94m0.9805[0m     +  0.0000  11.7432
     26      0.9760        0.5192       0.7238      0.7238        0.9805        0.0000  11.3531
     27      0.9760        0.5403       0.7238      0.7238        [94m0.9805[0m     +  0.0000  11.6190
     28      0.9760        0.5138       0.7238      0.7238        [94m0.9805[0m     +  0.0000  11.4162
     29      0.9760        0.5331       0.7238      0.7238        0.9806        0.0000  11.8364
     30      0.9808        0.5271       0.7238      0.7238        0.9805        0.0000  11.7610
     31      0.9760        0.5185       0.7238      0.7238        0.9805        0.0000  11.8213
     32      0.9808        0.5222       0.7238      0.7238        0.9805        0.0000  12.0684
     33      0.9808        0.5164       0.7238      0.7238        0.9805        0.0000  11.6154
     34      0.9808        0.5190       0.7238      0.7238        0.9805        0.0000  11.3902
     35      0.9808        0.5132       0.7238      0.7238        0.9805        0.0000  11.6990
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24, 56, 112, 208]
F1 Micro Score after query 4: 0.7328124999999999
F1 Macro Score after query 4: 0.30206165888531694
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_3.pt

Iteration:  5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.7474[0m        [32m0.9872[0m       [35m0.7293[0m      [31m0.7293[0m        [94m0.9810[0m     +  0.0000  12.6187
      2      [36m0.8880[0m        [32m0.6944[0m       0.7120      0.7120        [94m0.9694[0m     +  0.0000  12.2589
      3      [36m0.9349[0m        [32m0.5866[0m       0.6842      0.6842        1.0306        0.0000  12.3541
      4      [36m0.9401[0m        [32m0.5434[0m       0.6958      0.6958        0.9949        0.0000  12.1356
      5      [36m0.9688[0m        [32m0.4976[0m       0.7189      0.7189        [94m0.9239[0m     +  0.0000  12.3371
      6      0.9661        [32m0.4774[0m       [35m0.7684[0m      [31m0.7684[0m        [94m0.8793[0m     +  0.0000  12.5417
      7      0.9688        [32m0.4493[0m       [35m0.7694[0m      [31m0.7694[0m        [94m0.8724[0m     +  0.0000  12.4785
      8      [36m0.9714[0m        [32m0.4439[0m       [35m0.7700[0m      [31m0.7700[0m        [94m0.8583[0m     +  0.0000  12.4471
      9      [36m0.9740[0m        [32m0.4171[0m       [35m0.7712[0m      [31m0.7712[0m        [94m0.8538[0m     +  0.0000  12.6502
     10      [36m0.9766[0m        0.4346       [35m0.7736[0m      [31m0.7736[0m        [94m0.8440[0m     +  0.0000  12.1348
     11      [36m0.9792[0m        0.4204       0.7717      0.7717        [94m0.8400[0m     +  0.0000  12.6999
     12      0.9714        [32m0.4066[0m       0.7705      0.7705        [94m0.8381[0m     +  0.0000  12.2745
     13      0.9766        [32m0.4053[0m       0.7714      0.7714        [94m0.8356[0m     +  0.0000  12.1014
     14      0.9792        [32m0.3992[0m       0.7712      0.7712        [94m0.8355[0m     +  0.0000  12.5899
     15      0.9792        0.4058       0.7715      0.7715        [94m0.8328[0m     +  0.0000  12.3396
     16      [36m0.9818[0m        0.4112       0.7710      0.7710        [94m0.8320[0m     +  0.0000  12.7299
     17      0.9792        [32m0.3964[0m       0.7717      0.7717        [94m0.8320[0m     +  0.0000  12.9647
     18      0.9818        0.4016       0.7714      0.7714        [94m0.8319[0m     +  0.0000  12.3847
     19      0.9818        [32m0.3932[0m       0.7703      0.7703        0.8322        0.0000  12.5721
     20      0.9818        0.4032       0.7703      0.7703        [94m0.8316[0m     +  0.0000  12.3226
     21      0.9766        [32m0.3923[0m       0.7700      0.7700        0.8318        0.0000  12.6989
     22      [36m0.9844[0m        0.4043       0.7700      0.7700        0.8318        0.0000  12.3695
     23      [36m0.9870[0m        0.4031       0.7698      0.7698        0.8318        0.0000  12.1033
     24      0.9818        0.3937       0.7694      0.7694        0.8317        0.0000  12.4143
     25      0.9844        0.3941       0.7694      0.7694        [94m0.8316[0m     +  0.0000  12.5565
     26      0.9818        0.4048       0.7693      0.7693        0.8316        0.0000  12.5252
     27      0.9818        0.4042       0.7693      0.7693        0.8316        0.0000  12.3468
     28      0.9792        0.4036       0.7691      0.7691        0.8316        0.0000  12.5267
     29      0.9792        0.4006       0.7691      0.7691        0.8316        0.0000  12.7149
     30      0.9792        0.4106       0.7691      0.7691        0.8316        0.0000  12.1494
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.7800347222222223
F1 Macro Score after query 5: 0.3696132139560888
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_4.pt

Iteration:  6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8011[0m        [32m0.7992[0m       [35m0.8247[0m      [31m0.8247[0m        [94m0.7286[0m     +  0.0000  13.9159
      2      [36m0.9048[0m        [32m0.5695[0m       [35m0.8276[0m      [31m0.8276[0m        [94m0.6875[0m     +  0.0000  14.0272
      3      [36m0.9332[0m        [32m0.5057[0m       0.8248      0.8248        [94m0.6875[0m     +  0.0000  14.1499
      4      [36m0.9503[0m        [32m0.4420[0m       [35m0.8286[0m      [31m0.8286[0m        [94m0.6603[0m     +  0.0000  14.0431
      5      0.9460        [32m0.4097[0m       [35m0.8373[0m      [31m0.8373[0m        [94m0.6465[0m     +  0.0000  13.9313
      6      [36m0.9673[0m        [32m0.3683[0m       0.8149      0.8149        0.6570        0.0000  13.9639
      7      [36m0.9744[0m        [32m0.3471[0m       0.8146      0.8146        0.6556        0.0000  13.8084
      8      0.9716        0.3486       0.8130      0.8130        0.6529        0.0000  14.1660
      9      0.9744        [32m0.3282[0m       0.8153      0.8153        0.6479        0.0000  13.8702
     10      0.9730        0.3330       0.8161      0.8161        [94m0.6455[0m     +  0.0000  14.1505
     11      [36m0.9801[0m        [32m0.3120[0m       0.8054      0.8054        0.6663        0.0000  13.8223
     12      0.9787        0.3171       0.8042      0.8042        0.6676        0.0000  13.7770
     13      0.9801        0.3204       0.8040      0.8040        0.6695        0.0000  14.0071
     14      0.9787        [32m0.3028[0m       0.8054      0.8054        0.6673        0.0000  14.2267
     15      [36m0.9844[0m        0.3109       0.8050      0.8050        0.6677        0.0000  14.1356
     16      0.9844        0.3144       0.8066      0.8066        0.6651        0.0000  14.0107
     17      [36m0.9858[0m        [32m0.2978[0m       0.8063      0.8062        0.6641        0.0000  13.9156
     18      0.9815        0.3033       0.8061      0.8061        0.6623        0.0000  14.1983
     19      0.9858        0.3158       0.8056      0.8056        0.6633        0.0000  14.0586
     20      0.9858        0.3050       0.8052      0.8052        0.6637        0.0000  14.3839
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.8482638888888889
F1 Macro Score after query 6: 0.43568604045024634
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_5.pt

Iteration:  7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8267[0m        [32m0.6821[0m       [35m0.7720[0m      [31m0.7720[0m        [94m0.7809[0m     +  0.0000  16.6209
      2      [36m0.9011[0m        [32m0.5231[0m       [35m0.7970[0m      [31m0.7970[0m        [94m0.7159[0m     +  0.0000  16.4338
      3      [36m0.9280[0m        [32m0.4545[0m       0.7816      0.7816        0.7487        0.0000  16.9968
      4      [36m0.9375[0m        [32m0.4031[0m       [35m0.8085[0m      [31m0.8085[0m        [94m0.7005[0m     +  0.0000  17.2492
      5      [36m0.9422[0m        [32m0.3860[0m       0.8007      0.8007        [94m0.6882[0m     +  0.0000  16.8093
      6      [36m0.9517[0m        [32m0.3368[0m       [35m0.8210[0m      [31m0.8210[0m        [94m0.6147[0m     +  0.0000  16.4805
      7      [36m0.9668[0m        [32m0.3116[0m       [35m0.8222[0m      [31m0.8222[0m        [94m0.6113[0m     +  0.0000  16.9189
      8      0.9668        [32m0.3042[0m       0.8191      0.8191        0.6131        0.0000  17.0312
      9      [36m0.9684[0m        [32m0.2993[0m       0.8200      0.8200        [94m0.6108[0m     +  0.0000  16.9314
     10      [36m0.9715[0m        [32m0.2917[0m       [35m0.8231[0m      [31m0.8231[0m        [94m0.6030[0m     +  0.0000  16.5752
     11      [36m0.9723[0m        [32m0.2818[0m       0.8231      0.8231        [94m0.5965[0m     +  0.0000  16.3255
     12      [36m0.9739[0m        [32m0.2817[0m       0.8227      0.8227        [94m0.5965[0m     +  0.0000  17.0616
     13      0.9715        [32m0.2810[0m       [35m0.8234[0m      [31m0.8234[0m        [94m0.5930[0m     +  0.0000  16.8883
     14      [36m0.9763[0m        [32m0.2710[0m       [35m0.8247[0m      [31m0.8247[0m        [94m0.5918[0m     +  0.0000  16.7318
     15      [36m0.9771[0m        [32m0.2704[0m       0.8240      0.8240        0.5935        0.0000  16.4036
     16      0.9771        [32m0.2633[0m       0.8240      0.8240        0.5940        0.0000  17.0252
     17      0.9763        0.2703       0.8231      0.8231        0.5936        0.0000  17.3363
     18      [36m0.9786[0m        0.2640       0.8233      0.8233        0.5941        0.0000  16.5330
     19      0.9778        0.2709       0.8236      0.8236        0.5931        0.0000  16.9282
     20      [36m0.9794[0m        [32m0.2624[0m       0.8226      0.8226        0.5933        0.0000  16.9299
     21      0.9794        0.2657       0.8224      0.8224        0.5935        0.0000  16.6146
     22      0.9755        0.2680       0.8224      0.8224        0.5931        0.0000  16.6502
     23      0.9771        [32m0.2600[0m       0.8222      0.8222        0.5929        0.0000  16.5590
     24      0.9739        0.2656       0.8222      0.8222        0.5929        0.0000  16.5886
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.8329861111111111
F1 Macro Score after query 7: 0.5072742793933795
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_6.pt

Iteration:  8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8322[0m        [32m0.6016[0m       [35m0.8035[0m      [31m0.8035[0m        [94m0.5844[0m     +  0.0000  21.4667
      2      [36m0.8909[0m        [32m0.4667[0m       [35m0.8186[0m      [31m0.8186[0m        [94m0.5655[0m     +  0.0000  21.4379
      3      [36m0.9099[0m        [32m0.4118[0m       [35m0.8269[0m      [31m0.8269[0m        [94m0.5304[0m     +  0.0000  21.5646
      4      [36m0.9302[0m        [32m0.3645[0m       [35m0.8299[0m      [31m0.8299[0m        0.5306        0.0000  21.7991
      5      [36m0.9373[0m        [32m0.3324[0m       0.8253      0.8253        0.5701        0.0000  21.4685
      6      [36m0.9567[0m        [32m0.2748[0m       [35m0.8349[0m      [31m0.8349[0m        0.5362        0.0000  21.3759
      7      [36m0.9625[0m        [32m0.2537[0m       0.8347      0.8347        0.5445        0.0000  22.0806
      8      [36m0.9655[0m        [32m0.2405[0m       [35m0.8372[0m      [31m0.8372[0m        0.5523        0.0000  21.5304
      9      [36m0.9695[0m        [32m0.2304[0m       [35m0.8385[0m      [31m0.8385[0m        0.5552        0.0000  21.4512
     10      0.9686        [32m0.2240[0m       0.8384      0.8384        0.5522        0.0000  21.8892
     11      [36m0.9788[0m        [32m0.2119[0m       0.8370      0.8370        0.5647        0.0000  21.9040
     12      0.9784        [32m0.2112[0m       0.8344      0.8344        0.5672        0.0000  21.4850
     13      0.9775        [32m0.2082[0m       0.8354      0.8354        0.5676        0.0000  21.7171
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.8548611111111111
F1 Macro Score after query 8: 0.5597385651681315
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_7.pt

Iteration:  9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8777[0m        [32m0.4819[0m       [35m0.8417[0m      [31m0.8417[0m        [94m0.4855[0m     +  0.0000  30.1630
      2      [36m0.9082[0m        [32m0.3921[0m       [35m0.8609[0m      [31m0.8609[0m        [94m0.4499[0m     +  0.0000  31.0866
      3      [36m0.9205[0m        [32m0.3477[0m       0.8562      0.8562        0.4682        0.0000  30.4894
      4      [36m0.9317[0m        [32m0.3153[0m       0.8554      0.8554        0.4674        0.0000  30.4244
      5      [36m0.9379[0m        [32m0.2840[0m       0.8354      0.8354        0.5008        0.0000  30.7729
      6      [36m0.9515[0m        [32m0.2373[0m       0.8340      0.8340        0.5197        0.0000  30.4137
      7      [36m0.9624[0m        [32m0.2124[0m       0.8358      0.8358        0.5187        0.0000  30.2076
      8      [36m0.9705[0m        [32m0.1972[0m       0.8345      0.8345        0.5330        0.0000  30.6306
      9      [36m0.9735[0m        [32m0.1897[0m       0.8330      0.8330        0.5542        0.0000  30.3180
     10      0.9733        [32m0.1830[0m       0.8335      0.8335        0.5518        0.0000  30.2050
     11      [36m0.9797[0m        [32m0.1689[0m       0.8307      0.8307        0.5693        0.0000  30.9126
     12      [36m0.9809[0m        [32m0.1647[0m       0.8314      0.8314        0.5693        0.0000  30.4249
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.8447916666666668
F1 Macro Score after query 9: 0.5667338703651822
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_8.pt

Iteration:  10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9160[0m        [32m0.3662[0m       [35m0.8253[0m      [31m0.8253[0m        [94m0.5455[0m     +  0.0000  45.9751
      2      [36m0.9290[0m        [32m0.3120[0m       [35m0.8411[0m      [31m0.8411[0m        [94m0.4996[0m     +  0.0000  45.9360
      3      [36m0.9336[0m        [32m0.2799[0m       0.8212      0.8212        0.5330        0.0000  45.7267
      4      [36m0.9432[0m        [32m0.2480[0m       0.8241      0.8241        0.5357        0.0000  46.3464
      5      [36m0.9467[0m        [32m0.2219[0m       0.8057      0.8057        0.5923        0.0000  46.3112
      6      [36m0.9637[0m        [32m0.1808[0m       0.8113      0.8113        0.6288        0.0000  46.7981
      7      [36m0.9707[0m        [32m0.1637[0m       0.8156      0.8156        0.6229        0.0000  46.4863
      8      [36m0.9714[0m        [32m0.1535[0m       0.8179      0.8179        0.6437        0.0000  46.1112
      9      [36m0.9757[0m        [32m0.1411[0m       0.8222      0.8222        0.6391        0.0000  46.4225
     10      [36m0.9804[0m        [32m0.1307[0m       0.8207      0.8207        0.6641        0.0000  45.9577
     11      [36m0.9836[0m        [32m0.1204[0m       0.8210      0.8210        0.6537        0.0000  46.1223
     12      [36m0.9871[0m        [32m0.1134[0m       0.8236      0.8236        0.6551        0.0000  46.3901
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.8461805555555556
F1 Macro Score after query 10: 0.591571186985346
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_9.pt

Iteration:  11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9452[0m        [32m0.2498[0m       [35m0.8220[0m      [31m0.8220[0m        [94m0.5955[0m     +  0.0000  74.1150
      2      [36m0.9510[0m        [32m0.2113[0m       [35m0.8333[0m      [31m0.8333[0m        [94m0.5150[0m     +  0.0000  75.3105
      3      [36m0.9569[0m        [32m0.1856[0m       0.8097      0.8097        0.6060        0.0000  74.6075
      4      [36m0.9602[0m        [32m0.1672[0m       0.8181      0.8181        0.6024        0.0000  74.7939
      5      [36m0.9641[0m        [32m0.1455[0m       0.7667      0.7667        0.7842        0.0000  75.0083
      6      [36m0.9728[0m        [32m0.1211[0m       0.8208      0.8208        0.5624        0.0000  74.6313
      7      [36m0.9793[0m        [32m0.1014[0m       0.8238      0.8238        0.5711        0.0000  74.6182
      8      [36m0.9829[0m        [32m0.0903[0m       0.8207      0.8207        0.6207        0.0000  74.6822
      9      [36m0.9875[0m        [32m0.0792[0m       0.8220      0.8220        0.6348        0.0000  75.1222
     10      [36m0.9899[0m        [32m0.0710[0m       0.8217      0.8217        0.6660        0.0000  74.8617
     11      [36m0.9929[0m        [32m0.0630[0m       0.8163      0.8163        0.7237        0.0000  74.4224
     12      [36m0.9943[0m        [32m0.0591[0m       0.8156      0.8156        0.7257        0.0000  74.7015
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.8411458333333333
F1 Macro Score after query 11: 0.5699034774143147
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_10.pt

Iteration:  12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9697[0m        [32m0.1390[0m       [35m0.8076[0m      [31m0.8076[0m        [94m0.6581[0m     +  0.0000  124.2551
      2      [36m0.9710[0m        [32m0.1200[0m       0.6878      0.6878        1.1896        0.0000  124.8240
      3      [36m0.9734[0m        [32m0.1067[0m       [35m0.8104[0m      [31m0.8104[0m        [94m0.6315[0m     +  0.0000  124.7767
      4      [36m0.9759[0m        [32m0.0955[0m       0.7740      0.7740        0.8447        0.0000  125.1411
      5      [36m0.9784[0m        [32m0.0843[0m       [35m0.8127[0m      [31m0.8127[0m        0.6708        0.0000  124.9646
      6      [36m0.9844[0m        [32m0.0635[0m       [35m0.8431[0m      [31m0.8431[0m        [94m0.5434[0m     +  0.0000  124.9226
      7      [36m0.9884[0m        [32m0.0520[0m       0.8415      0.8415        0.5846        0.0000  125.1259
      8      [36m0.9914[0m        [32m0.0437[0m       0.8342      0.8342        0.6466        0.0000  124.8123
      9      [36m0.9932[0m        [32m0.0379[0m       0.8285      0.8285        0.6687        0.0000  124.5939
     10      [36m0.9949[0m        [32m0.0328[0m       0.8264      0.8264        0.6803        0.0000  125.0264
     11      [36m0.9957[0m        [32m0.0289[0m       0.8255      0.8255        0.7539        0.0000  124.7784
     12      [36m0.9968[0m        [32m0.0257[0m       0.8281      0.8281        0.7535        0.0000  126.1677
     13      [36m0.9975[0m        [32m0.0242[0m       0.8245      0.8245        0.7827        0.0000  125.1819
     14      [36m0.9980[0m        [32m0.0228[0m       0.8243      0.8243        0.7881        0.0000  124.8280
     15      0.9980        [32m0.0222[0m       0.8245      0.8245        0.7906        0.0000  125.6064
     16      [36m0.9982[0m        [32m0.0221[0m       0.8243      0.8243        0.8003        0.0000  125.3094
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.8444444444444444
F1 Macro Score after query 12: 0.5741806963732122
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_11.pt

Iteration:  13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9844[0m        [32m0.0635[0m       [35m0.7724[0m      [31m0.7724[0m        [94m0.8671[0m     +  0.0000  144.4903
      2      [36m0.9847[0m        [32m0.0568[0m       [35m0.7854[0m      [31m0.7854[0m        [94m0.8459[0m     +  0.0000  145.6355
      3      [36m0.9871[0m        [32m0.0496[0m       [35m0.7878[0m      [31m0.7878[0m        0.9040        0.0000  145.7207
      4      [36m0.9881[0m        [32m0.0442[0m       0.7800      0.7800        0.9056        0.0000  145.8618
      5      [36m0.9897[0m        [32m0.0405[0m       [35m0.8177[0m      [31m0.8177[0m        [94m0.7110[0m     +  0.0000  145.9105
      6      [36m0.9930[0m        [32m0.0291[0m       0.8163      0.8163        0.7724        0.0000  145.6453
      7      [36m0.9961[0m        [32m0.0199[0m       0.8111      0.8111        0.7829        0.0000  145.9056
      8      [36m0.9978[0m        [32m0.0161[0m       0.8135      0.8135        0.8739        0.0000  145.7734
      9      [36m0.9985[0m        [32m0.0130[0m       0.8161      0.8161        0.8397        0.0000  145.9409
     10      [36m0.9989[0m        [32m0.0118[0m       0.8170      0.8170        0.9077        0.0000  145.5098
     11      [36m0.9992[0m        [32m0.0103[0m       [35m0.8187[0m      [31m0.8187[0m        0.9250        0.0000  143.3723
     12      [36m0.9993[0m        [32m0.0089[0m       [35m0.8191[0m      [31m0.8191[0m        0.9265        0.0000  139.3202
     13      [36m0.9995[0m        [32m0.0086[0m       [35m0.8207[0m      [31m0.8207[0m        0.9301        0.0000  139.5173
     14      0.9995        [32m0.0082[0m       [35m0.8222[0m      [31m0.8222[0m        0.9309        0.0000  140.5789
     15      [36m0.9996[0m        0.0083       [35m0.8224[0m      [31m0.8224[0m        0.9349        0.0000  140.3436
Stopping since valid_loss has not improved in the last 11 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.8519097222222222
F1 Macro Score after query 13: 0.5823458391474272
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\model_checkpoint_iteration_12.pt
Pickle file saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoS/margin_sampling_seed48\AL_margin_sampling_results_for_multiclass_classification_s48.pickle
