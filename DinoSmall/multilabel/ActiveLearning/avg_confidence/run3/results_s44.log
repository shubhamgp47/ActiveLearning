Adjusted shapes after loading:
X_initial_np: (8, 224, 224, 3), y_initial_np: (8, 3)
X_pool_np: (26872, 224, 224, 3), y_pool_np: (26872, 3)
X_test_np: (5760, 224, 224, 3), y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3), y_val_np: (5760, 3)
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.5460[0m        [32m0.7305[0m       [35m0.1863[0m      [31m0.4349[0m        [94m0.7025[0m     +  0.0000  12.0769
      2      [36m0.7190[0m        [32m0.6576[0m       [35m0.2182[0m      0.2878        [94m0.6934[0m     +  0.0000  10.3394
      3      0.6389        [32m0.6449[0m       [35m0.2687[0m      [31m0.4495[0m        [94m0.6918[0m     +  0.0000  10.4324
      4      0.5913        [32m0.6354[0m       [35m0.2983[0m      0.4066        [94m0.6754[0m     +  0.0000  10.4920
      5      [36m0.9048[0m        [32m0.5740[0m       0.2793      [31m0.4647[0m        0.6930        0.0000  10.4826
      6      [36m0.9259[0m        [32m0.5448[0m       0.2785      [31m0.4678[0m        0.6885        0.0000  10.4949
      7      0.8201        [32m0.5390[0m       0.2720      0.4648        0.6880        0.0000  10.5007
      8      0.8320        0.5509       0.2694      [31m0.4776[0m        0.6883        0.0000  10.4277
      9      0.8130        0.5648       0.2823      0.4742        0.6833        0.0000  10.3884
     10      [36m0.9630[0m        [32m0.5210[0m       0.2793      [31m0.4887[0m        0.6835        0.0000  10.4254
     11      0.8796        0.5431       0.2795      0.4866        0.6838        0.0000  10.4423
     12      0.8519        [32m0.5153[0m       0.2839      0.4838        0.6831        0.0000  10.4594
     13      0.9259        [32m0.4990[0m       0.2877      [31m0.4894[0m        0.6845        0.0000  10.4236
     14      0.9259        [32m0.4931[0m       0.2835      [31m0.4896[0m        0.6848        0.0000  10.6263
Stopping since valid_loss has not improved in the last 11 epochs.
Pre F1 micro score = 0.5246
Pre F1 macro score = 0.5115
Pre Accuracy = 0.3703

Iteration: 1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.5722[0m        [32m0.6240[0m       [35m0.1818[0m      [31m0.2345[0m        [94m0.6298[0m     +  0.0000  10.5327
      2      [36m0.6258[0m        [32m0.5826[0m       [35m0.1998[0m      [31m0.3705[0m        [94m0.6176[0m     +  0.0000  10.4706
      3      [36m0.6296[0m        [32m0.5647[0m       0.1870      [31m0.5529[0m        [94m0.6164[0m     +  0.0000  10.4173
      4      [36m0.7571[0m        [32m0.5233[0m       [35m0.2278[0m      [31m0.5844[0m        [94m0.6030[0m     +  0.0000  10.5867
      5      [36m0.7874[0m        [32m0.5109[0m       0.2200      0.5675        [94m0.6011[0m     +  0.0000  10.4632
      6      [36m0.8225[0m        [32m0.4804[0m       [35m0.2339[0m      0.5774        [94m0.5956[0m     +  0.0000  10.5317
      7      0.8110        [32m0.4749[0m       [35m0.2455[0m      0.5781        [94m0.5911[0m     +  0.0000  10.6559
      8      [36m0.8714[0m        [32m0.4525[0m       [35m0.2458[0m      0.5835        [94m0.5897[0m     +  0.0000  10.4383
      9      0.8352        0.4627       [35m0.2569[0m      [31m0.5992[0m        [94m0.5873[0m     +  0.0000  10.5353
     10      0.8225        0.4641       [35m0.2589[0m      [31m0.6087[0m        [94m0.5840[0m     +  0.0000  10.5250
     11      [36m0.8898[0m        [32m0.4282[0m       [35m0.2622[0m      [31m0.6088[0m        [94m0.5825[0m     +  0.0000  10.5186
     12      0.8619        0.4449       0.2618      0.6041        [94m0.5815[0m     +  0.0000  10.5379
     13      [36m0.8918[0m        [32m0.4205[0m       [35m0.2653[0m      0.6000        [94m0.5792[0m     +  0.0000  10.7216
     14      [36m0.8943[0m        0.4307       [35m0.2701[0m      0.6047        [94m0.5781[0m     +  0.0000  10.5473
     15      0.8672        0.4294       [35m0.2703[0m      0.6011        [94m0.5770[0m     +  0.0000  10.6458
     16      [36m0.9025[0m        0.4329       [35m0.2720[0m      0.6022        [94m0.5765[0m     +  0.0000  10.6565
     17      0.8881        0.4373       [35m0.2731[0m      0.6041        [94m0.5763[0m     +  0.0000  10.7184
     18      [36m0.9050[0m        0.4218       [35m0.2753[0m      0.6080        [94m0.5761[0m     +  0.0000  10.5035
     19      0.8804        0.4299       [35m0.2767[0m      [31m0.6090[0m        [94m0.5760[0m     +  0.0000  10.5234
     20      0.8829        0.4324       [35m0.2785[0m      [31m0.6096[0m        [94m0.5754[0m     +  0.0000  10.5445
     21      0.8786        0.4263       [35m0.2786[0m      0.6094        [94m0.5752[0m     +  0.0000  10.4858
     22      0.8746        0.4370       [35m0.2788[0m      [31m0.6102[0m        [94m0.5751[0m     +  0.0000  10.7259
     23      [36m0.9107[0m        0.4259       [35m0.2797[0m      [31m0.6105[0m        [94m0.5750[0m     +  0.0000  10.5309
     24      0.8918        0.4229       [35m0.2806[0m      0.6105        [94m0.5748[0m     +  0.0000  10.4748
     25      [36m0.9171[0m        [32m0.3998[0m       [35m0.2818[0m      [31m0.6106[0m        [94m0.5746[0m     +  0.0000  10.5879
     26      0.8535        0.4377       [35m0.2819[0m      [31m0.6108[0m        [94m0.5745[0m     +  0.0000  10.6901
     27      0.8501        0.4247       [35m0.2821[0m      [31m0.6109[0m        [94m0.5745[0m     +  0.0000  10.5133
     28      0.8554        0.4255       [35m0.2830[0m      [31m0.6115[0m        [94m0.5744[0m     +  0.0000  10.5570
     29      [36m0.9241[0m        0.4232       [35m0.2833[0m      [31m0.6118[0m        [94m0.5744[0m     +  0.0000  10.8113
     30      0.8766        0.4104       [35m0.2837[0m      [31m0.6118[0m        [94m0.5744[0m     +  0.0000  10.4850
     31      0.9038        0.4270       0.2837      [31m0.6119[0m        0.5744        0.0000  10.4354
     32      0.9169        0.4227       0.2837      0.6119        [94m0.5744[0m     +  0.0000  10.5942
     33      0.8697        0.4397       0.2837      0.6119        [94m0.5744[0m     +  0.0000  10.7592
     34      0.8918        0.4323       0.2837      0.6119        [94m0.5744[0m     +  0.0000  10.5428
     35      0.8887        0.4279       0.2837      [31m0.6120[0m        0.5744        0.0000  10.5364
     36      0.8867        0.4163       0.2837      0.6120        [94m0.5744[0m     +  0.0000  10.5522
     37      0.8874        0.4332       0.2837      0.6120        [94m0.5743[0m     +  0.0000  10.5214
     38      0.8915        0.4240       0.2837      0.6120        [94m0.5743[0m     +  0.0000  10.4654
     39      0.9012        0.4185       0.2837      0.6120        [94m0.5743[0m     +  0.0000  10.6408
     40      0.8910        0.4171       0.2837      0.6120        [94m0.5743[0m     +  0.0000  10.6761
     41      0.8786        0.4218       0.2837      0.6120        [94m0.5743[0m     +  0.0000  10.5728
     42      0.8918        0.4221       0.2837      0.6120        [94m0.5743[0m     +  0.0000  10.5545
     43      0.9157        0.4130       0.2837      0.6120        0.5743        0.0000  10.5635
Stopping since valid_loss has not improved in the last 11 epochs.
[24]
F1 Micro Score after query 1: 0.6275016139444803
F1 Macro Score after query 1: 0.6082230723832397
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_0.pt

Iteration: 2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.6812[0m        [32m0.5638[0m       [35m0.4757[0m      [31m0.5765[0m        [94m0.5401[0m     +  0.0000  10.6470
      2      [36m0.7889[0m        [32m0.4774[0m       [35m0.6019[0m      [31m0.7866[0m        [94m0.4750[0m     +  0.0000  10.6247
      3      [36m0.8378[0m        [32m0.4314[0m       [35m0.6314[0m      [31m0.7936[0m        [94m0.4423[0m     +  0.0000  10.7341
      4      [36m0.8968[0m        [32m0.3814[0m       [35m0.6556[0m      [31m0.8046[0m        [94m0.4238[0m     +  0.0000  10.6997
      5      [36m0.9034[0m        [32m0.3684[0m       [35m0.6722[0m      [31m0.8148[0m        [94m0.4091[0m     +  0.0000  10.6365
      6      0.8989        [32m0.3511[0m       0.6628      [31m0.8149[0m        [94m0.4024[0m     +  0.0000  10.6473
      7      [36m0.9128[0m        [32m0.3443[0m       0.6708      [31m0.8171[0m        [94m0.4003[0m     +  0.0000  10.6602
      8      [36m0.9337[0m        [32m0.3377[0m       [35m0.6731[0m      0.8145        [94m0.3969[0m     +  0.0000  10.6794
      9      0.8938        [32m0.3327[0m       [35m0.6773[0m      0.8144        [94m0.3949[0m     +  0.0000  10.6803
     10      0.9276        0.3358       0.6720      0.8154        [94m0.3920[0m     +  0.0000  10.6225
     11      0.9262        [32m0.3290[0m       0.6738      0.8155        [94m0.3910[0m     +  0.0000  10.6865
     12      0.9294        [32m0.3201[0m       0.6714      0.8146        [94m0.3899[0m     +  0.0000  10.8243
     13      0.9225        0.3224       0.6717      0.8149        [94m0.3892[0m     +  0.0000  10.7822
     14      0.9235        0.3261       0.6714      0.8151        [94m0.3885[0m     +  0.0000  10.9014
     15      [36m0.9357[0m        [32m0.3190[0m       0.6708      0.8150        [94m0.3880[0m     +  0.0000  10.8789
     16      0.9095        0.3292       0.6703      0.8150        [94m0.3878[0m     +  0.0000  10.5422
     17      0.9225        [32m0.3160[0m       0.6705      0.8150        [94m0.3876[0m     +  0.0000  10.6234
     18      0.9250        0.3239       0.6707      0.8151        [94m0.3874[0m     +  0.0000  10.6761
     19      [36m0.9418[0m        0.3193       0.6714      0.8153        [94m0.3872[0m     +  0.0000  10.5533
     20      0.9188        [32m0.3132[0m       0.6717      0.8152        [94m0.3869[0m     +  0.0000  10.5625
     21      0.9085        0.3137       0.6708      0.8151        [94m0.3868[0m     +  0.0000  10.7310
     22      [36m0.9437[0m        0.3144       0.6708      0.8151        [94m0.3868[0m     +  0.0000  10.6244
     23      0.9377        0.3134       0.6707      0.8149        [94m0.3866[0m     +  0.0000  10.6927
     24      0.9303        [32m0.3001[0m       0.6708      0.8150        [94m0.3866[0m     +  0.0000  10.5757
     25      0.9310        0.3207       0.6710      0.8149        [94m0.3864[0m     +  0.0000  10.6509
     26      0.9164        0.3168       0.6707      0.8147        [94m0.3864[0m     +  0.0000  10.7189
     27      0.9211        0.3260       0.6710      0.8151        [94m0.3864[0m     +  0.0000  10.6713
     28      0.9303        0.3109       0.6712      0.8152        [94m0.3863[0m     +  0.0000  10.6712
     29      0.9156        0.3186       0.6712      0.8153        [94m0.3863[0m     +  0.0000  10.6663
     30      0.9103        0.3227       0.6714      0.8154        [94m0.3863[0m     +  0.0000  10.6611
     31      0.9431        0.3193       0.6712      0.8153        [94m0.3863[0m     +  0.0000  10.6196
     32      0.9058        0.3238       0.6710      0.8152        [94m0.3863[0m     +  0.0000  10.6373
     33      0.9225        0.3178       0.6710      0.8151        0.3863        0.0000  10.6268
     34      0.9286        0.3142       0.6710      0.8153        [94m0.3863[0m     +  0.0000  10.5718
     35      0.9280        0.3159       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.7918
     36      0.9280        0.3083       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.6135
     37      0.9103        0.3262       0.6712      0.8152        [94m0.3862[0m     +  0.0000  10.7132
     38      0.9357        0.3209       0.6712      0.8152        [94m0.3862[0m     +  0.0000  10.6860
     39      0.9280        0.3145       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.6988
     40      0.9118        0.3305       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.6617
     41      0.9294        0.3128       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.5938
     42      0.9152        0.3209       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.6468
     43      0.9262        0.3112       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.8537
     44      0.9280        0.3142       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.5892
     45      0.9357        0.3214       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.7660
     46      0.9377        0.3132       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.5937
     47      0.9437        0.3134       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.7100
     48      0.9303        0.3177       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.7857
     49      0.9046        0.3166       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.7296
     50      0.9030        0.3188       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.7275
     51      0.9394        0.3137       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.6124
     52      0.9225        0.3190       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.6537
     53      0.9188        0.3191       0.6710      0.8151        [94m0.3862[0m     +  0.0000  10.6406
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56]
F1 Micro Score after query 2: 0.8430500153421295
F1 Macro Score after query 2: 0.8289971975351813
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_1.pt

Iteration: 3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8268[0m        [32m0.3743[0m       [35m0.6993[0m      [31m0.8393[0m        [94m0.3679[0m     +  0.0000  10.9046
      2      [36m0.9024[0m        [32m0.3154[0m       0.6708      0.7857        [94m0.3576[0m     +  0.0000  10.8925
      3      [36m0.9212[0m        [32m0.2949[0m       0.6757      0.7949        [94m0.3453[0m     +  0.0000  10.8912
      4      [36m0.9359[0m        [32m0.2792[0m       0.6899      0.8314        [94m0.3351[0m     +  0.0000  10.9834
      5      0.9359        [32m0.2658[0m       0.6802      0.8002        [94m0.3334[0m     +  0.0000  10.8764
      6      [36m0.9388[0m        [32m0.2614[0m       0.6849      0.8115        [94m0.3283[0m     +  0.0000  10.8984
      7      [36m0.9446[0m        [32m0.2593[0m       0.6814      0.8080        0.3290        0.0000  10.8295
      8      0.9356        [32m0.2511[0m       0.6852      0.8146        [94m0.3258[0m     +  0.0000  10.8275
      9      0.9426        [32m0.2489[0m       0.6875      0.8191        [94m0.3225[0m     +  0.0000  10.7863
     10      [36m0.9601[0m        [32m0.2430[0m       0.6896      0.8211        [94m0.3216[0m     +  0.0000  10.9401
     11      0.9538        0.2465       0.6979      0.8345        [94m0.3199[0m     +  0.0000  10.9282
     12      0.9505        0.2435       0.6934      0.8289        [94m0.3197[0m     +  0.0000  10.8490
     13      0.9594        [32m0.2423[0m       0.6953      0.8305        [94m0.3195[0m     +  0.0000  10.9181
     14      0.9563        [32m0.2350[0m       0.6965      0.8307        [94m0.3195[0m     +  0.0000  10.8654
     15      0.9477        0.2491       0.6955      0.8301        [94m0.3186[0m     +  0.0000  11.1050
     16      0.9541        0.2369       0.6981      0.8340        [94m0.3180[0m     +  0.0000  10.8274
     17      [36m0.9655[0m        [32m0.2349[0m       0.6964      0.8320        0.3180        0.0000  10.8439
     18      0.9464        0.2350       0.6976      0.8343        [94m0.3179[0m     +  0.0000  10.8999
     19      0.9628        0.2437       0.6962      0.8330        0.3180        0.0000  10.8360
     20      0.9442        0.2365       0.6955      0.8309        0.3179        0.0000  10.8922
     21      0.9478        0.2482       0.6955      0.8317        [94m0.3178[0m     +  0.0000  10.9277
     22      0.9473        0.2439       0.6951      0.8315        [94m0.3178[0m     +  0.0000  10.9247
     23      0.9567        0.2380       0.6955      0.8320        [94m0.3177[0m     +  0.0000  10.9224
     24      0.9495        0.2373       0.6962      0.8328        [94m0.3177[0m     +  0.0000  10.8645
     25      0.9577        0.2437       0.6972      0.8343        [94m0.3175[0m     +  0.0000  10.8824
     26      0.9577        0.2357       0.6972      0.8343        [94m0.3175[0m     +  0.0000  10.9224
     27      0.9438        0.2383       0.6969      0.8337        0.3175        0.0000  10.9000
     28      0.9487        0.2417       0.6969      0.8335        0.3175        0.0000  10.7701
     29      0.9568        0.2354       0.6967      0.8335        0.3175        0.0000  10.8682
     30      0.9511        0.2354       0.6967      0.8335        [94m0.3175[0m     +  0.0000  10.9370
     31      0.9502        [32m0.2311[0m       0.6969      0.8336        [94m0.3174[0m     +  0.0000  11.0792
     32      0.9532        0.2427       0.6965      0.8334        [94m0.3174[0m     +  0.0000  10.8212
     33      0.9635        0.2371       0.6969      0.8337        [94m0.3174[0m     +  0.0000  10.8795
     34      0.9461        0.2395       0.6969      0.8337        [94m0.3174[0m     +  0.0000  10.8613
     35      0.9476        0.2408       0.6972      0.8340        [94m0.3174[0m     +  0.0000  10.9586
     36      0.9464        0.2350       0.6970      0.8340        [94m0.3174[0m     +  0.0000  10.8782
     37      0.9475        0.2399       0.6972      0.8340        0.3174        0.0000  11.1405
     38      0.9509        0.2337       0.6970      0.8340        [94m0.3174[0m     +  0.0000  10.7459
     39      0.9541        0.2342       0.6969      0.8339        [94m0.3174[0m     +  0.0000  10.8788
     40      0.9473        0.2349       0.6969      0.8339        [94m0.3174[0m     +  0.0000  10.8231
     41      0.9565        0.2446       0.6969      0.8339        [94m0.3174[0m     +  0.0000  10.8637
     42      [36m0.9657[0m        0.2341       0.6969      0.8339        0.3174        0.0000  10.9521
     43      0.9636        0.2373       0.6967      0.8338        0.3174        0.0000  10.8597
     44      0.9506        0.2396       0.6970      0.8340        [94m0.3174[0m     +  0.0000  11.0297
     45      0.9506        0.2398       0.6967      0.8338        [94m0.3174[0m     +  0.0000  10.8284
     46      0.9526        0.2347       0.6967      0.8338        [94m0.3174[0m     +  0.0000  10.8679
     47      0.9506        0.2449       0.6967      0.8338        [94m0.3174[0m     +  0.0000  10.8957
     48      0.9655        0.2357       0.6969      0.8339        [94m0.3174[0m     +  0.0000  11.2812
     49      0.9506        0.2412       0.6969      0.8339        [94m0.3174[0m     +  0.0000  10.8943
     50      0.9569        [32m0.2291[0m       0.6969      0.8339        [94m0.3174[0m     +  0.0000  10.8592
     51      0.9599        0.2362       0.6969      0.8339        [94m0.3174[0m     +  0.0000  10.7967
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112]
F1 Micro Score after query 3: 0.8842884130982369
F1 Macro Score after query 3: 0.8746760554231024
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_2.pt

Iteration: 4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9149[0m        [32m0.2781[0m       [35m0.7219[0m      [31m0.8455[0m        [94m0.3184[0m     +  0.0000  11.3526
      2      [36m0.9335[0m        [32m0.2451[0m       0.7108      0.8437        [94m0.3036[0m     +  0.0000  11.0186
      3      [36m0.9478[0m        [32m0.2282[0m       [35m0.7345[0m      [31m0.8599[0m        [94m0.3005[0m     +  0.0000  11.1328
      4      [36m0.9533[0m        [32m0.2161[0m       0.7245      0.8517        [94m0.2953[0m     +  0.0000  11.1808
      5      [36m0.9621[0m        [32m0.2064[0m       [35m0.7368[0m      0.8576        0.2960        0.0000  11.1066
      6      [36m0.9676[0m        0.2075       0.7217      0.8531        [94m0.2909[0m     +  0.0000  11.1475
      7      0.9608        [32m0.2051[0m       0.7273      0.8561        [94m0.2904[0m     +  0.0000  11.1500
      8      0.9604        [32m0.1929[0m       0.7267      0.8557        [94m0.2883[0m     +  0.0000  11.2282
      9      0.9589        0.1984       [35m0.7385[0m      [31m0.8637[0m        0.2911        0.0000  11.1794
     10      0.9644        [32m0.1915[0m       0.7273      0.8558        [94m0.2867[0m     +  0.0000  11.4009
     11      [36m0.9715[0m        0.1940       0.7234      0.8544        [94m0.2866[0m     +  0.0000  11.1336
     12      0.9665        0.1936       0.7226      0.8542        [94m0.2859[0m     +  0.0000  11.1814
     13      0.9715        [32m0.1886[0m       0.7227      0.8541        [94m0.2854[0m     +  0.0000  11.2101
     14      0.9694        0.1909       0.7243      0.8553        [94m0.2841[0m     +  0.0000  11.2576
     15      0.9588        0.1926       0.7293      0.8591        0.2842        0.0000  11.1940
     16      0.9672        [32m0.1869[0m       0.7238      0.8556        [94m0.2839[0m     +  0.0000  11.1650
     17      [36m0.9767[0m        0.1885       0.7240      0.8561        [94m0.2839[0m     +  0.0000  11.2316
     18      0.9643        [32m0.1858[0m       0.7222      0.8548        0.2839        0.0000  11.3361
     19      0.9718        [32m0.1840[0m       0.7215      0.8541        0.2842        0.0000  11.3684
     20      0.9681        0.1859       0.7219      0.8544        [94m0.2837[0m     +  0.0000  11.1639
     21      0.9654        0.1894       0.7215      0.8542        [94m0.2835[0m     +  0.0000  11.4311
     22      0.9711        [32m0.1826[0m       0.7217      0.8543        [94m0.2834[0m     +  0.0000  11.6944
     23      0.9691        0.1877       0.7222      0.8547        [94m0.2834[0m     +  0.0000  11.2814
     24      0.9712        [32m0.1807[0m       0.7240      0.8561        0.2834        0.0000  11.2508
     25      0.9699        0.1848       0.7229      0.8554        0.2835        0.0000  11.2487
     26      0.9722        0.1860       0.7227      0.8553        0.2835        0.0000  11.3752
     27      0.9677        0.1868       0.7222      0.8545        0.2834        0.0000  11.3109
     28      0.9664        0.1816       0.7219      0.8544        0.2834        0.0000  11.2601
     29      0.9693        0.1843       0.7224      0.8550        [94m0.2833[0m     +  0.0000  11.2320
     30      0.9712        0.1863       0.7226      0.8551        [94m0.2833[0m     +  0.0000  11.2508
     31      0.9635        0.1879       0.7224      0.8551        [94m0.2832[0m     +  0.0000  11.2602
     32      0.9700        0.1876       0.7224      0.8551        [94m0.2832[0m     +  0.0000  11.3576
     33      0.9689        [32m0.1796[0m       0.7227      0.8552        [94m0.2832[0m     +  0.0000  11.2182
     34      0.9673        0.1903       0.7224      0.8550        [94m0.2832[0m     +  0.0000  11.2190
     35      0.9721        0.1820       0.7219      0.8546        0.2832        0.0000  11.1581
     36      0.9658        0.1853       0.7219      0.8546        [94m0.2832[0m     +  0.0000  11.2815
     37      0.9737        0.1858       0.7217      0.8544        [94m0.2832[0m     +  0.0000  11.1834
     38      0.9716        0.1822       0.7219      0.8545        [94m0.2832[0m     +  0.0000  11.1748
     39      0.9676        0.1863       0.7219      0.8545        [94m0.2831[0m     +  0.0000  11.2389
     40      0.9711        0.1882       0.7220      0.8546        [94m0.2831[0m     +  0.0000  11.1917
     41      0.9693        0.1911       0.7220      0.8546        [94m0.2831[0m     +  0.0000  11.2715
     42      0.9711        0.1821       0.7220      0.8546        [94m0.2831[0m     +  0.0000  11.3342
     43      0.9676        0.1817       0.7220      0.8546        [94m0.2831[0m     +  0.0000  11.2334
     44      0.9701        0.1864       0.7219      0.8545        [94m0.2831[0m     +  0.0000  11.1832
     45      0.9672        [32m0.1780[0m       0.7219      0.8545        [94m0.2831[0m     +  0.0000  11.3542
     46      0.9694        0.1895       0.7219      0.8545        [94m0.2831[0m     +  0.0000  11.3596
     47      0.9677        0.1850       0.7219      0.8545        0.2831        0.0000  11.3280
     48      0.9724        0.1813       0.7219      0.8545        [94m0.2831[0m     +  0.0000  11.4521
     49      0.9741        0.1822       0.7219      0.8545        [94m0.2831[0m     +  0.0000  11.1785
     50      0.9698        0.1888       0.7219      0.8545        [94m0.2831[0m     +  0.0000  11.2686
     51      0.9704        0.1877       0.7219      0.8545        [94m0.2831[0m     +  0.0000  11.2668
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208]
F1 Micro Score after query 4: 0.8826678904696343
F1 Macro Score after query 4: 0.8699860407656527
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_3.pt

Iteration: 5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9149[0m        [32m0.2341[0m       [35m0.7050[0m      [31m0.8464[0m        [94m0.2806[0m     +  0.0000  11.9455
      2      [36m0.9397[0m        [32m0.2054[0m       0.6941      0.8316        0.2825        0.0000  11.9505
      3      [36m0.9508[0m        [32m0.1958[0m       0.7014      0.8393        [94m0.2777[0m     +  0.0000  11.9428
      4      0.9504        [32m0.1869[0m       [35m0.7127[0m      [31m0.8487[0m        [94m0.2700[0m     +  0.0000  11.8244
      5      [36m0.9559[0m        [32m0.1820[0m       [35m0.7352[0m      [31m0.8620[0m        [94m0.2623[0m     +  0.0000  11.8960
      6      [36m0.9673[0m        [32m0.1701[0m       [35m0.7443[0m      0.8609        [94m0.2576[0m     +  0.0000  12.0004
      7      0.9595        0.1708       0.7422      0.8586        [94m0.2575[0m     +  0.0000  11.9030
      8      0.9611        [32m0.1673[0m       [35m0.7524[0m      [31m0.8663[0m        [94m0.2553[0m     +  0.0000  12.0951
      9      [36m0.9725[0m        [32m0.1640[0m       0.7509      0.8652        0.2558        0.0000  11.9535
     10      [36m0.9726[0m        0.1651       0.7517      0.8660        [94m0.2550[0m     +  0.0000  12.1532
     11      [36m0.9746[0m        [32m0.1594[0m       [35m0.7668[0m      [31m0.8745[0m        [94m0.2502[0m     +  0.0000  11.9958
     12      [36m0.9810[0m        [32m0.1564[0m       0.7668      [31m0.8748[0m        [94m0.2501[0m     +  0.0000  11.9870
     13      0.9747        0.1587       [35m0.7670[0m      0.8746        [94m0.2500[0m     +  0.0000  11.8166
     14      0.9773        0.1584       0.7656      0.8742        0.2501        0.0000  11.9061
     15      0.9766        0.1567       [35m0.7688[0m      [31m0.8757[0m        [94m0.2494[0m     +  0.0000  11.8599
     16      0.9775        [32m0.1559[0m       0.7682      0.8756        [94m0.2493[0m     +  0.0000  11.9871
     17      [36m0.9824[0m        [32m0.1541[0m       0.7688      0.8755        [94m0.2488[0m     +  0.0000  11.9079
     18      [36m0.9850[0m        [32m0.1522[0m       [35m0.7691[0m      0.8756        [94m0.2484[0m     +  0.0000  11.8315
     19      0.9791        0.1543       0.7689      [31m0.8759[0m        [94m0.2483[0m     +  0.0000  11.9688
     20      0.9740        0.1554       0.7670      0.8745        0.2487        0.0000  11.8125
     21      0.9780        [32m0.1518[0m       0.7677      0.8752        0.2487        0.0000  11.8391
     22      0.9806        0.1530       0.7677      0.8754        0.2489        0.0000  11.9596
     23      0.9816        0.1567       0.7674      0.8754        0.2489        0.0000  11.9027
     24      0.9796        0.1522       0.7681      0.8753        0.2488        0.0000  12.0779
     25      0.9839        [32m0.1510[0m       0.7672      0.8745        0.2487        0.0000  12.0158
     26      0.9805        [32m0.1497[0m       0.7651      0.8732        0.2489        0.0000  11.8405
     27      0.9842        0.1507       0.7663      0.8738        0.2487        0.0000  12.0006
     28      0.9844        0.1498       0.7658      0.8733        0.2487        0.0000  11.9299
     29      0.9813        0.1501       0.7661      0.8737        0.2487        0.0000  11.8996
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.8986147054346172
F1 Macro Score after query 5: 0.8843640126394693
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_4.pt

Iteration: 6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9325[0m        [32m0.2030[0m       [35m0.7670[0m      [31m0.8595[0m        [94m0.2383[0m     +  0.0000  13.1226
      2      [36m0.9452[0m        [32m0.1798[0m       [35m0.7877[0m      [31m0.8776[0m        [94m0.2302[0m     +  0.0000  13.1878
      3      [36m0.9659[0m        [32m0.1614[0m       0.7868      [31m0.8803[0m        [94m0.2260[0m     +  0.0000  13.0987
      4      0.9649        [32m0.1555[0m       [35m0.7972[0m      [31m0.8886[0m        [94m0.2223[0m     +  0.0000  13.1512
      5      [36m0.9760[0m        [32m0.1480[0m       [35m0.8061[0m      [31m0.8944[0m        [94m0.2181[0m     +  0.0000  13.1361
      6      [36m0.9815[0m        [32m0.1382[0m       0.8010      0.8923        0.2184        0.0000  13.1934
      7      [36m0.9850[0m        [32m0.1341[0m       0.7993      0.8906        0.2189        0.0000  13.1196
      8      [36m0.9888[0m        [32m0.1313[0m       0.8002      0.8918        [94m0.2176[0m     +  0.0000  13.3937
      9      0.9881        [32m0.1281[0m       0.7986      0.8903        [94m0.2175[0m     +  0.0000  13.3537
     10      [36m0.9902[0m        [32m0.1268[0m       0.7974      0.8901        0.2178        0.0000  13.4175
     11      [36m0.9913[0m        [32m0.1251[0m       0.8000      0.8922        [94m0.2135[0m     +  0.0000  13.0994
     12      0.9905        [32m0.1230[0m       0.8033      0.8933        [94m0.2120[0m     +  0.0000  13.2215
     13      [36m0.9922[0m        [32m0.1224[0m       0.8045      0.8939        [94m0.2115[0m     +  0.0000  13.1416
     14      0.9900        [32m0.1196[0m       0.8026      0.8933        [94m0.2113[0m     +  0.0000  13.0986
     15      [36m0.9930[0m        [32m0.1193[0m       0.8035      0.8935        [94m0.2112[0m     +  0.0000  13.1407
     16      0.9921        0.1202       0.8021      0.8930        [94m0.2109[0m     +  0.0000  13.0680
     17      0.9917        0.1202       0.8010      0.8922        [94m0.2108[0m     +  0.0000  13.0947
     18      [36m0.9946[0m        [32m0.1176[0m       0.8016      0.8927        [94m0.2105[0m     +  0.0000  13.2140
     19      0.9943        [32m0.1160[0m       0.8002      0.8924        0.2109        0.0000  13.1478
     20      0.9940        0.1180       0.8030      0.8934        [94m0.2098[0m     +  0.0000  13.2090
     21      0.9941        0.1161       0.8014      0.8925        0.2104        0.0000  13.1980
     22      [36m0.9956[0m        0.1166       0.7993      0.8919        0.2107        0.0000  13.1845
     23      [36m0.9961[0m        [32m0.1134[0m       0.8009      0.8925        0.2102        0.0000  13.1873
     24      0.9937        0.1152       0.7993      0.8917        0.2105        0.0000  13.0887
     25      0.9945        0.1168       0.8003      0.8923        0.2104        0.0000  13.2389
     26      0.9941        0.1170       0.7990      0.8917        0.2106        0.0000  13.3198
     27      0.9935        0.1156       0.7976      0.8908        0.2107        0.0000  13.0756
     28      0.9932        0.1152       0.7974      0.8906        0.2106        0.0000  13.1461
     29      [36m0.9965[0m        0.1155       0.7974      0.8906        0.2107        0.0000  13.0831
     30      0.9946        0.1140       0.7988      0.8914        0.2104        0.0000  13.1516
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.9219648686460439
F1 Macro Score after query 6: 0.9125705700445557
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_5.pt

Iteration: 7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9507[0m        [32m0.1573[0m       [35m0.7875[0m      [31m0.8903[0m        [94m0.2176[0m     +  0.0000  15.3907
      2      [36m0.9710[0m        [32m0.1347[0m       0.7743      0.8810        0.2216        0.0000  15.2553
      3      [36m0.9734[0m        [32m0.1282[0m       0.7840      0.8845        [94m0.2168[0m     +  0.0000  15.2522
      4      [36m0.9771[0m        [32m0.1187[0m       0.7823      0.8838        0.2196        0.0000  15.3972
      5      [36m0.9823[0m        [32m0.1123[0m       0.7866      0.8859        0.2170        0.0000  15.3339
      6      [36m0.9870[0m        [32m0.1048[0m       0.7858      0.8853        [94m0.2132[0m     +  0.0000  15.4276
      7      [36m0.9896[0m        [32m0.1019[0m       0.7875      0.8861        [94m0.2111[0m     +  0.0000  15.1893
      8      [36m0.9897[0m        [32m0.0970[0m       0.7814      0.8836        0.2152        0.0000  15.3179
      9      [36m0.9901[0m        [32m0.0968[0m       0.7873      0.8860        0.2133        0.0000  15.3585
     10      [36m0.9936[0m        [32m0.0936[0m       [35m0.7892[0m      0.8869        0.2113        0.0000  15.3479
     11      0.9912        [32m0.0926[0m       [35m0.7988[0m      0.8900        [94m0.2025[0m     +  0.0000  15.4165
     12      [36m0.9936[0m        [32m0.0880[0m       [35m0.7990[0m      0.8899        [94m0.2010[0m     +  0.0000  15.3047
     13      0.9927        0.0887       0.7967      0.8883        0.2049        0.0000  15.3546
     14      [36m0.9938[0m        [32m0.0876[0m       [35m0.8007[0m      [31m0.8909[0m        0.2012        0.0000  15.2547
     15      [36m0.9958[0m        [32m0.0860[0m       0.7944      0.8883        0.2048        0.0000  15.3200
     16      0.9953        [32m0.0856[0m       [35m0.8047[0m      [31m0.8914[0m        [94m0.1991[0m     +  0.0000  15.3929
     17      0.9948        [32m0.0838[0m       0.8030      0.8905        0.2000        0.0000  15.2714
     18      0.9953        0.0845       [35m0.8049[0m      0.8911        [94m0.1989[0m     +  0.0000  15.5019
     19      0.9947        0.0852       [35m0.8050[0m      0.8911        0.1990        0.0000  15.4483
     20      [36m0.9967[0m        0.0862       0.8050      0.8913        [94m0.1981[0m     +  0.0000  15.4523
     21      0.9956        0.0840       [35m0.8075[0m      [31m0.8921[0m        [94m0.1970[0m     +  0.0000  15.4215
     22      0.9958        [32m0.0832[0m       [35m0.8078[0m      [31m0.8923[0m        [94m0.1966[0m     +  0.0000  15.3332
     23      0.9948        0.0864       0.8066      0.8915        0.1972        0.0000  15.3507
     24      0.9954        0.0842       0.8043      0.8905        0.1985        0.0000  15.3399
     25      0.9954        0.0837       0.8057      0.8910        0.1978        0.0000  15.4497
     26      0.9949        0.0850       0.8073      [31m0.8925[0m        [94m0.1964[0m     +  0.0000  15.4138
     27      0.9944        0.0840       0.8076      [31m0.8927[0m        [94m0.1963[0m     +  0.0000  15.4233
     28      0.9949        0.0849       0.8073      0.8925        [94m0.1963[0m     +  0.0000  15.4820
     29      0.9950        [32m0.0828[0m       0.8078      0.8925        [94m0.1959[0m     +  0.0000  15.1685
     30      0.9964        0.0845       [35m0.8080[0m      [31m0.8927[0m        0.1962        0.0000  15.4869
     31      0.9954        0.0848       0.8071      0.8923        [94m0.1959[0m     +  0.0000  15.3509
     32      0.9958        0.0835       0.8075      0.8923        [94m0.1955[0m     +  0.0000  15.5471
     33      0.9944        0.0833       0.8069      0.8921        0.1956        0.0000  15.3969
     34      0.9960        0.0841       0.8071      0.8923        0.1959        0.0000  15.4231
     35      0.9954        0.0831       0.8059      0.8917        0.1961        0.0000  15.2525
     36      0.9950        0.0832       0.8068      0.8921        0.1959        0.0000  15.3410
     37      0.9955        0.0831       0.8069      0.8921        0.1956        0.0000  15.2979
     38      0.9961        0.0831       0.8069      0.8921        0.1956        0.0000  15.2911
     39      0.9950        [32m0.0827[0m       0.8069      0.8921        0.1956        0.0000  15.3326
     40      0.9950        0.0830       0.8069      0.8922        0.1956        0.0000  15.3906
     41      0.9958        0.0849       0.8069      0.8922        0.1956        0.0000  15.4667
     42      0.9955        0.0840       0.8073      0.8924        0.1955        0.0000  15.3790
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.9211110260922795
F1 Macro Score after query 7: 0.9124488067762991
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_6.pt

Iteration: 8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9695[0m        [32m0.1131[0m       [35m0.8097[0m      [31m0.8753[0m        [94m0.2060[0m     +  0.0000  19.2345
      2      [36m0.9762[0m        [32m0.0994[0m       0.8089      [31m0.8859[0m        [94m0.1926[0m     +  0.0000  19.2329
      3      [36m0.9818[0m        [32m0.0891[0m       0.7865      0.8765        0.2020        0.0000  19.0025
      4      [36m0.9839[0m        [32m0.0850[0m       0.7797      0.8739        0.2086        0.0000  19.1738
      5      [36m0.9868[0m        [32m0.0789[0m       0.7898      0.8689        0.2087        0.0000  19.2813
      6      [36m0.9917[0m        [32m0.0715[0m       0.7840      0.8826        0.1982        0.0000  19.2920
      7      [36m0.9924[0m        [32m0.0686[0m       0.7896      [31m0.8866[0m        0.2000        0.0000  19.2532
      8      0.9922        [32m0.0663[0m       0.7955      [31m0.8883[0m        0.1975        0.0000  19.3295
      9      [36m0.9947[0m        [32m0.0652[0m       0.7833      0.8818        0.2005        0.0000  19.1322
     10      0.9940        [32m0.0626[0m       0.7903      0.8816        0.1979        0.0000  19.2922
     11      [36m0.9953[0m        [32m0.0587[0m       0.8005      0.8880        [94m0.1899[0m     +  0.0000  19.2051
     12      [36m0.9955[0m        [32m0.0584[0m       0.7958      0.8865        0.1942        0.0000  19.2486
     13      0.9955        [32m0.0583[0m       0.7955      0.8851        0.1926        0.0000  19.2641
     14      [36m0.9957[0m        [32m0.0568[0m       0.7969      0.8868        0.1933        0.0000  19.3672
     15      [36m0.9960[0m        [32m0.0566[0m       0.8007      0.8874        0.1899        0.0000  19.2878
     16      [36m0.9962[0m        [32m0.0551[0m       0.8040      [31m0.8908[0m        0.1900        0.0000  19.0622
     17      [36m0.9965[0m        [32m0.0551[0m       0.8038      0.8903        [94m0.1890[0m     +  0.0000  18.3449
     18      0.9963        0.0554       0.8073      [31m0.8924[0m        [94m0.1883[0m     +  0.0000  18.2820
     19      [36m0.9970[0m        [32m0.0548[0m       0.8057      0.8921        0.1896        0.0000  18.2796
     20      0.9969        [32m0.0539[0m       0.8068      0.8917        [94m0.1877[0m     +  0.0000  18.3233
     21      0.9969        0.0542       [35m0.8120[0m      [31m0.8940[0m        [94m0.1861[0m     +  0.0000  18.2671
     22      [36m0.9972[0m        [32m0.0529[0m       [35m0.8127[0m      0.8939        [94m0.1853[0m     +  0.0000  18.2763
     23      0.9970        0.0535       0.8120      [31m0.8944[0m        0.1861        0.0000  18.2648
     24      0.9964        0.0530       0.8120      0.8943        0.1864        0.0000  18.2668
     25      0.9970        [32m0.0528[0m       0.8113      0.8934        0.1853        0.0000  18.2616
     26      0.9970        0.0528       [35m0.8146[0m      [31m0.8950[0m        [94m0.1844[0m     +  0.0000  18.2860
     27      0.9972        0.0528       0.8137      0.8948        0.1849        0.0000  18.3581
     28      0.9967        0.0536       0.8130      0.8948        0.1856        0.0000  18.2327
     29      0.9968        0.0542       0.8134      0.8948        0.1854        0.0000  18.3459
     30      0.9971        [32m0.0524[0m       0.8144      [31m0.8952[0m        0.1849        0.0000  18.2787
     31      0.9971        0.0525       0.8146      0.8951        0.1845        0.0000  18.2938
     32      0.9967        0.0532       0.8135      0.8947        0.1852        0.0000  18.3554
     33      0.9969        0.0529       0.8139      0.8949        0.1851        0.0000  18.2840
     34      0.9969        0.0526       0.8146      [31m0.8952[0m        0.1848        0.0000  18.2457
     35      0.9971        0.0525       [35m0.8151[0m      [31m0.8955[0m        0.1845        0.0000  18.2940
     36      [36m0.9976[0m        0.0529       0.8146      0.8951        0.1844        0.0000  18.2493
     37      0.9969        [32m0.0517[0m       0.8146      0.8950        [94m0.1844[0m     +  0.0000  18.2628
     38      0.9973        0.0522       0.8148      0.8950        [94m0.1843[0m     +  0.0000  18.2670
     39      0.9968        0.0524       [35m0.8155[0m      0.8953        [94m0.1842[0m     +  0.0000  18.1718
     40      [36m0.9977[0m        0.0522       0.8149      0.8951        0.1843        0.0000  18.2643
     41      0.9972        0.0526       0.8148      0.8950        0.1844        0.0000  18.2769
     42      0.9967        0.0524       0.8149      0.8951        0.1844        0.0000  18.2236
     43      0.9973        0.0528       0.8148      0.8950        0.1844        0.0000  18.1325
     44      0.9968        0.0526       0.8149      0.8952        0.1844        0.0000  18.2830
     45      0.9968        0.0517       0.8148      0.8952        0.1845        0.0000  18.2365
     46      0.9967        0.0536       0.8149      0.8953        0.1845        0.0000  18.2980
     47      0.9972        0.0524       0.8149      0.8953        0.1845        0.0000  18.2952
     48      0.9973        0.0518       0.8149      0.8953        0.1845        0.0000  18.2379
     49      0.9968        0.0525       0.8149      0.8953        0.1845        0.0000  18.2522
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.9218580083890011
F1 Macro Score after query 8: 0.9132729521351534
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_7.pt

Iteration: 9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9735[0m        [32m0.0826[0m       [35m0.8165[0m      [31m0.8976[0m        [94m0.1739[0m     +  0.0000  24.8154
      2      [36m0.9804[0m        [32m0.0699[0m       0.7977      0.8919        0.1950        0.0000  24.9179
      3      [36m0.9844[0m        [32m0.0625[0m       0.8064      0.8910        0.1885        0.0000  24.9342
      4      [36m0.9867[0m        [32m0.0568[0m       0.7939      0.8820        0.1930        0.0000  24.9021
      5      0.9861        [32m0.0529[0m       0.8069      0.8888        0.1838        0.0000  24.8778
      6      [36m0.9914[0m        [32m0.0453[0m       [35m0.8238[0m      0.8948        0.1801        0.0000  24.9503
      7      [36m0.9942[0m        [32m0.0424[0m       0.8123      0.8925        0.1861        0.0000  24.9195
      8      [36m0.9947[0m        [32m0.0400[0m       0.8151      0.8935        0.1884        0.0000  24.9409
      9      [36m0.9960[0m        [32m0.0385[0m       0.8179      0.8927        0.1877        0.0000  24.8949
     10      [36m0.9960[0m        [32m0.0366[0m       [35m0.8250[0m      0.8975        0.1838        0.0000  24.9237
     11      [36m0.9968[0m        [32m0.0349[0m       0.8203      0.8928        0.1903        0.0000  24.8893
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.9242799628367915
F1 Macro Score after query 9: 0.912348165236097
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_8.pt

Iteration: 10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9698[0m        [32m0.0813[0m       [35m0.8087[0m      [31m0.8935[0m        [94m0.1772[0m     +  0.0000  37.4716
      2      [36m0.9763[0m        [32m0.0703[0m       [35m0.8177[0m      [31m0.8968[0m        0.1826        0.0000  38.2700
      3      [36m0.9795[0m        [32m0.0626[0m       0.8149      0.8919        0.1782        0.0000  38.2970
      4      [36m0.9827[0m        [32m0.0566[0m       [35m0.8226[0m      0.8958        0.1785        0.0000  37.9995
      5      [36m0.9848[0m        [32m0.0508[0m       [35m0.8313[0m      0.8949        [94m0.1709[0m     +  0.0000  37.8436
      6      [36m0.9890[0m        [32m0.0437[0m       0.8215      0.8868        0.1799        0.0000  37.8278
      7      [36m0.9900[0m        [32m0.0408[0m       0.8170      0.8821        0.1826        0.0000  37.6877
      8      [36m0.9916[0m        [32m0.0385[0m       0.8198      0.8825        0.1851        0.0000  37.8835
      9      [36m0.9928[0m        [32m0.0358[0m       0.8220      0.8852        0.1855        0.0000  37.7687
     10      [36m0.9939[0m        [32m0.0336[0m       0.8243      0.8858        0.1875        0.0000  37.8597
     11      [36m0.9947[0m        [32m0.0317[0m       0.8280      0.8899        0.1845        0.0000  37.8543
     12      [36m0.9958[0m        [32m0.0301[0m       0.8271      0.8899        0.1882        0.0000  38.4463
     13      [36m0.9959[0m        [32m0.0295[0m       0.8293      0.8902        0.1860        0.0000  38.5343
     14      [36m0.9962[0m        [32m0.0286[0m       0.8297      0.8888        0.1867        0.0000  38.5732
     15      0.9961        [32m0.0278[0m       0.8253      0.8905        0.1905        0.0000  38.7306
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.923363904865537
F1 Macro Score after query 10: 0.9109507867781752
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_9.pt

Iteration: 11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9794[0m        [32m0.0563[0m       [35m0.8425[0m      [31m0.8909[0m        [94m0.1608[0m     +  0.0000  60.0001
      2      [36m0.9815[0m        [32m0.0491[0m       0.8345      [31m0.8974[0m        [94m0.1582[0m     +  0.0000  60.0738
      3      [36m0.9840[0m        [32m0.0434[0m       0.8264      0.8901        0.1720        0.0000  60.0714
      4      [36m0.9872[0m        [32m0.0383[0m       0.8273      0.8919        0.1748        0.0000  60.3124
      5      [36m0.9884[0m        [32m0.0348[0m       0.8267      0.8918        0.1850        0.0000  60.1405
      6      [36m0.9909[0m        [32m0.0298[0m       0.8391      0.8945        0.1800        0.0000  60.4789
      7      [36m0.9927[0m        [32m0.0266[0m       0.8396      0.8957        0.1804        0.0000  60.0626
      8      [36m0.9932[0m        [32m0.0246[0m       0.8413      0.8950        0.1845        0.0000  60.2432
      9      [36m0.9942[0m        [32m0.0231[0m       0.8420      0.8965        0.1874        0.0000  60.3471
     10      [36m0.9950[0m        [32m0.0211[0m       [35m0.8429[0m      0.8958        0.1925        0.0000  60.5235
     11      [36m0.9954[0m        [32m0.0202[0m       [35m0.8434[0m      [31m0.8981[0m        0.1891        0.0000  60.5996
     12      [36m0.9966[0m        [32m0.0184[0m       [35m0.8438[0m      [31m0.8993[0m        0.1925        0.0000  60.0243
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.9263420724094881
F1 Macro Score after query 11: 0.9136547348757516
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_10.pt

Iteration: 12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9779[0m        [32m0.0515[0m       [35m0.8212[0m      [31m0.8913[0m        [94m0.1798[0m     +  0.0000  99.3032
      2      [36m0.9804[0m        [32m0.0450[0m       0.8160      [31m0.8913[0m        0.1830        0.0000  98.7483
      3      [36m0.9836[0m        [32m0.0393[0m       0.8109      0.8896        0.2000        0.0000  98.6968
      4      [36m0.9848[0m        [32m0.0352[0m       0.8116      0.8901        0.2108        0.0000  98.6817
      5      [36m0.9866[0m        [32m0.0316[0m       0.8170      [31m0.8929[0m        0.2092        0.0000  98.3198
      6      [36m0.9897[0m        [32m0.0258[0m       0.8168      [31m0.8941[0m        0.2051        0.0000  98.7757
      7      [36m0.9911[0m        [32m0.0230[0m       0.8113      0.8912        0.2164        0.0000  98.8020
      8      [36m0.9927[0m        [32m0.0206[0m       0.8128      0.8913        0.2208        0.0000  98.5691
      9      [36m0.9933[0m        [32m0.0191[0m       0.8156      0.8921        0.2247        0.0000  98.7695
     10      [36m0.9939[0m        [32m0.0177[0m       0.8076      0.8875        0.2479        0.0000  98.6187
     11      [36m0.9957[0m        [32m0.0153[0m       [35m0.8262[0m      [31m0.8953[0m        0.2176        0.0000  98.4715
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.9129353233830846
F1 Macro Score after query 12: 0.9010254523512758
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_11.pt

Iteration: 13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9788[0m        [32m0.0475[0m       [35m0.8512[0m      [31m0.9079[0m        [94m0.1354[0m     +  0.0000  113.1919
      2      [36m0.9812[0m        [32m0.0415[0m       0.8358      0.8998        0.1559        0.0000  114.2253
      3      [36m0.9837[0m        [32m0.0371[0m       0.8347      0.9010        0.1607        0.0000  114.4209
      4      [36m0.9860[0m        [32m0.0330[0m       0.8073      0.8873        0.2064        0.0000  114.1008
      5      [36m0.9872[0m        [32m0.0296[0m       0.7943      0.8822        0.2306        0.0000  114.7779
      6      [36m0.9896[0m        [32m0.0244[0m       0.8156      0.8925        0.2143        0.0000  114.9507
      7      [36m0.9915[0m        [32m0.0215[0m       0.8167      0.8932        0.2165        0.0000  115.0378
      8      [36m0.9927[0m        [32m0.0193[0m       0.8193      0.8955        0.2207        0.0000  114.8929
      9      [36m0.9933[0m        [32m0.0178[0m       0.8259      0.8978        0.2155        0.0000  114.3790
     10      [36m0.9947[0m        [32m0.0159[0m       0.8264      0.8993        0.2311        0.0000  114.8399
     11      [36m0.9953[0m        [32m0.0144[0m       0.8328      0.8989        0.2268        0.0000  115.0770
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.9176199616122841
F1 Macro Score after query 13: 0.9071423430006679
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\model_checkpoint_iteration_12.pt
Pickle file saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed44_lowLR\AL_average_confidence_results_for_multilabel_classification_s44.pickle
