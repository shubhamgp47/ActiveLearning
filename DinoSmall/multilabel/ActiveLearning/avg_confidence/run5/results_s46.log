Adjusted shapes after loading:
X_initial_np: (8, 224, 224, 3), y_initial_np: (8, 3)
X_pool_np: (26872, 224, 224, 3), y_pool_np: (26872, 3)
X_test_np: (5760, 224, 224, 3), y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3), y_val_np: (5760, 3)
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.5556[0m        [32m0.6869[0m       [35m0.1859[0m      [31m0.5403[0m        [94m0.6979[0m     +  0.0000  12.6188
      2      [36m0.6833[0m        [32m0.6858[0m       0.1476      0.4470        [94m0.6882[0m     +  0.0000  10.3406
      3      0.6722        [32m0.6446[0m       [35m0.1981[0m      0.4584        0.6915        0.0000  10.2654
      4      [36m0.8350[0m        [32m0.6036[0m       [35m0.2899[0m      0.4102        [94m0.6826[0m     +  0.0000  10.6517
      5      0.7222        0.6210       0.2344      0.4254        0.6899        0.0000  10.7502
      6      0.8350        [32m0.5965[0m       0.2651      0.4395        0.6858        0.0000  10.9375
      7      0.8148        [32m0.5547[0m       0.2773      0.4432        0.6834        0.0000  10.4065
      8      0.7593        [32m0.5228[0m       0.2642      0.4369        0.6838        0.0000  11.2820
      9      [36m0.9630[0m        0.5230       0.2616      0.4589        0.6835        0.0000  10.4039
     10      0.9259        [32m0.4998[0m       0.2655      0.4598        [94m0.6817[0m     +  0.0000  10.7496
     11      0.8426        0.5413       0.2724      0.4593        [94m0.6814[0m     +  0.0000  11.0310
     12      0.9153        0.5251       0.2658      0.4609        0.6814        0.0000  10.6408
     13      0.9153        0.5339       0.2634      0.4553        0.6827        0.0000  10.3920
     14      0.9259        0.5187       0.2644      0.4610        0.6825        0.0000  10.7175
     15      0.9630        [32m0.4953[0m       0.2726      0.4624        0.6815        0.0000  10.6095
     16      0.9333        [32m0.4932[0m       0.2736      0.4620        [94m0.6811[0m     +  0.0000  10.4247
     17      0.9630        [32m0.4929[0m       0.2734      0.4633        0.6813        0.0000  10.9074
     18      0.8487        0.5119       0.2719      0.4635        0.6816        0.0000  10.4197
     19      0.9630        0.4939       0.2708      0.4634        0.6817        0.0000  10.6905
     20      0.9630        [32m0.4892[0m       0.2694      0.4647        0.6819        0.0000  10.4833
     21      0.9167        0.5014       0.2681      0.4648        0.6820        0.0000  10.3651
     22      0.9630        [32m0.4857[0m       0.2677      0.4650        0.6819        0.0000  10.8082
     23      0.9259        0.4931       0.2681      0.4656        0.6819        0.0000  10.4221
     24      0.9630        0.5094       0.2682      0.4662        0.6819        0.0000  10.6304
     25      0.9630        0.4895       0.2684      0.4665        0.6819        0.0000  10.6723
     26      0.9259        [32m0.4809[0m       0.2684      0.4665        0.6819        0.0000  10.4512
Stopping since valid_loss has not improved in the last 11 epochs.
Pre F1 micro score = 0.5200
Pre F1 macro score = 0.5085
Pre Accuracy = 0.3510

Iteration: 1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.7330[0m        [32m0.5829[0m       [35m0.2332[0m      [31m0.3001[0m        [94m0.6307[0m     +  0.0000  10.5637
      2      0.4669        0.6105       [35m0.3000[0m      0.2851        [94m0.6302[0m     +  0.0000  10.9825
      3      0.5841        [32m0.5500[0m       [35m0.3111[0m      0.2811        [94m0.6287[0m     +  0.0000  10.6213
      4      0.5873        [32m0.5242[0m       [35m0.3156[0m      0.2889        [94m0.6254[0m     +  0.0000  10.6456
      5      0.6259        [32m0.4973[0m       0.3028      [31m0.3235[0m        [94m0.6238[0m     +  0.0000  10.9702
      6      [36m0.7371[0m        [32m0.4728[0m       0.3095      [31m0.3530[0m        [94m0.6237[0m     +  0.0000  10.7991
      7      0.7004        0.4801       0.3078      [31m0.3635[0m        [94m0.6232[0m     +  0.0000  11.0198
      8      [36m0.8407[0m        [32m0.4580[0m       0.3082      [31m0.3685[0m        [94m0.6229[0m     +  0.0000  10.6889
      9      0.8121        [32m0.4441[0m       0.3095      0.3676        [94m0.6211[0m     +  0.0000  10.6888
     10      0.7912        0.4584       0.3116      [31m0.3769[0m        [94m0.6202[0m     +  0.0000  10.7502
     11      0.8156        0.4546       0.3132      0.3767        [94m0.6198[0m     +  0.0000  10.8026
     12      0.8158        [32m0.4403[0m       0.3116      [31m0.3792[0m        0.6200        0.0000  10.8023
     13      [36m0.8450[0m        0.4503       0.3127      [31m0.3799[0m        [94m0.6198[0m     +  0.0000  10.5461
     14      0.8410        [32m0.4296[0m       0.3125      [31m0.3813[0m        [94m0.6197[0m     +  0.0000  10.7119
     15      0.8241        0.4395       0.3139      [31m0.3822[0m        [94m0.6190[0m     +  0.0000  10.6785
     16      0.8313        0.4369       0.3135      0.3815        [94m0.6188[0m     +  0.0000  10.4372
     17      [36m0.8528[0m        0.4310       0.3130      0.3820        0.6189        0.0000  10.7802
     18      0.8407        [32m0.4246[0m       0.3132      0.3818        [94m0.6187[0m     +  0.0000  10.4401
     19      0.8279        0.4271       0.3127      0.3817        [94m0.6185[0m     +  0.0000  10.9073
     20      [36m0.8880[0m        [32m0.4216[0m       0.3127      0.3820        0.6186        0.0000  10.7220
     21      0.8682        0.4218       0.3132      [31m0.3830[0m        0.6186        0.0000  10.8770
     22      0.8528        0.4444       0.3132      0.3830        0.6186        0.0000  11.2836
     23      0.8638        0.4280       0.3128      0.3826        [94m0.6185[0m     +  0.0000  10.7115
     24      0.8830        0.4249       0.3127      0.3825        [94m0.6185[0m     +  0.0000  10.7716
     25      0.8758        0.4254       0.3128      0.3827        0.6185        0.0000  10.5465
     26      0.8313        0.4285       0.3128      0.3827        0.6185        0.0000  10.5652
     27      0.8218        0.4295       0.3128      0.3828        0.6185        0.0000  10.7148
     28      0.7357        0.4277       0.3130      0.3828        0.6185        0.0000  10.5623
     29      0.8444        0.4407       0.3130      0.3828        [94m0.6185[0m     +  0.0000  10.9584
     30      0.7928        0.4513       0.3130      0.3828        [94m0.6184[0m     +  0.0000  10.8454
     31      0.7886        0.4473       0.3130      0.3827        0.6184        0.0000  11.0003
     32      0.8158        0.4347       0.3132      0.3828        [94m0.6184[0m     +  0.0000  10.5630
     33      0.8569        0.4317       0.3132      0.3828        [94m0.6184[0m     +  0.0000  10.9531
     34      0.8528        0.4445       0.3132      0.3828        [94m0.6184[0m     +  0.0000  10.4845
     35      0.7528        0.4415       0.3132      0.3828        [94m0.6184[0m     +  0.0000  10.8395
     36      0.8407        0.4391       0.3132      0.3828        [94m0.6184[0m     +  0.0000  10.4358
     37      0.8818        [32m0.4167[0m       0.3132      0.3828        [94m0.6184[0m     +  0.0000  10.5785
     38      0.8241        0.4211       0.3132      0.3828        0.6184        0.0000  10.7200
     39      0.8013        0.4198       0.3132      0.3828        [94m0.6184[0m     +  0.0000  10.7144
     40      0.7723        0.4246       0.3134      0.3828        [94m0.6183[0m     +  0.0000  10.4174
     41      0.8491        0.4371       0.3134      0.3828        [94m0.6183[0m     +  0.0000  10.7390
     42      0.7928        0.4354       0.3134      0.3828        [94m0.6183[0m     +  0.0000  10.7202
Stopping since valid_loss has not improved in the last 11 epochs.
[24]
F1 Micro Score after query 1: 0.4303508771929824
F1 Macro Score after query 1: 0.3592341515867667
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_0.pt

Iteration: 2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.5377[0m        [32m0.5779[0m       [35m0.4422[0m      [31m0.2726[0m        [94m0.5640[0m     +  0.0000  10.5782
      2      [36m0.6634[0m        [32m0.5018[0m       [35m0.5181[0m      [31m0.5676[0m        [94m0.5133[0m     +  0.0000  10.6560
      3      [36m0.8364[0m        [32m0.4467[0m       [35m0.5877[0m      [31m0.6955[0m        [94m0.4772[0m     +  0.0000  10.9199
      4      0.8335        [32m0.4234[0m       [35m0.6335[0m      [31m0.7836[0m        [94m0.4529[0m     +  0.0000  10.7300
      5      [36m0.8968[0m        [32m0.3951[0m       [35m0.6771[0m      [31m0.8227[0m        [94m0.4362[0m     +  0.0000  10.6754
      6      0.8815        [32m0.3850[0m       0.6651      0.8181        [94m0.4313[0m     +  0.0000  10.9359
      7      0.8757        [32m0.3733[0m       0.6682      [31m0.8232[0m        [94m0.4268[0m     +  0.0000  10.9321
      8      [36m0.9024[0m        [32m0.3704[0m       0.6736      [31m0.8274[0m        [94m0.4224[0m     +  0.0000  10.5583
      9      [36m0.9192[0m        [32m0.3536[0m       [35m0.6887[0m      [31m0.8334[0m        [94m0.4183[0m     +  0.0000  10.7871
     10      [36m0.9336[0m        [32m0.3427[0m       0.6844      0.8326        [94m0.4146[0m     +  0.0000  11.1170
     11      0.9199        [32m0.3367[0m       0.6818      0.8282        [94m0.4132[0m     +  0.0000  10.5969
     12      0.9219        0.3413       0.6821      0.8277        [94m0.4122[0m     +  0.0000  10.6225
     13      0.9111        0.3389       0.6840      0.8283        [94m0.4105[0m     +  0.0000  10.8902
     14      0.9219        [32m0.3266[0m       [35m0.6891[0m      0.8314        [94m0.4091[0m     +  0.0000  10.8515
     15      [36m0.9417[0m        0.3377       [35m0.6901[0m      0.8325        [94m0.4079[0m     +  0.0000  10.6945
     16      0.9357        0.3313       0.6894      0.8318        [94m0.4078[0m     +  0.0000  10.6576
     17      0.9156        0.3340       0.6894      0.8327        [94m0.4075[0m     +  0.0000  10.9636
     18      0.9417        [32m0.3263[0m       0.6880      0.8310        [94m0.4072[0m     +  0.0000  10.6511
     19      0.9417        0.3302       [35m0.6910[0m      0.8314        [94m0.4070[0m     +  0.0000  10.6252
     20      0.9417        0.3281       0.6898      0.8307        [94m0.4066[0m     +  0.0000  10.9998
     21      0.9199        0.3314       0.6887      0.8312        [94m0.4064[0m     +  0.0000  11.0303
     22      [36m0.9441[0m        0.3338       0.6882      0.8312        [94m0.4062[0m     +  0.0000  11.0146
     23      0.9313        0.3297       0.6892      0.8317        [94m0.4060[0m     +  0.0000  10.6765
     24      0.9299        0.3371       0.6905      0.8329        [94m0.4058[0m     +  0.0000  10.9411
     25      0.9388        [32m0.3199[0m       [35m0.6911[0m      0.8333        [94m0.4056[0m     +  0.0000  11.1692
     26      0.9363        0.3434       0.6911      [31m0.8335[0m        [94m0.4056[0m     +  0.0000  10.8759
     27      0.9414        0.3412       [35m0.6920[0m      [31m0.8337[0m        [94m0.4055[0m     +  0.0000  10.5994
     28      0.9414        0.3306       0.6911      0.8331        [94m0.4054[0m     +  0.0000  10.7824
     29      0.9306        0.3250       0.6899      0.8327        [94m0.4054[0m     +  0.0000  10.6370
     30      0.9192        0.3410       0.6910      0.8330        [94m0.4054[0m     +  0.0000  10.6685
     31      0.9368        0.3257       0.6910      0.8330        [94m0.4053[0m     +  0.0000  11.0161
     32      0.9357        [32m0.3166[0m       0.6910      0.8330        [94m0.4053[0m     +  0.0000  10.6582
     33      0.9377        0.3320       0.6910      0.8331        [94m0.4053[0m     +  0.0000  10.5803
     34      0.9319        0.3275       0.6913      0.8334        [94m0.4052[0m     +  0.0000  10.5758
     35      0.9319        0.3367       0.6915      0.8335        [94m0.4052[0m     +  0.0000  10.6120
     36      0.9357        0.3270       0.6913      0.8334        [94m0.4052[0m     +  0.0000  10.6850
     37      [36m0.9485[0m        [32m0.3112[0m       0.6910      0.8331        [94m0.4052[0m     +  0.0000  10.8806
     38      0.9417        0.3174       0.6910      0.8331        [94m0.4052[0m     +  0.0000  10.7658
     39      0.9468        0.3221       0.6910      0.8331        [94m0.4052[0m     +  0.0000  10.6835
     40      0.9016        0.3320       0.6911      0.8331        [94m0.4052[0m     +  0.0000  10.6434
     41      0.9161        0.3289       0.6911      0.8331        [94m0.4052[0m     +  0.0000  10.6591
     42      0.9257        0.3366       0.6911      0.8331        [94m0.4052[0m     +  0.0000  11.1398
     43      0.9414        0.3221       0.6911      0.8331        [94m0.4052[0m     +  0.0000  10.8094
     44      0.9299        0.3322       0.6910      0.8330        [94m0.4052[0m     +  0.0000  10.6026
     45      0.9248        0.3278       0.6910      0.8330        [94m0.4052[0m     +  0.0000  10.6450
     46      0.9467        0.3284       0.6910      0.8330        [94m0.4052[0m     +  0.0000  10.5976
     47      0.9219        0.3290       0.6910      0.8330        [94m0.4052[0m     +  0.0000  10.7707
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56]
F1 Micro Score after query 2: 0.8404607858608173
F1 Macro Score after query 2: 0.8153704402049401
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_1.pt

Iteration: 3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8567[0m        [32m0.3952[0m       [35m0.7214[0m      [31m0.8539[0m        [94m0.3775[0m     +  0.0000  11.5613
      2      [36m0.8937[0m        [32m0.3384[0m       [35m0.7318[0m      0.8533        [94m0.3617[0m     +  0.0000  10.9244
      3      [36m0.8940[0m        [32m0.3137[0m       0.7302      0.8535        [94m0.3528[0m     +  0.0000  10.8044
      4      [36m0.9175[0m        [32m0.2924[0m       0.7231      0.8479        [94m0.3507[0m     +  0.0000  11.2881
      5      0.9035        [32m0.2913[0m       0.7276      0.8520        [94m0.3406[0m     +  0.0000  11.1492
      6      [36m0.9480[0m        [32m0.2672[0m       [35m0.7460[0m      [31m0.8686[0m        [94m0.3281[0m     +  0.0000  10.9310
      7      [36m0.9500[0m        [32m0.2665[0m       [35m0.7476[0m      [31m0.8706[0m        [94m0.3247[0m     +  0.0000  10.9596
      8      0.9366        [32m0.2628[0m       [35m0.7481[0m      [31m0.8713[0m        [94m0.3236[0m     +  0.0000  11.0231
      9      0.9325        [32m0.2574[0m       [35m0.7533[0m      [31m0.8728[0m        [94m0.3197[0m     +  0.0000  11.3067
     10      [36m0.9527[0m        0.2645       0.7531      [31m0.8733[0m        [94m0.3195[0m     +  0.0000  10.9917
     11      0.9480        0.2613       [35m0.7562[0m      [31m0.8746[0m        [94m0.3174[0m     +  0.0000  10.9913
     12      0.9520        [32m0.2557[0m       [35m0.7573[0m      [31m0.8749[0m        [94m0.3167[0m     +  0.0000  10.8078
     13      [36m0.9689[0m        0.2562       [35m0.7582[0m      [31m0.8751[0m        [94m0.3160[0m     +  0.0000  11.0535
     14      0.9568        [32m0.2521[0m       [35m0.7604[0m      [31m0.8761[0m        0.3162        0.0000  10.9762
     15      0.9616        [32m0.2489[0m       0.7589      0.8755        [94m0.3156[0m     +  0.0000  11.3662
     16      0.9589        [32m0.2402[0m       0.7576      0.8751        [94m0.3149[0m     +  0.0000  11.0421
     17      0.9467        0.2486       0.7599      0.8758        [94m0.3149[0m     +  0.0000  11.1946
     18      0.9672        0.2509       0.7594      0.8758        [94m0.3143[0m     +  0.0000  10.8499
     19      [36m0.9701[0m        0.2423       [35m0.7613[0m      [31m0.8766[0m        0.3144        0.0000  11.0690
     20      0.9521        0.2465       0.7613      [31m0.8768[0m        [94m0.3141[0m     +  0.0000  11.4902
     21      0.9663        0.2416       0.7599      0.8761        [94m0.3138[0m     +  0.0000  11.2269
     22      0.9667        [32m0.2381[0m       0.7604      0.8763        [94m0.3136[0m     +  0.0000  10.8501
     23      0.9620        0.2560       0.7608      0.8764        0.3137        0.0000  10.9915
     24      [36m0.9720[0m        0.2402       0.7604      0.8762        0.3137        0.0000  11.1638
     25      0.9618        0.2452       [35m0.7615[0m      0.8767        [94m0.3136[0m     +  0.0000  11.2093
     26      0.9686        [32m0.2378[0m       0.7613      0.8767        [94m0.3134[0m     +  0.0000  10.8403
     27      0.9568        0.2451       0.7609      0.8764        [94m0.3133[0m     +  0.0000  10.9451
     28      0.9629        0.2415       0.7609      0.8764        [94m0.3133[0m     +  0.0000  10.9273
     29      0.9597        0.2509       0.7613      0.8767        [94m0.3132[0m     +  0.0000  11.1787
     30      0.9624        0.2425       [35m0.7616[0m      [31m0.8768[0m        0.3132        0.0000  11.8978
     31      0.9720        0.2420       [35m0.7618[0m      [31m0.8769[0m        [94m0.3132[0m     +  0.0000  11.3178
     32      0.9535        0.2470       0.7613      0.8767        [94m0.3132[0m     +  0.0000  10.9132
     33      0.9575        0.2424       0.7613      0.8767        [94m0.3132[0m     +  0.0000  10.9287
     34      [36m0.9724[0m        [32m0.2377[0m       0.7615      0.8768        [94m0.3131[0m     +  0.0000  10.9603
     35      0.9624        0.2451       0.7615      0.8768        [94m0.3131[0m     +  0.0000  10.9280
     36      0.9631        [32m0.2368[0m       0.7615      0.8768        [94m0.3131[0m     +  0.0000  11.1160
     37      0.9595        0.2453       0.7615      0.8768        0.3131        0.0000  11.0383
     38      0.9546        0.2460       0.7615      0.8768        [94m0.3131[0m     +  0.0000  11.1157
     39      0.9649        0.2422       0.7615      0.8768        [94m0.3131[0m     +  0.0000  11.0863
     40      0.9689        [32m0.2358[0m       0.7615      0.8768        0.3131        0.0000  11.0692
     41      0.9560        0.2439       0.7615      0.8768        [94m0.3131[0m     +  0.0000  11.0847
     42      0.9631        0.2492       0.7615      0.8768        [94m0.3131[0m     +  0.0000  11.4452
     43      0.9636        0.2436       0.7615      0.8768        [94m0.3131[0m     +  0.0000  11.4295
     44      0.9557        0.2484       0.7616      0.8769        [94m0.3131[0m     +  0.0000  11.0472
     45      0.9499        0.2583       0.7616      0.8769        [94m0.3131[0m     +  0.0000  10.8507
     46      0.9720        0.2386       0.7616      0.8769        [94m0.3131[0m     +  0.0000  10.8661
     47      0.9689        0.2365       0.7616      0.8769        [94m0.3131[0m     +  0.0000  10.9131
     48      0.9687        0.2426       0.7616      0.8769        [94m0.3131[0m     +  0.0000  10.9130
     49      0.9705        0.2413       0.7616      0.8769        [94m0.3130[0m     +  0.0000  11.0997
     50      0.9597        0.2469       0.7616      0.8769        [94m0.3130[0m     +  0.0000  10.8509
     51      [36m0.9764[0m        0.2413       0.7616      0.8769        [94m0.3130[0m     +  0.0000  11.1629
     52      0.9602        0.2477       0.7616      0.8769        [94m0.3130[0m     +  0.0000  11.5693
     53      0.9764        0.2396       0.7616      0.8769        [94m0.3130[0m     +  0.0000  10.7577
     54      0.9680        0.2410       0.7616      0.8769        [94m0.3130[0m     +  0.0000  11.0223
     55      0.9724        0.2416       0.7616      0.8769        [94m0.3130[0m     +  0.0000  10.9426
     56      0.9645        0.2503       0.7616      0.8769        0.3130        0.0000  10.9436
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112]
F1 Micro Score after query 3: 0.8682275251388264
F1 Macro Score after query 3: 0.8462576972231185
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_2.pt

Iteration: 4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8813[0m        [32m0.2990[0m       [35m0.7227[0m      [31m0.8125[0m        [94m0.3085[0m     +  0.0000  11.2111
      2      [36m0.9050[0m        [32m0.2634[0m       [35m0.7597[0m      [31m0.8522[0m        [94m0.2906[0m     +  0.0000  11.2087
      3      [36m0.9150[0m        [32m0.2542[0m       0.7472      0.8246        0.2915        0.0000  11.4137
      4      [36m0.9342[0m        [32m0.2382[0m       0.7568      0.8383        [94m0.2845[0m     +  0.0000  11.5519
      5      [36m0.9378[0m        [32m0.2363[0m       [35m0.7674[0m      [31m0.8540[0m        [94m0.2801[0m     +  0.0000  11.3242
      6      0.9369        [32m0.2245[0m       [35m0.7859[0m      [31m0.8846[0m        [94m0.2693[0m     +  0.0000  11.8589
      7      0.9346        [32m0.2207[0m       0.7830      0.8819        [94m0.2675[0m     +  0.0000  11.5469
      8      [36m0.9487[0m        [32m0.2158[0m       [35m0.7889[0m      [31m0.8862[0m        [94m0.2672[0m     +  0.0000  11.2457
      9      [36m0.9521[0m        [32m0.2142[0m       0.7875      0.8854        [94m0.2644[0m     +  0.0000  11.2197
     10      [36m0.9539[0m        [32m0.2061[0m       0.7889      [31m0.8871[0m        [94m0.2630[0m     +  0.0000  11.4053
     11      [36m0.9560[0m        0.2122       0.7852      0.8863        [94m0.2629[0m     +  0.0000  11.1898
     12      [36m0.9628[0m        [32m0.1992[0m       0.7847      0.8868        0.2641        0.0000  11.2360
     13      0.9560        0.2076       0.7842      0.8862        [94m0.2623[0m     +  0.0000  11.1987
     14      [36m0.9648[0m        0.2001       0.7861      0.8862        [94m0.2615[0m     +  0.0000  11.3151
     15      0.9535        0.2016       0.7863      [31m0.8875[0m        0.2615        0.0000  11.2594
     16      [36m0.9694[0m        [32m0.1990[0m       0.7851      0.8871        0.2621        0.0000  11.2214
     17      0.9587        0.1999       0.7852      0.8871        0.2617        0.0000  11.3609
     18      0.9611        0.2014       0.7849      0.8870        0.2618        0.0000  11.3113
     19      0.9666        0.2044       0.7847      0.8869        0.2618        0.0000  11.3131
     20      0.9587        [32m0.1962[0m       0.7837      0.8867        0.2622        0.0000  11.2656
     21      0.9625        0.1972       0.7844      0.8867        0.2618        0.0000  11.4846
     22      0.9595        0.2004       0.7845      0.8869        0.2619        0.0000  11.3904
     23      0.9556        0.1994       0.7847      0.8869        0.2617        0.0000  11.4733
     24      0.9662        [32m0.1956[0m       0.7833      0.8865        0.2616        0.0000  11.4872
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208]
F1 Micro Score after query 4: 0.8972262550622755
F1 Macro Score after query 4: 0.8800208608482918
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_3.pt

Iteration: 5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8969[0m        [32m0.2397[0m       [35m0.7441[0m      [31m0.8102[0m        [94m0.2675[0m     +  0.0000  12.2649
      2      [36m0.9353[0m        [32m0.2081[0m       [35m0.7523[0m      [31m0.8189[0m        0.2691        0.0000  12.1712
      3      0.9343        [32m0.2000[0m       [35m0.7856[0m      [31m0.8584[0m        [94m0.2504[0m     +  0.0000  11.9030
      4      [36m0.9486[0m        [32m0.1885[0m       [35m0.7922[0m      [31m0.8690[0m        [94m0.2474[0m     +  0.0000  12.2007
      5      0.9400        [32m0.1837[0m       [35m0.8068[0m      [31m0.8864[0m        [94m0.2370[0m     +  0.0000  12.1279
      6      [36m0.9590[0m        [32m0.1771[0m       [35m0.8071[0m      [31m0.8938[0m        [94m0.2340[0m     +  0.0000  11.9999
      7      [36m0.9776[0m        [32m0.1730[0m       0.8066      0.8923        [94m0.2334[0m     +  0.0000  12.0016
      8      0.9698        [32m0.1684[0m       0.8052      0.8922        [94m0.2334[0m     +  0.0000  11.9597
      9      0.9735        [32m0.1641[0m       0.8042      0.8914        0.2335        0.0000  12.4111
     10      0.9753        [32m0.1611[0m       [35m0.8123[0m      [31m0.8946[0m        [94m0.2300[0m     +  0.0000  12.2657
     11      [36m0.9816[0m        [32m0.1571[0m       0.8068      0.8923        0.2310        0.0000  12.5291
     12      [36m0.9816[0m        0.1588       0.8052      0.8914        0.2315        0.0000  11.8853
     13      0.9753        [32m0.1570[0m       0.8071      0.8922        0.2311        0.0000  11.9260
     14      [36m0.9838[0m        [32m0.1555[0m       0.8085      0.8926        0.2304        0.0000  12.1876
     15      0.9791        0.1569       0.8064      0.8919        0.2303        0.0000  11.9056
     16      0.9825        [32m0.1550[0m       0.8113      0.8941        [94m0.2293[0m     +  0.0000  12.1525
     17      0.9811        [32m0.1538[0m       0.8122      0.8944        0.2295        0.0000  12.4950
     18      0.9812        [32m0.1528[0m       0.8109      0.8940        0.2294        0.0000  11.8593
     19      [36m0.9859[0m        0.1547       0.8085      0.8926        0.2302        0.0000  11.9362
     20      0.9850        [32m0.1527[0m       0.8111      0.8942        [94m0.2291[0m     +  0.0000  12.0001
     21      0.9847        0.1558       [35m0.8128[0m      [31m0.8947[0m        [94m0.2287[0m     +  0.0000  11.9145
     22      0.9826        [32m0.1517[0m       0.8123      0.8946        0.2290        0.0000  12.2029
     23      0.9779        0.1534       [35m0.8135[0m      [31m0.8950[0m        0.2289        0.0000  11.9250
     24      0.9814        0.1536       [35m0.8148[0m      [31m0.8953[0m        [94m0.2286[0m     +  0.0000  11.9559
     25      0.9855        0.1548       0.8139      0.8949        [94m0.2286[0m     +  0.0000  12.1688
     26      0.9790        [32m0.1479[0m       0.8141      0.8950        [94m0.2285[0m     +  0.0000  11.8746
     27      0.9817        0.1528       0.8139      0.8949        0.2286        0.0000  12.0350
     28      0.9825        0.1499       0.8135      0.8947        [94m0.2285[0m     +  0.0000  11.9514
     29      0.9824        0.1502       0.8130      0.8945        [94m0.2285[0m     +  0.0000  11.9539
     30      0.9834        0.1551       0.8137      0.8948        [94m0.2284[0m     +  0.0000  11.8576
     31      0.9847        0.1531       0.8141      0.8950        0.2285        0.0000  11.9247
     32      0.9826        0.1518       0.8139      0.8949        0.2285        0.0000  12.0581
     33      0.9789        0.1498       0.8142      0.8950        0.2286        0.0000  12.3747
     34      0.9826        0.1503       0.8141      0.8951        0.2285        0.0000  12.2643
     35      0.9832        0.1546       0.8142      0.8951        0.2285        0.0000  12.0069
     36      0.9846        0.1549       0.8142      0.8951        0.2285        0.0000  11.8873
     37      0.9816        0.1516       0.8142      0.8951        0.2285        0.0000  11.9709
     38      0.9834        0.1494       0.8142      0.8951        0.2285        0.0000  12.0756
     39      0.9826        0.1520       0.8142      0.8951        0.2285        0.0000  12.0783
     40      0.9782        0.1486       0.8142      0.8951        0.2285        0.0000  12.1282
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.9103395061728394
F1 Macro Score after query 5: 0.8938324337806494
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_4.pt

Iteration: 6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9339[0m        [32m0.1944[0m       [35m0.7970[0m      [31m0.8921[0m        [94m0.2264[0m     +  0.0000  13.2541
      2      [36m0.9488[0m        [32m0.1727[0m       [35m0.8010[0m      0.8782        [94m0.2241[0m     +  0.0000  13.4811
      3      [36m0.9621[0m        [32m0.1583[0m       [35m0.8031[0m      0.8874        [94m0.2196[0m     +  0.0000  13.3336
      4      [36m0.9697[0m        [32m0.1558[0m       [35m0.8078[0m      [31m0.8922[0m        [94m0.2152[0m     +  0.0000  13.2424
      5      [36m0.9777[0m        [32m0.1471[0m       0.7936      0.8859        0.2206        0.0000  13.2968
      6      [36m0.9795[0m        [32m0.1395[0m       0.8023      0.8891        [94m0.2147[0m     +  0.0000  13.3353
      7      [36m0.9832[0m        [32m0.1343[0m       0.7979      0.8870        0.2168        0.0000  13.2186
      8      [36m0.9847[0m        [32m0.1329[0m       0.7955      0.8857        0.2188        0.0000  13.3377
      9      0.9847        [32m0.1307[0m       0.7974      0.8863        0.2170        0.0000  13.4685
     10      [36m0.9854[0m        [32m0.1289[0m       0.7981      0.8862        0.2160        0.0000  13.7853
     11      [36m0.9863[0m        [32m0.1253[0m       0.8047      0.8878        [94m0.2142[0m     +  0.0000  13.5666
     12      0.9858        0.1264       0.8030      0.8869        0.2147        0.0000  13.2229
     13      [36m0.9881[0m        [32m0.1232[0m       0.8030      0.8871        [94m0.2142[0m     +  0.0000  13.2062
     14      [36m0.9889[0m        [32m0.1230[0m       0.8003      0.8862        0.2151        0.0000  13.4027
     15      0.9879        [32m0.1214[0m       0.8000      0.8856        0.2154        0.0000  13.3233
     16      0.9875        0.1243       0.8047      0.8869        0.2143        0.0000  13.2189
     17      0.9873        0.1221       0.8050      0.8865        [94m0.2141[0m     +  0.0000  13.4855
     18      0.9882        [32m0.1205[0m       0.8049      0.8864        0.2145        0.0000  13.1599
     19      [36m0.9897[0m        0.1209       0.8040      0.8864        0.2143        0.0000  13.3595
     20      [36m0.9900[0m        [32m0.1182[0m       0.8028      0.8860        0.2147        0.0000  13.4242
     21      0.9888        [32m0.1179[0m       0.8047      0.8869        0.2143        0.0000  13.2312
     22      [36m0.9900[0m        0.1210       0.8038      0.8865        0.2142        0.0000  13.1876
     23      [36m0.9905[0m        0.1203       0.8043      0.8866        0.2141        0.0000  13.1721
     24      0.9895        0.1186       0.8043      0.8868        0.2141        0.0000  13.3016
     25      0.9893        0.1189       0.8043      0.8867        [94m0.2141[0m     +  0.0000  13.2704
     26      0.9898        0.1225       0.8031      0.8864        0.2143        0.0000  13.2769
     27      0.9896        0.1192       0.8036      0.8864        0.2142        0.0000  13.3179
     28      0.9903        0.1182       0.8040      0.8866        0.2143        0.0000  13.0916
     29      [36m0.9912[0m        0.1201       0.8033      0.8863        0.2143        0.0000  13.6256
     30      [36m0.9932[0m        0.1204       0.8035      0.8865        0.2143        0.0000  13.4241
     31      0.9908        [32m0.1160[0m       0.8030      0.8862        0.2143        0.0000  13.1628
     32      0.9907        0.1185       0.8033      0.8866        0.2143        0.0000  13.5157
     33      0.9893        0.1204       0.8030      0.8863        0.2143        0.0000  13.2137
     34      0.9903        0.1204       0.8033      0.8865        0.2143        0.0000  13.3549
     35      0.9896        0.1180       0.8035      0.8865        0.2143        0.0000  13.2625
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.9086274812697923
F1 Macro Score after query 6: 0.8936267628661746
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_5.pt

Iteration: 7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9504[0m        [32m0.1591[0m       [35m0.8102[0m      [31m0.8914[0m        [94m0.2067[0m     +  0.0000  15.4352
      2      [36m0.9666[0m        [32m0.1401[0m       0.7990      0.8871        0.2139        0.0000  15.3750
      3      [36m0.9731[0m        [32m0.1328[0m       0.7899      0.8860        0.2191        0.0000  15.5832
      4      [36m0.9749[0m        [32m0.1222[0m       [35m0.8120[0m      [31m0.8919[0m        0.2077        0.0000  15.5784
      5      [36m0.9815[0m        [32m0.1175[0m       0.8047      0.8837        0.2088        0.0000  15.3732
      6      [36m0.9842[0m        [32m0.1116[0m       0.8071      0.8884        [94m0.2052[0m     +  0.0000  15.4965
      7      [36m0.9870[0m        [32m0.1074[0m       0.8050      0.8862        0.2054        0.0000  15.4802
      8      0.9854        [32m0.1057[0m       0.8099      0.8883        [94m0.2036[0m     +  0.0000  15.3781
      9      0.9867        [32m0.1028[0m       0.7988      0.8824        0.2077        0.0000  15.6719
     10      [36m0.9878[0m        [32m0.1009[0m       0.8024      0.8863        0.2042        0.0000  15.5496
     11      [36m0.9884[0m        [32m0.0971[0m       0.8016      0.8856        0.2038        0.0000  15.4067
     12      [36m0.9900[0m        0.0980       0.8019      0.8860        0.2040        0.0000  15.3472
     13      0.9881        [32m0.0966[0m       0.8005      0.8844        [94m0.2036[0m     +  0.0000  15.5492
     14      [36m0.9914[0m        [32m0.0938[0m       0.8017      0.8861        [94m0.2033[0m     +  0.0000  15.4999
     15      0.9911        [32m0.0931[0m       0.8005      0.8856        0.2036        0.0000  15.4234
     16      0.9913        [32m0.0931[0m       0.8073      0.8879        [94m0.2015[0m     +  0.0000  15.3870
     17      [36m0.9917[0m        0.0934       0.8068      0.8881        [94m0.2013[0m     +  0.0000  15.3751
     18      0.9915        [32m0.0917[0m       0.8066      0.8877        0.2014        0.0000  15.3319
     19      0.9916        [32m0.0908[0m       0.8095      0.8892        [94m0.2008[0m     +  0.0000  16.0626
     20      0.9916        [32m0.0902[0m       0.8082      0.8884        [94m0.2006[0m     +  0.0000  15.3520
     21      [36m0.9918[0m        0.0916       0.8095      0.8876        [94m0.2001[0m     +  0.0000  15.6128
     22      0.9918        [32m0.0893[0m       0.8102      0.8881        0.2002        0.0000  15.5792
     23      0.9907        0.0909       0.8095      0.8879        0.2004        0.0000  15.6866
     24      [36m0.9927[0m        0.0915       0.8090      0.8880        0.2004        0.0000  15.4372
     25      0.9921        0.0918       0.8099      0.8877        0.2005        0.0000  15.5156
     26      0.9917        0.0901       0.8092      0.8880        0.2004        0.0000  15.7519
     27      0.9918        0.0921       0.8101      0.8887        0.2005        0.0000  15.2645
     28      0.9925        0.0901       0.8099      0.8888        0.2005        0.0000  15.3557
     29      [36m0.9931[0m        0.0895       0.8095      0.8885        0.2004        0.0000  15.3323
     30      [36m0.9934[0m        0.0900       0.8092      0.8883        0.2003        0.0000  15.2772
     31      0.9931        [32m0.0892[0m       0.8099      0.8888        0.2003        0.0000  15.7177
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.907662388175807
F1 Macro Score after query 7: 0.8913765181590678
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_6.pt

Iteration: 8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9656[0m        [32m0.1220[0m       [35m0.8095[0m      [31m0.8829[0m        [94m0.1959[0m     +  0.0000  19.4539
      2      [36m0.9756[0m        [32m0.1062[0m       0.8068      0.8687        0.2011        0.0000  19.4869
      3      [36m0.9795[0m        [32m0.0977[0m       0.8087      0.8693        0.2032        0.0000  19.4647
      4      [36m0.9825[0m        [32m0.0912[0m       [35m0.8111[0m      0.8745        0.2005        0.0000  19.6195
      5      [36m0.9844[0m        [32m0.0854[0m       0.7946      0.8618        0.2076        0.0000  19.4209
      6      [36m0.9868[0m        [32m0.0787[0m       [35m0.8125[0m      0.8760        0.1977        0.0000  19.4711
      7      [36m0.9888[0m        [32m0.0750[0m       [35m0.8130[0m      0.8787        0.1968        0.0000  19.3688
      8      [36m0.9890[0m        [32m0.0737[0m       [35m0.8146[0m      [31m0.8833[0m        0.1979        0.0000  19.5933
      9      0.9886        [32m0.0703[0m       [35m0.8153[0m      0.8802        0.1983        0.0000  19.3961
     10      [36m0.9902[0m        [32m0.0679[0m       0.8106      0.8809        0.2016        0.0000  19.6445
     11      [36m0.9917[0m        [32m0.0656[0m       0.8141      [31m0.8843[0m        0.1995        0.0000  19.7718
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.9090485074626866
F1 Macro Score after query 8: 0.8932964778951358
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_7.pt

Iteration: 9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9634[0m        [32m0.1167[0m       [35m0.8321[0m      [31m0.8960[0m        [94m0.1835[0m     +  0.0000  26.4537
      2      [36m0.9693[0m        [32m0.1020[0m       0.8177      0.8927        0.1887        0.0000  26.4965
      3      [36m0.9756[0m        [32m0.0915[0m       [35m0.8335[0m      [31m0.9008[0m        [94m0.1797[0m     +  0.0000  26.5007
      4      [36m0.9781[0m        [32m0.0832[0m       0.8116      0.8876        0.1885        0.0000  26.3747
      5      [36m0.9824[0m        [32m0.0757[0m       0.8333      0.9006        [94m0.1773[0m     +  0.0000  26.2806
      6      [36m0.9862[0m        [32m0.0672[0m       0.7951      0.8817        0.2032        0.0000  26.5639
      7      [36m0.9870[0m        [32m0.0633[0m       0.7969      0.8831        0.2047        0.0000  26.1874
      8      [36m0.9893[0m        [32m0.0598[0m       0.8030      0.8864        0.2007        0.0000  26.3809
      9      [36m0.9912[0m        [32m0.0576[0m       0.7962      0.8822        0.2058        0.0000  26.4803
     10      [36m0.9924[0m        [32m0.0545[0m       0.7984      0.8842        0.2090        0.0000  26.0116
     11      [36m0.9940[0m        [32m0.0523[0m       0.8082      0.8884        0.2004        0.0000  24.9792
     12      [36m0.9949[0m        [32m0.0505[0m       0.8057      0.8870        0.2026        0.0000  24.9536
     13      0.9947        [32m0.0489[0m       0.8118      0.8903        0.2001        0.0000  25.0748
     14      [36m0.9953[0m        [32m0.0484[0m       0.8116      0.8906        0.2001        0.0000  25.3024
     15      0.9951        [32m0.0478[0m       0.8134      0.8908        0.2000        0.0000  24.9634
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.913396795913629
F1 Macro Score after query 9: 0.8998537231829425
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_8.pt

Iteration: 10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9722[0m        [32m0.0824[0m       [35m0.8205[0m      [31m0.8968[0m        [94m0.1734[0m     +  0.0000  36.6775
      2      [36m0.9774[0m        [32m0.0710[0m       [35m0.8264[0m      0.8951        0.1735        0.0000  36.8483
      3      [36m0.9818[0m        [32m0.0638[0m       0.8087      0.8902        0.1830        0.0000  36.8037
      4      [36m0.9824[0m        [32m0.0566[0m       0.8118      0.8930        0.1873        0.0000  36.8269
      5      [36m0.9840[0m        [32m0.0535[0m       0.7931      0.8837        0.2167        0.0000  36.8365
      6      [36m0.9878[0m        [32m0.0450[0m       0.8153      0.8877        0.1905        0.0000  36.8848
      7      [36m0.9898[0m        [32m0.0424[0m       0.8144      0.8852        0.1956        0.0000  37.0511
      8      [36m0.9913[0m        [32m0.0389[0m       0.8175      0.8855        0.1962        0.0000  36.8189
      9      [36m0.9926[0m        [32m0.0377[0m       0.8210      0.8892        0.1933        0.0000  36.8726
     10      [36m0.9933[0m        [32m0.0350[0m       0.8137      0.8880        0.2060        0.0000  36.9062
     11      [36m0.9942[0m        [32m0.0332[0m       0.8219      0.8941        0.1957        0.0000  36.8438
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.9209065679925994
F1 Macro Score after query 10: 0.907576774665789
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_9.pt

Iteration: 11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9694[0m        [32m0.0795[0m       [35m0.8394[0m      [31m0.8996[0m        [94m0.1596[0m     +  0.0000  57.8546
      2      [36m0.9741[0m        [32m0.0682[0m       [35m0.8462[0m      [31m0.8997[0m        0.1599        0.0000  57.9539
      3      [36m0.9775[0m        [32m0.0596[0m       0.8460      0.8957        0.1630        0.0000  58.0729
      4      [36m0.9796[0m        [32m0.0535[0m       [35m0.8490[0m      0.8982        0.1633        0.0000  57.9016
      5      [36m0.9830[0m        [32m0.0476[0m       0.8333      0.8874        0.1822        0.0000  58.4530
      6      [36m0.9857[0m        [32m0.0413[0m       0.8373      0.8963        0.1756        0.0000  58.1562
      7      [36m0.9877[0m        [32m0.0378[0m       0.8347      0.8939        0.1796        0.0000  57.8811
      8      [36m0.9895[0m        [32m0.0351[0m       0.8236      0.8899        0.1921        0.0000  57.9265
      9      [36m0.9907[0m        [32m0.0326[0m       0.8321      0.8907        0.1898        0.0000  58.0499
     10      [36m0.9912[0m        [32m0.0310[0m       0.8240      0.8896        0.1944        0.0000  57.7536
     11      [36m0.9926[0m        [32m0.0280[0m       0.8210      0.8904        0.1994        0.0000  57.6806
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.9234108527131782
F1 Macro Score after query 11: 0.9118990477907679
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_10.pt

Iteration: 12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9724[0m        [32m0.0695[0m       [35m0.8170[0m      [31m0.8936[0m        [94m0.1772[0m     +  0.0000  94.7277
      2      [36m0.9773[0m        [32m0.0583[0m       [35m0.8311[0m      [31m0.8959[0m        [94m0.1692[0m     +  0.0000  95.1798
      3      [36m0.9796[0m        [32m0.0517[0m       0.8266      0.8955        0.1760        0.0000  95.2921
      4      [36m0.9815[0m        [32m0.0452[0m       0.8248      0.8955        0.1848        0.0000  95.4318
      5      [36m0.9833[0m        [32m0.0410[0m       0.8233      0.8926        0.1947        0.0000  95.0141
      6      [36m0.9865[0m        [32m0.0343[0m       [35m0.8326[0m      [31m0.8979[0m        0.1850        0.0000  95.1586
      7      [36m0.9882[0m        [32m0.0309[0m       0.8306      0.8953        0.1951        0.0000  95.9003
      8      [36m0.9898[0m        [32m0.0285[0m       0.8264      0.8949        0.2055        0.0000  96.4395
      9      [36m0.9902[0m        [32m0.0260[0m       0.8285      0.8946        0.2081        0.0000  95.9377
     10      [36m0.9913[0m        [32m0.0243[0m       0.8300      0.8947        0.2121        0.0000  96.2828
     11      [36m0.9929[0m        [32m0.0218[0m       0.8323      0.8976        0.2138        0.0000  95.7707
     12      [36m0.9943[0m        [32m0.0202[0m       0.8286      0.8945        0.2196        0.0000  95.7452
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.9214764583493874
F1 Macro Score after query 12: 0.9099209938359105
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_11.pt

Iteration: 13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9779[0m        [32m0.0523[0m       [35m0.8417[0m      [31m0.9036[0m        [94m0.1626[0m     +  0.0000  110.2582
      2      [36m0.9812[0m        [32m0.0455[0m       0.8365      0.8985        0.1814        0.0000  111.0612
      3      [36m0.9824[0m        [32m0.0415[0m       0.8387      0.9010        0.1758        0.0000  110.7986
      4      [36m0.9833[0m        [32m0.0376[0m       0.8229      0.8975        0.1906        0.0000  110.8593
      5      [36m0.9851[0m        [32m0.0335[0m       0.8375      0.9014        0.1906        0.0000  110.9611
      6      [36m0.9882[0m        [32m0.0281[0m       0.8339      0.9003        0.1971        0.0000  110.8688
      7      [36m0.9904[0m        [32m0.0248[0m       0.8366      0.9013        0.2011        0.0000  110.7787
      8      [36m0.9917[0m        [32m0.0226[0m       0.8347      0.8989        0.2102        0.0000  111.0797
      9      [36m0.9925[0m        [32m0.0209[0m       0.8318      0.8972        0.2184        0.0000  110.9346
     10      [36m0.9931[0m        [32m0.0191[0m       0.8299      0.8963        0.2297        0.0000  111.5173
     11      [36m0.9945[0m        [32m0.0171[0m       0.8337      0.8974        0.2264        0.0000  110.6618
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.9288025889967638
F1 Macro Score after query 13: 0.9160943669814513
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\model_checkpoint_iteration_12.pt
Pickle file saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed46_lowLR\AL_average_confidence_results_for_multilabel_classification_s46.pickle
