Adjusted shapes after loading:
X_initial_np: (8, 224, 224, 3), y_initial_np: (8, 3)
X_pool_np: (26872, 224, 224, 3), y_pool_np: (26872, 3)
X_test_np: (5760, 224, 224, 3), y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3), y_val_np: (5760, 3)
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.6963[0m        [32m0.6694[0m       [35m0.0210[0m      [31m0.2447[0m        [94m0.7251[0m     +  0.0000  12.5189
      2      0.6035        [32m0.6429[0m       [35m0.1413[0m      [31m0.4842[0m        [94m0.7204[0m     +  0.0000  10.6561
      3      [36m0.7090[0m        0.6429       [35m0.2264[0m      0.4404        [94m0.6974[0m     +  0.0000  10.6724
      4      [36m0.8320[0m        [32m0.6066[0m       0.1587      0.4439        0.7094        0.0000  10.5606
      5      0.7389        [32m0.5923[0m       [35m0.2753[0m      0.4530        0.6996        0.0000  10.7347
      6      0.8320        [32m0.5545[0m       0.2490      0.4541        0.7084        0.0000  10.9253
      7      [36m0.8889[0m        [32m0.5539[0m       0.2568      0.4521        0.7083        0.0000  10.7030
      8      0.8333        [32m0.5391[0m       0.2441      0.4534        0.7119        0.0000  10.8145
      9      0.8333        [32m0.5271[0m       0.2625      0.4558        0.7121        0.0000  10.7616
     10      0.8333        [32m0.5047[0m       0.2462      0.4523        0.7132        0.0000  10.8006
     11      0.8333        0.5390       0.2391      0.4512        0.7137        0.0000  10.7653
     12      0.8796        0.5228       0.2403      0.4510        0.7143        0.0000  10.8146
     13      0.8796        0.5083       0.2385      0.4500        0.7141        0.0000  10.9244
Stopping since valid_loss has not improved in the last 11 epochs.
Pre F1 micro score = 0.4609
Pre F1 macro score = 0.4592
Pre Accuracy = 0.2559

Iteration: 1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.7156[0m        [32m0.6131[0m       [35m0.4479[0m      [31m0.0000[0m        [94m0.6492[0m     +  0.0000  10.8236
      2      0.1270        [32m0.5781[0m       [35m0.4622[0m      [31m0.0267[0m        [94m0.6351[0m     +  0.0000  10.9486
      3      0.3238        [32m0.5598[0m       0.4583      [31m0.0309[0m        0.6397        0.0000  10.7293
      4      0.3889        [32m0.5305[0m       [35m0.4653[0m      [31m0.0587[0m        0.6360        0.0000  10.9237
      5      0.4103        [32m0.5295[0m       [35m0.4665[0m      [31m0.1056[0m        [94m0.6345[0m     +  0.0000  10.7618
      6      0.4040        [32m0.5020[0m       [35m0.4734[0m      [31m0.1388[0m        [94m0.6315[0m     +  0.0000  10.9191
      7      [36m0.7532[0m        [32m0.4709[0m       [35m0.4790[0m      [31m0.1544[0m        [94m0.6314[0m     +  0.0000  10.9047
      8      0.5176        0.4898       [35m0.4901[0m      [31m0.1759[0m        [94m0.6291[0m     +  0.0000  10.7694
      9      0.6263        0.4738       [35m0.4910[0m      [31m0.2068[0m        [94m0.6267[0m     +  0.0000  10.9098
     10      0.6056        [32m0.4627[0m       [35m0.4918[0m      [31m0.2202[0m        [94m0.6257[0m     +  0.0000  10.8607
     11      0.5503        [32m0.4623[0m       [35m0.4922[0m      [31m0.2264[0m        [94m0.6254[0m     +  0.0000  10.8380
     12      0.7284        [32m0.4480[0m       [35m0.4958[0m      [31m0.2280[0m        0.6257        0.0000  10.9828
     13      0.7342        0.4555       0.4944      [31m0.2331[0m        0.6257        0.0000  10.7364
     14      [36m0.7536[0m        [32m0.4447[0m       [35m0.4960[0m      [31m0.2379[0m        0.6254        0.0000  11.0332
     15      [36m0.8048[0m        0.4451       0.4906      [31m0.2485[0m        [94m0.6245[0m     +  0.0000  10.5622
     16      0.7374        [32m0.4419[0m       0.4911      [31m0.2526[0m        0.6246        0.0000  10.7518
     17      0.7365        0.4475       0.4915      [31m0.2557[0m        0.6245        0.0000  10.9957
     18      [36m0.8121[0m        [32m0.4357[0m       0.4920      [31m0.2562[0m        [94m0.6244[0m     +  0.0000  10.9278
     19      [36m0.8413[0m        0.4601       0.4924      [31m0.2571[0m        [94m0.6243[0m     +  0.0000  10.9036
     20      0.7215        0.4455       0.4903      [31m0.2632[0m        [94m0.6241[0m     +  0.0000  10.9216
     21      0.7532        0.4418       0.4894      [31m0.2641[0m        [94m0.6240[0m     +  0.0000  10.9049
     22      0.7688        0.4494       0.4896      [31m0.2651[0m        0.6240        0.0000  10.8805
     23      [36m0.8442[0m        [32m0.4310[0m       0.4905      [31m0.2655[0m        0.6240        0.0000  10.9041
     24      0.8278        0.4463       0.4896      [31m0.2658[0m        [94m0.6239[0m     +  0.0000  10.7345
     25      0.8171        0.4376       0.4889      [31m0.2667[0m        [94m0.6239[0m     +  0.0000  10.9849
     26      0.7172        [32m0.4269[0m       0.4889      [31m0.2679[0m        [94m0.6238[0m     +  0.0000  10.9735
     27      0.7190        0.4352       0.4891      [31m0.2689[0m        [94m0.6238[0m     +  0.0000  10.9013
     28      [36m0.8897[0m        [32m0.4231[0m       0.4891      [31m0.2696[0m        0.6238        0.0000  10.8009
     29      0.7921        0.4540       0.4889      0.2694        [94m0.6238[0m     +  0.0000  10.8388
     30      0.8268        [32m0.4207[0m       0.4887      [31m0.2704[0m        [94m0.6238[0m     +  0.0000  10.8475
     31      0.8586        0.4436       0.4887      0.2703        0.6238        0.0000  10.7610
     32      0.7100        0.4569       0.4892      [31m0.2708[0m        [94m0.6238[0m     +  0.0000  10.9959
     33      0.6615        0.4483       0.4892      [31m0.2709[0m        [94m0.6238[0m     +  0.0000  10.9897
     34      0.6524        0.4494       0.4892      0.2706        0.6238        0.0000  10.8559
     35      0.7302        0.4498       0.4892      0.2707        0.6238        0.0000  10.7637
     36      0.5667        0.4529       0.4894      0.2708        0.6238        0.0000  10.9008
     37      0.8755        [32m0.4126[0m       0.4892      0.2707        0.6238        0.0000  10.7005
     38      0.6374        0.4579       0.4892      0.2707        [94m0.6238[0m     +  0.0000  10.9266
     39      0.8564        0.4295       0.4892      0.2707        [94m0.6238[0m     +  0.0000  10.7928
     40      0.7394        0.4525       0.4892      0.2707        [94m0.6238[0m     +  0.0000  10.9267
     41      0.6312        0.4491       0.4892      0.2707        [94m0.6238[0m     +  0.0000  10.8248
     42      0.7504        0.4446       0.4892      0.2707        0.6238        0.0000  11.0188
Stopping since valid_loss has not improved in the last 11 epochs.
[24]
F1 Micro Score after query 1: 0.37685027905848095
F1 Macro Score after query 1: 0.30760531768370514
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_0.pt

Iteration: 2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.7690[0m        [32m0.5882[0m       [35m0.2118[0m      [31m0.5546[0m        [94m0.7064[0m     +  0.0000  10.9880
      2      [36m0.8895[0m        [32m0.4838[0m       [35m0.2646[0m      [31m0.5871[0m        [94m0.6645[0m     +  0.0000  10.8417
      3      [36m0.9110[0m        [32m0.4544[0m       [35m0.3234[0m      [31m0.6176[0m        [94m0.6312[0m     +  0.0000  11.0159
      4      [36m0.9418[0m        [32m0.4138[0m       [35m0.3328[0m      [31m0.6354[0m        [94m0.6070[0m     +  0.0000  10.9672
      5      [36m0.9523[0m        [32m0.3787[0m       [35m0.3943[0m      [31m0.6620[0m        [94m0.5901[0m     +  0.0000  11.0038
      6      [36m0.9666[0m        [32m0.3607[0m       [35m0.4156[0m      [31m0.6771[0m        [94m0.5772[0m     +  0.0000  11.1719
      7      0.9519        0.3690       [35m0.4448[0m      [31m0.6867[0m        [94m0.5729[0m     +  0.0000  11.1094
      8      0.9535        [32m0.3483[0m       0.4311      0.6867        [94m0.5680[0m     +  0.0000  11.2020
      9      0.9582        0.3553       0.4439      [31m0.6940[0m        [94m0.5619[0m     +  0.0000  10.9711
     10      [36m0.9691[0m        [32m0.3307[0m       0.4420      [31m0.6955[0m        [94m0.5580[0m     +  0.0000  11.0294
     11      [36m0.9807[0m        0.3390       [35m0.4467[0m      [31m0.6972[0m        [94m0.5571[0m     +  0.0000  11.1094
     12      0.9642        [32m0.3272[0m       [35m0.4505[0m      [31m0.6983[0m        0.5574        0.0000  10.9430
     13      0.9700        0.3311       [35m0.4549[0m      [31m0.7016[0m        [94m0.5545[0m     +  0.0000  11.0126
     14      0.9752        [32m0.3174[0m       [35m0.4583[0m      [31m0.7024[0m        [94m0.5538[0m     +  0.0000  10.9683
     15      0.9726        0.3267       [35m0.4585[0m      [31m0.7033[0m        [94m0.5522[0m     +  0.0000  10.7396
     16      0.9712        0.3278       [35m0.4618[0m      [31m0.7044[0m        [94m0.5518[0m     +  0.0000  11.1408
     17      0.9750        0.3211       0.4618      [31m0.7046[0m        [94m0.5508[0m     +  0.0000  11.0615
     18      0.9698        0.3239       [35m0.4622[0m      [31m0.7047[0m        [94m0.5503[0m     +  0.0000  10.9999
     19      0.9682        0.3193       [35m0.4649[0m      [31m0.7049[0m        0.5508        0.0000  11.0767
     20      0.9712        [32m0.3073[0m       0.4620      [31m0.7052[0m        [94m0.5496[0m     +  0.0000  11.0315
     21      0.9790        0.3374       0.4625      [31m0.7054[0m        [94m0.5493[0m     +  0.0000  11.0669
     22      0.9763        0.3284       0.4630      [31m0.7056[0m        [94m0.5492[0m     +  0.0000  11.1250
     23      0.9740        0.3228       0.4632      [31m0.7057[0m        [94m0.5489[0m     +  0.0000  11.0341
     24      0.9720        0.3260       0.4637      [31m0.7059[0m        [94m0.5487[0m     +  0.0000  11.0145
     25      0.9721        0.3157       0.4632      [31m0.7061[0m        [94m0.5483[0m     +  0.0000  11.0834
     26      0.9700        0.3212       0.4639      [31m0.7064[0m        [94m0.5482[0m     +  0.0000  11.0150
     27      0.9793        0.3252       0.4641      [31m0.7065[0m        [94m0.5482[0m     +  0.0000  11.2029
     28      0.9793        0.3155       0.4635      0.7063        [94m0.5481[0m     +  0.0000  10.9983
     29      0.9691        0.3251       0.4639      0.7064        [94m0.5480[0m     +  0.0000  11.0462
     30      0.9791        0.3233       0.4637      0.7064        [94m0.5479[0m     +  0.0000  11.1409
     31      0.9751        0.3200       0.4639      0.7065        0.5479        0.0000  11.0018
     32      0.9788        0.3172       0.4637      0.7064        [94m0.5479[0m     +  0.0000  11.0312
     33      [36m0.9841[0m        0.3176       0.4637      0.7064        0.5479        0.0000  11.0920
     34      0.9710        0.3087       0.4635      0.7063        [94m0.5479[0m     +  0.0000  10.9842
     35      0.9676        0.3373       0.4635      0.7063        [94m0.5478[0m     +  0.0000  11.1100
     36      0.9716        0.3250       0.4634      0.7062        [94m0.5478[0m     +  0.0000  11.0937
     37      0.9841        0.3114       0.4634      0.7062        [94m0.5478[0m     +  0.0000  11.1359
     38      [36m0.9881[0m        0.3167       0.4635      0.7063        [94m0.5478[0m     +  0.0000  11.0311
     39      0.9666        0.3168       0.4634      0.7062        [94m0.5478[0m     +  0.0000  10.8758
     40      0.9807        0.3231       0.4635      0.7062        [94m0.5478[0m     +  0.0000  11.0303
     41      0.9807        0.3115       0.4637      0.7063        [94m0.5478[0m     +  0.0000  11.1128
     42      0.9793        0.3163       0.4637      0.7063        [94m0.5477[0m     +  0.0000  11.0469
     43      0.9830        0.3108       0.4637      0.7063        [94m0.5477[0m     +  0.0000  10.9993
     44      0.9645        0.3266       0.4637      0.7063        [94m0.5477[0m     +  0.0000  10.9190
     45      0.9762        0.3227       0.4635      0.7063        [94m0.5477[0m     +  0.0000  10.9409
     46      0.9804        0.3180       0.4635      0.7063        [94m0.5477[0m     +  0.0000  10.9999
     47      0.9624        0.3326       0.4635      0.7063        [94m0.5477[0m     +  0.0000  10.8108
     48      0.9763        0.3329       0.4635      0.7063        [94m0.5477[0m     +  0.0000  10.9394
     49      0.9756        0.3289       0.4635      0.7063        [94m0.5477[0m     +  0.0000  10.9085
     50      0.9740        0.3231       0.4635      0.7063        [94m0.5477[0m     +  0.0000  11.0147
     51      0.9740        0.3187       0.4635      0.7063        [94m0.5477[0m     +  0.0000  11.0465
     52      0.9804        0.3165       0.4635      0.7063        [94m0.5477[0m     +  0.0000  11.1095
     53      0.9753        0.3205       0.4635      0.7063        [94m0.5477[0m     +  0.0000  11.2337
     54      0.9740        0.3209       0.4635      0.7063        [94m0.5477[0m     +  0.0000  11.1445
     55      0.9777        0.3135       0.4635      0.7063        [94m0.5477[0m     +  0.0000  11.1590
     56      0.9740        0.3182       0.4635      0.7063        [94m0.5477[0m     +  0.0000  11.0346
     57      0.9706        0.3223       0.4635      0.7063        [94m0.5477[0m     +  0.0000  10.9595
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56]
F1 Micro Score after query 2: 0.7251962883654534
F1 Macro Score after query 2: 0.7026242691520319
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_1.pt

Iteration: 3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9740[0m        [32m0.2947[0m       [35m0.2866[0m      [31m0.6113[0m        [94m0.6570[0m     +  0.0000  11.2986
      2      [36m0.9761[0m        [32m0.2603[0m       [35m0.4663[0m      [31m0.6935[0m        [94m0.5617[0m     +  0.0000  11.3648
      3      [36m0.9862[0m        [32m0.2330[0m       [35m0.5036[0m      [31m0.7198[0m        [94m0.5279[0m     +  0.0000  11.1829
      4      0.9840        [32m0.2217[0m       [35m0.5316[0m      [31m0.7388[0m        [94m0.5053[0m     +  0.0000  11.1722
      5      [36m0.9898[0m        [32m0.2129[0m       [35m0.5344[0m      [31m0.7393[0m        [94m0.4973[0m     +  0.0000  11.2516
      6      [36m0.9932[0m        [32m0.2049[0m       [35m0.5354[0m      0.7383        0.4980        0.0000  10.9284
      7      0.9932        0.2081       0.5302      0.7375        [94m0.4949[0m     +  0.0000  11.0620
      8      0.9897        [32m0.2023[0m       0.5330      [31m0.7395[0m        [94m0.4942[0m     +  0.0000  11.2431
      9      0.9930        [32m0.1937[0m       0.5290      0.7388        0.4946        0.0000  11.2349
     10      0.9932        0.1978       0.5306      0.7388        [94m0.4927[0m     +  0.0000  11.0726
     11      [36m0.9949[0m        0.1953       0.5339      [31m0.7413[0m        [94m0.4920[0m     +  0.0000  11.3309
     12      0.9915        [32m0.1935[0m       0.5354      0.7411        [94m0.4915[0m     +  0.0000  11.2309
     13      0.9949        [32m0.1862[0m       0.5339      0.7412        [94m0.4903[0m     +  0.0000  11.1532
     14      0.9932        0.1962       [35m0.5370[0m      [31m0.7421[0m        [94m0.4901[0m     +  0.0000  11.3093
     15      0.9932        0.1891       [35m0.5382[0m      0.7417        0.4905        0.0000  11.1606
     16      0.9949        0.1904       [35m0.5385[0m      0.7421        0.4901        0.0000  10.8871
     17      0.9932        0.1895       [35m0.5387[0m      [31m0.7423[0m        [94m0.4896[0m     +  0.0000  11.0794
     18      0.9949        0.1897       0.5387      0.7421        0.4908        0.0000  11.1564
     19      0.9949        [32m0.1836[0m       [35m0.5394[0m      [31m0.7430[0m        0.4900        0.0000  11.2177
     20      0.9949        0.1884       [35m0.5399[0m      0.7430        0.4899        0.0000  11.1774
     21      0.9949        0.1867       0.5399      [31m0.7434[0m        0.4896        0.0000  11.1093
     22      0.9949        0.1887       [35m0.5401[0m      0.7434        [94m0.4892[0m     +  0.0000  11.1621
     23      0.9915        0.1960       0.5398      0.7431        0.4895        0.0000  11.2453
     24      0.9932        0.1930       [35m0.5406[0m      [31m0.7439[0m        [94m0.4888[0m     +  0.0000  11.3619
     25      0.9949        0.1852       [35m0.5413[0m      [31m0.7442[0m        [94m0.4886[0m     +  0.0000  11.1971
     26      0.9949        [32m0.1822[0m       [35m0.5415[0m      [31m0.7442[0m        [94m0.4885[0m     +  0.0000  11.0469
     27      0.9915        0.1871       0.5408      0.7442        0.4886        0.0000  11.2340
     28      0.9949        0.1877       0.5411      0.7442        [94m0.4885[0m     +  0.0000  11.2836
     29      0.9949        0.1826       [35m0.5417[0m      [31m0.7444[0m        [94m0.4885[0m     +  0.0000  11.2509
     30      0.9949        [32m0.1811[0m       0.5406      0.7444        [94m0.4883[0m     +  0.0000  11.3556
     31      0.9915        0.1921       0.5406      0.7442        0.4884        0.0000  11.0940
     32      0.9932        0.1906       0.5406      0.7443        0.4884        0.0000  11.1619
     33      0.9930        0.1899       0.5408      [31m0.7444[0m        [94m0.4883[0m     +  0.0000  11.3109
     34      0.9949        0.1879       0.5408      [31m0.7446[0m        [94m0.4882[0m     +  0.0000  11.2513
     35      0.9949        0.1909       0.5406      0.7444        0.4883        0.0000  11.2455
     36      0.9932        0.1947       0.5406      0.7444        0.4883        0.0000  11.2710
     37      0.9949        0.1849       0.5406      0.7444        0.4883        0.0000  11.0947
     38      0.9949        0.1824       0.5406      0.7444        0.4883        0.0000  11.4507
     39      0.9930        0.1920       0.5406      0.7444        0.4883        0.0000  11.1094
     40      0.9949        0.1882       0.5406      0.7445        0.4883        0.0000  11.2927
     41      0.9949        0.1885       0.5406      0.7445        0.4883        0.0000  11.2987
     42      0.9932        0.1908       0.5406      0.7446        0.4883        0.0000  11.2869
     43      0.9949        0.1888       0.5406      0.7446        0.4883        0.0000  11.3674
     44      0.9932        0.1917       0.5408      0.7446        0.4882        0.0000  11.1410
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112]
F1 Micro Score after query 3: 0.7589750433738156
F1 Macro Score after query 3: 0.7395429443329027
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_2.pt

Iteration: 4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9746[0m        [32m0.2438[0m       [35m0.4097[0m      [31m0.7113[0m        [94m0.5257[0m     +  0.0000  11.6978
      2      [36m0.9772[0m        [32m0.2189[0m       [35m0.5585[0m      [31m0.7626[0m        [94m0.4331[0m     +  0.0000  11.5866
      3      [36m0.9816[0m        [32m0.1983[0m       0.5516      0.7159        0.4352        0.0000  11.6667
      4      [36m0.9871[0m        [32m0.1829[0m       0.5212      0.6266        0.4493        0.0000  11.7135
      5      [36m0.9925[0m        [32m0.1749[0m       0.5405      0.6820        0.4337        0.0000  11.8380
      6      [36m0.9964[0m        [32m0.1647[0m       [35m0.5689[0m      0.7312        [94m0.4206[0m     +  0.0000  11.5407
      7      0.9955        [32m0.1644[0m       [35m0.5700[0m      0.7342        [94m0.4187[0m     +  0.0000  11.0716
      8      0.9945        [32m0.1601[0m       [35m0.5740[0m      0.7263        0.4196        0.0000  11.7905
      9      0.9964        0.1651       0.5674      0.7185        0.4214        0.0000  11.6509
     10      0.9964        [32m0.1574[0m       [35m0.5767[0m      0.7428        [94m0.4147[0m     +  0.0000  11.7109
     11      0.9964        0.1574       [35m0.5799[0m      0.7483        [94m0.4133[0m     +  0.0000  11.6349
     12      [36m0.9974[0m        [32m0.1549[0m       [35m0.5830[0m      0.7504        [94m0.4122[0m     +  0.0000  11.4649
     13      0.9973        [32m0.1509[0m       0.5825      0.7483        0.4133        0.0000  11.6498
     14      0.9964        0.1573       [35m0.5837[0m      0.7571        [94m0.4105[0m     +  0.0000  11.6495
     15      0.9974        0.1548       0.5835      0.7527        0.4110        0.0000  11.6516
     16      [36m0.9983[0m        0.1559       [35m0.5847[0m      0.7589        [94m0.4094[0m     +  0.0000  11.6502
     17      0.9974        0.1510       [35m0.5905[0m      0.7581        [94m0.4093[0m     +  0.0000  11.6349
     18      0.9983        [32m0.1497[0m       0.5896      0.7615        [94m0.4081[0m     +  0.0000  11.6192
     19      0.9974        0.1517       [35m0.5906[0m      0.7587        0.4087        0.0000  11.5876
     20      0.9974        [32m0.1477[0m       0.5898      0.7589        0.4084        0.0000  11.5260
     21      0.9973        0.1528       [35m0.5908[0m      0.7593        0.4083        0.0000  11.6512
     22      0.9974        0.1503       [35m0.5920[0m      0.7609        [94m0.4079[0m     +  0.0000  11.6656
     23      0.9974        0.1525       [35m0.5924[0m      0.7591        0.4082        0.0000  11.6819
     24      0.9974        0.1504       [35m0.5931[0m      [31m0.7631[0m        [94m0.4075[0m     +  0.0000  11.6348
     25      0.9964        0.1545       0.5915      0.7598        0.4080        0.0000  11.6519
     26      0.9974        0.1501       0.5929      0.7624        0.4077        0.0000  11.6669
     27      0.9974        0.1522       [35m0.5934[0m      0.7628        0.4076        0.0000  11.6358
     28      0.9974        0.1478       0.5932      0.7629        [94m0.4074[0m     +  0.0000  11.7424
     29      0.9974        0.1511       [35m0.5938[0m      [31m0.7634[0m        [94m0.4074[0m     +  0.0000  11.7594
     30      0.9983        0.1478       [35m0.5941[0m      [31m0.7637[0m        [94m0.4073[0m     +  0.0000  11.6043
     31      0.9974        0.1541       [35m0.5943[0m      [31m0.7638[0m        [94m0.4073[0m     +  0.0000  11.7429
     32      0.9974        0.1519       0.5938      0.7635        [94m0.4073[0m     +  0.0000  11.5272
     33      0.9966        0.1502       0.5939      0.7637        [94m0.4072[0m     +  0.0000  11.4457
     34      0.9974        0.1501       [35m0.5944[0m      [31m0.7638[0m        [94m0.4071[0m     +  0.0000  11.5733
     35      0.9974        0.1510       [35m0.5951[0m      [31m0.7642[0m        [94m0.4071[0m     +  0.0000  11.7440
     36      0.9983        0.1538       0.5948      0.7639        0.4072        0.0000  11.7909
     37      0.9974        0.1536       0.5951      0.7640        0.4072        0.0000  11.5403
     38      0.9974        [32m0.1474[0m       0.5951      0.7640        0.4072        0.0000  11.5856
     39      0.9974        0.1534       [35m0.5953[0m      0.7640        0.4072        0.0000  11.5115
     40      0.9964        0.1494       0.5946      0.7634        0.4072        0.0000  11.5149
     41      0.9983        0.1527       0.5950      0.7636        0.4072        0.0000  11.6947
     42      0.9966        0.1494       0.5948      0.7635        0.4072        0.0000  11.5701
     43      0.9955        0.1482       0.5944      0.7633        0.4072        0.0000  11.7745
     44      0.9974        [32m0.1455[0m       0.5944      0.7633        0.4072        0.0000  11.8215
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208]
F1 Micro Score after query 4: 0.8007555723460521
F1 Macro Score after query 4: 0.7761259314067015
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_3.pt

Iteration: 5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9848[0m        [32m0.1841[0m       [35m0.3628[0m      [31m0.7006[0m        [94m0.5385[0m     +  0.0000  12.2919
      2      [36m0.9900[0m        [32m0.1626[0m       [35m0.5880[0m      [31m0.7906[0m        [94m0.4211[0m     +  0.0000  12.2599
      3      [36m0.9923[0m        [32m0.1546[0m       [35m0.6134[0m      [31m0.8051[0m        [94m0.3994[0m     +  0.0000  12.2586
      4      0.9919        [32m0.1526[0m       [35m0.6422[0m      [31m0.8127[0m        [94m0.3845[0m     +  0.0000  12.2299
      5      [36m0.9933[0m        [32m0.1464[0m       [35m0.6726[0m      [31m0.8196[0m        [94m0.3725[0m     +  0.0000  12.3062
      6      [36m0.9948[0m        [32m0.1423[0m       [35m0.6880[0m      0.8159        [94m0.3685[0m     +  0.0000  12.2745
      7      0.9943        [32m0.1398[0m       0.6851      0.8161        [94m0.3674[0m     +  0.0000  12.2764
      8      [36m0.9952[0m        [32m0.1353[0m       [35m0.6932[0m      [31m0.8199[0m        [94m0.3658[0m     +  0.0000  12.0413
      9      [36m0.9957[0m        [32m0.1347[0m       0.6865      0.8170        0.3665        0.0000  12.2137
     10      0.9948        0.1365       0.6833      0.8195        [94m0.3654[0m     +  0.0000  12.3370
     11      0.9952        [32m0.1320[0m       [35m0.6939[0m      [31m0.8217[0m        [94m0.3630[0m     +  0.0000  12.4010
     12      0.9952        [32m0.1300[0m       0.6908      0.8199        0.3634        0.0000  12.0866
     13      0.9948        0.1323       0.6903      0.8207        0.3639        0.0000  12.2122
     14      [36m0.9962[0m        0.1314       0.6939      [31m0.8230[0m        [94m0.3630[0m     +  0.0000  12.3388
     15      0.9948        0.1310       0.6918      0.8190        0.3633        0.0000  12.1662
     16      0.9962        [32m0.1257[0m       [35m0.6946[0m      0.8218        [94m0.3629[0m     +  0.0000  12.2278
     17      [36m0.9971[0m        0.1302       [35m0.6950[0m      0.8230        [94m0.3624[0m     +  0.0000  12.1958
     18      0.9957        0.1283       [35m0.6976[0m      [31m0.8236[0m        [94m0.3624[0m     +  0.0000  12.0569
     19      0.9957        0.1265       0.6941      0.8226        0.3625        0.0000  11.9309
     20      0.9962        0.1287       0.6946      0.8228        0.3626        0.0000  12.3846
     21      0.9962        0.1304       0.6946      0.8226        0.3625        0.0000  12.0705
     22      0.9971        0.1275       0.6946      0.8224        0.3626        0.0000  12.3057
     23      0.9971        [32m0.1253[0m       0.6951      0.8226        0.3626        0.0000  12.3545
     24      0.9952        0.1268       0.6936      0.8223        0.3627        0.0000  12.2309
     25      0.9957        0.1272       0.6951      0.8226        0.3627        0.0000  11.9783
     26      0.9953        0.1302       0.6951      0.8227        0.3627        0.0000  12.3686
     27      0.9957        [32m0.1230[0m       0.6960      0.8227        0.3626        0.0000  12.2915
     28      0.9971        0.1268       0.6946      0.8223        0.3626        0.0000  12.2267
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.8355384615384615
F1 Macro Score after query 5: 0.8162526470363506
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_4.pt

Iteration: 6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9815[0m        [32m0.1650[0m       [35m0.5960[0m      [31m0.8117[0m        [94m0.3759[0m     +  0.0000  13.3223
      2      [36m0.9897[0m        [32m0.1442[0m       [35m0.6424[0m      [31m0.8142[0m        [94m0.3593[0m     +  0.0000  13.3207
      3      [36m0.9907[0m        [32m0.1370[0m       0.6295      [31m0.8190[0m        0.3662        0.0000  13.3859
      4      [36m0.9929[0m        [32m0.1310[0m       0.6273      0.8185        0.3644        0.0000  13.4345
      5      [36m0.9942[0m        [32m0.1211[0m       0.6233      0.8125        0.3666        0.0000  13.5733
      6      [36m0.9945[0m        0.1222       0.6266      0.8115        0.3659        0.0000  13.4779
      7      [36m0.9952[0m        [32m0.1175[0m       0.6319      0.8157        0.3644        0.0000  13.5552
      8      0.9945        [32m0.1171[0m       0.6276      0.8114        0.3654        0.0000  13.1964
      9      [36m0.9955[0m        [32m0.1129[0m       0.6243      0.8138        0.3658        0.0000  13.5719
     10      [36m0.9960[0m        0.1144       0.6299      0.8118        0.3622        0.0000  13.1975
     11      0.9958        [32m0.1092[0m       0.6375      0.8164        0.3627        0.0000  13.4787
     12      0.9952        0.1108       0.6366      0.8154        0.3617        0.0000  13.5720
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.8180582385347355
F1 Macro Score after query 6: 0.8040159025409386
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_5.pt

Iteration: 7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9759[0m        [32m0.1715[0m       [35m0.4688[0m      [31m0.7623[0m        [94m0.4279[0m     +  0.0000  15.6754
      2      [36m0.9826[0m        [32m0.1527[0m       [35m0.5280[0m      [31m0.7876[0m        [94m0.3942[0m     +  0.0000  15.5883
      3      [36m0.9859[0m        [32m0.1417[0m       [35m0.5632[0m      [31m0.8046[0m        [94m0.3811[0m     +  0.0000  15.7151
      4      [36m0.9863[0m        [32m0.1345[0m       [35m0.5828[0m      [31m0.8141[0m        [94m0.3746[0m     +  0.0000  15.5581
      5      [36m0.9884[0m        [32m0.1266[0m       [35m0.6106[0m      [31m0.8172[0m        [94m0.3718[0m     +  0.0000  15.5596
      6      [36m0.9894[0m        [32m0.1175[0m       0.6082      0.8112        0.3730        0.0000  15.6991
      7      [36m0.9909[0m        [32m0.1134[0m       [35m0.6175[0m      0.8151        [94m0.3695[0m     +  0.0000  15.6082
      8      [36m0.9916[0m        [32m0.1126[0m       0.6104      0.8106        [94m0.3695[0m     +  0.0000  15.7004
      9      0.9909        [32m0.1123[0m       0.6174      0.8150        [94m0.3684[0m     +  0.0000  15.6841
     10      [36m0.9927[0m        [32m0.1105[0m       0.6109      0.8140        0.3716        0.0000  15.4498
     11      [36m0.9935[0m        [32m0.1065[0m       [35m0.6264[0m      0.8154        [94m0.3640[0m     +  0.0000  15.6692
     12      0.9932        [32m0.1049[0m       [35m0.6276[0m      0.8140        [94m0.3631[0m     +  0.0000  15.8868
     13      [36m0.9939[0m        0.1066       0.6273      0.8153        [94m0.3626[0m     +  0.0000  15.6837
     14      [36m0.9943[0m        [32m0.1030[0m       [35m0.6312[0m      0.8161        [94m0.3614[0m     +  0.0000  15.9653
     15      0.9938        0.1045       [35m0.6323[0m      [31m0.8184[0m        0.3615        0.0000  15.6365
     16      0.9935        [32m0.1020[0m       [35m0.6425[0m      [31m0.8205[0m        [94m0.3575[0m     +  0.0000  15.7618
     17      0.9937        0.1027       0.6398      0.8184        0.3583        0.0000  15.7474
     18      0.9942        [32m0.1016[0m       [35m0.6448[0m      [31m0.8212[0m        [94m0.3567[0m     +  0.0000  15.3334
     19      [36m0.9944[0m        [32m0.1006[0m       [35m0.6451[0m      [31m0.8214[0m        0.3567        0.0000  15.8267
     20      [36m0.9948[0m        [32m0.1000[0m       [35m0.6455[0m      [31m0.8215[0m        0.3569        0.0000  15.3855
     21      0.9937        [32m0.0997[0m       [35m0.6477[0m      0.8215        [94m0.3553[0m     +  0.0000  15.5427
     22      [36m0.9950[0m        [32m0.0983[0m       0.6467      0.8209        0.3557        0.0000  15.7310
     23      0.9946        0.0996       [35m0.6484[0m      [31m0.8226[0m        [94m0.3552[0m     +  0.0000  15.4628
     24      0.9947        0.0986       0.6484      [31m0.8230[0m        [94m0.3547[0m     +  0.0000  15.7146
     25      0.9937        0.1004       0.6477      0.8225        0.3550        0.0000  15.6996
     26      0.9948        [32m0.0982[0m       0.6481      0.8227        0.3547        0.0000  15.8272
     27      0.9938        0.0985       0.6479      0.8227        0.3547        0.0000  15.5738
     28      0.9943        0.0987       0.6474      0.8224        [94m0.3547[0m     +  0.0000  15.6675
     29      0.9949        0.0994       0.6476      0.8224        0.3547        0.0000  15.7773
     30      0.9940        0.1008       0.6479      0.8225        0.3550        0.0000  15.6838
     31      0.9940        0.0998       0.6477      0.8226        0.3550        0.0000  15.7634
     32      0.9945        0.0992       0.6481      0.8229        0.3549        0.0000  15.6985
     33      0.9948        [32m0.0976[0m       0.6476      0.8225        0.3548        0.0000  15.5552
     34      0.9945        0.0980       0.6477      0.8227        0.3548        0.0000  15.5903
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.8277000358808755
F1 Macro Score after query 7: 0.8205157070956309
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_6.pt

Iteration: 8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9835[0m        [32m0.1291[0m       [35m0.3998[0m      [31m0.7601[0m        [94m0.4495[0m     +  0.0000  19.4824
      2      [36m0.9856[0m        [32m0.1154[0m       [35m0.4852[0m      [31m0.7832[0m        [94m0.4296[0m     +  0.0000  19.3302
      3      [36m0.9872[0m        [32m0.1076[0m       [35m0.5087[0m      [31m0.7877[0m        [94m0.4152[0m     +  0.0000  19.4209
      4      [36m0.9875[0m        [32m0.1013[0m       [35m0.5538[0m      [31m0.7985[0m        [94m0.4080[0m     +  0.0000  19.5608
      5      [36m0.9887[0m        [32m0.0956[0m       [35m0.6068[0m      [31m0.8048[0m        [94m0.3717[0m     +  0.0000  19.5294
      6      [36m0.9922[0m        [32m0.0873[0m       [35m0.6389[0m      [31m0.8236[0m        [94m0.3567[0m     +  0.0000  19.4380
      7      0.9922        [32m0.0837[0m       0.6316      [31m0.8258[0m        0.3604        0.0000  19.6427
      8      [36m0.9928[0m        [32m0.0811[0m       [35m0.6406[0m      [31m0.8283[0m        [94m0.3545[0m     +  0.0000  19.6536
      9      [36m0.9938[0m        [32m0.0780[0m       0.6389      0.8274        0.3547        0.0000  19.5000
     10      0.9936        [32m0.0776[0m       0.6194      0.8193        0.3655        0.0000  19.4507
     11      [36m0.9945[0m        [32m0.0725[0m       0.6349      0.8262        0.3587        0.0000  19.5913
     12      [36m0.9952[0m        [32m0.0716[0m       0.6373      0.8268        0.3571        0.0000  19.6568
     13      [36m0.9953[0m        [32m0.0695[0m       0.6332      0.8229        0.3576        0.0000  19.4995
     14      [36m0.9955[0m        0.0698       0.6351      0.8238        0.3576        0.0000  19.3139
     15      0.9953        [32m0.0677[0m       0.6368      0.8257        0.3580        0.0000  19.6397
     16      [36m0.9959[0m        [32m0.0666[0m       [35m0.6453[0m      0.8274        [94m0.3527[0m     +  0.0000  19.3883
     17      [36m0.9963[0m        0.0667       0.6429      0.8267        0.3541        0.0000  19.3919
     18      [36m0.9966[0m        [32m0.0659[0m       0.6425      0.8267        0.3540        0.0000  19.4370
     19      0.9962        [32m0.0653[0m       0.6420      0.8264        0.3538        0.0000  19.7321
     20      0.9964        [32m0.0644[0m       0.6385      0.8247        0.3558        0.0000  19.3951
     21      [36m0.9968[0m        0.0649       0.6417      0.8247        0.3528        0.0000  19.3302
     22      0.9964        [32m0.0638[0m       0.6436      0.8263        0.3531        0.0000  19.3759
     23      0.9964        0.0642       0.6413      0.8253        0.3537        0.0000  19.5431
     24      0.9964        0.0645       0.6417      0.8256        0.3540        0.0000  19.3568
     25      [36m0.9968[0m        0.0639       0.6385      0.8237        0.3543        0.0000  19.3143
     26      0.9966        [32m0.0632[0m       0.6399      0.8247        0.3543        0.0000  19.6113
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.8135686328366408
F1 Macro Score after query 8: 0.8125390786183698
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_7.pt

Iteration: 9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9832[0m        [32m0.1039[0m       [35m0.6035[0m      [31m0.8138[0m        [94m0.3793[0m     +  0.0000  26.5806
      2      [36m0.9869[0m        [32m0.0885[0m       0.5335      0.7901        0.4191        0.0000  26.4262
      3      [36m0.9881[0m        [32m0.0818[0m       0.5222      0.7808        0.4428        0.0000  26.5839
      4      [36m0.9897[0m        [32m0.0749[0m       0.5950      0.7933        0.4263        0.0000  26.5800
      5      [36m0.9912[0m        [32m0.0687[0m       0.5465      0.7737        0.4700        0.0000  26.2828
      6      [36m0.9922[0m        [32m0.0605[0m       0.5880      0.7933        0.4303        0.0000  26.2855
      7      [36m0.9941[0m        [32m0.0556[0m       0.6035      0.7993        0.4219        0.0000  26.4235
      8      [36m0.9949[0m        [32m0.0528[0m       0.6009      0.7965        0.4294        0.0000  26.2601
      9      [36m0.9950[0m        [32m0.0509[0m       0.6000      0.7985        0.4307        0.0000  26.4872
     10      [36m0.9954[0m        [32m0.0480[0m       [35m0.6052[0m      0.7995        0.4314        0.0000  26.6453
     11      [36m0.9959[0m        [32m0.0453[0m       [35m0.6467[0m      [31m0.8152[0m        0.3932        0.0000  26.4112
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.798753640858904
F1 Macro Score after query 9: 0.8024226524909404
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_8.pt

Iteration: 10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9834[0m        [32m0.1024[0m       [35m0.5778[0m      [31m0.8217[0m        [94m0.3619[0m     +  0.0000  38.3529
      2      [36m0.9868[0m        [32m0.0882[0m       0.5755      0.8177        0.3812        0.0000  38.5448
      3      [36m0.9878[0m        [32m0.0781[0m       [35m0.6361[0m      [31m0.8348[0m        [94m0.3562[0m     +  0.0000  38.9360
      4      [36m0.9893[0m        [32m0.0705[0m       [35m0.6389[0m      [31m0.8407[0m        [94m0.3539[0m     +  0.0000  38.6692
      5      [36m0.9898[0m        [32m0.0649[0m       0.6283      0.8307        0.3666        0.0000  38.4993
      6      [36m0.9928[0m        [32m0.0558[0m       0.6247      0.8318        0.3548        0.0000  38.7959
      7      0.9927        [32m0.0519[0m       [35m0.6528[0m      [31m0.8424[0m        [94m0.3458[0m     +  0.0000  38.4333
      8      [36m0.9941[0m        [32m0.0487[0m       [35m0.6665[0m      [31m0.8469[0m        [94m0.3433[0m     +  0.0000  38.8268
      9      [36m0.9942[0m        [32m0.0460[0m       [35m0.6984[0m      [31m0.8579[0m        [94m0.3327[0m     +  0.0000  38.5757
     10      [36m0.9947[0m        [32m0.0438[0m       0.6755      0.8489        0.3415        0.0000  38.5813
     11      [36m0.9954[0m        [32m0.0401[0m       [35m0.7033[0m      [31m0.8584[0m        0.3327        0.0000  38.7799
     12      [36m0.9960[0m        [32m0.0384[0m       [35m0.7057[0m      [31m0.8597[0m        [94m0.3326[0m     +  0.0000  38.8109
     13      [36m0.9960[0m        [32m0.0377[0m       [35m0.7083[0m      0.8591        0.3339        0.0000  38.6259
     14      [36m0.9964[0m        [32m0.0367[0m       0.7000      0.8563        0.3389        0.0000  38.4813
     15      [36m0.9965[0m        [32m0.0351[0m       0.7071      0.8592        0.3356        0.0000  38.5796
     16      [36m0.9969[0m        [32m0.0349[0m       0.7007      0.8553        0.3360        0.0000  38.3279
     17      0.9968        [32m0.0337[0m       0.7024      0.8559        0.3364        0.0000  38.6708
     18      [36m0.9970[0m        [32m0.0334[0m       0.6974      0.8541        0.3386        0.0000  38.8907
     19      0.9969        [32m0.0330[0m       0.7047      0.8572        0.3365        0.0000  38.5423
     20      [36m0.9973[0m        [32m0.0327[0m       0.6969      0.8543        0.3388        0.0000  38.7168
     21      0.9972        [32m0.0318[0m       0.6931      0.8525        0.3408        0.0000  38.5912
     22      0.9972        0.0319       0.6898      0.8509        0.3422        0.0000  38.8896
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.8379721669980119
F1 Macro Score after query 10: 0.8365859941049386
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_9.pt

Iteration: 11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9738[0m        [32m0.1178[0m       [35m0.7816[0m      [31m0.8447[0m        [94m0.2616[0m     +  0.0000  59.9048
      2      [36m0.9820[0m        [32m0.0929[0m       [35m0.7878[0m      [31m0.8535[0m        [94m0.2457[0m     +  0.0000  60.5593
      3      [36m0.9850[0m        [32m0.0820[0m       [35m0.8012[0m      [31m0.8739[0m        [94m0.2345[0m     +  0.0000  60.6556
      4      [36m0.9861[0m        [32m0.0743[0m       [35m0.8085[0m      [31m0.8779[0m        [94m0.2279[0m     +  0.0000  60.2285
      5      [36m0.9880[0m        [32m0.0661[0m       [35m0.8144[0m      [31m0.8783[0m        [94m0.2195[0m     +  0.0000  60.1818
      6      [36m0.9905[0m        [32m0.0563[0m       [35m0.8238[0m      [31m0.8971[0m        [94m0.2095[0m     +  0.0000  60.2448
      7      [36m0.9914[0m        [32m0.0524[0m       0.8224      0.8953        [94m0.2088[0m     +  0.0000  60.1507
      8      [36m0.9923[0m        [32m0.0492[0m       [35m0.8247[0m      [31m0.8972[0m        0.2104        0.0000  60.2006
      9      [36m0.9935[0m        [32m0.0455[0m       0.8214      0.8958        0.2145        0.0000  60.3120
     10      [36m0.9938[0m        [32m0.0434[0m       0.8227      0.8969        0.2139        0.0000  60.7016
     11      [36m0.9950[0m        [32m0.0397[0m       0.8168      [31m0.8979[0m        0.2266        0.0000  60.3668
     12      [36m0.9956[0m        [32m0.0379[0m       0.8198      [31m0.8986[0m        0.2251        0.0000  60.6858
     13      [36m0.9961[0m        [32m0.0365[0m       0.8184      0.8979        0.2267        0.0000  60.2154
     14      [36m0.9964[0m        [32m0.0348[0m       0.8182      0.8979        0.2288        0.0000  60.5907
     15      [36m0.9965[0m        [32m0.0344[0m       0.8193      [31m0.8989[0m        0.2288        0.0000  60.1668
     16      [36m0.9968[0m        [32m0.0331[0m       0.8141      0.8974        0.2376        0.0000  60.6538
     17      [36m0.9970[0m        [32m0.0324[0m       0.8135      0.8968        0.2350        0.0000  60.8405
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.9123294008790193
F1 Macro Score after query 11: 0.9019037701016779
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_10.pt

Iteration: 12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9832[0m        [32m0.0726[0m       [35m0.8370[0m      [31m0.8885[0m        [94m0.1944[0m     +  0.0000  100.0539
      2      [36m0.9854[0m        [32m0.0560[0m       [35m0.8448[0m      [31m0.8959[0m        [94m0.1876[0m     +  0.0000  99.6779
      3      [36m0.9877[0m        [32m0.0453[0m       0.8326      0.8785        0.2026        0.0000  99.3982
      4      [36m0.9886[0m        [32m0.0389[0m       [35m0.8474[0m      0.8944        [94m0.1833[0m     +  0.0000  99.7402
      5      [36m0.9896[0m        [32m0.0344[0m       0.8465      0.8943        0.1873        0.0000  99.1654
      6      [36m0.9924[0m        [32m0.0277[0m       0.8474      [31m0.9024[0m        0.1870        0.0000  99.6959
      7      [36m0.9938[0m        [32m0.0237[0m       [35m0.8510[0m      [31m0.9025[0m        0.1838        0.0000  99.1446
      8      [36m0.9946[0m        [32m0.0216[0m       0.8488      0.9023        0.1917        0.0000  99.6617
      9      [36m0.9954[0m        [32m0.0190[0m       0.8477      0.9011        0.1935        0.0000  99.0522
     10      [36m0.9963[0m        [32m0.0171[0m       0.8481      0.8998        0.2024        0.0000  99.1475
     11      [36m0.9970[0m        [32m0.0150[0m       0.8406      0.9008        0.2155        0.0000  99.1767
     12      [36m0.9978[0m        [32m0.0135[0m       0.8490      [31m0.9048[0m        0.2139        0.0000  99.6511
     13      [36m0.9982[0m        [32m0.0128[0m       0.8389      0.9024        0.2236        0.0000  99.0224
     14      [36m0.9984[0m        [32m0.0118[0m       0.8448      0.9030        0.2195        0.0000  99.3951
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.9275541795665635
F1 Macro Score after query 12: 0.9148487226802867
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_11.pt

Iteration: 13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9890[0m        [32m0.0331[0m       [35m0.8351[0m      [31m0.8940[0m        [94m0.2021[0m     +  0.0000  114.4929
      2      [36m0.9898[0m        [32m0.0284[0m       [35m0.8368[0m      0.8895        [94m0.1927[0m     +  0.0000  115.0509
      3      [36m0.9906[0m        [32m0.0253[0m       [35m0.8477[0m      [31m0.9003[0m        0.1951        0.0000  115.5116
      4      [36m0.9915[0m        [32m0.0222[0m       0.8248      0.8828        0.2159        0.0000  115.6162
      5      [36m0.9923[0m        [32m0.0199[0m       [35m0.8510[0m      [31m0.9051[0m        0.2121        0.0000  115.1250
      6      [36m0.9946[0m        [32m0.0155[0m       0.8486      0.8987        0.2034        0.0000  111.5742
      7      [36m0.9957[0m        [32m0.0128[0m       0.8446      0.9007        0.2122        0.0000  110.4566
      8      [36m0.9968[0m        [32m0.0113[0m       0.8464      0.8997        0.2173        0.0000  111.4864
      9      [36m0.9973[0m        [32m0.0097[0m       0.8448      0.8969        0.2274        0.0000  110.6978
     10      [36m0.9977[0m        [32m0.0089[0m       0.8458      0.9014        0.2318        0.0000  110.9959
     11      [36m0.9980[0m        [32m0.0078[0m       0.8387      0.9011        0.2468        0.0000  110.6421
     12      [36m0.9984[0m        [32m0.0070[0m       0.8358      0.8984        0.2565        0.0000  110.7497
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.9260826090300746
F1 Macro Score after query 13: 0.9131999120074258
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\model_checkpoint_iteration_12.pt
Pickle file saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed42_lowLR\AL_max_score_results_for_multilabel_classification_s42.pickle
