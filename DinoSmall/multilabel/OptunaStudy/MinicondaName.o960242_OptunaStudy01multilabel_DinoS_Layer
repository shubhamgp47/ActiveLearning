### Starting TaskPrologue of job 960242 on tg080 at Sat 21 Dec 2024 09:52:06 PM CET
Running on cores 12-13,28-29,44-45,60-61 with governor ondemand
Sat Dec 21 21:52:06 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3080        On  |   00000000:DA:00.0 Off |                  N/A |
| 30%   40C    P8             17W /  300W |       2MiB /  10240MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

/home/hpc/iwfa/iwfa044h/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).
  from pandas.core.computation.check import NUMEXPR_INSTALLED
[I 2024-12-21 21:52:16,948] A new study created in RDB with name: OptunaStudy01multilabel_DinoS_Layer
Number of trials completed: 0
Number of pruned trials: 0
Total number of trails completed: 0
Number of trials to run: 200
==================== Training of trial number:0 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.32
Learning rate: 2.5e-07
Optimizer: SGD
Momentum term: 0.9295783646545879
Step size: 12
Gamma: 0.2
Best F1-score on Validation data until now: 0
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Epoch 1/100, Training Loss: 0.6963, Training F1-score: 0.3067, Validation Loss: 0.6850, Validation F1-score: 0.3025
Best F1-score till now on the current trail on Validation data: 0.3025294724612465
Best F1-score till now on Validation data: 0.3025294724612465
Epoch 2/100, Training Loss: 0.6871, Training F1-score: 0.3135, Validation Loss: 0.6760, Validation F1-score: 0.3474
Best F1-score till now on the current trail on Validation data: 0.3473691172406777
Best F1-score till now on Validation data: 0.3473691172406777
Epoch 3/100, Training Loss: 0.6794, Training F1-score: 0.3104, Validation Loss: 0.6682, Validation F1-score: 0.2750
Epoch 4/100, Training Loss: 0.6730, Training F1-score: 0.2964, Validation Loss: 0.6615, Validation F1-score: 0.1932
Epoch 5/100, Training Loss: 0.6670, Training F1-score: 0.2767, Validation Loss: 0.6557, Validation F1-score: 0.2008
Epoch 6/100, Training Loss: 0.6615, Training F1-score: 0.2559, Validation Loss: 0.6506, Validation F1-score: 0.2063
Epoch 7/100, Training Loss: 0.6577, Training F1-score: 0.2426, Validation Loss: 0.6461, Validation F1-score: 0.2102
Epoch 8/100, Training Loss: 0.6537, Training F1-score: 0.2301, Validation Loss: 0.6422, Validation F1-score: 0.2144
Epoch 9/100, Training Loss: 0.6499, Training F1-score: 0.2282, Validation Loss: 0.6387, Validation F1-score: 0.2175
Epoch 10/100, Training Loss: 0.6472, Training F1-score: 0.2258, Validation Loss: 0.6356, Validation F1-score: 0.2194
Epoch 11/100, Training Loss: 0.6444, Training F1-score: 0.2280, Validation Loss: 0.6328, Validation F1-score: 0.2213
[I 2024-12-21 22:12:43,421] Trial 0 finished with value: 0.3473691172406777 and parameters: {'dropout': 0.32, 'learning_rate': 2.5e-07, 'optimizer': 'SGD', 'momentum_term': 0.9295783646545879, 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma', 'step_size': 12, 'gamma': 0.2}. Best is trial 0 with value: 0.3473691172406777.
Epoch 12/100, Training Loss: 0.6421, Training F1-score: 0.2281, Validation Loss: 0.6302, Validation F1-score: 0.2227
Early stopping triggered after 12 epochs.
Best F1-score till now on Validation data: 0.3473691172406777
==================== Training of trial number:1 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.6.ls2.gamma
Dropout: 0.28
Learning rate: 2.5e-05
Optimizer: Adam
Momentum term: 0
Step size: 20
Gamma: 0.1
Best F1-score on Validation data until now: 0.3473691172406777
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.2474, Training F1-score: 0.8451, Validation Loss: 0.2216, Validation F1-score: 0.8430
Best F1-score till now on the current trail on Validation data: 0.8429744329687446
Best F1-score till now on Validation data: 0.8429744329687446
Epoch 2/100, Training Loss: 0.1534, Training F1-score: 0.9088, Validation Loss: 0.2043, Validation F1-score: 0.8506
Best F1-score till now on the current trail on Validation data: 0.8505796778096236
Best F1-score till now on Validation data: 0.8505796778096236
Epoch 3/100, Training Loss: 0.1305, Training F1-score: 0.9213, Validation Loss: 0.1973, Validation F1-score: 0.8724
Best F1-score till now on the current trail on Validation data: 0.8724401966545652
Best F1-score till now on Validation data: 0.8724401966545652
Epoch 4/100, Training Loss: 0.1153, Training F1-score: 0.9298, Validation Loss: 0.2053, Validation F1-score: 0.8786
Best F1-score till now on the current trail on Validation data: 0.8785708899550939
Best F1-score till now on Validation data: 0.8785708899550939
Epoch 5/100, Training Loss: 0.1043, Training F1-score: 0.9367, Validation Loss: 0.2024, Validation F1-score: 0.8733
Epoch 6/100, Training Loss: 0.0981, Training F1-score: 0.9413, Validation Loss: 0.2006, Validation F1-score: 0.8702
Epoch 7/100, Training Loss: 0.0924, Training F1-score: 0.9460, Validation Loss: 0.2123, Validation F1-score: 0.8585
Epoch 8/100, Training Loss: 0.0869, Training F1-score: 0.9492, Validation Loss: 0.2291, Validation F1-score: 0.8527
Epoch 9/100, Training Loss: 0.0838, Training F1-score: 0.9509, Validation Loss: 0.2516, Validation F1-score: 0.8291
Epoch 10/100, Training Loss: 0.0801, Training F1-score: 0.9525, Validation Loss: 0.2449, Validation F1-score: 0.8561
Epoch 11/100, Training Loss: 0.0785, Training F1-score: 0.9546, Validation Loss: 0.2291, Validation F1-score: 0.8544
Epoch 12/100, Training Loss: 0.0750, Training F1-score: 0.9561, Validation Loss: 0.2408, Validation F1-score: 0.8391
Epoch 13/100, Training Loss: 0.0731, Training F1-score: 0.9585, Validation Loss: 0.2234, Validation F1-score: 0.8574
[I 2024-12-21 22:36:25,966] Trial 1 finished with value: 0.8785708899550939 and parameters: {'dropout': 0.28, 'learning_rate': 2.5e-05, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.6.ls2.gamma', 'step_size': 20, 'gamma': 0.1}. Best is trial 1 with value: 0.8785708899550939.
Epoch 14/100, Training Loss: 0.0710, Training F1-score: 0.9587, Validation Loss: 0.2388, Validation F1-score: 0.8438
Early stopping triggered after 14 epochs.
Best F1-score till now on Validation data: 0.8785708899550939
==================== Training of trial number:2 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.6.ls2.gamma
Dropout: 0.2
Learning rate: 1e-07
Optimizer: SGD
Momentum term: 0.9398573652206884
Step size: 10
Gamma: 0.3
Best F1-score on Validation data until now: 0.8785708899550939
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.7209, Training F1-score: 0.4722, Validation Loss: 0.7183, Validation F1-score: 0.4673
Best F1-score till now on the current trail on Validation data: 0.4673202817059156
Epoch 2/100, Training Loss: 0.7143, Training F1-score: 0.4651, Validation Loss: 0.7116, Validation F1-score: 0.4659
Epoch 3/100, Training Loss: 0.7084, Training F1-score: 0.4603, Validation Loss: 0.7054, Validation F1-score: 0.4570
Epoch 4/100, Training Loss: 0.7027, Training F1-score: 0.4515, Validation Loss: 0.6997, Validation F1-score: 0.4299
Epoch 5/100, Training Loss: 0.6974, Training F1-score: 0.4417, Validation Loss: 0.6943, Validation F1-score: 0.3978
Epoch 6/100, Training Loss: 0.6924, Training F1-score: 0.4330, Validation Loss: 0.6894, Validation F1-score: 0.3825
Epoch 7/100, Training Loss: 0.6879, Training F1-score: 0.4207, Validation Loss: 0.6848, Validation F1-score: 0.3834
Epoch 8/100, Training Loss: 0.6839, Training F1-score: 0.4100, Validation Loss: 0.6805, Validation F1-score: 0.3836
Epoch 9/100, Training Loss: 0.6799, Training F1-score: 0.3974, Validation Loss: 0.6765, Validation F1-score: 0.3797
Epoch 10/100, Training Loss: 0.6761, Training F1-score: 0.3896, Validation Loss: 0.6728, Validation F1-score: 0.3769
[I 2024-12-21 22:55:27,791] Trial 2 finished with value: 0.8785708899550939 and parameters: {'dropout': 0.2, 'learning_rate': 1e-07, 'optimizer': 'SGD', 'momentum_term': 0.9398573652206884, 'layer_freeze_upto': 'dino_model.blocks.6.ls2.gamma', 'step_size': 10, 'gamma': 0.3}. Best is trial 1 with value: 0.8785708899550939.
Epoch 11/100, Training Loss: 0.6746, Training F1-score: 0.3795, Validation Loss: 0.6719, Validation F1-score: 0.3776
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8785708899550939
==================== Training of trial number:3 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.2
Learning rate: 2.5e-06
Optimizer: SGD
Momentum term: 0.9870777228996512
Step size: 18
Gamma: 0.5
Best F1-score on Validation data until now: 0.8785708899550939
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.6235, Training F1-score: 0.2873, Validation Loss: 0.5789, Validation F1-score: 0.2705
Best F1-score till now on the current trail on Validation data: 0.27045278423558067
Epoch 2/100, Training Loss: 0.5425, Training F1-score: 0.3803, Validation Loss: 0.4956, Validation F1-score: 0.5132
Best F1-score till now on the current trail on Validation data: 0.5132290953002384
Epoch 3/100, Training Loss: 0.4280, Training F1-score: 0.6833, Validation Loss: 0.3772, Validation F1-score: 0.7539
Best F1-score till now on the current trail on Validation data: 0.7539001280082395
Epoch 4/100, Training Loss: 0.3124, Training F1-score: 0.8353, Validation Loss: 0.2985, Validation F1-score: 0.8079
Best F1-score till now on the current trail on Validation data: 0.8078751736239272
Epoch 5/100, Training Loss: 0.2582, Training F1-score: 0.8566, Validation Loss: 0.2681, Validation F1-score: 0.8218
Best F1-score till now on the current trail on Validation data: 0.8217569302872699
Epoch 6/100, Training Loss: 0.2342, Training F1-score: 0.8651, Validation Loss: 0.2519, Validation F1-score: 0.8310
Best F1-score till now on the current trail on Validation data: 0.8309763263213586
Epoch 7/100, Training Loss: 0.2213, Training F1-score: 0.8705, Validation Loss: 0.2428, Validation F1-score: 0.8353
Best F1-score till now on the current trail on Validation data: 0.8352987335657875
Epoch 8/100, Training Loss: 0.2126, Training F1-score: 0.8733, Validation Loss: 0.2370, Validation F1-score: 0.8372
Best F1-score till now on the current trail on Validation data: 0.8371720123180543
Epoch 9/100, Training Loss: 0.2062, Training F1-score: 0.8765, Validation Loss: 0.2322, Validation F1-score: 0.8416
Best F1-score till now on the current trail on Validation data: 0.8415613516452686
Epoch 10/100, Training Loss: 0.2007, Training F1-score: 0.8801, Validation Loss: 0.2292, Validation F1-score: 0.8438
Best F1-score till now on the current trail on Validation data: 0.8437989856957224
Epoch 11/100, Training Loss: 0.1970, Training F1-score: 0.8821, Validation Loss: 0.2260, Validation F1-score: 0.8414
Epoch 12/100, Training Loss: 0.1925, Training F1-score: 0.8857, Validation Loss: 0.2228, Validation F1-score: 0.8490
Best F1-score till now on the current trail on Validation data: 0.8490392292807002
Epoch 13/100, Training Loss: 0.1889, Training F1-score: 0.8881, Validation Loss: 0.2207, Validation F1-score: 0.8521
Best F1-score till now on the current trail on Validation data: 0.8521203091430163
Epoch 14/100, Training Loss: 0.1859, Training F1-score: 0.8889, Validation Loss: 0.2180, Validation F1-score: 0.8564
Best F1-score till now on the current trail on Validation data: 0.856403962750154
Epoch 15/100, Training Loss: 0.1828, Training F1-score: 0.8914, Validation Loss: 0.2164, Validation F1-score: 0.8533
Epoch 16/100, Training Loss: 0.1800, Training F1-score: 0.8939, Validation Loss: 0.2144, Validation F1-score: 0.8590
Best F1-score till now on the current trail on Validation data: 0.8589960918799786
Epoch 17/100, Training Loss: 0.1768, Training F1-score: 0.8948, Validation Loss: 0.2140, Validation F1-score: 0.8590
Epoch 18/100, Training Loss: 0.1742, Training F1-score: 0.8978, Validation Loss: 0.2113, Validation F1-score: 0.8640
Best F1-score till now on the current trail on Validation data: 0.8640451232795733
Epoch 19/100, Training Loss: 0.1727, Training F1-score: 0.8974, Validation Loss: 0.2115, Validation F1-score: 0.8623
Epoch 20/100, Training Loss: 0.1708, Training F1-score: 0.8989, Validation Loss: 0.2099, Validation F1-score: 0.8637
Epoch 21/100, Training Loss: 0.1701, Training F1-score: 0.9004, Validation Loss: 0.2096, Validation F1-score: 0.8667
Best F1-score till now on the current trail on Validation data: 0.8667275089034104
Epoch 22/100, Training Loss: 0.1688, Training F1-score: 0.9010, Validation Loss: 0.2090, Validation F1-score: 0.8679
Best F1-score till now on the current trail on Validation data: 0.8678845119980019
Epoch 23/100, Training Loss: 0.1681, Training F1-score: 0.9006, Validation Loss: 0.2094, Validation F1-score: 0.8650
Epoch 24/100, Training Loss: 0.1665, Training F1-score: 0.9016, Validation Loss: 0.2081, Validation F1-score: 0.8706
Best F1-score till now on the current trail on Validation data: 0.8705660023398586
Epoch 25/100, Training Loss: 0.1664, Training F1-score: 0.9024, Validation Loss: 0.2077, Validation F1-score: 0.8671
Epoch 26/100, Training Loss: 0.1644, Training F1-score: 0.9039, Validation Loss: 0.2071, Validation F1-score: 0.8691
Epoch 27/100, Training Loss: 0.1637, Training F1-score: 0.9031, Validation Loss: 0.2062, Validation F1-score: 0.8705
Epoch 28/100, Training Loss: 0.1633, Training F1-score: 0.9041, Validation Loss: 0.2051, Validation F1-score: 0.8718
Best F1-score till now on the current trail on Validation data: 0.8718141434547285
Epoch 29/100, Training Loss: 0.1618, Training F1-score: 0.9053, Validation Loss: 0.2052, Validation F1-score: 0.8706
Epoch 30/100, Training Loss: 0.1607, Training F1-score: 0.9058, Validation Loss: 0.2047, Validation F1-score: 0.8693
Epoch 31/100, Training Loss: 0.1594, Training F1-score: 0.9066, Validation Loss: 0.2036, Validation F1-score: 0.8734
Best F1-score till now on the current trail on Validation data: 0.87339742485384
Epoch 32/100, Training Loss: 0.1583, Training F1-score: 0.9065, Validation Loss: 0.2042, Validation F1-score: 0.8718
Epoch 33/100, Training Loss: 0.1583, Training F1-score: 0.9066, Validation Loss: 0.2035, Validation F1-score: 0.8743
Best F1-score till now on the current trail on Validation data: 0.874287362435874
Epoch 34/100, Training Loss: 0.1577, Training F1-score: 0.9074, Validation Loss: 0.2038, Validation F1-score: 0.8712
Epoch 35/100, Training Loss: 0.1557, Training F1-score: 0.9096, Validation Loss: 0.2040, Validation F1-score: 0.8687
Epoch 36/100, Training Loss: 0.1555, Training F1-score: 0.9086, Validation Loss: 0.2023, Validation F1-score: 0.8734
Epoch 37/100, Training Loss: 0.1542, Training F1-score: 0.9095, Validation Loss: 0.2018, Validation F1-score: 0.8737
Epoch 38/100, Training Loss: 0.1542, Training F1-score: 0.9105, Validation Loss: 0.2018, Validation F1-score: 0.8735
Epoch 39/100, Training Loss: 0.1538, Training F1-score: 0.9103, Validation Loss: 0.2018, Validation F1-score: 0.8742
Epoch 40/100, Training Loss: 0.1527, Training F1-score: 0.9102, Validation Loss: 0.2010, Validation F1-score: 0.8750
Best F1-score till now on the current trail on Validation data: 0.8749851522737325
Epoch 41/100, Training Loss: 0.1530, Training F1-score: 0.9100, Validation Loss: 0.2008, Validation F1-score: 0.8752
Best F1-score till now on the current trail on Validation data: 0.875243758913283
Epoch 42/100, Training Loss: 0.1517, Training F1-score: 0.9115, Validation Loss: 0.2011, Validation F1-score: 0.8749
Epoch 43/100, Training Loss: 0.1515, Training F1-score: 0.9114, Validation Loss: 0.2008, Validation F1-score: 0.8755
Best F1-score till now on the current trail on Validation data: 0.8754635879348563
Epoch 44/100, Training Loss: 0.1516, Training F1-score: 0.9112, Validation Loss: 0.1999, Validation F1-score: 0.8763
Best F1-score till now on the current trail on Validation data: 0.8763317778672439
Epoch 45/100, Training Loss: 0.1516, Training F1-score: 0.9108, Validation Loss: 0.2002, Validation F1-score: 0.8761
Epoch 46/100, Training Loss: 0.1504, Training F1-score: 0.9115, Validation Loss: 0.2000, Validation F1-score: 0.8757
Epoch 47/100, Training Loss: 0.1501, Training F1-score: 0.9125, Validation Loss: 0.1997, Validation F1-score: 0.8762
Epoch 48/100, Training Loss: 0.1490, Training F1-score: 0.9126, Validation Loss: 0.1998, Validation F1-score: 0.8754
Epoch 49/100, Training Loss: 0.1497, Training F1-score: 0.9128, Validation Loss: 0.1998, Validation F1-score: 0.8756
Epoch 50/100, Training Loss: 0.1493, Training F1-score: 0.9123, Validation Loss: 0.1990, Validation F1-score: 0.8756
Epoch 51/100, Training Loss: 0.1489, Training F1-score: 0.9127, Validation Loss: 0.1993, Validation F1-score: 0.8764
Best F1-score till now on the current trail on Validation data: 0.8764339147408776
Epoch 52/100, Training Loss: 0.1484, Training F1-score: 0.9127, Validation Loss: 0.1986, Validation F1-score: 0.8767
Best F1-score till now on the current trail on Validation data: 0.8766769739997861
Epoch 53/100, Training Loss: 0.1478, Training F1-score: 0.9130, Validation Loss: 0.1981, Validation F1-score: 0.8771
Best F1-score till now on the current trail on Validation data: 0.8770519390135664
Epoch 54/100, Training Loss: 0.1473, Training F1-score: 0.9145, Validation Loss: 0.1988, Validation F1-score: 0.8767
Epoch 55/100, Training Loss: 0.1467, Training F1-score: 0.9140, Validation Loss: 0.1984, Validation F1-score: 0.8772
Best F1-score till now on the current trail on Validation data: 0.8771752920418727
Epoch 56/100, Training Loss: 0.1466, Training F1-score: 0.9143, Validation Loss: 0.1985, Validation F1-score: 0.8769
Epoch 57/100, Training Loss: 0.1467, Training F1-score: 0.9141, Validation Loss: 0.1982, Validation F1-score: 0.8759
Epoch 58/100, Training Loss: 0.1464, Training F1-score: 0.9141, Validation Loss: 0.1983, Validation F1-score: 0.8758
Epoch 59/100, Training Loss: 0.1461, Training F1-score: 0.9153, Validation Loss: 0.1979, Validation F1-score: 0.8773
Best F1-score till now on the current trail on Validation data: 0.8772807645985603
Epoch 60/100, Training Loss: 0.1463, Training F1-score: 0.9134, Validation Loss: 0.1983, Validation F1-score: 0.8756
Epoch 61/100, Training Loss: 0.1459, Training F1-score: 0.9142, Validation Loss: 0.1978, Validation F1-score: 0.8763
Epoch 62/100, Training Loss: 0.1454, Training F1-score: 0.9143, Validation Loss: 0.1978, Validation F1-score: 0.8764
Epoch 63/100, Training Loss: 0.1453, Training F1-score: 0.9148, Validation Loss: 0.1979, Validation F1-score: 0.8761
Epoch 64/100, Training Loss: 0.1446, Training F1-score: 0.9146, Validation Loss: 0.1976, Validation F1-score: 0.8763
Epoch 65/100, Training Loss: 0.1455, Training F1-score: 0.9145, Validation Loss: 0.1983, Validation F1-score: 0.8759
Epoch 66/100, Training Loss: 0.1454, Training F1-score: 0.9146, Validation Loss: 0.1979, Validation F1-score: 0.8767
Epoch 67/100, Training Loss: 0.1445, Training F1-score: 0.9160, Validation Loss: 0.1975, Validation F1-score: 0.8771
Epoch 68/100, Training Loss: 0.1439, Training F1-score: 0.9163, Validation Loss: 0.1976, Validation F1-score: 0.8763
[I 2024-12-22 00:52:05,389] Trial 3 finished with value: 0.8785708899550939 and parameters: {'dropout': 0.2, 'learning_rate': 2.5e-06, 'optimizer': 'SGD', 'momentum_term': 0.9870777228996512, 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma', 'step_size': 18, 'gamma': 0.5}. Best is trial 1 with value: 0.8785708899550939.
Epoch 69/100, Training Loss: 0.1442, Training F1-score: 0.9147, Validation Loss: 0.1976, Validation F1-score: 0.8759
Early stopping triggered after 69 epochs.
Best F1-score till now on Validation data: 0.8785708899550939
==================== Training of trial number:4 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.3
Learning rate: 2.5e-05
Optimizer: RMSprop
Momentum term: 0
Step size: 10
Gamma: 0.4
Best F1-score on Validation data until now: 0.8785708899550939
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.2428, Training F1-score: 0.8541, Validation Loss: 0.2301, Validation F1-score: 0.8285
Best F1-score till now on the current trail on Validation data: 0.8284847342585712
Epoch 2/100, Training Loss: 0.1578, Training F1-score: 0.9058, Validation Loss: 0.2047, Validation F1-score: 0.8766
Best F1-score till now on the current trail on Validation data: 0.8765734042978077
Epoch 3/100, Training Loss: 0.1343, Training F1-score: 0.9188, Validation Loss: 0.2076, Validation F1-score: 0.8786
Best F1-score till now on the current trail on Validation data: 0.8786261076082628
Best F1-score till now on Validation data: 0.8786261076082628
Epoch 4/100, Training Loss: 0.1180, Training F1-score: 0.9300, Validation Loss: 0.2057, Validation F1-score: 0.8749
Epoch 5/100, Training Loss: 0.1071, Training F1-score: 0.9355, Validation Loss: 0.2079, Validation F1-score: 0.8761
Epoch 6/100, Training Loss: 0.0986, Training F1-score: 0.9428, Validation Loss: 0.2248, Validation F1-score: 0.8684
Epoch 7/100, Training Loss: 0.0923, Training F1-score: 0.9448, Validation Loss: 0.2056, Validation F1-score: 0.8674
Epoch 8/100, Training Loss: 0.0879, Training F1-score: 0.9485, Validation Loss: 0.2150, Validation F1-score: 0.8585
Epoch 9/100, Training Loss: 0.0837, Training F1-score: 0.9498, Validation Loss: 0.2371, Validation F1-score: 0.8691
Epoch 10/100, Training Loss: 0.0809, Training F1-score: 0.9529, Validation Loss: 0.2195, Validation F1-score: 0.8598
Epoch 11/100, Training Loss: 0.0734, Training F1-score: 0.9579, Validation Loss: 0.2262, Validation F1-score: 0.8623
Epoch 12/100, Training Loss: 0.0722, Training F1-score: 0.9587, Validation Loss: 0.2160, Validation F1-score: 0.8559
[I 2024-12-22 01:13:48,244] Trial 4 finished with value: 0.8786261076082628 and parameters: {'dropout': 0.3, 'learning_rate': 2.5e-05, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma', 'step_size': 10, 'gamma': 0.4}. Best is trial 4 with value: 0.8786261076082628.
Epoch 13/100, Training Loss: 0.0713, Training F1-score: 0.9589, Validation Loss: 0.2223, Validation F1-score: 0.8567
Early stopping triggered after 13 epochs.
Best F1-score till now on Validation data: 0.8786261076082628
==================== Training of trial number:5 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.32
Learning rate: 2.5e-06
Optimizer: SGD
Momentum term: 0.9275797437154848
Step size: 15
Gamma: 0.3
Best F1-score on Validation data until now: 0.8786261076082628
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 01:15:30,984] Trial 5 pruned. 
Epoch 1/100, Training Loss: 0.6579, Training F1-score: 0.3141, Validation Loss: 0.6306, Validation F1-score: 0.2365
Best F1-score till now on Validation data: 0.8786261076082628
==================== Training of trial number:6 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.6.ls2.gamma
Dropout: 0.35
Learning rate: 2.5e-06
Optimizer: SGD
Momentum term: 0.9097316076054753
Step size: 18
Gamma: 0.2
Best F1-score on Validation data until now: 0.8786261076082628
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 01:17:12,982] Trial 6 pruned. 
Epoch 1/100, Training Loss: 0.6857, Training F1-score: 0.4730, Validation Loss: 0.6490, Validation F1-score: 0.3576
Best F1-score till now on Validation data: 0.8786261076082628
==================== Training of trial number:7 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.4
Learning rate: 2.5e-05
Optimizer: SGD
Momentum term: 0.882054639144088
Step size: 8
Gamma: 0.1
Best F1-score on Validation data until now: 0.8786261076082628
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 01:18:51,963] Trial 7 pruned. 
Epoch 1/100, Training Loss: 0.6352, Training F1-score: 0.3005, Validation Loss: 0.5913, Validation F1-score: 0.2412
Best F1-score till now on Validation data: 0.8786261076082628
==================== Training of trial number:8 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.2.ls2.gamma
Dropout: 0.3
Learning rate: 2.5e-07
Optimizer: SGD
Momentum term: 0.9373072128540301
Step size: 8
Gamma: 0.2
Best F1-score on Validation data until now: 0.8786261076082628
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 01:20:30,085] Trial 8 pruned. 
Epoch 1/100, Training Loss: 0.6755, Training F1-score: 0.1863, Validation Loss: 0.6711, Validation F1-score: 0.0292
Best F1-score till now on Validation data: 0.8786261076082628
==================== Training of trial number:9 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.2.ls2.gamma
Dropout: 0.35
Learning rate: 2.5e-05
Optimizer: SGD
Momentum term: 0.8623116967854335
Step size: 13
Gamma: 0.5
Best F1-score on Validation data until now: 0.8786261076082628
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 01:22:13,283] Trial 9 pruned. 
Epoch 1/100, Training Loss: 0.6269, Training F1-score: 0.2537, Validation Loss: 0.5816, Validation F1-score: 0.2425
Best F1-score till now on Validation data: 0.8786261076082628
==================== Training of trial number:10 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8786261076082628
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1834, Training F1-score: 0.8871, Validation Loss: 0.2000, Validation F1-score: 0.8845
Best F1-score till now on the current trail on Validation data: 0.8845134368464649
Best F1-score till now on Validation data: 0.8845134368464649
Epoch 2/100, Training Loss: 0.1288, Training F1-score: 0.9203, Validation Loss: 0.2014, Validation F1-score: 0.8734
Epoch 3/100, Training Loss: 0.1108, Training F1-score: 0.9329, Validation Loss: 0.2052, Validation F1-score: 0.8564
Epoch 4/100, Training Loss: 0.0990, Training F1-score: 0.9410, Validation Loss: 0.2145, Validation F1-score: 0.8642
Epoch 5/100, Training Loss: 0.0925, Training F1-score: 0.9447, Validation Loss: 0.2446, Validation F1-score: 0.8625
Epoch 6/100, Training Loss: 0.0862, Training F1-score: 0.9492, Validation Loss: 0.2478, Validation F1-score: 0.8535
Epoch 7/100, Training Loss: 0.0712, Training F1-score: 0.9592, Validation Loss: 0.2561, Validation F1-score: 0.8535
Epoch 8/100, Training Loss: 0.0684, Training F1-score: 0.9605, Validation Loss: 0.2230, Validation F1-score: 0.8520
Epoch 9/100, Training Loss: 0.0666, Training F1-score: 0.9616, Validation Loss: 0.2345, Validation F1-score: 0.8501
Epoch 10/100, Training Loss: 0.0655, Training F1-score: 0.9622, Validation Loss: 0.2190, Validation F1-score: 0.8497
[I 2024-12-22 01:40:50,130] Trial 10 finished with value: 0.8845134368464649 and parameters: {'dropout': 0.3, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 6, 'gamma': 0.4}. Best is trial 10 with value: 0.8845134368464649.
Epoch 11/100, Training Loss: 0.0644, Training F1-score: 0.9627, Validation Loss: 0.2247, Validation F1-score: 0.8565
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8845134368464649
==================== Training of trial number:11 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8845134368464649
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1822, Training F1-score: 0.8891, Validation Loss: 0.2247, Validation F1-score: 0.8708
Best F1-score till now on the current trail on Validation data: 0.8708307689496536
Epoch 2/100, Training Loss: 0.1292, Training F1-score: 0.9198, Validation Loss: 0.2196, Validation F1-score: 0.8369
Epoch 3/100, Training Loss: 0.1099, Training F1-score: 0.9325, Validation Loss: 0.2091, Validation F1-score: 0.8543
Epoch 4/100, Training Loss: 0.0995, Training F1-score: 0.9396, Validation Loss: 0.2638, Validation F1-score: 0.8120
Epoch 5/100, Training Loss: 0.0922, Training F1-score: 0.9451, Validation Loss: 0.2240, Validation F1-score: 0.8460
Epoch 6/100, Training Loss: 0.0754, Training F1-score: 0.9565, Validation Loss: 0.2446, Validation F1-score: 0.8625
Epoch 7/100, Training Loss: 0.0719, Training F1-score: 0.9587, Validation Loss: 0.2517, Validation F1-score: 0.8398
Epoch 8/100, Training Loss: 0.0706, Training F1-score: 0.9597, Validation Loss: 0.2371, Validation F1-score: 0.8405
Epoch 9/100, Training Loss: 0.0679, Training F1-score: 0.9601, Validation Loss: 0.2639, Validation F1-score: 0.8357
Epoch 10/100, Training Loss: 0.0667, Training F1-score: 0.9615, Validation Loss: 0.3215, Validation F1-score: 0.8555
[I 2024-12-22 01:59:12,906] Trial 11 finished with value: 0.8845134368464649 and parameters: {'dropout': 0.3, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 5, 'gamma': 0.4}. Best is trial 10 with value: 0.8845134368464649.
Epoch 11/100, Training Loss: 0.0609, Training F1-score: 0.9641, Validation Loss: 0.2563, Validation F1-score: 0.8393
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8845134368464649
==================== Training of trial number:12 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8845134368464649
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1854, Training F1-score: 0.8873, Validation Loss: 0.1914, Validation F1-score: 0.8930
Best F1-score till now on the current trail on Validation data: 0.8930030233872418
Best F1-score till now on Validation data: 0.8930030233872418
Epoch 2/100, Training Loss: 0.1275, Training F1-score: 0.9220, Validation Loss: 0.2169, Validation F1-score: 0.8405
Epoch 3/100, Training Loss: 0.1094, Training F1-score: 0.9342, Validation Loss: 0.2282, Validation F1-score: 0.8716
Epoch 4/100, Training Loss: 0.0991, Training F1-score: 0.9410, Validation Loss: 0.2190, Validation F1-score: 0.8708
Epoch 5/100, Training Loss: 0.0923, Training F1-score: 0.9444, Validation Loss: 0.2588, Validation F1-score: 0.8371
Epoch 6/100, Training Loss: 0.0753, Training F1-score: 0.9569, Validation Loss: 0.2352, Validation F1-score: 0.8479
Epoch 7/100, Training Loss: 0.0725, Training F1-score: 0.9581, Validation Loss: 0.2297, Validation F1-score: 0.8520
Epoch 8/100, Training Loss: 0.0700, Training F1-score: 0.9592, Validation Loss: 0.2459, Validation F1-score: 0.8653
Epoch 9/100, Training Loss: 0.0683, Training F1-score: 0.9605, Validation Loss: 0.2538, Validation F1-score: 0.8560
Epoch 10/100, Training Loss: 0.0662, Training F1-score: 0.9618, Validation Loss: 0.2739, Validation F1-score: 0.8422
[I 2024-12-22 02:17:41,018] Trial 12 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.3, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 5, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0608, Training F1-score: 0.9650, Validation Loss: 0.2520, Validation F1-score: 0.8525
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:13 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1818, Training F1-score: 0.8881, Validation Loss: 0.2082, Validation F1-score: 0.8737
Best F1-score till now on the current trail on Validation data: 0.8736719185541536
Epoch 2/100, Training Loss: 0.1250, Training F1-score: 0.9231, Validation Loss: 0.2141, Validation F1-score: 0.8119
Epoch 3/100, Training Loss: 0.1081, Training F1-score: 0.9348, Validation Loss: 0.2119, Validation F1-score: 0.8651
Epoch 4/100, Training Loss: 0.0959, Training F1-score: 0.9422, Validation Loss: 0.2215, Validation F1-score: 0.8413
Epoch 5/100, Training Loss: 0.0890, Training F1-score: 0.9469, Validation Loss: 0.2071, Validation F1-score: 0.8634
Epoch 6/100, Training Loss: 0.0735, Training F1-score: 0.9577, Validation Loss: 0.2151, Validation F1-score: 0.8619
Epoch 7/100, Training Loss: 0.0713, Training F1-score: 0.9590, Validation Loss: 0.2969, Validation F1-score: 0.8612
Epoch 8/100, Training Loss: 0.0684, Training F1-score: 0.9604, Validation Loss: 0.2257, Validation F1-score: 0.8687
Epoch 9/100, Training Loss: 0.0671, Training F1-score: 0.9606, Validation Loss: 0.2803, Validation F1-score: 0.8687
Epoch 10/100, Training Loss: 0.0646, Training F1-score: 0.9620, Validation Loss: 0.2503, Validation F1-score: 0.8521
[I 2024-12-22 02:36:29,193] Trial 13 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.25, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 5, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0597, Training F1-score: 0.9652, Validation Loss: 0.2327, Validation F1-score: 0.8556
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:14 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1826, Training F1-score: 0.8882, Validation Loss: 0.2212, Validation F1-score: 0.8714
Best F1-score till now on the current trail on Validation data: 0.8714441893450808
Epoch 2/100, Training Loss: 0.1269, Training F1-score: 0.9217, Validation Loss: 0.2028, Validation F1-score: 0.8700
Epoch 3/100, Training Loss: 0.1090, Training F1-score: 0.9351, Validation Loss: 0.2368, Validation F1-score: 0.8656
Epoch 4/100, Training Loss: 0.0974, Training F1-score: 0.9412, Validation Loss: 0.2604, Validation F1-score: 0.8660
Epoch 5/100, Training Loss: 0.0899, Training F1-score: 0.9470, Validation Loss: 0.2213, Validation F1-score: 0.8622
Epoch 6/100, Training Loss: 0.0741, Training F1-score: 0.9573, Validation Loss: 0.2519, Validation F1-score: 0.8668
Epoch 7/100, Training Loss: 0.0714, Training F1-score: 0.9586, Validation Loss: 0.2444, Validation F1-score: 0.8685
Epoch 8/100, Training Loss: 0.0684, Training F1-score: 0.9600, Validation Loss: 0.2603, Validation F1-score: 0.8673
Epoch 9/100, Training Loss: 0.0668, Training F1-score: 0.9616, Validation Loss: 0.2494, Validation F1-score: 0.8473
Epoch 10/100, Training Loss: 0.0651, Training F1-score: 0.9619, Validation Loss: 0.2369, Validation F1-score: 0.8658
[I 2024-12-22 02:54:55,875] Trial 14 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.25, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 5, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0596, Training F1-score: 0.9658, Validation Loss: 0.2390, Validation F1-score: 0.8705
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:15 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.5
Learning rate: 1e-05
Optimizer: RMSprop
Momentum term: 0
Step size: 7
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 02:56:35,650] Trial 15 pruned. 
Epoch 1/100, Training Loss: 0.3605, Training F1-score: 0.7669, Validation Loss: 0.2495, Validation F1-score: 0.8371
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:16 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 9
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1812, Training F1-score: 0.8877, Validation Loss: 0.2026, Validation F1-score: 0.8656
Best F1-score till now on the current trail on Validation data: 0.8656491362128191
Epoch 2/100, Training Loss: 0.1275, Training F1-score: 0.9209, Validation Loss: 0.2298, Validation F1-score: 0.8447
Epoch 3/100, Training Loss: 0.1087, Training F1-score: 0.9349, Validation Loss: 0.2517, Validation F1-score: 0.8568
Epoch 4/100, Training Loss: 0.0975, Training F1-score: 0.9414, Validation Loss: 0.2148, Validation F1-score: 0.8619
Epoch 5/100, Training Loss: 0.0902, Training F1-score: 0.9466, Validation Loss: 0.2675, Validation F1-score: 0.8634
Epoch 6/100, Training Loss: 0.0844, Training F1-score: 0.9505, Validation Loss: 0.2144, Validation F1-score: 0.8656
Epoch 7/100, Training Loss: 0.0811, Training F1-score: 0.9528, Validation Loss: 0.2688, Validation F1-score: 0.8561
Epoch 8/100, Training Loss: 0.0774, Training F1-score: 0.9557, Validation Loss: 0.2352, Validation F1-score: 0.8604
Epoch 9/100, Training Loss: 0.0743, Training F1-score: 0.9559, Validation Loss: 0.2467, Validation F1-score: 0.8448
Epoch 10/100, Training Loss: 0.0618, Training F1-score: 0.9644, Validation Loss: 0.2785, Validation F1-score: 0.8677
Best F1-score till now on the current trail on Validation data: 0.867678731422008
Epoch 11/100, Training Loss: 0.0603, Training F1-score: 0.9654, Validation Loss: 0.2268, Validation F1-score: 0.8635
Epoch 12/100, Training Loss: 0.0580, Training F1-score: 0.9661, Validation Loss: 0.2847, Validation F1-score: 0.8634
Epoch 13/100, Training Loss: 0.0571, Training F1-score: 0.9659, Validation Loss: 0.2940, Validation F1-score: 0.8603
Epoch 14/100, Training Loss: 0.0569, Training F1-score: 0.9666, Validation Loss: 0.2272, Validation F1-score: 0.8608
Epoch 15/100, Training Loss: 0.0554, Training F1-score: 0.9668, Validation Loss: 0.2902, Validation F1-score: 0.8666
Epoch 16/100, Training Loss: 0.0552, Training F1-score: 0.9670, Validation Loss: 0.2631, Validation F1-score: 0.8543
Epoch 17/100, Training Loss: 0.0534, Training F1-score: 0.9681, Validation Loss: 0.2545, Validation F1-score: 0.8661
Epoch 18/100, Training Loss: 0.0540, Training F1-score: 0.9681, Validation Loss: 0.2955, Validation F1-score: 0.8494
Epoch 19/100, Training Loss: 0.0482, Training F1-score: 0.9713, Validation Loss: 0.2908, Validation F1-score: 0.8664
[I 2024-12-22 03:29:49,692] Trial 16 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.25, 'learning_rate': 0.00025, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 9, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 20/100, Training Loss: 0.0479, Training F1-score: 0.9721, Validation Loss: 0.2713, Validation F1-score: 0.8505
Early stopping triggered after 20 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:17 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 1e-06
Optimizer: RMSprop
Momentum term: 0
Step size: 12
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 03:31:34,343] Trial 17 pruned. 
Epoch 1/100, Training Loss: 0.6108, Training F1-score: 0.2417, Validation Loss: 0.5665, Validation F1-score: 0.2567
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:18 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1822, Training F1-score: 0.8894, Validation Loss: 0.2119, Validation F1-score: 0.8745
Best F1-score till now on the current trail on Validation data: 0.874549219762159
Epoch 2/100, Training Loss: 0.1274, Training F1-score: 0.9217, Validation Loss: 0.2212, Validation F1-score: 0.8603
Epoch 3/100, Training Loss: 0.1086, Training F1-score: 0.9349, Validation Loss: 0.2148, Validation F1-score: 0.8548
Epoch 4/100, Training Loss: 0.0975, Training F1-score: 0.9431, Validation Loss: 0.2247, Validation F1-score: 0.8619
Epoch 5/100, Training Loss: 0.0902, Training F1-score: 0.9473, Validation Loss: 0.2648, Validation F1-score: 0.8571
Epoch 6/100, Training Loss: 0.0737, Training F1-score: 0.9579, Validation Loss: 0.2439, Validation F1-score: 0.8616
Epoch 7/100, Training Loss: 0.0711, Training F1-score: 0.9597, Validation Loss: 0.2543, Validation F1-score: 0.8487
Epoch 8/100, Training Loss: 0.0690, Training F1-score: 0.9605, Validation Loss: 0.2561, Validation F1-score: 0.8664
Epoch 9/100, Training Loss: 0.0672, Training F1-score: 0.9613, Validation Loss: 0.2254, Validation F1-score: 0.8472
Epoch 10/100, Training Loss: 0.0657, Training F1-score: 0.9626, Validation Loss: 0.2627, Validation F1-score: 0.8520
[I 2024-12-22 03:50:38,903] Trial 18 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.28, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.0.ls2.gamma', 'step_size': 5, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0597, Training F1-score: 0.9667, Validation Loss: 0.2482, Validation F1-score: 0.8560
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:19 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 7
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1971, Training F1-score: 0.8768, Validation Loss: 0.1942, Validation F1-score: 0.8718
Best F1-score till now on the current trail on Validation data: 0.8717866196684416
Epoch 2/100, Training Loss: 0.1311, Training F1-score: 0.9198, Validation Loss: 0.2041, Validation F1-score: 0.8597
Epoch 3/100, Training Loss: 0.1126, Training F1-score: 0.9312, Validation Loss: 0.2183, Validation F1-score: 0.8739
Best F1-score till now on the current trail on Validation data: 0.8739393318783625
Epoch 4/100, Training Loss: 0.1026, Training F1-score: 0.9376, Validation Loss: 0.2268, Validation F1-score: 0.8433
Epoch 5/100, Training Loss: 0.0958, Training F1-score: 0.9434, Validation Loss: 0.2547, Validation F1-score: 0.8655
Epoch 6/100, Training Loss: 0.0898, Training F1-score: 0.9460, Validation Loss: 0.2473, Validation F1-score: 0.8677
Epoch 7/100, Training Loss: 0.0864, Training F1-score: 0.9482, Validation Loss: 0.2293, Validation F1-score: 0.8336
Epoch 8/100, Training Loss: 0.0707, Training F1-score: 0.9589, Validation Loss: 0.2378, Validation F1-score: 0.8567
Epoch 9/100, Training Loss: 0.0693, Training F1-score: 0.9599, Validation Loss: 0.2215, Validation F1-score: 0.8543
Epoch 10/100, Training Loss: 0.0680, Training F1-score: 0.9606, Validation Loss: 0.2492, Validation F1-score: 0.8591
Epoch 11/100, Training Loss: 0.0667, Training F1-score: 0.9621, Validation Loss: 0.2578, Validation F1-score: 0.8475
Epoch 12/100, Training Loss: 0.0656, Training F1-score: 0.9615, Validation Loss: 0.2542, Validation F1-score: 0.8485
[I 2024-12-22 04:12:44,099] Trial 19 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.4, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.5.ls2.gamma', 'step_size': 7, 'gamma': 0.3}. Best is trial 12 with value: 0.8930030233872418.
Epoch 13/100, Training Loss: 0.0640, Training F1-score: 0.9622, Validation Loss: 0.2499, Validation F1-score: 0.8371
Early stopping triggered after 13 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:20 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.5
Learning rate: 1e-07
Optimizer: RMSprop
Momentum term: 0
Step size: 14
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 04:14:29,422] Trial 20 pruned. 
Epoch 1/100, Training Loss: 0.6865, Training F1-score: 0.4831, Validation Loss: 0.6702, Validation F1-score: 0.4439
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:21 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 04:16:08,100] Trial 21 pruned. 
Epoch 1/100, Training Loss: 0.1812, Training F1-score: 0.8895, Validation Loss: 0.2231, Validation F1-score: 0.8500
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:22 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 7
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 04:17:52,368] Trial 22 pruned. 
Epoch 1/100, Training Loss: 0.1809, Training F1-score: 0.8880, Validation Loss: 0.2159, Validation F1-score: 0.8487
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:23 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 04:19:39,234] Trial 23 pruned. 
Epoch 1/100, Training Loss: 0.1819, Training F1-score: 0.8900, Validation Loss: 0.2166, Validation F1-score: 0.8637
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:24 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 1e-06
Optimizer: RMSprop
Momentum term: 0
Step size: 10
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 04:21:22,349] Trial 24 pruned. 
Epoch 1/100, Training Loss: 0.6155, Training F1-score: 0.2516, Validation Loss: 0.5670, Validation F1-score: 0.2707
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:25 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 04:23:03,501] Trial 25 pruned. 
Epoch 1/100, Training Loss: 0.1835, Training F1-score: 0.8850, Validation Loss: 0.2304, Validation F1-score: 0.8654
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:26 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.3
Learning rate: 1e-05
Optimizer: RMSprop
Momentum term: 0
Step size: 8
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 04:24:45,313] Trial 26 pruned. 
Epoch 1/100, Training Loss: 0.3454, Training F1-score: 0.7778, Validation Loss: 0.2428, Validation F1-score: 0.8364
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:27 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.2.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 04:26:29,978] Trial 27 pruned. 
Epoch 1/100, Training Loss: 0.1875, Training F1-score: 0.8834, Validation Loss: 0.2180, Validation F1-score: 0.8337
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:28 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 9
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1903, Training F1-score: 0.8832, Validation Loss: 0.2196, Validation F1-score: 0.8760
Best F1-score till now on the current trail on Validation data: 0.8760404670612448
Epoch 2/100, Training Loss: 0.1337, Training F1-score: 0.9176, Validation Loss: 0.2140, Validation F1-score: 0.8521
Epoch 3/100, Training Loss: 0.1142, Training F1-score: 0.9310, Validation Loss: 0.2073, Validation F1-score: 0.8698
Epoch 4/100, Training Loss: 0.1035, Training F1-score: 0.9365, Validation Loss: 0.2312, Validation F1-score: 0.8504
Epoch 5/100, Training Loss: 0.0969, Training F1-score: 0.9416, Validation Loss: 0.2391, Validation F1-score: 0.8670
Epoch 6/100, Training Loss: 0.0903, Training F1-score: 0.9461, Validation Loss: 0.2860, Validation F1-score: 0.8613
Epoch 7/100, Training Loss: 0.0871, Training F1-score: 0.9495, Validation Loss: 0.2307, Validation F1-score: 0.8575
Epoch 8/100, Training Loss: 0.0839, Training F1-score: 0.9506, Validation Loss: 0.2395, Validation F1-score: 0.8697
Epoch 9/100, Training Loss: 0.0807, Training F1-score: 0.9520, Validation Loss: 0.2563, Validation F1-score: 0.8599
Epoch 10/100, Training Loss: 0.0680, Training F1-score: 0.9604, Validation Loss: 0.2828, Validation F1-score: 0.8632
[I 2024-12-22 04:45:07,068] Trial 28 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.4, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma', 'step_size': 9, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0662, Training F1-score: 0.9613, Validation Loss: 0.2425, Validation F1-score: 0.8617
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:29 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.32
Learning rate: 2.5e-07
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 04:46:51,579] Trial 29 pruned. 
Epoch 1/100, Training Loss: 0.6748, Training F1-score: 0.3587, Validation Loss: 0.6469, Validation F1-score: 0.2689
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:30 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.5
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 11
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 04:48:34,649] Trial 30 pruned. 
Epoch 1/100, Training Loss: 0.1967, Training F1-score: 0.8798, Validation Loss: 0.2174, Validation F1-score: 0.8608
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:31 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 9
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 04:50:15,392] Trial 31 pruned. 
Epoch 1/100, Training Loss: 0.1821, Training F1-score: 0.8877, Validation Loss: 0.2060, Validation F1-score: 0.8520
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:32 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 7
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 04:51:57,141] Trial 32 pruned. 
Epoch 1/100, Training Loss: 0.1817, Training F1-score: 0.8866, Validation Loss: 0.2139, Validation F1-score: 0.8687
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:33 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.28
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1831, Training F1-score: 0.8858, Validation Loss: 0.2219, Validation F1-score: 0.8783
Best F1-score till now on the current trail on Validation data: 0.8782637372403066
Epoch 2/100, Training Loss: 0.1267, Training F1-score: 0.9213, Validation Loss: 0.2155, Validation F1-score: 0.8641
Epoch 3/100, Training Loss: 0.1099, Training F1-score: 0.9324, Validation Loss: 0.2591, Validation F1-score: 0.8599
Epoch 4/100, Training Loss: 0.0986, Training F1-score: 0.9406, Validation Loss: 0.2762, Validation F1-score: 0.8566
Epoch 5/100, Training Loss: 0.0919, Training F1-score: 0.9439, Validation Loss: 0.2302, Validation F1-score: 0.8680
Epoch 6/100, Training Loss: 0.0682, Training F1-score: 0.9604, Validation Loss: 0.2557, Validation F1-score: 0.8636
Epoch 7/100, Training Loss: 0.0659, Training F1-score: 0.9611, Validation Loss: 0.2438, Validation F1-score: 0.8618
Epoch 8/100, Training Loss: 0.0651, Training F1-score: 0.9621, Validation Loss: 0.2617, Validation F1-score: 0.8600
Epoch 9/100, Training Loss: 0.0635, Training F1-score: 0.9629, Validation Loss: 0.2816, Validation F1-score: 0.8603
Epoch 10/100, Training Loss: 0.0622, Training F1-score: 0.9638, Validation Loss: 0.2598, Validation F1-score: 0.8614
[I 2024-12-22 05:10:42,624] Trial 33 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.28, 'learning_rate': 0.00025, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 5, 'gamma': 0.1}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0601, Training F1-score: 0.9657, Validation Loss: 0.2578, Validation F1-score: 0.8559
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:34 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.6.ls2.gamma
Dropout: 0.2
Learning rate: 1e-07
Optimizer: Adam
Momentum term: 0
Step size: 9
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 05:12:28,218] Trial 34 pruned. 
Epoch 1/100, Training Loss: 0.7357, Training F1-score: 0.4346, Validation Loss: 0.7170, Validation F1-score: 0.4378
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:35 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 11
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1786, Training F1-score: 0.8894, Validation Loss: 0.2123, Validation F1-score: 0.8726
Best F1-score till now on the current trail on Validation data: 0.8725583367804056
Epoch 2/100, Training Loss: 0.1267, Training F1-score: 0.9226, Validation Loss: 0.2061, Validation F1-score: 0.8496
Epoch 3/100, Training Loss: 0.1083, Training F1-score: 0.9349, Validation Loss: 0.2648, Validation F1-score: 0.8664
Epoch 4/100, Training Loss: 0.0966, Training F1-score: 0.9424, Validation Loss: 0.2212, Validation F1-score: 0.8646
Epoch 5/100, Training Loss: 0.0909, Training F1-score: 0.9455, Validation Loss: 0.2345, Validation F1-score: 0.8373
Epoch 6/100, Training Loss: 0.0860, Training F1-score: 0.9491, Validation Loss: 0.2872, Validation F1-score: 0.8401
Epoch 7/100, Training Loss: 0.0805, Training F1-score: 0.9528, Validation Loss: 0.2602, Validation F1-score: 0.8592
Epoch 8/100, Training Loss: 0.0775, Training F1-score: 0.9548, Validation Loss: 0.2359, Validation F1-score: 0.8587
Epoch 9/100, Training Loss: 0.0756, Training F1-score: 0.9563, Validation Loss: 0.2351, Validation F1-score: 0.8389
Epoch 10/100, Training Loss: 0.0728, Training F1-score: 0.9580, Validation Loss: 0.2654, Validation F1-score: 0.8508
[I 2024-12-22 05:31:02,912] Trial 35 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.25, 'learning_rate': 0.00025, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma', 'step_size': 11, 'gamma': 0.5}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0708, Training F1-score: 0.9588, Validation Loss: 0.3613, Validation F1-score: 0.8638
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:36 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.32
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1951, Training F1-score: 0.8797, Validation Loss: 0.2132, Validation F1-score: 0.8761
Best F1-score till now on the current trail on Validation data: 0.8760718034052396
Epoch 2/100, Training Loss: 0.1297, Training F1-score: 0.9197, Validation Loss: 0.2314, Validation F1-score: 0.8500
Epoch 3/100, Training Loss: 0.1102, Training F1-score: 0.9341, Validation Loss: 0.2209, Validation F1-score: 0.8672
Epoch 4/100, Training Loss: 0.0977, Training F1-score: 0.9417, Validation Loss: 0.2597, Validation F1-score: 0.8689
Epoch 5/100, Training Loss: 0.0920, Training F1-score: 0.9445, Validation Loss: 0.2365, Validation F1-score: 0.8685
Epoch 6/100, Training Loss: 0.0846, Training F1-score: 0.9494, Validation Loss: 0.2861, Validation F1-score: 0.8543
Epoch 7/100, Training Loss: 0.0819, Training F1-score: 0.9513, Validation Loss: 0.2705, Validation F1-score: 0.8614
Epoch 8/100, Training Loss: 0.0780, Training F1-score: 0.9538, Validation Loss: 0.2980, Validation F1-score: 0.8504
Epoch 9/100, Training Loss: 0.0757, Training F1-score: 0.9555, Validation Loss: 0.2806, Validation F1-score: 0.8551
Epoch 10/100, Training Loss: 0.0750, Training F1-score: 0.9554, Validation Loss: 0.2346, Validation F1-score: 0.8588
[I 2024-12-22 05:49:33,488] Trial 36 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.32, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.5.ls2.gamma', 'step_size': 16, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0700, Training F1-score: 0.9582, Validation Loss: 0.2181, Validation F1-score: 0.8569
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:37 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.2
Learning rate: 1e-05
Optimizer: RMSprop
Momentum term: 0
Step size: 20
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 05:51:18,881] Trial 37 pruned. 
Epoch 1/100, Training Loss: 0.3212, Training F1-score: 0.7966, Validation Loss: 0.2397, Validation F1-score: 0.8439
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:38 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.35
Learning rate: 2.5e-06
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 05:53:00,706] Trial 38 pruned. 
Epoch 1/100, Training Loss: 0.5762, Training F1-score: 0.3971, Validation Loss: 0.4727, Validation F1-score: 0.5626
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:39 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.6.ls2.gamma
Dropout: 0.3
Learning rate: 2.5e-07
Optimizer: RMSprop
Momentum term: 0
Step size: 8
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 05:54:42,799] Trial 39 pruned. 
Epoch 1/100, Training Loss: 0.6623, Training F1-score: 0.3787, Validation Loss: 0.6367, Validation F1-score: 0.2930
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:40 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.25
Learning rate: 1e-06
Optimizer: SGD
Momentum term: 0.8187962684586123
Step size: 11
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 05:56:21,441] Trial 40 pruned. 
Epoch 1/100, Training Loss: 0.7339, Training F1-score: 0.4410, Validation Loss: 0.7217, Validation F1-score: 0.4349
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:41 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1855, Training F1-score: 0.8854, Validation Loss: 0.2101, Validation F1-score: 0.8792
Best F1-score till now on the current trail on Validation data: 0.8791726741597906
Epoch 2/100, Training Loss: 0.1276, Training F1-score: 0.9215, Validation Loss: 0.2246, Validation F1-score: 0.8757
Epoch 3/100, Training Loss: 0.1100, Training F1-score: 0.9332, Validation Loss: 0.2073, Validation F1-score: 0.8410
Epoch 4/100, Training Loss: 0.0990, Training F1-score: 0.9413, Validation Loss: 0.2302, Validation F1-score: 0.8514
Epoch 5/100, Training Loss: 0.0917, Training F1-score: 0.9465, Validation Loss: 0.2803, Validation F1-score: 0.8538
Epoch 6/100, Training Loss: 0.0747, Training F1-score: 0.9566, Validation Loss: 0.2779, Validation F1-score: 0.7995
Epoch 7/100, Training Loss: 0.0719, Training F1-score: 0.9589, Validation Loss: 0.2270, Validation F1-score: 0.8471
Epoch 8/100, Training Loss: 0.0694, Training F1-score: 0.9599, Validation Loss: 0.2367, Validation F1-score: 0.8569
Epoch 9/100, Training Loss: 0.0677, Training F1-score: 0.9607, Validation Loss: 0.2660, Validation F1-score: 0.8359
Epoch 10/100, Training Loss: 0.0659, Training F1-score: 0.9619, Validation Loss: 0.2714, Validation F1-score: 0.8601
[I 2024-12-22 06:14:56,598] Trial 41 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.28, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.0.ls2.gamma', 'step_size': 5, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0609, Training F1-score: 0.9650, Validation Loss: 0.2519, Validation F1-score: 0.8491
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:42 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 06:16:32,877] Trial 42 pruned. 
Epoch 1/100, Training Loss: 0.1835, Training F1-score: 0.8866, Validation Loss: 0.2075, Validation F1-score: 0.8446
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:43 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.3
Learning rate: 2.5e-05
Optimizer: RMSprop
Momentum term: 0
Step size: 7
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 06:18:14,462] Trial 43 pruned. 
Epoch 1/100, Training Loss: 0.2371, Training F1-score: 0.8568, Validation Loss: 0.2162, Validation F1-score: 0.8561
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:44 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 06:19:59,078] Trial 44 pruned. 
Epoch 1/100, Training Loss: 0.1831, Training F1-score: 0.8875, Validation Loss: 0.1966, Validation F1-score: 0.8700
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:45 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.2.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: SGD
Momentum term: 0.8014977302163634
Step size: 8
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 06:21:37,585] Trial 45 pruned. 
Epoch 1/100, Training Loss: 0.5629, Training F1-score: 0.3961, Validation Loss: 0.4495, Validation F1-score: 0.6653
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:46 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.35
Learning rate: 2.5e-06
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 06:23:21,087] Trial 46 pruned. 
Epoch 1/100, Training Loss: 0.5494, Training F1-score: 0.4437, Validation Loss: 0.4534, Validation F1-score: 0.6598
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:47 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.28
Learning rate: 1e-07
Optimizer: RMSprop
Momentum term: 0
Step size: 18
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 06:25:02,356] Trial 47 pruned. 
Epoch 1/100, Training Loss: 0.6898, Training F1-score: 0.3375, Validation Loss: 0.6745, Validation F1-score: 0.1878
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:48 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1840, Training F1-score: 0.8860, Validation Loss: 0.2187, Validation F1-score: 0.8739
Best F1-score till now on the current trail on Validation data: 0.8738598353591839
Epoch 2/100, Training Loss: 0.1271, Training F1-score: 0.9228, Validation Loss: 0.1990, Validation F1-score: 0.8833
Best F1-score till now on the current trail on Validation data: 0.883318158992087
Epoch 3/100, Training Loss: 0.1099, Training F1-score: 0.9334, Validation Loss: 0.1987, Validation F1-score: 0.8718
Epoch 4/100, Training Loss: 0.0998, Training F1-score: 0.9397, Validation Loss: 0.2709, Validation F1-score: 0.8490
Epoch 5/100, Training Loss: 0.0910, Training F1-score: 0.9464, Validation Loss: 0.2612, Validation F1-score: 0.8655
Epoch 6/100, Training Loss: 0.0757, Training F1-score: 0.9556, Validation Loss: 0.2552, Validation F1-score: 0.8484
Epoch 7/100, Training Loss: 0.0730, Training F1-score: 0.9587, Validation Loss: 0.2279, Validation F1-score: 0.8516
Epoch 8/100, Training Loss: 0.0700, Training F1-score: 0.9598, Validation Loss: 0.2596, Validation F1-score: 0.8590
Epoch 9/100, Training Loss: 0.0685, Training F1-score: 0.9601, Validation Loss: 0.2529, Validation F1-score: 0.8522
Epoch 10/100, Training Loss: 0.0665, Training F1-score: 0.9612, Validation Loss: 0.2345, Validation F1-score: 0.8659
Epoch 11/100, Training Loss: 0.0613, Training F1-score: 0.9649, Validation Loss: 0.2561, Validation F1-score: 0.8600
[I 2024-12-22 06:45:30,640] Trial 48 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.3, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 5, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 12/100, Training Loss: 0.0600, Training F1-score: 0.9655, Validation Loss: 0.2657, Validation F1-score: 0.8569
Early stopping triggered after 12 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:49 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.00025
Optimizer: SGD
Momentum term: 0.9760853941755931
Step size: 7
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 06:47:13,290] Trial 49 pruned. 
Epoch 1/100, Training Loss: 0.2370, Training F1-score: 0.8529, Validation Loss: 0.2127, Validation F1-score: 0.8521
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:50 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.6.ls2.gamma
Dropout: 0.25
Learning rate: 2.5e-05
Optimizer: RMSprop
Momentum term: 0
Step size: 13
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 06:49:01,000] Trial 50 pruned. 
Epoch 1/100, Training Loss: 0.2350, Training F1-score: 0.8580, Validation Loss: 0.2191, Validation F1-score: 0.8376
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:51 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 7
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 06:50:45,029] Trial 51 pruned. 
Epoch 1/100, Training Loss: 0.1998, Training F1-score: 0.8765, Validation Loss: 0.2390, Validation F1-score: 0.8688
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:52 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1967, Training F1-score: 0.8774, Validation Loss: 0.1960, Validation F1-score: 0.8809
Best F1-score till now on the current trail on Validation data: 0.8808869686970313
Epoch 2/100, Training Loss: 0.1325, Training F1-score: 0.9191, Validation Loss: 0.2483, Validation F1-score: 0.8206
Epoch 3/100, Training Loss: 0.1133, Training F1-score: 0.9303, Validation Loss: 0.2044, Validation F1-score: 0.8749
Epoch 4/100, Training Loss: 0.1026, Training F1-score: 0.9392, Validation Loss: 0.2093, Validation F1-score: 0.8700
Epoch 5/100, Training Loss: 0.0948, Training F1-score: 0.9432, Validation Loss: 0.2127, Validation F1-score: 0.8557
Epoch 6/100, Training Loss: 0.0902, Training F1-score: 0.9463, Validation Loss: 0.2588, Validation F1-score: 0.8432
Epoch 7/100, Training Loss: 0.0740, Training F1-score: 0.9570, Validation Loss: 0.2516, Validation F1-score: 0.8604
Epoch 8/100, Training Loss: 0.0718, Training F1-score: 0.9575, Validation Loss: 0.2464, Validation F1-score: 0.8566
Epoch 9/100, Training Loss: 0.0696, Training F1-score: 0.9589, Validation Loss: 0.2516, Validation F1-score: 0.8608
Epoch 10/100, Training Loss: 0.0691, Training F1-score: 0.9595, Validation Loss: 0.2665, Validation F1-score: 0.8538
[I 2024-12-22 07:09:27,994] Trial 52 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.4, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.5.ls2.gamma', 'step_size': 6, 'gamma': 0.3}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0670, Training F1-score: 0.9605, Validation Loss: 0.2565, Validation F1-score: 0.8599
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:53 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 07:11:13,110] Trial 53 pruned. 
Epoch 1/100, Training Loss: 0.1978, Training F1-score: 0.8760, Validation Loss: 0.2085, Validation F1-score: 0.8435
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:54 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 7
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 07:13:00,980] Trial 54 pruned. 
Epoch 1/100, Training Loss: 0.1884, Training F1-score: 0.8833, Validation Loss: 0.2206, Validation F1-score: 0.8448
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:55 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.5
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 8
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 07:14:49,243] Trial 55 pruned. 
Epoch 1/100, Training Loss: 0.2021, Training F1-score: 0.8761, Validation Loss: 0.1973, Validation F1-score: 0.8619
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:56 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.32
Learning rate: 1e-06
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 07:16:47,747] Trial 56 pruned. 
Epoch 1/100, Training Loss: 0.6262, Training F1-score: 0.2611, Validation Loss: 0.5810, Validation F1-score: 0.2679
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:57 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 2.5e-07
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 07:18:35,355] Trial 57 pruned. 
Epoch 1/100, Training Loss: 0.6508, Training F1-score: 0.3383, Validation Loss: 0.6229, Validation F1-score: 0.2585
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:58 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.2.ls2.gamma
Dropout: 0.2
Learning rate: 1e-05
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 07:20:23,284] Trial 58 pruned. 
Epoch 1/100, Training Loss: 0.3290, Training F1-score: 0.7829, Validation Loss: 0.2394, Validation F1-score: 0.8363
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:59 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 9
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 07:22:12,137] Trial 59 pruned. 
Epoch 1/100, Training Loss: 0.1858, Training F1-score: 0.8855, Validation Loss: 0.2099, Validation F1-score: 0.8479
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:60 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: SGD
Momentum term: 0.8430950763448567
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 07:23:59,727] Trial 60 pruned. 
Epoch 1/100, Training Loss: 0.5356, Training F1-score: 0.4785, Validation Loss: 0.3885, Validation F1-score: 0.7341
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:61 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 10
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 07:25:46,994] Trial 61 pruned. 
Epoch 1/100, Training Loss: 0.1919, Training F1-score: 0.8820, Validation Loss: 0.2347, Validation F1-score: 0.8325
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:62 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 9
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1917, Training F1-score: 0.8831, Validation Loss: 0.2074, Validation F1-score: 0.8738
Best F1-score till now on the current trail on Validation data: 0.873822391213677
Epoch 2/100, Training Loss: 0.1347, Training F1-score: 0.9169, Validation Loss: 0.2184, Validation F1-score: 0.8743
Best F1-score till now on the current trail on Validation data: 0.874342164174457
Epoch 3/100, Training Loss: 0.1149, Training F1-score: 0.9303, Validation Loss: 0.2012, Validation F1-score: 0.8719
Epoch 4/100, Training Loss: 0.1042, Training F1-score: 0.9374, Validation Loss: 0.2266, Validation F1-score: 0.8667
Epoch 5/100, Training Loss: 0.0965, Training F1-score: 0.9438, Validation Loss: 0.2064, Validation F1-score: 0.8586
Epoch 6/100, Training Loss: 0.0908, Training F1-score: 0.9452, Validation Loss: 0.2201, Validation F1-score: 0.8570
Epoch 7/100, Training Loss: 0.0857, Training F1-score: 0.9499, Validation Loss: 0.2303, Validation F1-score: 0.8572
Epoch 8/100, Training Loss: 0.0836, Training F1-score: 0.9519, Validation Loss: 0.2906, Validation F1-score: 0.8669
Epoch 9/100, Training Loss: 0.0804, Training F1-score: 0.9522, Validation Loss: 0.2575, Validation F1-score: 0.8607
Epoch 10/100, Training Loss: 0.0672, Training F1-score: 0.9609, Validation Loss: 0.2483, Validation F1-score: 0.8576
Epoch 11/100, Training Loss: 0.0653, Training F1-score: 0.9626, Validation Loss: 0.2459, Validation F1-score: 0.8638
[I 2024-12-22 07:46:28,660] Trial 62 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.4, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma', 'step_size': 9, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 12/100, Training Loss: 0.0648, Training F1-score: 0.9622, Validation Loss: 0.2969, Validation F1-score: 0.8306
Early stopping triggered after 12 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:63 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 7
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 07:48:13,967] Trial 63 pruned. 
Epoch 1/100, Training Loss: 0.1903, Training F1-score: 0.8845, Validation Loss: 0.2124, Validation F1-score: 0.8204
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:64 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 8
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 07:50:00,017] Trial 64 pruned. 
Epoch 1/100, Training Loss: 0.1816, Training F1-score: 0.8869, Validation Loss: 0.2076, Validation F1-score: 0.8609
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:65 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 10
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1882, Training F1-score: 0.8836, Validation Loss: 0.1983, Validation F1-score: 0.8801
Best F1-score till now on the current trail on Validation data: 0.8800761366222546
Epoch 2/100, Training Loss: 0.1312, Training F1-score: 0.9199, Validation Loss: 0.2151, Validation F1-score: 0.8765
Epoch 3/100, Training Loss: 0.1142, Training F1-score: 0.9302, Validation Loss: 0.2166, Validation F1-score: 0.8697
Epoch 4/100, Training Loss: 0.1036, Training F1-score: 0.9382, Validation Loss: 0.2125, Validation F1-score: 0.8615
Epoch 5/100, Training Loss: 0.0966, Training F1-score: 0.9430, Validation Loss: 0.2053, Validation F1-score: 0.8636
Epoch 6/100, Training Loss: 0.0901, Training F1-score: 0.9461, Validation Loss: 0.2665, Validation F1-score: 0.8438
Epoch 7/100, Training Loss: 0.0874, Training F1-score: 0.9486, Validation Loss: 0.2858, Validation F1-score: 0.8565
Epoch 8/100, Training Loss: 0.0840, Training F1-score: 0.9508, Validation Loss: 0.2773, Validation F1-score: 0.8552
Epoch 9/100, Training Loss: 0.0803, Training F1-score: 0.9528, Validation Loss: 0.3120, Validation F1-score: 0.8628
Epoch 10/100, Training Loss: 0.0788, Training F1-score: 0.9548, Validation Loss: 0.2578, Validation F1-score: 0.8451
[I 2024-12-22 08:08:52,567] Trial 65 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.4, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 10, 'gamma': 0.2}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0630, Training F1-score: 0.9638, Validation Loss: 0.2643, Validation F1-score: 0.8598
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:66 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 1e-07
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 08:10:37,149] Trial 66 pruned. 
Epoch 1/100, Training Loss: 0.6653, Training F1-score: 0.2800, Validation Loss: 0.6491, Validation F1-score: 0.2290
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:67 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.35
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 12
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1939, Training F1-score: 0.8795, Validation Loss: 0.2138, Validation F1-score: 0.8782
Best F1-score till now on the current trail on Validation data: 0.8782451901327454
Epoch 2/100, Training Loss: 0.1301, Training F1-score: 0.9199, Validation Loss: 0.2030, Validation F1-score: 0.8753
Epoch 3/100, Training Loss: 0.1086, Training F1-score: 0.9337, Validation Loss: 0.2306, Validation F1-score: 0.8665
Epoch 4/100, Training Loss: 0.0995, Training F1-score: 0.9405, Validation Loss: 0.2176, Validation F1-score: 0.8632
Epoch 5/100, Training Loss: 0.0914, Training F1-score: 0.9458, Validation Loss: 0.2203, Validation F1-score: 0.8643
Epoch 6/100, Training Loss: 0.0867, Training F1-score: 0.9489, Validation Loss: 0.2311, Validation F1-score: 0.8539
Epoch 7/100, Training Loss: 0.0823, Training F1-score: 0.9518, Validation Loss: 0.2526, Validation F1-score: 0.8457
Epoch 8/100, Training Loss: 0.0789, Training F1-score: 0.9539, Validation Loss: 0.2265, Validation F1-score: 0.8768
Epoch 9/100, Training Loss: 0.0759, Training F1-score: 0.9553, Validation Loss: 0.2533, Validation F1-score: 0.8686
Epoch 10/100, Training Loss: 0.0732, Training F1-score: 0.9574, Validation Loss: 0.2360, Validation F1-score: 0.8654
[I 2024-12-22 08:29:48,746] Trial 67 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.35, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.0.ls2.gamma', 'step_size': 12, 'gamma': 0.3}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0727, Training F1-score: 0.9571, Validation Loss: 0.2562, Validation F1-score: 0.8589
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:68 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.5
Learning rate: 2.5e-06
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 08:31:38,174] Trial 68 pruned. 
Epoch 1/100, Training Loss: 0.5682, Training F1-score: 0.4103, Validation Loss: 0.4783, Validation F1-score: 0.5491
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:69 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.3
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 7
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1827, Training F1-score: 0.8860, Validation Loss: 0.1968, Validation F1-score: 0.8769
Best F1-score till now on the current trail on Validation data: 0.8768871268507544
Epoch 2/100, Training Loss: 0.1296, Training F1-score: 0.9204, Validation Loss: 0.2358, Validation F1-score: 0.8289
Epoch 3/100, Training Loss: 0.1101, Training F1-score: 0.9342, Validation Loss: 0.2063, Validation F1-score: 0.8850
Best F1-score till now on the current trail on Validation data: 0.8850217361398922
Epoch 4/100, Training Loss: 0.1000, Training F1-score: 0.9399, Validation Loss: 0.2246, Validation F1-score: 0.8596
Epoch 5/100, Training Loss: 0.0930, Training F1-score: 0.9448, Validation Loss: 0.2775, Validation F1-score: 0.8493
Epoch 6/100, Training Loss: 0.0887, Training F1-score: 0.9468, Validation Loss: 0.2457, Validation F1-score: 0.8376
Epoch 7/100, Training Loss: 0.0837, Training F1-score: 0.9509, Validation Loss: 0.2718, Validation F1-score: 0.8538
Epoch 8/100, Training Loss: 0.0646, Training F1-score: 0.9627, Validation Loss: 0.2572, Validation F1-score: 0.8594
Epoch 9/100, Training Loss: 0.0620, Training F1-score: 0.9639, Validation Loss: 0.2715, Validation F1-score: 0.8575
Epoch 10/100, Training Loss: 0.0612, Training F1-score: 0.9643, Validation Loss: 0.2476, Validation F1-score: 0.8443
Epoch 11/100, Training Loss: 0.0602, Training F1-score: 0.9651, Validation Loss: 0.2622, Validation F1-score: 0.8523
Epoch 12/100, Training Loss: 0.0594, Training F1-score: 0.9650, Validation Loss: 0.2783, Validation F1-score: 0.8527
[I 2024-12-22 08:54:00,586] Trial 69 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.3, 'learning_rate': 0.00025, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma', 'step_size': 7, 'gamma': 0.1}. Best is trial 12 with value: 0.8930030233872418.
Epoch 13/100, Training Loss: 0.0592, Training F1-score: 0.9663, Validation Loss: 0.2645, Validation F1-score: 0.8545
Early stopping triggered after 13 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:70 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 8
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1814, Training F1-score: 0.8884, Validation Loss: 0.1918, Validation F1-score: 0.8775
Best F1-score till now on the current trail on Validation data: 0.877486213382818
Epoch 2/100, Training Loss: 0.1261, Training F1-score: 0.9229, Validation Loss: 0.2647, Validation F1-score: 0.8536
Epoch 3/100, Training Loss: 0.1076, Training F1-score: 0.9348, Validation Loss: 0.2735, Validation F1-score: 0.8160
Epoch 4/100, Training Loss: 0.0978, Training F1-score: 0.9416, Validation Loss: 0.2159, Validation F1-score: 0.8557
Epoch 5/100, Training Loss: 0.0904, Training F1-score: 0.9464, Validation Loss: 0.2192, Validation F1-score: 0.8704
Epoch 6/100, Training Loss: 0.0845, Training F1-score: 0.9495, Validation Loss: 0.2438, Validation F1-score: 0.8577
Epoch 7/100, Training Loss: 0.0808, Training F1-score: 0.9526, Validation Loss: 0.2454, Validation F1-score: 0.8686
Epoch 8/100, Training Loss: 0.0765, Training F1-score: 0.9555, Validation Loss: 0.2725, Validation F1-score: 0.8538
Epoch 9/100, Training Loss: 0.0638, Training F1-score: 0.9631, Validation Loss: 0.2653, Validation F1-score: 0.8588
Epoch 10/100, Training Loss: 0.0623, Training F1-score: 0.9632, Validation Loss: 0.2550, Validation F1-score: 0.8361
[I 2024-12-22 09:12:50,450] Trial 70 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.25, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma', 'step_size': 8, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0609, Training F1-score: 0.9640, Validation Loss: 0.2334, Validation F1-score: 0.8579
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:71 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.28
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:14:34,843] Trial 71 pruned. 
Epoch 1/100, Training Loss: 0.1798, Training F1-score: 0.8869, Validation Loss: 0.2115, Validation F1-score: 0.8520
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:72 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.28
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:16:18,737] Trial 72 pruned. 
Epoch 1/100, Training Loss: 0.1821, Training F1-score: 0.8861, Validation Loss: 0.2275, Validation F1-score: 0.8413
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:73 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.28
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:18:03,258] Trial 73 pruned. 
Epoch 1/100, Training Loss: 0.1820, Training F1-score: 0.8874, Validation Loss: 0.2341, Validation F1-score: 0.8491
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:74 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.28
Learning rate: 2.5e-05
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:19:51,681] Trial 74 pruned. 
Epoch 1/100, Training Loss: 0.2508, Training F1-score: 0.8462, Validation Loss: 0.2092, Validation F1-score: 0.8712
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:75 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.6.ls2.gamma
Dropout: 0.32
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:21:36,800] Trial 75 pruned. 
Epoch 1/100, Training Loss: 0.1856, Training F1-score: 0.8828, Validation Loss: 0.2090, Validation F1-score: 0.8532
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:76 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.2
Learning rate: 1e-05
Optimizer: RMSprop
Momentum term: 0
Step size: 9
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:23:22,607] Trial 76 pruned. 
Epoch 1/100, Training Loss: 0.3406, Training F1-score: 0.7832, Validation Loss: 0.2437, Validation F1-score: 0.8338
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:77 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.2.ls2.gamma
Dropout: 0.28
Learning rate: 2.5e-07
Optimizer: RMSprop
Momentum term: 0
Step size: 7
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:25:08,030] Trial 77 pruned. 
Epoch 1/100, Training Loss: 0.6623, Training F1-score: 0.3501, Validation Loss: 0.6332, Validation F1-score: 0.2378
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:78 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.25
Learning rate: 1e-06
Optimizer: SGD
Momentum term: 0.8861282271842594
Step size: 5
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:26:53,994] Trial 78 pruned. 
Epoch 1/100, Training Loss: 0.6798, Training F1-score: 0.4266, Validation Loss: 0.6723, Validation F1-score: 0.2946
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:79 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:28:40,862] Trial 79 pruned. 
Epoch 1/100, Training Loss: 0.1877, Training F1-score: 0.8841, Validation Loss: 0.2037, Validation F1-score: 0.8683
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:80 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 14
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:30:27,890] Trial 80 pruned. 
Epoch 1/100, Training Loss: 0.1850, Training F1-score: 0.8860, Validation Loss: 0.2066, Validation F1-score: 0.8661
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:81 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 10
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1806, Training F1-score: 0.8877, Validation Loss: 0.1997, Validation F1-score: 0.8843
Best F1-score till now on the current trail on Validation data: 0.8843149661151105
Epoch 2/100, Training Loss: 0.1260, Training F1-score: 0.9227, Validation Loss: 0.1972, Validation F1-score: 0.8721
Epoch 3/100, Training Loss: 0.1076, Training F1-score: 0.9340, Validation Loss: 0.2279, Validation F1-score: 0.8409
Epoch 4/100, Training Loss: 0.0989, Training F1-score: 0.9419, Validation Loss: 0.2110, Validation F1-score: 0.8677
Epoch 5/100, Training Loss: 0.0912, Training F1-score: 0.9459, Validation Loss: 0.2435, Validation F1-score: 0.8628
Epoch 6/100, Training Loss: 0.0865, Training F1-score: 0.9494, Validation Loss: 0.2383, Validation F1-score: 0.8612
Epoch 7/100, Training Loss: 0.0822, Training F1-score: 0.9515, Validation Loss: 0.2615, Validation F1-score: 0.8464
Epoch 8/100, Training Loss: 0.0776, Training F1-score: 0.9533, Validation Loss: 0.3040, Validation F1-score: 0.8116
Epoch 9/100, Training Loss: 0.0759, Training F1-score: 0.9557, Validation Loss: 0.2892, Validation F1-score: 0.8455
Epoch 10/100, Training Loss: 0.0719, Training F1-score: 0.9577, Validation Loss: 0.2246, Validation F1-score: 0.8537
[I 2024-12-22 09:49:31,282] Trial 81 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.25, 'learning_rate': 0.00025, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma', 'step_size': 10, 'gamma': 0.5}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0613, Training F1-score: 0.9636, Validation Loss: 0.2531, Validation F1-score: 0.8651
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:82 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 12
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:51:14,256] Trial 82 pruned. 
Epoch 1/100, Training Loss: 0.1785, Training F1-score: 0.8886, Validation Loss: 0.2263, Validation F1-score: 0.8101
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:83 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 9
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:52:58,278] Trial 83 pruned. 
Epoch 1/100, Training Loss: 0.1796, Training F1-score: 0.8867, Validation Loss: 0.1921, Validation F1-score: 0.8720
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:84 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.28
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 11
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 09:54:43,296] Trial 84 pruned. 
Epoch 1/100, Training Loss: 0.1805, Training F1-score: 0.8886, Validation Loss: 0.2090, Validation F1-score: 0.8554
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:85 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 11
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1978, Training F1-score: 0.8785, Validation Loss: 0.2033, Validation F1-score: 0.8749
Best F1-score till now on the current trail on Validation data: 0.8749035170210818
Epoch 2/100, Training Loss: 0.1335, Training F1-score: 0.9178, Validation Loss: 0.2065, Validation F1-score: 0.8762
Best F1-score till now on the current trail on Validation data: 0.8762123783314953
Epoch 3/100, Training Loss: 0.1135, Training F1-score: 0.9312, Validation Loss: 0.2157, Validation F1-score: 0.8510
Epoch 4/100, Training Loss: 0.1033, Training F1-score: 0.9384, Validation Loss: 0.2121, Validation F1-score: 0.8731
Epoch 5/100, Training Loss: 0.0964, Training F1-score: 0.9436, Validation Loss: 0.2342, Validation F1-score: 0.8541
Epoch 6/100, Training Loss: 0.0899, Training F1-score: 0.9469, Validation Loss: 0.2189, Validation F1-score: 0.8671
Epoch 7/100, Training Loss: 0.0849, Training F1-score: 0.9497, Validation Loss: 0.2809, Validation F1-score: 0.8569
Epoch 8/100, Training Loss: 0.0817, Training F1-score: 0.9531, Validation Loss: 0.2258, Validation F1-score: 0.8438
Epoch 9/100, Training Loss: 0.0787, Training F1-score: 0.9539, Validation Loss: 0.2128, Validation F1-score: 0.8458
Epoch 10/100, Training Loss: 0.0758, Training F1-score: 0.9552, Validation Loss: 0.2589, Validation F1-score: 0.8497
Epoch 11/100, Training Loss: 0.0734, Training F1-score: 0.9569, Validation Loss: 0.2139, Validation F1-score: 0.8588
[I 2024-12-22 10:15:29,124] Trial 85 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.4, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 11, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 12/100, Training Loss: 0.0639, Training F1-score: 0.9633, Validation Loss: 0.2851, Validation F1-score: 0.8595
Early stopping triggered after 12 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:86 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.25
Learning rate: 1e-07
Optimizer: RMSprop
Momentum term: 0
Step size: 13
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 10:17:14,379] Trial 86 pruned. 
Epoch 1/100, Training Loss: 0.6996, Training F1-score: 0.4595, Validation Loss: 0.6769, Validation F1-score: 0.4159
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:87 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.35
Learning rate: 2.5e-06
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 10:19:00,509] Trial 87 pruned. 
Epoch 1/100, Training Loss: 0.5581, Training F1-score: 0.4272, Validation Loss: 0.4428, Validation F1-score: 0.6664
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:88 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 8
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 10:20:46,341] Trial 88 pruned. 
Epoch 1/100, Training Loss: 0.1831, Training F1-score: 0.8880, Validation Loss: 0.2056, Validation F1-score: 0.8724
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:89 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.6.ls2.gamma
Dropout: 0.3
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 10:22:33,295] Trial 89 pruned. 
Epoch 1/100, Training Loss: 0.1860, Training F1-score: 0.8827, Validation Loss: 0.1934, Validation F1-score: 0.8706
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:90 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 7
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1909, Training F1-score: 0.8828, Validation Loss: 0.2024, Validation F1-score: 0.8771
Best F1-score till now on the current trail on Validation data: 0.8770632657897165
Epoch 2/100, Training Loss: 0.1353, Training F1-score: 0.9161, Validation Loss: 0.2188, Validation F1-score: 0.8805
Best F1-score till now on the current trail on Validation data: 0.8805204055247361
Epoch 3/100, Training Loss: 0.1143, Training F1-score: 0.9305, Validation Loss: 0.2268, Validation F1-score: 0.8476
Epoch 4/100, Training Loss: 0.1047, Training F1-score: 0.9366, Validation Loss: 0.2513, Validation F1-score: 0.8669
Epoch 5/100, Training Loss: 0.0969, Training F1-score: 0.9423, Validation Loss: 0.2573, Validation F1-score: 0.8607
Epoch 6/100, Training Loss: 0.0909, Training F1-score: 0.9460, Validation Loss: 0.2373, Validation F1-score: 0.8649
Epoch 7/100, Training Loss: 0.0867, Training F1-score: 0.9489, Validation Loss: 0.2394, Validation F1-score: 0.8724
Epoch 8/100, Training Loss: 0.0728, Training F1-score: 0.9584, Validation Loss: 0.2537, Validation F1-score: 0.8637
Epoch 9/100, Training Loss: 0.0701, Training F1-score: 0.9594, Validation Loss: 0.2500, Validation F1-score: 0.8558
Epoch 10/100, Training Loss: 0.0678, Training F1-score: 0.9613, Validation Loss: 0.2816, Validation F1-score: 0.8684
Epoch 11/100, Training Loss: 0.0670, Training F1-score: 0.9609, Validation Loss: 0.2724, Validation F1-score: 0.8623
[I 2024-12-22 10:43:10,078] Trial 90 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.4, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma', 'step_size': 7, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 12/100, Training Loss: 0.0657, Training F1-score: 0.9619, Validation Loss: 0.2814, Validation F1-score: 0.8611
Early stopping triggered after 12 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:91 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.32
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 16
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 10:44:53,002] Trial 91 pruned. 
Epoch 1/100, Training Loss: 0.1932, Training F1-score: 0.8808, Validation Loss: 0.2578, Validation F1-score: 0.8638
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:92 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.32
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 14
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1911, Training F1-score: 0.8832, Validation Loss: 0.1989, Validation F1-score: 0.8808
Best F1-score till now on the current trail on Validation data: 0.8808104058143558
Epoch 2/100, Training Loss: 0.1270, Training F1-score: 0.9219, Validation Loss: 0.2194, Validation F1-score: 0.8353
Epoch 3/100, Training Loss: 0.1084, Training F1-score: 0.9342, Validation Loss: 0.2167, Validation F1-score: 0.8764
Epoch 4/100, Training Loss: 0.0971, Training F1-score: 0.9424, Validation Loss: 0.2528, Validation F1-score: 0.8551
Epoch 5/100, Training Loss: 0.0903, Training F1-score: 0.9466, Validation Loss: 0.2133, Validation F1-score: 0.8605
Epoch 6/100, Training Loss: 0.0838, Training F1-score: 0.9508, Validation Loss: 0.2236, Validation F1-score: 0.8741
Epoch 7/100, Training Loss: 0.0810, Training F1-score: 0.9523, Validation Loss: 0.2915, Validation F1-score: 0.8660
Epoch 8/100, Training Loss: 0.0778, Training F1-score: 0.9547, Validation Loss: 0.2155, Validation F1-score: 0.8598
Epoch 9/100, Training Loss: 0.0756, Training F1-score: 0.9555, Validation Loss: 0.2425, Validation F1-score: 0.8620
Epoch 10/100, Training Loss: 0.0728, Training F1-score: 0.9572, Validation Loss: 0.2550, Validation F1-score: 0.8472
[I 2024-12-22 11:04:29,229] Trial 92 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.32, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.5.ls2.gamma', 'step_size': 14, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0710, Training F1-score: 0.9583, Validation Loss: 0.2545, Validation F1-score: 0.8598
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:93 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.32
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 11
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1910, Training F1-score: 0.8817, Validation Loss: 0.1922, Validation F1-score: 0.8830
Best F1-score till now on the current trail on Validation data: 0.8830256386089618
Epoch 2/100, Training Loss: 0.1258, Training F1-score: 0.9228, Validation Loss: 0.2051, Validation F1-score: 0.8729
Epoch 3/100, Training Loss: 0.1079, Training F1-score: 0.9340, Validation Loss: 0.2215, Validation F1-score: 0.8510
Epoch 4/100, Training Loss: 0.0965, Training F1-score: 0.9431, Validation Loss: 0.2520, Validation F1-score: 0.8433
Epoch 5/100, Training Loss: 0.0892, Training F1-score: 0.9473, Validation Loss: 0.2663, Validation F1-score: 0.8558
Epoch 6/100, Training Loss: 0.0857, Training F1-score: 0.9494, Validation Loss: 0.2169, Validation F1-score: 0.8525
Epoch 7/100, Training Loss: 0.0808, Training F1-score: 0.9524, Validation Loss: 0.2288, Validation F1-score: 0.8703
Epoch 8/100, Training Loss: 0.0776, Training F1-score: 0.9548, Validation Loss: 0.2568, Validation F1-score: 0.8425
Epoch 9/100, Training Loss: 0.0756, Training F1-score: 0.9556, Validation Loss: 0.2555, Validation F1-score: 0.8596
Epoch 10/100, Training Loss: 0.0730, Training F1-score: 0.9574, Validation Loss: 0.2747, Validation F1-score: 0.8516
[I 2024-12-22 11:23:54,892] Trial 93 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.32, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.5.ls2.gamma', 'step_size': 11, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0704, Training F1-score: 0.9588, Validation Loss: 0.2339, Validation F1-score: 0.8651
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:94 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.32
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1905, Training F1-score: 0.8819, Validation Loss: 0.2012, Validation F1-score: 0.8775
Best F1-score till now on the current trail on Validation data: 0.8775223408035685
Epoch 2/100, Training Loss: 0.1276, Training F1-score: 0.9215, Validation Loss: 0.1936, Validation F1-score: 0.8803
Best F1-score till now on the current trail on Validation data: 0.8802910186861469
Epoch 3/100, Training Loss: 0.1092, Training F1-score: 0.9343, Validation Loss: 0.2083, Validation F1-score: 0.8597
Epoch 4/100, Training Loss: 0.0972, Training F1-score: 0.9429, Validation Loss: 0.2241, Validation F1-score: 0.8461
Epoch 5/100, Training Loss: 0.0902, Training F1-score: 0.9460, Validation Loss: 0.2568, Validation F1-score: 0.8548
Epoch 6/100, Training Loss: 0.0752, Training F1-score: 0.9551, Validation Loss: 0.2307, Validation F1-score: 0.8650
Epoch 7/100, Training Loss: 0.0718, Training F1-score: 0.9576, Validation Loss: 0.2467, Validation F1-score: 0.8555
Epoch 8/100, Training Loss: 0.0710, Training F1-score: 0.9597, Validation Loss: 0.2537, Validation F1-score: 0.8434
Epoch 9/100, Training Loss: 0.0686, Training F1-score: 0.9598, Validation Loss: 0.2398, Validation F1-score: 0.8476
Epoch 10/100, Training Loss: 0.0670, Training F1-score: 0.9610, Validation Loss: 0.2571, Validation F1-score: 0.8374
Epoch 11/100, Training Loss: 0.0612, Training F1-score: 0.9652, Validation Loss: 0.2424, Validation F1-score: 0.8557
[I 2024-12-22 11:44:46,235] Trial 94 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.32, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.5.ls2.gamma', 'step_size': 5, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 12/100, Training Loss: 0.0601, Training F1-score: 0.9652, Validation Loss: 0.2579, Validation F1-score: 0.8468
Early stopping triggered after 12 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:95 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.5
Learning rate: 2.5e-05
Optimizer: SGD
Momentum term: 0.9611117236381744
Step size: 18
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 11:46:34,169] Trial 95 pruned. 
Epoch 1/100, Training Loss: 0.5310, Training F1-score: 0.4827, Validation Loss: 0.3758, Validation F1-score: 0.7364
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:96 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 10
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1824, Training F1-score: 0.8884, Validation Loss: 0.2044, Validation F1-score: 0.8769
Best F1-score till now on the current trail on Validation data: 0.8768748832244059
Epoch 2/100, Training Loss: 0.1262, Training F1-score: 0.9227, Validation Loss: 0.2374, Validation F1-score: 0.8517
Epoch 3/100, Training Loss: 0.1079, Training F1-score: 0.9342, Validation Loss: 0.2227, Validation F1-score: 0.8646
Epoch 4/100, Training Loss: 0.0975, Training F1-score: 0.9419, Validation Loss: 0.2147, Validation F1-score: 0.8580
Epoch 5/100, Training Loss: 0.0898, Training F1-score: 0.9473, Validation Loss: 0.2338, Validation F1-score: 0.8576
Epoch 6/100, Training Loss: 0.0847, Training F1-score: 0.9508, Validation Loss: 0.2763, Validation F1-score: 0.8189
Epoch 7/100, Training Loss: 0.0815, Training F1-score: 0.9527, Validation Loss: 0.2629, Validation F1-score: 0.8190
Epoch 8/100, Training Loss: 0.0783, Training F1-score: 0.9542, Validation Loss: 0.3265, Validation F1-score: 0.8561
Epoch 9/100, Training Loss: 0.0743, Training F1-score: 0.9556, Validation Loss: 0.2764, Validation F1-score: 0.8613
Epoch 10/100, Training Loss: 0.0717, Training F1-score: 0.9584, Validation Loss: 0.2744, Validation F1-score: 0.8660
[I 2024-12-22 12:05:49,098] Trial 96 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.28, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 10, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0607, Training F1-score: 0.9642, Validation Loss: 0.2829, Validation F1-score: 0.8569
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:97 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.25
Learning rate: 2.5e-07
Optimizer: Adam
Momentum term: 0
Step size: 17
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 12:07:37,853] Trial 97 pruned. 
Epoch 1/100, Training Loss: 0.6742, Training F1-score: 0.3900, Validation Loss: 0.6424, Validation F1-score: 0.2510
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:98 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 1e-05
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 12:09:26,155] Trial 98 pruned. 
Epoch 1/100, Training Loss: 0.3349, Training F1-score: 0.7909, Validation Loss: 0.2413, Validation F1-score: 0.8377
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:99 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.2.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 15
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1865, Training F1-score: 0.8847, Validation Loss: 0.2021, Validation F1-score: 0.8792
Best F1-score till now on the current trail on Validation data: 0.8791508566071133
Epoch 2/100, Training Loss: 0.1239, Training F1-score: 0.9235, Validation Loss: 0.1906, Validation F1-score: 0.8755
Epoch 3/100, Training Loss: 0.1037, Training F1-score: 0.9385, Validation Loss: 0.2026, Validation F1-score: 0.8758
Epoch 4/100, Training Loss: 0.0956, Training F1-score: 0.9429, Validation Loss: 0.2347, Validation F1-score: 0.8400
Epoch 5/100, Training Loss: 0.0887, Training F1-score: 0.9477, Validation Loss: 0.2079, Validation F1-score: 0.8628
Epoch 6/100, Training Loss: 0.0832, Training F1-score: 0.9512, Validation Loss: 0.2257, Validation F1-score: 0.8557
Epoch 7/100, Training Loss: 0.0789, Training F1-score: 0.9538, Validation Loss: 0.2144, Validation F1-score: 0.8580
Epoch 8/100, Training Loss: 0.0750, Training F1-score: 0.9557, Validation Loss: 0.2229, Validation F1-score: 0.8614
Epoch 9/100, Training Loss: 0.0723, Training F1-score: 0.9573, Validation Loss: 0.2311, Validation F1-score: 0.8656
Epoch 10/100, Training Loss: 0.0696, Training F1-score: 0.9591, Validation Loss: 0.2221, Validation F1-score: 0.8500
[I 2024-12-22 12:28:57,036] Trial 99 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.25, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.2.ls2.gamma', 'step_size': 15, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0683, Training F1-score: 0.9597, Validation Loss: 0.2515, Validation F1-score: 0.8687
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:100 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.2
Learning rate: 1e-06
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 12:30:42,259] Trial 100 pruned. 
Epoch 1/100, Training Loss: 0.6322, Training F1-score: 0.3291, Validation Loss: 0.5884, Validation F1-score: 0.2468
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:101 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1839, Training F1-score: 0.8871, Validation Loss: 0.1999, Validation F1-score: 0.8808
Best F1-score till now on the current trail on Validation data: 0.8808063743020105
Epoch 2/100, Training Loss: 0.1278, Training F1-score: 0.9216, Validation Loss: 0.2147, Validation F1-score: 0.8683
Epoch 3/100, Training Loss: 0.1094, Training F1-score: 0.9331, Validation Loss: 0.2250, Validation F1-score: 0.8651
Epoch 4/100, Training Loss: 0.0987, Training F1-score: 0.9413, Validation Loss: 0.2075, Validation F1-score: 0.8576
Epoch 5/100, Training Loss: 0.0917, Training F1-score: 0.9456, Validation Loss: 0.2382, Validation F1-score: 0.8711
Epoch 6/100, Training Loss: 0.0752, Training F1-score: 0.9562, Validation Loss: 0.2375, Validation F1-score: 0.8589
Epoch 7/100, Training Loss: 0.0722, Training F1-score: 0.9584, Validation Loss: 0.2433, Validation F1-score: 0.8515
Epoch 8/100, Training Loss: 0.0701, Training F1-score: 0.9592, Validation Loss: 0.2627, Validation F1-score: 0.8616
Epoch 9/100, Training Loss: 0.0675, Training F1-score: 0.9607, Validation Loss: 0.2485, Validation F1-score: 0.8606
Epoch 10/100, Training Loss: 0.0662, Training F1-score: 0.9616, Validation Loss: 0.2611, Validation F1-score: 0.8524
[I 2024-12-22 12:50:21,014] Trial 101 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.28, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.0.ls2.gamma', 'step_size': 5, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0604, Training F1-score: 0.9647, Validation Loss: 0.2430, Validation F1-score: 0.8605
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:102 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 12:52:08,641] Trial 102 pruned. 
Epoch 1/100, Training Loss: 0.1843, Training F1-score: 0.8869, Validation Loss: 0.2089, Validation F1-score: 0.8612
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:103 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 20
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1830, Training F1-score: 0.8873, Validation Loss: 0.2117, Validation F1-score: 0.8809
Best F1-score till now on the current trail on Validation data: 0.8808597947129884
Epoch 2/100, Training Loss: 0.1281, Training F1-score: 0.9212, Validation Loss: 0.1995, Validation F1-score: 0.8791
Epoch 3/100, Training Loss: 0.1087, Training F1-score: 0.9334, Validation Loss: 0.2078, Validation F1-score: 0.8713
Epoch 4/100, Training Loss: 0.0978, Training F1-score: 0.9414, Validation Loss: 0.2644, Validation F1-score: 0.8624
Epoch 5/100, Training Loss: 0.0902, Training F1-score: 0.9459, Validation Loss: 0.2328, Validation F1-score: 0.8603
Epoch 6/100, Training Loss: 0.0847, Training F1-score: 0.9496, Validation Loss: 0.2283, Validation F1-score: 0.8536
Epoch 7/100, Training Loss: 0.0802, Training F1-score: 0.9530, Validation Loss: 0.2265, Validation F1-score: 0.8693
Epoch 8/100, Training Loss: 0.0780, Training F1-score: 0.9534, Validation Loss: 0.2473, Validation F1-score: 0.8460
Epoch 9/100, Training Loss: 0.0763, Training F1-score: 0.9551, Validation Loss: 0.2494, Validation F1-score: 0.8607
Epoch 10/100, Training Loss: 0.0733, Training F1-score: 0.9565, Validation Loss: 0.2800, Validation F1-score: 0.8664
[I 2024-12-22 13:11:49,576] Trial 103 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.28, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.0.ls2.gamma', 'step_size': 20, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0717, Training F1-score: 0.9584, Validation Loss: 0.2750, Validation F1-score: 0.8694
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:104 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 19
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1831, Training F1-score: 0.8877, Validation Loss: 0.2043, Validation F1-score: 0.8793
Best F1-score till now on the current trail on Validation data: 0.879343530973738
Epoch 2/100, Training Loss: 0.1283, Training F1-score: 0.9217, Validation Loss: 0.1957, Validation F1-score: 0.8815
Best F1-score till now on the current trail on Validation data: 0.8814828935079752
Epoch 3/100, Training Loss: 0.1097, Training F1-score: 0.9348, Validation Loss: 0.2062, Validation F1-score: 0.8679
Epoch 4/100, Training Loss: 0.0988, Training F1-score: 0.9422, Validation Loss: 0.1972, Validation F1-score: 0.8785
Epoch 5/100, Training Loss: 0.0917, Training F1-score: 0.9460, Validation Loss: 0.2270, Validation F1-score: 0.8503
Epoch 6/100, Training Loss: 0.0857, Training F1-score: 0.9498, Validation Loss: 0.2324, Validation F1-score: 0.8584
Epoch 7/100, Training Loss: 0.0821, Training F1-score: 0.9527, Validation Loss: 0.2268, Validation F1-score: 0.8675
Epoch 8/100, Training Loss: 0.0784, Training F1-score: 0.9535, Validation Loss: 0.2121, Validation F1-score: 0.8653
Epoch 9/100, Training Loss: 0.0753, Training F1-score: 0.9557, Validation Loss: 0.2295, Validation F1-score: 0.8607
Epoch 10/100, Training Loss: 0.0733, Training F1-score: 0.9582, Validation Loss: 0.2374, Validation F1-score: 0.8725
Epoch 11/100, Training Loss: 0.0704, Training F1-score: 0.9586, Validation Loss: 0.2468, Validation F1-score: 0.8601
[I 2024-12-22 13:33:03,480] Trial 104 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.28, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.0.ls2.gamma', 'step_size': 19, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 12/100, Training Loss: 0.0696, Training F1-score: 0.9586, Validation Loss: 0.2271, Validation F1-score: 0.8540
Early stopping triggered after 12 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:105 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.3
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 13:34:48,683] Trial 105 pruned. 
Epoch 1/100, Training Loss: 0.1849, Training F1-score: 0.8855, Validation Loss: 0.2387, Validation F1-score: 0.8330
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:106 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 13:36:38,667] Trial 106 pruned. 
Epoch 1/100, Training Loss: 0.1917, Training F1-score: 0.8822, Validation Loss: 0.2158, Validation F1-score: 0.8147
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:107 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.32
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 13:38:26,339] Trial 107 pruned. 
Epoch 1/100, Training Loss: 0.1888, Training F1-score: 0.8836, Validation Loss: 0.2464, Validation F1-score: 0.8669
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:108 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.28
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 7
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 13:40:14,863] Trial 108 pruned. 
Epoch 1/100, Training Loss: 0.1824, Training F1-score: 0.8868, Validation Loss: 0.2534, Validation F1-score: 0.8371
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:109 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 1e-07
Optimizer: SGD
Momentum term: 0.8495936137473201
Step size: 9
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 13:42:04,083] Trial 109 pruned. 
Epoch 1/100, Training Loss: 0.7273, Training F1-score: 0.3327, Validation Loss: 0.7282, Validation F1-score: 0.3156
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:110 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.28
Learning rate: 2.5e-06
Optimizer: Adam
Momentum term: 0
Step size: 13
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 13:43:53,508] Trial 110 pruned. 
Epoch 1/100, Training Loss: 0.5613, Training F1-score: 0.3957, Validation Loss: 0.4554, Validation F1-score: 0.6749
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:111 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 13:45:40,898] Trial 111 pruned. 
Epoch 1/100, Training Loss: 0.1836, Training F1-score: 0.8876, Validation Loss: 0.2063, Validation F1-score: 0.8711
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:112 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1850, Training F1-score: 0.8857, Validation Loss: 0.1922, Validation F1-score: 0.8817
Best F1-score till now on the current trail on Validation data: 0.8816613131343439
Epoch 2/100, Training Loss: 0.1300, Training F1-score: 0.9208, Validation Loss: 0.2190, Validation F1-score: 0.8766
Epoch 3/100, Training Loss: 0.1110, Training F1-score: 0.9323, Validation Loss: 0.2300, Validation F1-score: 0.8649
Epoch 4/100, Training Loss: 0.1001, Training F1-score: 0.9388, Validation Loss: 0.1966, Validation F1-score: 0.8634
Epoch 5/100, Training Loss: 0.0922, Training F1-score: 0.9457, Validation Loss: 0.2306, Validation F1-score: 0.8250
Epoch 6/100, Training Loss: 0.0756, Training F1-score: 0.9565, Validation Loss: 0.2358, Validation F1-score: 0.8595
Epoch 7/100, Training Loss: 0.0732, Training F1-score: 0.9573, Validation Loss: 0.2395, Validation F1-score: 0.8504
Epoch 8/100, Training Loss: 0.0707, Training F1-score: 0.9579, Validation Loss: 0.2452, Validation F1-score: 0.8406
Epoch 9/100, Training Loss: 0.0689, Training F1-score: 0.9596, Validation Loss: 0.2327, Validation F1-score: 0.8442
Epoch 10/100, Training Loss: 0.0667, Training F1-score: 0.9607, Validation Loss: 0.2597, Validation F1-score: 0.8310
[I 2024-12-22 14:05:13,673] Trial 112 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.3, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 5, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0613, Training F1-score: 0.9644, Validation Loss: 0.2915, Validation F1-score: 0.8500
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:113 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 14:07:01,228] Trial 113 pruned. 
Epoch 1/100, Training Loss: 0.1836, Training F1-score: 0.8881, Validation Loss: 0.2303, Validation F1-score: 0.8765
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:114 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1849, Training F1-score: 0.8863, Validation Loss: 0.2207, Validation F1-score: 0.8779
Best F1-score till now on the current trail on Validation data: 0.8778824734655938
Epoch 2/100, Training Loss: 0.1287, Training F1-score: 0.9210, Validation Loss: 0.1941, Validation F1-score: 0.8649
Epoch 3/100, Training Loss: 0.1095, Training F1-score: 0.9341, Validation Loss: 0.2080, Validation F1-score: 0.8473
Epoch 4/100, Training Loss: 0.0980, Training F1-score: 0.9417, Validation Loss: 0.2146, Validation F1-score: 0.8334
Epoch 5/100, Training Loss: 0.0911, Training F1-score: 0.9456, Validation Loss: 0.2459, Validation F1-score: 0.8601
Epoch 6/100, Training Loss: 0.0720, Training F1-score: 0.9583, Validation Loss: 0.2245, Validation F1-score: 0.8600
Epoch 7/100, Training Loss: 0.0690, Training F1-score: 0.9604, Validation Loss: 0.2345, Validation F1-score: 0.8607
Epoch 8/100, Training Loss: 0.0685, Training F1-score: 0.9610, Validation Loss: 0.2381, Validation F1-score: 0.8599
Epoch 9/100, Training Loss: 0.0672, Training F1-score: 0.9610, Validation Loss: 0.2339, Validation F1-score: 0.8546
Epoch 10/100, Training Loss: 0.0665, Training F1-score: 0.9617, Validation Loss: 0.2296, Validation F1-score: 0.8549
[I 2024-12-22 14:26:31,440] Trial 114 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.3, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 5, 'gamma': 0.1}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0647, Training F1-score: 0.9634, Validation Loss: 0.2365, Validation F1-score: 0.8550
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:115 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.35
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1878, Training F1-score: 0.8847, Validation Loss: 0.2076, Validation F1-score: 0.8815
Best F1-score till now on the current trail on Validation data: 0.8815389434118203
Epoch 2/100, Training Loss: 0.1297, Training F1-score: 0.9191, Validation Loss: 0.2112, Validation F1-score: 0.8554
Epoch 3/100, Training Loss: 0.1120, Training F1-score: 0.9317, Validation Loss: 0.2208, Validation F1-score: 0.8645
Epoch 4/100, Training Loss: 0.1010, Training F1-score: 0.9402, Validation Loss: 0.2392, Validation F1-score: 0.8719
Epoch 5/100, Training Loss: 0.0945, Training F1-score: 0.9427, Validation Loss: 0.2369, Validation F1-score: 0.8662
Epoch 6/100, Training Loss: 0.0890, Training F1-score: 0.9470, Validation Loss: 0.2706, Validation F1-score: 0.8633
Epoch 7/100, Training Loss: 0.0748, Training F1-score: 0.9569, Validation Loss: 0.2398, Validation F1-score: 0.8543
Epoch 8/100, Training Loss: 0.0714, Training F1-score: 0.9590, Validation Loss: 0.2555, Validation F1-score: 0.8362
Epoch 9/100, Training Loss: 0.0697, Training F1-score: 0.9591, Validation Loss: 0.2637, Validation F1-score: 0.8653
Epoch 10/100, Training Loss: 0.0683, Training F1-score: 0.9612, Validation Loss: 0.2728, Validation F1-score: 0.8404
[I 2024-12-22 14:46:08,011] Trial 115 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.35, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma', 'step_size': 6, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0669, Training F1-score: 0.9614, Validation Loss: 0.2890, Validation F1-score: 0.8461
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:116 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1822, Training F1-score: 0.8883, Validation Loss: 0.2091, Validation F1-score: 0.8794
Best F1-score till now on the current trail on Validation data: 0.8794306661586121
Epoch 2/100, Training Loss: 0.1289, Training F1-score: 0.9208, Validation Loss: 0.2176, Validation F1-score: 0.8787
Epoch 3/100, Training Loss: 0.1102, Training F1-score: 0.9334, Validation Loss: 0.2177, Validation F1-score: 0.8524
Epoch 4/100, Training Loss: 0.0996, Training F1-score: 0.9411, Validation Loss: 0.2393, Validation F1-score: 0.8645
Epoch 5/100, Training Loss: 0.0915, Training F1-score: 0.9449, Validation Loss: 0.2604, Validation F1-score: 0.8368
Epoch 6/100, Training Loss: 0.0734, Training F1-score: 0.9575, Validation Loss: 0.2512, Validation F1-score: 0.8689
Epoch 7/100, Training Loss: 0.0696, Training F1-score: 0.9594, Validation Loss: 0.2435, Validation F1-score: 0.8391
Epoch 8/100, Training Loss: 0.0680, Training F1-score: 0.9609, Validation Loss: 0.2584, Validation F1-score: 0.8623
Epoch 9/100, Training Loss: 0.0663, Training F1-score: 0.9616, Validation Loss: 0.2398, Validation F1-score: 0.8623
Epoch 10/100, Training Loss: 0.0648, Training F1-score: 0.9623, Validation Loss: 0.2741, Validation F1-score: 0.8624
[I 2024-12-22 15:05:32,153] Trial 116 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.25, 'learning_rate': 0.00025, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 5, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0578, Training F1-score: 0.9663, Validation Loss: 0.2624, Validation F1-score: 0.8616
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:117 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.6.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:07:21,732] Trial 117 pruned. 
Epoch 1/100, Training Loss: 0.1973, Training F1-score: 0.8786, Validation Loss: 0.1981, Validation F1-score: 0.8739
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:118 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.5
Learning rate: 2.5e-05
Optimizer: RMSprop
Momentum term: 0
Step size: 7
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:09:13,090] Trial 118 pruned. 
Epoch 1/100, Training Loss: 0.2584, Training F1-score: 0.8431, Validation Loss: 0.2309, Validation F1-score: 0.8448
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:119 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:11:02,534] Trial 119 pruned. 
Epoch 1/100, Training Loss: 0.1890, Training F1-score: 0.8825, Validation Loss: 0.2117, Validation F1-score: 0.8547
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:120 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:12:50,066] Trial 120 pruned. 
Epoch 1/100, Training Loss: 0.1834, Training F1-score: 0.8857, Validation Loss: 0.2182, Validation F1-score: 0.8555
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:121 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.2001, Training F1-score: 0.8765, Validation Loss: 0.1970, Validation F1-score: 0.8842
Best F1-score till now on the current trail on Validation data: 0.8842327664377693
Epoch 2/100, Training Loss: 0.1312, Training F1-score: 0.9202, Validation Loss: 0.2088, Validation F1-score: 0.8771
Epoch 3/100, Training Loss: 0.1129, Training F1-score: 0.9306, Validation Loss: 0.2218, Validation F1-score: 0.8478
Epoch 4/100, Training Loss: 0.1009, Training F1-score: 0.9386, Validation Loss: 0.2104, Validation F1-score: 0.8619
Epoch 5/100, Training Loss: 0.0924, Training F1-score: 0.9456, Validation Loss: 0.2126, Validation F1-score: 0.8738
Epoch 6/100, Training Loss: 0.0888, Training F1-score: 0.9482, Validation Loss: 0.2248, Validation F1-score: 0.8709
Epoch 7/100, Training Loss: 0.0736, Training F1-score: 0.9571, Validation Loss: 0.2225, Validation F1-score: 0.8600
Epoch 8/100, Training Loss: 0.0705, Training F1-score: 0.9591, Validation Loss: 0.2309, Validation F1-score: 0.8648
Epoch 9/100, Training Loss: 0.0692, Training F1-score: 0.9597, Validation Loss: 0.2371, Validation F1-score: 0.8497
Epoch 10/100, Training Loss: 0.0674, Training F1-score: 0.9603, Validation Loss: 0.2562, Validation F1-score: 0.8545
[I 2024-12-22 15:32:17,579] Trial 121 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.4, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.5.ls2.gamma', 'step_size': 6, 'gamma': 0.3}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0665, Training F1-score: 0.9617, Validation Loss: 0.2597, Validation F1-score: 0.8562
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:122 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:34:07,938] Trial 122 pruned. 
Epoch 1/100, Training Loss: 0.1994, Training F1-score: 0.8755, Validation Loss: 0.2357, Validation F1-score: 0.8727
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:123 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:35:54,977] Trial 123 pruned. 
Epoch 1/100, Training Loss: 0.1928, Training F1-score: 0.8802, Validation Loss: 0.2090, Validation F1-score: 0.8494
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:124 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:37:44,251] Trial 124 pruned. 
Epoch 1/100, Training Loss: 0.1991, Training F1-score: 0.8782, Validation Loss: 0.1966, Validation F1-score: 0.8764
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:125 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.2.ls2.gamma
Dropout: 0.32
Learning rate: 2.5e-07
Optimizer: Adam
Momentum term: 0
Step size: 10
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:39:35,146] Trial 125 pruned. 
Epoch 1/100, Training Loss: 0.6442, Training F1-score: 0.3467, Validation Loss: 0.6215, Validation F1-score: 0.2443
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:126 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.2
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:41:23,244] Trial 126 pruned. 
Epoch 1/100, Training Loss: 0.1852, Training F1-score: 0.8852, Validation Loss: 0.2816, Validation F1-score: 0.7784
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:127 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.28
Learning rate: 1e-06
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:43:13,127] Trial 127 pruned. 
Epoch 1/100, Training Loss: 0.6249, Training F1-score: 0.3181, Validation Loss: 0.5788, Validation F1-score: 0.3603
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:128 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.25
Learning rate: 1e-05
Optimizer: Adam
Momentum term: 0
Step size: 8
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:45:01,832] Trial 128 pruned. 
Epoch 1/100, Training Loss: 0.3470, Training F1-score: 0.7656, Validation Loss: 0.2427, Validation F1-score: 0.8464
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:129 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 12
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:46:48,495] Trial 129 pruned. 
Epoch 1/100, Training Loss: 0.1883, Training F1-score: 0.8850, Validation Loss: 0.2156, Validation F1-score: 0.8648
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:130 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 7
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:48:36,215] Trial 130 pruned. 
Epoch 1/100, Training Loss: 0.1812, Training F1-score: 0.8859, Validation Loss: 0.2344, Validation F1-score: 0.8708
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:131 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:50:26,139] Trial 131 pruned. 
Epoch 1/100, Training Loss: 0.1907, Training F1-score: 0.8839, Validation Loss: 0.2239, Validation F1-score: 0.8765
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:132 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 9
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 15:52:14,941] Trial 132 pruned. 
Epoch 1/100, Training Loss: 0.1899, Training F1-score: 0.8848, Validation Loss: 0.2047, Validation F1-score: 0.8718
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:133 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 8
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1903, Training F1-score: 0.8843, Validation Loss: 0.1966, Validation F1-score: 0.8790
Best F1-score till now on the current trail on Validation data: 0.8789517291386707
Epoch 2/100, Training Loss: 0.1330, Training F1-score: 0.9181, Validation Loss: 0.2376, Validation F1-score: 0.8408
Epoch 3/100, Training Loss: 0.1153, Training F1-score: 0.9296, Validation Loss: 0.2255, Validation F1-score: 0.8372
Epoch 4/100, Training Loss: 0.1039, Training F1-score: 0.9373, Validation Loss: 0.2289, Validation F1-score: 0.8518
Epoch 5/100, Training Loss: 0.0966, Training F1-score: 0.9429, Validation Loss: 0.2828, Validation F1-score: 0.8634
Epoch 6/100, Training Loss: 0.0908, Training F1-score: 0.9459, Validation Loss: 0.2254, Validation F1-score: 0.8434
Epoch 7/100, Training Loss: 0.0871, Training F1-score: 0.9491, Validation Loss: 0.3209, Validation F1-score: 0.8581
Epoch 8/100, Training Loss: 0.0834, Training F1-score: 0.9510, Validation Loss: 0.2557, Validation F1-score: 0.8407
Epoch 9/100, Training Loss: 0.0707, Training F1-score: 0.9600, Validation Loss: 0.2711, Validation F1-score: 0.8424
Epoch 10/100, Training Loss: 0.0678, Training F1-score: 0.9606, Validation Loss: 0.2342, Validation F1-score: 0.8375
[I 2024-12-22 16:12:14,577] Trial 133 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.4, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma', 'step_size': 8, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0669, Training F1-score: 0.9618, Validation Loss: 0.2605, Validation F1-score: 0.8536
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:134 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 9
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:14:00,414] Trial 134 pruned. 
Epoch 1/100, Training Loss: 0.1904, Training F1-score: 0.8828, Validation Loss: 0.2439, Validation F1-score: 0.8646
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:135 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: SGD
Momentum term: 0.8218796527054248
Step size: 10
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:15:48,794] Trial 135 pruned. 
Epoch 1/100, Training Loss: 0.5258, Training F1-score: 0.4870, Validation Loss: 0.3814, Validation F1-score: 0.7441
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:136 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:17:41,166] Trial 136 pruned. 
Epoch 1/100, Training Loss: 0.1837, Training F1-score: 0.8887, Validation Loss: 0.2005, Validation F1-score: 0.8755
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:137 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.4
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 16
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:19:28,302] Trial 137 pruned. 
Epoch 1/100, Training Loss: 0.1903, Training F1-score: 0.8826, Validation Loss: 0.2455, Validation F1-score: 0.7638
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:138 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 11
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:21:16,943] Trial 138 pruned. 
Epoch 1/100, Training Loss: 0.1868, Training F1-score: 0.8861, Validation Loss: 0.2116, Validation F1-score: 0.8744
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:139 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.32
Learning rate: 1e-07
Optimizer: RMSprop
Momentum term: 0
Step size: 12
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:23:03,352] Trial 139 pruned. 
Epoch 1/100, Training Loss: 0.6665, Training F1-score: 0.2333, Validation Loss: 0.6496, Validation F1-score: 0.2318
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:140 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:24:51,317] Trial 140 pruned. 
Epoch 1/100, Training Loss: 0.1894, Training F1-score: 0.8822, Validation Loss: 0.2039, Validation F1-score: 0.8722
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:141 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 10
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:26:41,486] Trial 141 pruned. 
Epoch 1/100, Training Loss: 0.1896, Training F1-score: 0.8844, Validation Loss: 0.2145, Validation F1-score: 0.8683
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:142 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 9
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:28:28,564] Trial 142 pruned. 
Epoch 1/100, Training Loss: 0.1902, Training F1-score: 0.8834, Validation Loss: 0.2233, Validation F1-score: 0.8646
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:143 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 2.5e-06
Optimizer: RMSprop
Momentum term: 0
Step size: 9
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:30:18,229] Trial 143 pruned. 
Epoch 1/100, Training Loss: 0.5988, Training F1-score: 0.4306, Validation Loss: 0.5092, Validation F1-score: 0.5829
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:144 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 10
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:32:06,483] Trial 144 pruned. 
Epoch 1/100, Training Loss: 0.1903, Training F1-score: 0.8829, Validation Loss: 0.2232, Validation F1-score: 0.8405
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:145 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.35
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 10
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1858, Training F1-score: 0.8857, Validation Loss: 0.1875, Validation F1-score: 0.8827
Best F1-score till now on the current trail on Validation data: 0.8826511104202148
Epoch 2/100, Training Loss: 0.1304, Training F1-score: 0.9203, Validation Loss: 0.2657, Validation F1-score: 0.7708
Epoch 3/100, Training Loss: 0.1120, Training F1-score: 0.9315, Validation Loss: 0.2310, Validation F1-score: 0.8775
Epoch 4/100, Training Loss: 0.1020, Training F1-score: 0.9390, Validation Loss: 0.2086, Validation F1-score: 0.8647
Epoch 5/100, Training Loss: 0.0942, Training F1-score: 0.9438, Validation Loss: 0.2076, Validation F1-score: 0.8568
Epoch 6/100, Training Loss: 0.0898, Training F1-score: 0.9473, Validation Loss: 0.2342, Validation F1-score: 0.8549
Epoch 7/100, Training Loss: 0.0847, Training F1-score: 0.9502, Validation Loss: 0.2624, Validation F1-score: 0.7986
Epoch 8/100, Training Loss: 0.0810, Training F1-score: 0.9526, Validation Loss: 0.2313, Validation F1-score: 0.8658
Epoch 9/100, Training Loss: 0.0781, Training F1-score: 0.9545, Validation Loss: 0.2581, Validation F1-score: 0.8380
Epoch 10/100, Training Loss: 0.0758, Training F1-score: 0.9567, Validation Loss: 0.2827, Validation F1-score: 0.8346
[I 2024-12-22 16:51:32,481] Trial 145 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.35, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 10, 'gamma': 0.2}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0618, Training F1-score: 0.9647, Validation Loss: 0.2467, Validation F1-score: 0.8463
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:146 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.6.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:53:21,257] Trial 146 pruned. 
Epoch 1/100, Training Loss: 0.1830, Training F1-score: 0.8856, Validation Loss: 0.2605, Validation F1-score: 0.8668
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:147 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 11
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:55:17,556] Trial 147 pruned. 
Epoch 1/100, Training Loss: 0.1895, Training F1-score: 0.8821, Validation Loss: 0.1975, Validation F1-score: 0.8720
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:148 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.28
Learning rate: 2.5e-05
Optimizer: RMSprop
Momentum term: 0
Step size: 8
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:57:03,245] Trial 148 pruned. 
Epoch 1/100, Training Loss: 0.2383, Training F1-score: 0.8572, Validation Loss: 0.2122, Validation F1-score: 0.8645
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:149 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 9
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 16:58:51,537] Trial 149 pruned. 
Epoch 1/100, Training Loss: 0.1887, Training F1-score: 0.8851, Validation Loss: 0.2002, Validation F1-score: 0.8768
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:150 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.5
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 17:00:41,783] Trial 150 pruned. 
Epoch 1/100, Training Loss: 0.2056, Training F1-score: 0.8735, Validation Loss: 0.2175, Validation F1-score: 0.8435
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:151 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.35
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 12
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 17:02:28,418] Trial 151 pruned. 
Epoch 1/100, Training Loss: 0.1921, Training F1-score: 0.8811, Validation Loss: 0.1972, Validation F1-score: 0.8689
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:152 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.35
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 12
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1940, Training F1-score: 0.8793, Validation Loss: 0.2002, Validation F1-score: 0.8865
Best F1-score till now on the current trail on Validation data: 0.886529886411286
Epoch 2/100, Training Loss: 0.1284, Training F1-score: 0.9216, Validation Loss: 0.2229, Validation F1-score: 0.8544
Epoch 3/100, Training Loss: 0.1086, Training F1-score: 0.9346, Validation Loss: 0.2274, Validation F1-score: 0.8627
Epoch 4/100, Training Loss: 0.0992, Training F1-score: 0.9399, Validation Loss: 0.2288, Validation F1-score: 0.8200
Epoch 5/100, Training Loss: 0.0925, Training F1-score: 0.9451, Validation Loss: 0.2304, Validation F1-score: 0.8631
Epoch 6/100, Training Loss: 0.0869, Training F1-score: 0.9479, Validation Loss: 0.2219, Validation F1-score: 0.8560
Epoch 7/100, Training Loss: 0.0833, Training F1-score: 0.9513, Validation Loss: 0.1950, Validation F1-score: 0.8786
Epoch 8/100, Training Loss: 0.0803, Training F1-score: 0.9522, Validation Loss: 0.2445, Validation F1-score: 0.8683
Epoch 9/100, Training Loss: 0.0762, Training F1-score: 0.9548, Validation Loss: 0.2173, Validation F1-score: 0.8631
Epoch 10/100, Training Loss: 0.0748, Training F1-score: 0.9552, Validation Loss: 0.2681, Validation F1-score: 0.8596
[I 2024-12-22 17:22:23,815] Trial 152 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.35, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.0.ls2.gamma', 'step_size': 12, 'gamma': 0.3}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0717, Training F1-score: 0.9570, Validation Loss: 0.2223, Validation F1-score: 0.8425
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:153 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.35
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 11
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1992, Training F1-score: 0.8750, Validation Loss: 0.2064, Validation F1-score: 0.8796
Best F1-score till now on the current trail on Validation data: 0.879621993837909
Epoch 2/100, Training Loss: 0.1292, Training F1-score: 0.9208, Validation Loss: 0.2140, Validation F1-score: 0.8377
Epoch 3/100, Training Loss: 0.1103, Training F1-score: 0.9332, Validation Loss: 0.2056, Validation F1-score: 0.8654
Epoch 4/100, Training Loss: 0.1002, Training F1-score: 0.9407, Validation Loss: 0.2507, Validation F1-score: 0.8106
Epoch 5/100, Training Loss: 0.0919, Training F1-score: 0.9463, Validation Loss: 0.2287, Validation F1-score: 0.8516
Epoch 6/100, Training Loss: 0.0867, Training F1-score: 0.9488, Validation Loss: 0.2273, Validation F1-score: 0.8579
Epoch 7/100, Training Loss: 0.0843, Training F1-score: 0.9509, Validation Loss: 0.2587, Validation F1-score: 0.8403
Epoch 8/100, Training Loss: 0.0781, Training F1-score: 0.9547, Validation Loss: 0.2791, Validation F1-score: 0.8649
Epoch 9/100, Training Loss: 0.0776, Training F1-score: 0.9544, Validation Loss: 0.2780, Validation F1-score: 0.8561
Epoch 10/100, Training Loss: 0.0753, Training F1-score: 0.9558, Validation Loss: 0.2575, Validation F1-score: 0.8581
[I 2024-12-22 17:42:06,656] Trial 153 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.35, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.0.ls2.gamma', 'step_size': 11, 'gamma': 0.3}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0724, Training F1-score: 0.9567, Validation Loss: 0.2337, Validation F1-score: 0.8541
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:154 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.35
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 10
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1937, Training F1-score: 0.8810, Validation Loss: 0.1976, Validation F1-score: 0.8824
Best F1-score till now on the current trail on Validation data: 0.8824011090441685
Epoch 2/100, Training Loss: 0.1288, Training F1-score: 0.9210, Validation Loss: 0.2235, Validation F1-score: 0.8189
Epoch 3/100, Training Loss: 0.1090, Training F1-score: 0.9340, Validation Loss: 0.2111, Validation F1-score: 0.8639
Epoch 4/100, Training Loss: 0.0985, Training F1-score: 0.9404, Validation Loss: 0.2461, Validation F1-score: 0.8435
Epoch 5/100, Training Loss: 0.0931, Training F1-score: 0.9443, Validation Loss: 0.2495, Validation F1-score: 0.8683
Epoch 6/100, Training Loss: 0.0873, Training F1-score: 0.9480, Validation Loss: 0.2614, Validation F1-score: 0.8331
Epoch 7/100, Training Loss: 0.0823, Training F1-score: 0.9513, Validation Loss: 0.2405, Validation F1-score: 0.8606
Epoch 8/100, Training Loss: 0.0802, Training F1-score: 0.9525, Validation Loss: 0.2370, Validation F1-score: 0.8548
Epoch 9/100, Training Loss: 0.0771, Training F1-score: 0.9543, Validation Loss: 0.2534, Validation F1-score: 0.8262
Epoch 10/100, Training Loss: 0.0744, Training F1-score: 0.9562, Validation Loss: 0.2521, Validation F1-score: 0.8630
[I 2024-12-22 18:01:37,929] Trial 154 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.35, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.0.ls2.gamma', 'step_size': 10, 'gamma': 0.3}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0611, Training F1-score: 0.9645, Validation Loss: 0.2585, Validation F1-score: 0.8626
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:155 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.25
Learning rate: 2.5e-07
Optimizer: Adam
Momentum term: 0
Step size: 11
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 18:03:25,295] Trial 155 pruned. 
Epoch 1/100, Training Loss: 0.6668, Training F1-score: 0.3086, Validation Loss: 0.6380, Validation F1-score: 0.2487
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:156 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.2
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 18:05:17,116] Trial 156 pruned. 
Epoch 1/100, Training Loss: 0.1800, Training F1-score: 0.8878, Validation Loss: 0.2048, Validation F1-score: 0.8735
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:157 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.2.ls2.gamma
Dropout: 0.32
Learning rate: 1e-05
Optimizer: SGD
Momentum term: 0.9037307828484666
Step size: 13
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 18:07:05,852] Trial 157 pruned. 
Epoch 1/100, Training Loss: 0.6408, Training F1-score: 0.2992, Validation Loss: 0.5975, Validation F1-score: 0.2692
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:158 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 18:08:56,686] Trial 158 pruned. 
Epoch 1/100, Training Loss: 0.1919, Training F1-score: 0.8800, Validation Loss: 0.2038, Validation F1-score: 0.8690
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:159 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.28
Learning rate: 1e-06
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 18:10:46,614] Trial 159 pruned. 
Epoch 1/100, Training Loss: 0.6128, Training F1-score: 0.3494, Validation Loss: 0.5626, Validation F1-score: 0.3378
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:160 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1888, Training F1-score: 0.8833, Validation Loss: 0.1926, Validation F1-score: 0.8839
Best F1-score till now on the current trail on Validation data: 0.8838763480867543
Epoch 2/100, Training Loss: 0.1228, Training F1-score: 0.9257, Validation Loss: 0.2311, Validation F1-score: 0.8680
Epoch 3/100, Training Loss: 0.1043, Training F1-score: 0.9364, Validation Loss: 0.1994, Validation F1-score: 0.8660
Epoch 4/100, Training Loss: 0.0946, Training F1-score: 0.9436, Validation Loss: 0.2695, Validation F1-score: 0.8716
Epoch 5/100, Training Loss: 0.0880, Training F1-score: 0.9478, Validation Loss: 0.2546, Validation F1-score: 0.8724
Epoch 6/100, Training Loss: 0.0736, Training F1-score: 0.9573, Validation Loss: 0.2184, Validation F1-score: 0.8583
Epoch 7/100, Training Loss: 0.0706, Training F1-score: 0.9586, Validation Loss: 0.2269, Validation F1-score: 0.8517
Epoch 8/100, Training Loss: 0.0683, Training F1-score: 0.9598, Validation Loss: 0.2558, Validation F1-score: 0.8577
Epoch 9/100, Training Loss: 0.0658, Training F1-score: 0.9615, Validation Loss: 0.2413, Validation F1-score: 0.8465
Epoch 10/100, Training Loss: 0.0644, Training F1-score: 0.9618, Validation Loss: 0.2373, Validation F1-score: 0.8502
[I 2024-12-22 18:30:26,687] Trial 160 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.25, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.5.ls2.gamma', 'step_size': 5, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0594, Training F1-score: 0.9650, Validation Loss: 0.2279, Validation F1-score: 0.8551
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:161 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.3
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 7
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 18:32:11,617] Trial 161 pruned. 
Epoch 1/100, Training Loss: 0.1826, Training F1-score: 0.8865, Validation Loss: 0.2004, Validation F1-score: 0.8773
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:162 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.3
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 9
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1851, Training F1-score: 0.8857, Validation Loss: 0.2044, Validation F1-score: 0.8785
Best F1-score till now on the current trail on Validation data: 0.8784920538512172
Epoch 2/100, Training Loss: 0.1281, Training F1-score: 0.9215, Validation Loss: 0.2183, Validation F1-score: 0.8717
Epoch 3/100, Training Loss: 0.1092, Training F1-score: 0.9346, Validation Loss: 0.2263, Validation F1-score: 0.8680
Epoch 4/100, Training Loss: 0.0988, Training F1-score: 0.9417, Validation Loss: 0.2054, Validation F1-score: 0.8645
Epoch 5/100, Training Loss: 0.0910, Training F1-score: 0.9466, Validation Loss: 0.2425, Validation F1-score: 0.8496
Epoch 6/100, Training Loss: 0.0853, Training F1-score: 0.9503, Validation Loss: 0.2680, Validation F1-score: 0.8699
Epoch 7/100, Training Loss: 0.0840, Training F1-score: 0.9512, Validation Loss: 0.2457, Validation F1-score: 0.8518
Epoch 8/100, Training Loss: 0.0790, Training F1-score: 0.9529, Validation Loss: 0.2662, Validation F1-score: 0.8525
Epoch 9/100, Training Loss: 0.0776, Training F1-score: 0.9548, Validation Loss: 0.2830, Validation F1-score: 0.8544
Epoch 10/100, Training Loss: 0.0655, Training F1-score: 0.9618, Validation Loss: 0.2841, Validation F1-score: 0.8654
[I 2024-12-22 18:51:52,599] Trial 162 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.3, 'learning_rate': 0.00025, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.1.ls2.gamma', 'step_size': 9, 'gamma': 0.5}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0633, Training F1-score: 0.9631, Validation Loss: 0.2751, Validation F1-score: 0.8483
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:163 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.3
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 7
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 18:53:39,492] Trial 163 pruned. 
Epoch 1/100, Training Loss: 0.1822, Training F1-score: 0.8878, Validation Loss: 0.2346, Validation F1-score: 0.8480
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:164 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 5
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 18:55:24,990] Trial 164 pruned. 
Epoch 1/100, Training Loss: 0.1878, Training F1-score: 0.8843, Validation Loss: 0.2182, Validation F1-score: 0.8674
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:165 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 8
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 18:57:15,531] Trial 165 pruned. 
Epoch 1/100, Training Loss: 0.1903, Training F1-score: 0.8814, Validation Loss: 0.2059, Validation F1-score: 0.8491
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:166 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 6
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1915, Training F1-score: 0.8817, Validation Loss: 0.2193, Validation F1-score: 0.8791
Best F1-score till now on the current trail on Validation data: 0.8791118721669183
Epoch 2/100, Training Loss: 0.1384, Training F1-score: 0.9158, Validation Loss: 0.1972, Validation F1-score: 0.8618
Epoch 3/100, Training Loss: 0.1194, Training F1-score: 0.9280, Validation Loss: 0.2386, Validation F1-score: 0.8651
Epoch 4/100, Training Loss: 0.1082, Training F1-score: 0.9359, Validation Loss: 0.2455, Validation F1-score: 0.8323
Epoch 5/100, Training Loss: 0.1007, Training F1-score: 0.9395, Validation Loss: 0.2553, Validation F1-score: 0.8534
Epoch 6/100, Training Loss: 0.0950, Training F1-score: 0.9446, Validation Loss: 0.2488, Validation F1-score: 0.8251
Epoch 7/100, Training Loss: 0.0730, Training F1-score: 0.9582, Validation Loss: 0.2502, Validation F1-score: 0.8577
Epoch 8/100, Training Loss: 0.0704, Training F1-score: 0.9600, Validation Loss: 0.2529, Validation F1-score: 0.8507
Epoch 9/100, Training Loss: 0.0695, Training F1-score: 0.9600, Validation Loss: 0.2608, Validation F1-score: 0.8517
Epoch 10/100, Training Loss: 0.0684, Training F1-score: 0.9608, Validation Loss: 0.2761, Validation F1-score: 0.8541
[I 2024-12-22 19:17:02,347] Trial 166 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.4, 'learning_rate': 0.00025, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 6, 'gamma': 0.1}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0670, Training F1-score: 0.9620, Validation Loss: 0.2912, Validation F1-score: 0.8561
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:167 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 19:18:50,546] Trial 167 pruned. 
Epoch 1/100, Training Loss: 0.1812, Training F1-score: 0.8895, Validation Loss: 0.2086, Validation F1-score: 0.8733
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:168 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1877, Training F1-score: 0.8835, Validation Loss: 0.2014, Validation F1-score: 0.8861
Best F1-score till now on the current trail on Validation data: 0.8860944313430318
Epoch 2/100, Training Loss: 0.1258, Training F1-score: 0.9228, Validation Loss: 0.2217, Validation F1-score: 0.8461
Epoch 3/100, Training Loss: 0.1067, Training F1-score: 0.9361, Validation Loss: 0.2068, Validation F1-score: 0.8656
Epoch 4/100, Training Loss: 0.0964, Training F1-score: 0.9411, Validation Loss: 0.2329, Validation F1-score: 0.8728
Epoch 5/100, Training Loss: 0.0880, Training F1-score: 0.9479, Validation Loss: 0.2448, Validation F1-score: 0.8577
Epoch 6/100, Training Loss: 0.0835, Training F1-score: 0.9508, Validation Loss: 0.2226, Validation F1-score: 0.8641
Epoch 7/100, Training Loss: 0.0698, Training F1-score: 0.9592, Validation Loss: 0.2416, Validation F1-score: 0.8556
Epoch 8/100, Training Loss: 0.0678, Training F1-score: 0.9608, Validation Loss: 0.2350, Validation F1-score: 0.8552
Epoch 9/100, Training Loss: 0.0656, Training F1-score: 0.9611, Validation Loss: 0.2649, Validation F1-score: 0.8501
Epoch 10/100, Training Loss: 0.0642, Training F1-score: 0.9620, Validation Loss: 0.2653, Validation F1-score: 0.8661
[I 2024-12-22 19:38:17,206] Trial 168 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.28, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 6, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0629, Training F1-score: 0.9632, Validation Loss: 0.2768, Validation F1-score: 0.8619
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:169 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 19:40:03,264] Trial 169 pruned. 
Epoch 1/100, Training Loss: 0.1887, Training F1-score: 0.8858, Validation Loss: 0.2095, Validation F1-score: 0.8490
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:170 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.3
Learning rate: 1e-07
Optimizer: Adam
Momentum term: 0
Step size: 19
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 19:41:50,334] Trial 170 pruned. 
Epoch 1/100, Training Loss: 0.6813, Training F1-score: 0.4246, Validation Loss: 0.6641, Validation F1-score: 0.3165
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:171 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 9
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1825, Training F1-score: 0.8886, Validation Loss: 0.1914, Validation F1-score: 0.8799
Best F1-score till now on the current trail on Validation data: 0.8798560417536425
Epoch 2/100, Training Loss: 0.1262, Training F1-score: 0.9222, Validation Loss: 0.2221, Validation F1-score: 0.8434
Epoch 3/100, Training Loss: 0.1076, Training F1-score: 0.9343, Validation Loss: 0.2257, Validation F1-score: 0.8629
Epoch 4/100, Training Loss: 0.0973, Training F1-score: 0.9419, Validation Loss: 0.2093, Validation F1-score: 0.8645
Epoch 5/100, Training Loss: 0.0897, Training F1-score: 0.9474, Validation Loss: 0.2238, Validation F1-score: 0.8671
Epoch 6/100, Training Loss: 0.0851, Training F1-score: 0.9498, Validation Loss: 0.2464, Validation F1-score: 0.8412
Epoch 7/100, Training Loss: 0.0800, Training F1-score: 0.9524, Validation Loss: 0.2160, Validation F1-score: 0.8499
Epoch 8/100, Training Loss: 0.0769, Training F1-score: 0.9540, Validation Loss: 0.2708, Validation F1-score: 0.8514
Epoch 9/100, Training Loss: 0.0742, Training F1-score: 0.9560, Validation Loss: 0.2274, Validation F1-score: 0.8468
Epoch 10/100, Training Loss: 0.0624, Training F1-score: 0.9634, Validation Loss: 0.2380, Validation F1-score: 0.8490
[I 2024-12-22 20:01:28,241] Trial 171 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.25, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma', 'step_size': 9, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0610, Training F1-score: 0.9647, Validation Loss: 0.2491, Validation F1-score: 0.8526
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:172 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 7
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:03:14,605] Trial 172 pruned. 
Epoch 1/100, Training Loss: 0.1836, Training F1-score: 0.8875, Validation Loss: 0.2206, Validation F1-score: 0.8476
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:173 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 8
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1849, Training F1-score: 0.8874, Validation Loss: 0.2047, Validation F1-score: 0.8808
Best F1-score till now on the current trail on Validation data: 0.880796234055158
Epoch 2/100, Training Loss: 0.1279, Training F1-score: 0.9204, Validation Loss: 0.1871, Validation F1-score: 0.8756
Epoch 3/100, Training Loss: 0.1090, Training F1-score: 0.9344, Validation Loss: 0.2307, Validation F1-score: 0.8774
Epoch 4/100, Training Loss: 0.0973, Training F1-score: 0.9433, Validation Loss: 0.2411, Validation F1-score: 0.8607
Epoch 5/100, Training Loss: 0.0899, Training F1-score: 0.9465, Validation Loss: 0.2585, Validation F1-score: 0.8700
Epoch 6/100, Training Loss: 0.0841, Training F1-score: 0.9504, Validation Loss: 0.2317, Validation F1-score: 0.8420
Epoch 7/100, Training Loss: 0.0791, Training F1-score: 0.9543, Validation Loss: 0.2280, Validation F1-score: 0.8590
Epoch 8/100, Training Loss: 0.0763, Training F1-score: 0.9561, Validation Loss: 0.2626, Validation F1-score: 0.8559
Epoch 9/100, Training Loss: 0.0638, Training F1-score: 0.9633, Validation Loss: 0.2451, Validation F1-score: 0.8618
Epoch 10/100, Training Loss: 0.0614, Training F1-score: 0.9642, Validation Loss: 0.2271, Validation F1-score: 0.8651
[I 2024-12-22 20:22:59,930] Trial 173 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.25, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.3.ls2.gamma', 'step_size': 8, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0601, Training F1-score: 0.9644, Validation Loss: 0.2612, Validation F1-score: 0.8546
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:174 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 8
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:24:48,833] Trial 174 pruned. 
Epoch 1/100, Training Loss: 0.1837, Training F1-score: 0.8858, Validation Loss: 0.2112, Validation F1-score: 0.8341
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:175 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.32
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 7
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:26:36,795] Trial 175 pruned. 
Epoch 1/100, Training Loss: 0.1874, Training F1-score: 0.8822, Validation Loss: 0.2169, Validation F1-score: 0.8745
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:176 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.25
Learning rate: 2.5e-06
Optimizer: RMSprop
Momentum term: 0
Step size: 9
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:28:23,088] Trial 176 pruned. 
Epoch 1/100, Training Loss: 0.5470, Training F1-score: 0.4473, Validation Loss: 0.4524, Validation F1-score: 0.6427
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:177 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.3.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 8
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:30:11,127] Trial 177 pruned. 
Epoch 1/100, Training Loss: 0.1882, Training F1-score: 0.8819, Validation Loss: 0.2035, Validation F1-score: 0.8531
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:178 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.6.ls2.gamma
Dropout: 0.35
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 10
Gamma: 0.1
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:32:00,017] Trial 178 pruned. 
Epoch 1/100, Training Loss: 0.1877, Training F1-score: 0.8841, Validation Loss: 0.2187, Validation F1-score: 0.8519
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:179 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 6
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:33:49,447] Trial 179 pruned. 
Epoch 1/100, Training Loss: 0.1942, Training F1-score: 0.8810, Validation Loss: 0.1919, Validation F1-score: 0.8784
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:180 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 15
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:35:36,829] Trial 180 pruned. 
Epoch 1/100, Training Loss: 0.1829, Training F1-score: 0.8866, Validation Loss: 0.3065, Validation F1-score: 0.8604
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:181 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.5
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 11
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:37:31,071] Trial 181 pruned. 
Epoch 1/100, Training Loss: 0.1965, Training F1-score: 0.8789, Validation Loss: 0.2023, Validation F1-score: 0.8734
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:182 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 9
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:39:21,004] Trial 182 pruned. 
Epoch 1/100, Training Loss: 0.1816, Training F1-score: 0.8863, Validation Loss: 0.2007, Validation F1-score: 0.8783
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:183 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 10
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:41:09,572] Trial 183 pruned. 
Epoch 1/100, Training Loss: 0.1795, Training F1-score: 0.8894, Validation Loss: 0.2155, Validation F1-score: 0.8383
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:184 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: Adam
Momentum term: 0
Step size: 10
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:42:59,727] Trial 184 pruned. 
Epoch 1/100, Training Loss: 0.1803, Training F1-score: 0.8891, Validation Loss: 0.2227, Validation F1-score: 0.8082
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:185 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.0.ls2.gamma
Dropout: 0.25
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 14
Gamma: 0.2
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 20:44:48,077] Trial 185 pruned. 
Epoch 1/100, Training Loss: 0.1885, Training F1-score: 0.8841, Validation Loss: 0.1879, Validation F1-score: 0.8767
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:186 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.3
Learning rate: 0.0001
Optimizer: RMSprop
Momentum term: 0
Step size: 17
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1846, Training F1-score: 0.8867, Validation Loss: 0.1963, Validation F1-score: 0.8837
Best F1-score till now on the current trail on Validation data: 0.8836615788362264
Epoch 2/100, Training Loss: 0.1289, Training F1-score: 0.9203, Validation Loss: 0.1998, Validation F1-score: 0.8721
Epoch 3/100, Training Loss: 0.1104, Training F1-score: 0.9337, Validation Loss: 0.2178, Validation F1-score: 0.8747
Epoch 4/100, Training Loss: 0.1001, Training F1-score: 0.9393, Validation Loss: 0.2109, Validation F1-score: 0.8775
Epoch 5/100, Training Loss: 0.0924, Training F1-score: 0.9446, Validation Loss: 0.2119, Validation F1-score: 0.8717
Epoch 6/100, Training Loss: 0.0862, Training F1-score: 0.9495, Validation Loss: 0.2683, Validation F1-score: 0.8351
Epoch 7/100, Training Loss: 0.0828, Training F1-score: 0.9513, Validation Loss: 0.2477, Validation F1-score: 0.8401
Epoch 8/100, Training Loss: 0.0795, Training F1-score: 0.9536, Validation Loss: 0.2476, Validation F1-score: 0.8600
Epoch 9/100, Training Loss: 0.0770, Training F1-score: 0.9546, Validation Loss: 0.2650, Validation F1-score: 0.8608
Epoch 10/100, Training Loss: 0.0742, Training F1-score: 0.9558, Validation Loss: 0.2371, Validation F1-score: 0.8616
[I 2024-12-22 21:04:09,603] Trial 186 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.3, 'learning_rate': 0.0001, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 17, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0717, Training F1-score: 0.9573, Validation Loss: 0.2507, Validation F1-score: 0.8484
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:187 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.1.ls2.gamma
Dropout: 0.4
Learning rate: 2.5e-05
Optimizer: SGD
Momentum term: 0.9611765387091
Step size: 5
Gamma: 0.3
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 21:06:00,207] Trial 187 pruned. 
Epoch 1/100, Training Loss: 0.5313, Training F1-score: 0.4815, Validation Loss: 0.3805, Validation F1-score: 0.7538
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:188 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.28
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 9
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1871, Training F1-score: 0.8841, Validation Loss: 0.2048, Validation F1-score: 0.8794
Best F1-score till now on the current trail on Validation data: 0.8794009768168972
Epoch 2/100, Training Loss: 0.1236, Training F1-score: 0.9240, Validation Loss: 0.2049, Validation F1-score: 0.8643
Epoch 3/100, Training Loss: 0.1054, Training F1-score: 0.9365, Validation Loss: 0.2090, Validation F1-score: 0.8593
Epoch 4/100, Training Loss: 0.0944, Training F1-score: 0.9445, Validation Loss: 0.2173, Validation F1-score: 0.8421
Epoch 5/100, Training Loss: 0.0873, Training F1-score: 0.9488, Validation Loss: 0.2251, Validation F1-score: 0.8615
Epoch 6/100, Training Loss: 0.0833, Training F1-score: 0.9514, Validation Loss: 0.2357, Validation F1-score: 0.8636
Epoch 7/100, Training Loss: 0.0792, Training F1-score: 0.9527, Validation Loss: 0.2140, Validation F1-score: 0.8677
Epoch 8/100, Training Loss: 0.0759, Training F1-score: 0.9564, Validation Loss: 0.2808, Validation F1-score: 0.8553
Epoch 9/100, Training Loss: 0.0728, Training F1-score: 0.9571, Validation Loss: 0.2309, Validation F1-score: 0.8494
Epoch 10/100, Training Loss: 0.0614, Training F1-score: 0.9635, Validation Loss: 0.2573, Validation F1-score: 0.8489
[I 2024-12-22 21:25:55,260] Trial 188 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.28, 'learning_rate': 0.0001, 'optimizer': 'Adam', 'layer_freeze_upto': 'dino_model.blocks.4.ls2.gamma', 'step_size': 9, 'gamma': 0.4}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0600, Training F1-score: 0.9645, Validation Loss: 0.2491, Validation F1-score: 0.8655
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:189 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.5.ls2.gamma
Dropout: 0.25
Learning rate: 0.00025
Optimizer: RMSprop
Momentum term: 0
Step size: 5
Gamma: 0.5
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
Epoch 1/100, Training Loss: 0.1819, Training F1-score: 0.8869, Validation Loss: 0.1983, Validation F1-score: 0.8791
Best F1-score till now on the current trail on Validation data: 0.8790691870565815
Epoch 2/100, Training Loss: 0.1301, Training F1-score: 0.9199, Validation Loss: 0.2090, Validation F1-score: 0.8711
Epoch 3/100, Training Loss: 0.1115, Training F1-score: 0.9326, Validation Loss: 0.2384, Validation F1-score: 0.8334
Epoch 4/100, Training Loss: 0.1000, Training F1-score: 0.9404, Validation Loss: 0.2357, Validation F1-score: 0.8526
Epoch 5/100, Training Loss: 0.0927, Training F1-score: 0.9455, Validation Loss: 0.2300, Validation F1-score: 0.8632
Epoch 6/100, Training Loss: 0.0757, Training F1-score: 0.9561, Validation Loss: 0.2584, Validation F1-score: 0.8422
Epoch 7/100, Training Loss: 0.0726, Training F1-score: 0.9583, Validation Loss: 0.2738, Validation F1-score: 0.8613
Epoch 8/100, Training Loss: 0.0703, Training F1-score: 0.9583, Validation Loss: 0.2788, Validation F1-score: 0.8553
Epoch 9/100, Training Loss: 0.0686, Training F1-score: 0.9602, Validation Loss: 0.2565, Validation F1-score: 0.8443
Epoch 10/100, Training Loss: 0.0660, Training F1-score: 0.9607, Validation Loss: 0.2530, Validation F1-score: 0.8735
[I 2024-12-22 21:45:37,203] Trial 189 finished with value: 0.8930030233872418 and parameters: {'dropout': 0.25, 'learning_rate': 0.00025, 'optimizer': 'RMSprop', 'layer_freeze_upto': 'dino_model.blocks.5.ls2.gamma', 'step_size': 5, 'gamma': 0.5}. Best is trial 12 with value: 0.8930030233872418.
Epoch 11/100, Training Loss: 0.0600, Training F1-score: 0.9652, Validation Loss: 0.2994, Validation F1-score: 0.8572
Early stopping triggered after 11 epochs.
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:190 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.2.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 8
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 21:47:25,485] Trial 190 pruned. 
Epoch 1/100, Training Loss: 0.1969, Training F1-score: 0.8783, Validation Loss: 0.2014, Validation F1-score: 0.8635
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:191 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 11
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 21:49:11,220] Trial 191 pruned. 
Epoch 1/100, Training Loss: 0.1971, Training F1-score: 0.8779, Validation Loss: 0.2319, Validation F1-score: 0.8616
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:192 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 11
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
[I 2024-12-22 21:50:56,077] Trial 192 pruned. 
Epoch 1/100, Training Loss: 0.1977, Training F1-score: 0.8767, Validation Loss: 0.2103, Validation F1-score: 0.8728
Best F1-score till now on Validation data: 0.8930030233872418
==================== Training of trial number:193 ====================
Hyperparameters:
Layer freeze: dino_model.blocks.4.ls2.gamma
Dropout: 0.4
Learning rate: 0.0001
Optimizer: Adam
Momentum term: 0
Step size: 10
Gamma: 0.4
Best F1-score on Validation data until now: 0.8930030233872418
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
slurmstepd: error: *** JOB 960242 ON tg080 CANCELLED AT 2024-12-22T21:52:23 DUE TO TIME LIMIT ***
=== JOB_STATISTICS ===
=== current date     : Sun 22 Dec 2024 09:52:25 PM CET
= Job-ID             : 960242 on tinygpu
= Job-Name           : MinicondaName
= Job-Command        : /home/woody/iwfa/iwfa044h/runner.sh
= Initial workdir    : /home/woody/iwfa/iwfa044h
= Queue/Partition    : work
= Slurm account      : iwfa with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 1-00:00:20
= Total RAM usage    : 1.4 GiB of requested  GiB (%)   
= Node list          : tg080
= Subm/Elig/Start/End: 2024-12-21T21:28:24 / 2024-12-21T21:28:24 / 2024-12-21T21:52:03 / 2024-12-22T21:52:23
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           58.7G   104.9G   209.7G        N/A     152K     500K   1,000K        N/A    
    /home/vault          0.0K  1048.6G  2097.2G        N/A       1      200K     400K        N/A    
    /home/woody        987.6G  1000.0G  1500.0G        N/A   1,812K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA GeForce RTX 3080, 00000000:DA:00.0, 1272379, 45 %, 18 %, 458 MiB, 86406067 ms
