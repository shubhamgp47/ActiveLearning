### Starting TaskPrologue of job 974174 on tg090 at Wed 15 Jan 2025 10:56:18 PM CET
Running on cores 32-63 with governor ondemand
Wed Jan 15 22:56:18 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   38C    P0             53W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

[07;1;31;43m WARNING: You are over quota on at least one filesystem![0m
    Path              Used     SoftQ    HardQ    Gracetime  Filecount  FileQuota  FileHardQ  FileGrace    
[07;1;31;43m!!! /home/woody       1017.0G  1000.0G  1500.0G      6days   1,813K   5,000K   7,500K        N/A !!![0m
Adjusted shapes after loading:
X_initial_np: (8, 224, 224, 3), y_initial_np: (8, 3)
X_pool_np: (26872, 224, 224, 3), y_pool_np: (26872, 3)
X_test_np: (5760, 224, 224, 3), y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3), y_val_np: (5760, 3)
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.3889[0m        [32m0.7043[0m       [35m0.2675[0m      [31m0.3634[0m        [94m0.6507[0m     +  75.3152
      2      [36m0.4571[0m        [32m0.6808[0m       [35m0.3056[0m      0.3536        [94m0.6502[0m     +  72.6059
      3      0.3016        0.6975       [35m0.3184[0m      [31m0.6914[0m        0.6513        72.6017
      4      [36m0.5524[0m        [32m0.6546[0m       [35m0.3224[0m      [31m0.6945[0m        0.6531        72.5937
      5      0.4722        0.6593       0.3177      [31m0.6964[0m        0.6556        72.5998
      6      [36m0.6190[0m        [32m0.6248[0m       0.3042      0.6920        0.6579        72.6860
      7      0.5820        [32m0.6221[0m       0.2837      0.6855        0.6606        72.6861
      8      [36m0.6519[0m        [32m0.6100[0m       0.2767      0.3580        0.6631        72.6308
      9      [36m0.6690[0m        [32m0.6045[0m       0.2703      0.4074        0.6642        72.6907
     10      [36m0.8320[0m        [32m0.5796[0m       0.2490      0.4304        0.6656        72.6908
     11      0.7619        [32m0.5767[0m       0.2417      0.4465        0.6669        72.7053
     12      0.7778        [32m0.5555[0m       0.2410      0.4655        0.6674        72.7646
     13      [36m0.8796[0m        0.5576       0.2450      0.4791        0.6675        72.7005
Stopping since valid_loss has not improved in the last 12 epochs.
Pre F1 micro score = 0.5453
Pre F1 macro score = 0.5164
Pre Accuracy = 0.3111

Iteration: 1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.6282[0m        [32m0.6607[0m       [35m0.2078[0m      [31m0.5200[0m        [94m0.6560[0m     +  73.0955
      2      [36m0.7042[0m        [32m0.6164[0m       [35m0.2156[0m      [31m0.5567[0m        [94m0.6541[0m     +  73.0467
      3      [36m0.8134[0m        [32m0.5787[0m       [35m0.2587[0m      [31m0.6054[0m        [94m0.6466[0m     +  73.1206
      4      [36m0.8135[0m        0.5865       [35m0.2967[0m      [31m0.6370[0m        [94m0.6351[0m     +  73.1612
      5      [36m0.8515[0m        [32m0.5536[0m       [35m0.3198[0m      [31m0.6598[0m        [94m0.6249[0m     +  73.1873
      6      [36m0.8645[0m        [32m0.5386[0m       [35m0.3399[0m      [31m0.6785[0m        [94m0.6129[0m     +  73.1592
      7      [36m0.9057[0m        [32m0.5146[0m       [35m0.3613[0m      [31m0.6919[0m        [94m0.5979[0m     +  73.1744
      8      [36m0.9237[0m        [32m0.4884[0m       [35m0.3790[0m      [31m0.7000[0m        [94m0.5859[0m     +  73.1686
      9      [36m0.9569[0m        [32m0.4628[0m       [35m0.4033[0m      [31m0.7148[0m        [94m0.5744[0m     +  73.2272
     10      0.9437        [32m0.4432[0m       [35m0.4326[0m      [31m0.7303[0m        [94m0.5627[0m     +  73.1845
     11      [36m0.9651[0m        [32m0.4038[0m       [35m0.4661[0m      [31m0.7445[0m        [94m0.5495[0m     +  73.2061
     12      [36m0.9784[0m        [32m0.3855[0m       [35m0.5186[0m      [31m0.7599[0m        [94m0.5357[0m     +  73.1900
     13      0.9651        0.3914       [35m0.5510[0m      [31m0.7638[0m        [94m0.5227[0m     +  73.1683
     14      0.9543        [32m0.3635[0m       [35m0.5693[0m      [31m0.7658[0m        [94m0.5113[0m     +  73.1811
     15      0.9752        [32m0.3344[0m       [35m0.5727[0m      [31m0.7678[0m        [94m0.5026[0m     +  73.1385
     16      [36m0.9867[0m        [32m0.3121[0m       0.5701      [31m0.7702[0m        [94m0.4985[0m     +  73.1767
     17      0.9867        0.3142       [35m0.5780[0m      [31m0.7741[0m        [94m0.4892[0m     +  73.1521
     18      0.9752        [32m0.2876[0m       [35m0.5915[0m      [31m0.7754[0m        [94m0.4774[0m     +  73.1551
     19      [36m1.0000[0m        [32m0.2655[0m       [35m0.5953[0m      0.7678        [94m0.4700[0m     +  73.1371
     20      1.0000        0.2662       0.5920      0.7629        [94m0.4666[0m     +  73.1641
     21      1.0000        [32m0.2588[0m       0.5873      0.7569        [94m0.4651[0m     +  73.1489
     22      1.0000        [32m0.2464[0m       0.5795      0.7612        0.4674        73.1381
     23      1.0000        [32m0.2361[0m       0.5856      0.7642        [94m0.4588[0m     +  75.8400
     24      1.0000        [32m0.2114[0m       [35m0.6003[0m      0.7719        [94m0.4501[0m     +  73.2072
     25      1.0000        [32m0.2099[0m       [35m0.6097[0m      0.7748        [94m0.4471[0m     +  73.1924
     26      1.0000        [32m0.2091[0m       0.5958      0.7690        0.4472        73.1489
     27      1.0000        0.2148       0.5885      0.7565        [94m0.4447[0m     +  73.1022
     28      1.0000        [32m0.1899[0m       0.5920      0.7555        0.4467        73.1983
     29      1.0000        [32m0.1795[0m       0.5847      0.7534        [94m0.4429[0m     +  73.1403
     30      1.0000        0.1894       0.5727      0.7578        0.4446        73.1644
     31      1.0000        [32m0.1743[0m       0.5901      0.7577        [94m0.4378[0m     +  73.1295
     32      1.0000        [32m0.1742[0m       0.5950      0.7551        [94m0.4338[0m     +  73.1592
     33      1.0000        [32m0.1631[0m       0.5571      0.7524        0.4526        73.1691
     34      0.9885        [32m0.1565[0m       0.5984      0.7633        [94m0.4276[0m     +  73.1144
     35      1.0000        [32m0.1467[0m       [35m0.6219[0m      0.7559        [94m0.4250[0m     +  73.1764
     36      1.0000        0.1518       0.5646      0.7528        0.4446        73.1454
     37      1.0000        [32m0.1452[0m       0.5939      0.7605        0.4275        73.1318
     38      1.0000        0.1527       0.6189      0.7607        [94m0.4204[0m     +  73.1294
     39      1.0000        [32m0.1445[0m       0.5717      0.7519        0.4356        73.1700
     40      1.0000        [32m0.1423[0m       0.5674      0.7510        0.4336        73.1403
     41      1.0000        0.1429       0.5939      0.7560        0.4212        73.1331
     42      1.0000        [32m0.1377[0m       0.5950      0.7617        [94m0.4199[0m     +  73.1619
     43      1.0000        [32m0.1346[0m       0.5877      0.7640        0.4216        73.1787
     44      1.0000        0.1367       0.5892      0.7588        [94m0.4169[0m     +  73.1199
     45      1.0000        0.1379       0.5882      0.7546        0.4190        73.1880
     46      1.0000        [32m0.1228[0m       0.5590      0.7537        0.4315        73.1115
     47      1.0000        0.1319       0.5858      0.7618        [94m0.4161[0m     +  73.1391
     48      1.0000        0.1294       0.6144      0.7648        [94m0.4093[0m     +  73.1750
     49      1.0000        0.1243       0.5766      0.7600        0.4266        73.1708
     50      1.0000        [32m0.1188[0m       0.5927      0.7732        0.4124        73.1261
     51      1.0000        0.1194       0.6212      0.7723        [94m0.3984[0m     +  73.1492
     52      1.0000        [32m0.1148[0m       0.5905      0.7593        0.4140        73.1971
     53      1.0000        0.1155       0.5587      0.7468        0.4288        73.1179
     54      1.0000        0.1179       0.5771      0.7564        0.4175        73.1280
     55      1.0000        [32m0.1098[0m       0.6068      0.7627        0.4092        73.1249
     56      1.0000        [32m0.1033[0m       0.5806      0.7603        0.4144        73.1260
     57      1.0000        0.1099       0.5599      0.7602        0.4223        73.1381
     58      1.0000        [32m0.1025[0m       0.5972      0.7642        0.4076        73.1313
     59      1.0000        0.1084       0.5861      0.7573        0.4144        73.1499
     60      1.0000        [32m0.0984[0m       0.5500      0.7502        0.4287        73.1414
     61      1.0000        0.1016       0.5766      0.7607        0.4138        73.1301
     62      1.0000        [32m0.0948[0m       0.5948      0.7655        0.4048        73.1512
Stopping since valid_loss has not improved in the last 12 epochs.
[24]
F1 Micro Score after query 1: 0.7374220098234434
F1 Macro Score after query 1: 0.7186645757146849
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_0.pt

Iteration: 2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9271[0m        [32m0.2625[0m       [35m0.6023[0m      [31m0.6851[0m        [94m0.3924[0m     +  74.0805
      2      [36m0.9430[0m        [32m0.2267[0m       [35m0.6580[0m      [31m0.8312[0m        [94m0.3553[0m     +  74.0906
      3      [36m0.9551[0m        [32m0.1949[0m       [35m0.6852[0m      [31m0.8417[0m        [94m0.3325[0m     +  74.1014
      4      [36m0.9733[0m        [32m0.1762[0m       0.6582      0.8079        [94m0.3243[0m     +  74.1569
      5      [36m0.9847[0m        [32m0.1591[0m       0.6665      0.8402        [94m0.3198[0m     +  74.1315
      6      0.9829        [32m0.1484[0m       0.6818      0.8393        [94m0.3130[0m     +  74.0699
      7      [36m0.9957[0m        [32m0.1467[0m       0.6830      [31m0.8439[0m        [94m0.3091[0m     +  74.0477
      8      [36m1.0000[0m        [32m0.1381[0m       [35m0.6910[0m      [31m0.8550[0m        [94m0.3053[0m     +  74.0690
      9      1.0000        [32m0.1315[0m       [35m0.6936[0m      0.8449        [94m0.2995[0m     +  74.0915
     10      1.0000        [32m0.1185[0m       [35m0.7002[0m      0.8548        [94m0.2972[0m     +  74.1322
     11      1.0000        0.1213       [35m0.7009[0m      [31m0.8556[0m        [94m0.2947[0m     +  74.0762
     12      1.0000        [32m0.1135[0m       [35m0.7033[0m      0.8532        [94m0.2918[0m     +  74.0862
     13      1.0000        [32m0.1075[0m       [35m0.7047[0m      0.8516        [94m0.2893[0m     +  74.0819
     14      1.0000        0.1085       0.6939      0.8470        0.2897        74.0956
     15      1.0000        0.1075       0.6944      0.8508        0.2896        74.0734
     16      1.0000        [32m0.0985[0m       [35m0.7066[0m      0.8445        [94m0.2853[0m     +  74.0602
     17      1.0000        0.1006       0.6861      0.8550        0.2943        74.1077
     18      1.0000        [32m0.0970[0m       0.6943      0.8438        0.2874        74.0504
     19      1.0000        [32m0.0936[0m       0.7033      0.8521        [94m0.2807[0m     +  74.0514
     20      1.0000        [32m0.0889[0m       0.6991      0.8529        0.2814        74.0786
     21      1.0000        0.0925       0.7063      0.8390        [94m0.2794[0m     +  74.0663
     22      1.0000        0.0911       0.6988      [31m0.8585[0m        0.2815        74.0759
     23      1.0000        0.0940       0.7042      0.8527        [94m0.2775[0m     +  74.0378
     24      1.0000        [32m0.0852[0m       [35m0.7106[0m      0.8494        [94m0.2723[0m     +  74.0998
     25      1.0000        [32m0.0841[0m       0.7030      [31m0.8595[0m        0.2764        74.0940
     26      1.0000        [32m0.0830[0m       0.7030      0.8493        0.2737        74.0574
     27      1.0000        [32m0.0823[0m       0.7043      0.8532        [94m0.2709[0m     +  74.1014
     28      1.0000        [32m0.0810[0m       0.7024      0.8573        0.2714        74.1120
     29      1.0000        0.0869       0.6974      0.8522        0.2724        74.0332
     30      1.0000        [32m0.0765[0m       0.7059      0.8526        [94m0.2683[0m     +  74.0491
     31      1.0000        0.0819       0.7042      0.8528        [94m0.2666[0m     +  74.1068
     32      1.0000        [32m0.0747[0m       0.6979      0.8563        0.2706        74.0586
     33      1.0000        0.0781       0.7005      0.8523        0.2689        74.0341
     34      1.0000        [32m0.0739[0m       0.6957      0.8516        0.2698        74.0483
     35      1.0000        [32m0.0729[0m       0.7097      0.8499        [94m0.2639[0m     +  74.0465
     36      1.0000        0.0731       0.6957      0.8550        0.2684        74.0865
     37      1.0000        0.0730       0.7017      0.8573        0.2671        74.0470
     38      1.0000        [32m0.0721[0m       0.6974      0.8511        0.2694        74.0609
     39      1.0000        0.0735       0.7005      0.8404        0.2663        74.0649
     40      1.0000        [32m0.0696[0m       0.6991      0.8577        0.2664        74.0783
     41      1.0000        [32m0.0683[0m       0.6991      0.8560        0.2677        74.0669
     42      1.0000        0.0748       0.6951      0.8394        0.2661        74.0720
     43      1.0000        0.0692       0.6990      0.8530        0.2651        74.0816
     44      1.0000        0.0697       0.7061      0.8468        [94m0.2606[0m     +  74.0672
     45      1.0000        0.0694       0.6950      0.8564        0.2677        74.1485
     46      1.0000        0.0699       0.6967      0.8361        0.2649        74.0721
     47      1.0000        [32m0.0649[0m       0.6965      0.8557        0.2624        74.0777
     48      1.0000        [32m0.0628[0m       0.6920      0.8484        0.2642        74.0559
     49      1.0000        0.0633       0.7003      0.8438        0.2619        74.0478
     50      1.0000        [32m0.0625[0m       0.6962      0.8545        [94m0.2601[0m     +  74.0673
     51      1.0000        [32m0.0603[0m       0.6986      0.8475        0.2620        74.1172
     52      1.0000        0.0628       0.6937      0.8484        0.2613        74.0656
     53      1.0000        0.0644       0.7014      0.8514        [94m0.2581[0m     +  74.4169
     54      1.0000        0.0633       0.6979      0.8527        [94m0.2580[0m     +  74.2837
     55      1.0000        [32m0.0588[0m       0.6925      0.8482        0.2609        74.2485
     56      1.0000        0.0589       0.6943      0.8485        0.2616        74.1051
     57      1.0000        0.0598       0.6950      0.8434        0.2594        74.0577
     58      1.0000        0.0608       0.7087      0.8575        [94m0.2522[0m     +  74.0845
     59      1.0000        0.0609       0.6957      0.8500        0.2583        74.1570
     60      1.0000        0.0601       0.6967      0.8532        0.2593        74.3634
     61      1.0000        0.0613       0.6990      0.8402        0.2584        74.3028
     62      1.0000        0.0601       0.7007      0.8557        0.2543        74.1137
     63      1.0000        [32m0.0582[0m       0.6977      0.8474        0.2570        74.0767
     64      1.0000        [32m0.0561[0m       0.6970      0.8486        0.2556        74.0590
     65      1.0000        [32m0.0551[0m       0.6965      0.8499        0.2558        74.0795
     66      1.0000        0.0583       0.7066      0.8533        [94m0.2513[0m     +  74.0720
     67      1.0000        0.0573       0.6993      0.8507        0.2526        74.1220
     68      1.0000        [32m0.0540[0m       0.6937      0.8519        0.2571        74.1089
     69      1.0000        0.0555       0.6983      0.8503        0.2533        74.0871
     70      1.0000        0.0552       0.6917      0.8451        0.2561        74.0862
     71      1.0000        0.0589       0.6997      0.8453        0.2545        74.0777
     72      1.0000        [32m0.0514[0m       0.6977      0.8533        0.2535        74.0804
     73      1.0000        0.0535       0.7007      0.8483        [94m0.2509[0m     +  74.0870
     74      1.0000        0.0543       0.7050      0.8387        [94m0.2498[0m     +  74.1105
     75      1.0000        [32m0.0506[0m       0.6804      0.8513        0.2637        74.1364
     76      1.0000        0.0521       0.6866      0.8257        0.2620        74.0756
     77      1.0000        0.0542       0.6936      0.8518        0.2543        74.0765
     78      1.0000        0.0533       0.6885      0.8470        0.2558        74.0951
     79      1.0000        [32m0.0506[0m       0.6984      0.8505        0.2516        74.0605
     80      1.0000        0.0551       0.6896      0.8447        0.2560        74.0768
     81      1.0000        0.0524       0.6868      0.8351        0.2572        74.0937
     82      1.0000        [32m0.0502[0m       0.6880      0.8481        0.2544        74.1001
     83      1.0000        0.0554       0.6911      0.8464        0.2544        74.0729
     84      1.0000        0.0518       0.6948      0.8451        0.2518        74.0746
     85      1.0000        0.0504       0.6858      0.8496        0.2563        74.0627
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56]
F1 Micro Score after query 2: 0.8345313294126188
F1 Macro Score after query 2: 0.8212158456409773
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_1.pt

Iteration: 3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9352[0m        [32m0.1656[0m       [35m0.7517[0m      [31m0.8665[0m        [94m0.2458[0m     +  75.7147
      2      [36m0.9381[0m        0.1679       0.6444      0.7039        0.3208        75.8083
      3      [36m0.9725[0m        [32m0.1328[0m       0.7460      [31m0.8685[0m        [94m0.2314[0m     +  75.9204
      4      [36m0.9789[0m        [32m0.1135[0m       0.7418      0.8439        [94m0.2308[0m     +  75.8024
      5      [36m0.9815[0m        [32m0.1078[0m       0.7503      0.8660        [94m0.2244[0m     +  75.7682
      6      [36m0.9820[0m        [32m0.1017[0m       0.7497      0.8528        [94m0.2202[0m     +  75.7355
      7      [36m0.9942[0m        [32m0.0885[0m       0.7292      0.8515        0.2349        75.8176
      8      0.9887        [32m0.0822[0m       0.7477      0.8501        [94m0.2201[0m     +  75.7188
      9      [36m0.9975[0m        [32m0.0704[0m       [35m0.7587[0m      [31m0.8686[0m        [94m0.2132[0m     +  75.7702
     10      0.9956        0.0721       0.7413      0.8389        0.2237        75.7873
     11      [36m1.0000[0m        [32m0.0686[0m       0.7536      0.8684        0.2151        75.7750
     12      1.0000        0.0691       0.7488      0.8465        0.2167        75.7241
     13      0.9968        [32m0.0611[0m       0.7531      0.8661        [94m0.2127[0m     +  75.7444
     14      0.9956        [32m0.0599[0m       0.7517      0.8480        0.2138        75.7927
     15      1.0000        0.0611       0.7542      0.8651        0.2138        75.7300
     16      1.0000        [32m0.0593[0m       0.7575      0.8540        [94m0.2108[0m     +  75.7206
     17      0.9968        [32m0.0568[0m       0.7490      0.8627        0.2149        75.7654
     18      1.0000        0.0570       0.7587      0.8596        [94m0.2083[0m     +  75.7208
     19      1.0000        [32m0.0550[0m       0.7575      0.8655        0.2107        75.7689
     20      1.0000        [32m0.0537[0m       0.7545      0.8589        0.2103        75.7531
     21      0.9968        [32m0.0535[0m       0.7509      0.8605        0.2130        75.7040
     22      1.0000        [32m0.0518[0m       [35m0.7628[0m      0.8653        [94m0.2040[0m     +  75.7516
     23      1.0000        [32m0.0505[0m       0.7481      0.8527        0.2155        75.7909
     24      1.0000        0.0511       0.7540      0.8655        0.2090        75.7219
     25      1.0000        0.0507       0.7444      0.8558        0.2161        75.7182
     26      1.0000        [32m0.0488[0m       0.7575      0.8602        0.2080        75.7172
     27      1.0000        [32m0.0487[0m       0.7507      0.8632        0.2113        75.7119
     28      1.0000        [32m0.0450[0m       0.7507      0.8560        0.2128        75.7269
     29      1.0000        0.0460       0.7536      0.8622        0.2084        75.7248
     30      1.0000        0.0451       0.7531      0.8616        0.2094        75.7330
     31      1.0000        0.0483       0.7491      0.8598        0.2107        75.7261
     32      1.0000        0.0465       0.7569      0.8610        0.2077        75.7293
     33      1.0000        0.0475       0.7503      0.8550        0.2130        75.7047
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112]
F1 Micro Score after query 3: 0.8676884579293088
F1 Macro Score after query 3: 0.8508692119527036
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_2.pt

Iteration: 4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9572[0m        [32m0.1279[0m       [35m0.6905[0m      [31m0.6815[0m        [94m0.2857[0m     +  78.4997
      2      [36m0.9709[0m        [32m0.1000[0m       [35m0.7137[0m      [31m0.7683[0m        [94m0.2540[0m     +  78.5874
      3      [36m0.9820[0m        [32m0.0846[0m       [35m0.7557[0m      [31m0.8164[0m        [94m0.2262[0m     +  78.7685
      4      [36m0.9861[0m        [32m0.0814[0m       [35m0.7726[0m      [31m0.8580[0m        [94m0.2026[0m     +  78.5438
      5      [36m0.9881[0m        [32m0.0784[0m       [35m0.7854[0m      [31m0.8824[0m        [94m0.1969[0m     +  78.5981
      6      [36m0.9892[0m        [32m0.0757[0m       0.7816      [31m0.8872[0m        0.2250        78.5172
      7      0.9815        0.0771       0.7552      0.8747        0.2512        78.4887
      8      0.9824        [32m0.0725[0m       0.7752      0.8760        0.1994        78.5068
      9      [36m0.9905[0m        [32m0.0696[0m       0.7434      0.8279        0.2207        78.5250
     10      [36m0.9969[0m        [32m0.0570[0m       0.7306      0.8169        0.2353        78.7900
     11      [36m0.9980[0m        [32m0.0553[0m       0.7667      0.8721        0.2086        78.8209
     12      [36m1.0000[0m        [32m0.0473[0m       0.7566      0.8665        0.2171        79.0472
     13      1.0000        0.0476       0.7630      0.8668        0.2091        78.8138
     14      1.0000        [32m0.0438[0m       0.7780      0.8713        [94m0.1934[0m     +  78.7336
     15      1.0000        [32m0.0424[0m       0.7840      0.8745        [94m0.1884[0m     +  79.4271
     16      1.0000        [32m0.0419[0m       0.7769      0.8687        0.1915        78.8769
     17      1.0000        [32m0.0419[0m       0.7767      0.8708        0.1935        78.6896
     18      1.0000        [32m0.0391[0m       0.7771      0.8718        0.1928        78.8833
     19      1.0000        [32m0.0383[0m       0.7745      0.8676        0.1922        78.8188
     20      1.0000        0.0393       0.7764      0.8691        0.1905        78.5520
     21      1.0000        [32m0.0376[0m       0.7727      0.8668        0.1928        78.4915
     22      1.0000        [32m0.0358[0m       0.7781      0.8724        0.1902        78.4865
     23      1.0000        0.0368       0.7811      0.8743        [94m0.1882[0m     +  78.4857
     24      1.0000        0.0378       0.7720      0.8680        0.1943        78.8580
     25      1.0000        0.0364       0.7753      0.8693        0.1894        78.5693
     26      1.0000        0.0358       0.7806      0.8715        [94m0.1860[0m     +  78.5992
     27      1.0000        [32m0.0342[0m       0.7760      0.8685        0.1912        78.5151
     28      1.0000        [32m0.0334[0m       0.7762      0.8690        0.1919        78.5674
     29      1.0000        [32m0.0331[0m       0.7764      0.8722        0.1885        78.4638
     30      1.0000        0.0337       0.7734      0.8710        0.1930        78.4787
     31      1.0000        [32m0.0328[0m       0.7740      0.8665        0.1902        78.4601
     32      1.0000        0.0336       0.7748      0.8668        0.1887        78.4677
     33      1.0000        [32m0.0325[0m       0.7797      0.8704        0.1860        78.4718
     34      1.0000        [32m0.0310[0m       0.7781      0.8698        0.1889        78.4779
     35      1.0000        [32m0.0305[0m       0.7759      0.8700        0.1908        78.5054
     36      1.0000        0.0305       0.7767      0.8715        0.1876        78.4750
     37      1.0000        0.0305       0.7757      0.8697        0.1907        78.4530
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208]
F1 Micro Score after query 4: 0.8957613814756672
F1 Macro Score after query 4: 0.875477417739023
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_3.pt

Iteration: 5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9401[0m        [32m0.1161[0m       [35m0.7771[0m      [31m0.8420[0m        [94m0.2304[0m     +  83.7044
      2      [36m0.9537[0m        [32m0.0966[0m       [35m0.7941[0m      [31m0.8513[0m        [94m0.1996[0m     +  83.6567
      3      [36m0.9748[0m        [32m0.0747[0m       [35m0.7984[0m      [31m0.8627[0m        [94m0.1870[0m     +  83.6291
      4      [36m0.9815[0m        [32m0.0635[0m       [35m0.8090[0m      [31m0.8730[0m        [94m0.1787[0m     +  83.6872
      5      [36m0.9877[0m        [32m0.0554[0m       0.7948      0.8432        0.2004        83.6820
      6      [36m0.9907[0m        [32m0.0497[0m       0.7967      0.8491        0.1936        83.6328
      7      [36m0.9926[0m        [32m0.0463[0m       0.7785      0.8465        0.2249        83.6052
      8      [36m0.9963[0m        [32m0.0399[0m       0.7988      0.8590        0.1899        83.6178
      9      [36m0.9982[0m        [32m0.0377[0m       0.7910      0.8426        0.2192        83.6365
     10      0.9982        [32m0.0351[0m       0.7889      0.8465        0.2141        83.7401
     11      [36m1.0000[0m        [32m0.0343[0m       0.8009      0.8583        0.1921        83.7367
     12      0.9982        [32m0.0329[0m       0.7941      0.8646        0.1890        83.6865
     13      0.9959        0.0344       0.7734      0.8330        0.2439        83.6759
     14      1.0000        [32m0.0319[0m       [35m0.8095[0m      0.8712        [94m0.1774[0m     +  83.6382
     15      1.0000        [32m0.0303[0m       0.7984      0.8625        0.1951        83.6389
     16      1.0000        [32m0.0290[0m       0.8083      0.8692        0.1832        83.6774
     17      1.0000        [32m0.0274[0m       0.8094      0.8699        0.1823        83.6401
     18      1.0000        0.0276       0.8030      0.8730        0.1824        83.6043
     19      1.0000        [32m0.0262[0m       0.8057      [31m0.8740[0m        0.1825        83.6118
     20      1.0000        [32m0.0259[0m       [35m0.8127[0m      [31m0.8766[0m        0.1797        83.6600
     21      1.0000        [32m0.0252[0m       [35m0.8139[0m      [31m0.8767[0m        [94m0.1768[0m     +  83.6449
     22      1.0000        [32m0.0243[0m       0.8092      [31m0.8774[0m        0.1809        83.6985
     23      1.0000        [32m0.0238[0m       0.8057      0.8753        0.1857        83.6264
     24      1.0000        0.0239       0.8127      [31m0.8807[0m        0.1794        83.6268
     25      1.0000        [32m0.0236[0m       [35m0.8160[0m      [31m0.8839[0m        0.1777        83.6338
     26      1.0000        [32m0.0229[0m       [35m0.8168[0m      0.8821        0.1778        83.6531
     27      1.0000        [32m0.0225[0m       0.8038      0.8805        0.1883        83.6508
     28      1.0000        [32m0.0221[0m       0.8139      0.8781        0.1785        83.6733
     29      1.0000        [32m0.0210[0m       0.8043      0.8833        0.1885        83.6424
     30      1.0000        0.0211       0.8113      0.8809        0.1820        83.6498
     31      1.0000        [32m0.0200[0m       0.8082      0.8795        0.1868        83.6450
     32      1.0000        0.0205       0.8092      0.8804        0.1837        83.6645
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.9101185336368632
F1 Macro Score after query 5: 0.8900732672893961
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_4.pt

Iteration: 6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9472[0m        [32m0.0999[0m       [35m0.8092[0m      [31m0.8734[0m        [94m0.1694[0m     +  92.9399
      2      [36m0.9692[0m        [32m0.0735[0m       [35m0.8146[0m      0.8551        0.1907        93.0007
      3      [36m0.9792[0m        [32m0.0544[0m       [35m0.8243[0m      0.8689        0.1832        92.9856
      4      [36m0.9897[0m        [32m0.0443[0m       0.8144      0.8662        0.1836        92.9943
      5      [36m0.9918[0m        [32m0.0399[0m       0.8170      [31m0.8738[0m        0.1788        92.9958
      6      [36m0.9918[0m        [32m0.0351[0m       0.8035      0.8667        0.1935        92.9899
      7      0.9894        [32m0.0318[0m       0.8146      0.8539        0.2008        92.9782
      8      [36m0.9952[0m        [32m0.0288[0m       0.8101      0.8439        0.2157        92.9915
      9      0.9939        0.0314       0.8214      [31m0.8842[0m        0.1787        92.9920
     10      0.9929        0.0314       [35m0.8280[0m      [31m0.8983[0m        0.1917        92.9802
     11      [36m0.9959[0m        [32m0.0253[0m       0.8255      0.8796        0.1904        92.9741
     12      0.9912        0.0290       0.8069      0.8585        0.1962        93.0728
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.9280472239948948
F1 Macro Score after query 6: 0.9069511697179659
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_5.pt

Iteration: 7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9411[0m        [32m0.1025[0m       [35m0.8217[0m      [31m0.8806[0m        [94m0.1667[0m     +  109.3025
      2      [36m0.9669[0m        [32m0.0760[0m       [35m0.8220[0m      [31m0.8865[0m        [94m0.1595[0m     +  109.3494
      3      [36m0.9749[0m        [32m0.0598[0m       0.8108      0.8822        0.1770        109.3362
      4      [36m0.9838[0m        [32m0.0483[0m       0.8038      0.8805        0.1798        109.3932
      5      [36m0.9857[0m        [32m0.0432[0m       0.7873      0.8793        0.2043        109.2968
      6      [36m0.9885[0m        [32m0.0410[0m       0.8102      0.8815        0.1825        109.2847
      7      0.9817        0.0423       0.7877      0.8809        0.2014        109.3118
      8      [36m0.9907[0m        [32m0.0347[0m       0.8009      0.8857        0.2005        109.3240
      9      [36m0.9932[0m        [32m0.0301[0m       0.8163      [31m0.8914[0m        0.1868        109.2805
     10      0.9921        [32m0.0291[0m       [35m0.8255[0m      [31m0.8944[0m        0.1785        109.2854
     11      [36m0.9954[0m        [32m0.0257[0m       0.8219      0.8902        0.1829        109.2641
     12      [36m0.9965[0m        [32m0.0240[0m       0.8207      0.8923        0.1944        109.2941
     13      [36m0.9971[0m        [32m0.0214[0m       0.8224      0.8867        0.1863        109.2972
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.9353629976580796
F1 Macro Score after query 7: 0.9222036908042449
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_6.pt

Iteration: 8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9676[0m        [32m0.0728[0m       [35m0.8299[0m      [31m0.8989[0m        [94m0.1593[0m     +  138.8554
      2      [36m0.9773[0m        [32m0.0585[0m       0.8158      0.8833        0.1765        138.5198
      3      [36m0.9781[0m        [32m0.0546[0m       0.8257      0.8816        0.1667        138.4427
      4      [36m0.9845[0m        [32m0.0453[0m       0.8170      0.8855        0.1738        138.4021
      5      [36m0.9853[0m        [32m0.0417[0m       0.8247      0.8961        0.1701        138.3962
      6      [36m0.9875[0m        [32m0.0369[0m       0.8267      0.8936        0.1739        138.3154
      7      [36m0.9886[0m        [32m0.0327[0m       0.8184      0.8820        0.1865        138.2989
      8      [36m0.9921[0m        [32m0.0284[0m       0.8181      0.8900        0.1972        138.4265
      9      [36m0.9923[0m        0.0287       0.8227      0.8947        0.1859        138.2445
     10      [36m0.9952[0m        [32m0.0225[0m       0.8233      0.8952        0.1892        138.5904
     11      [36m0.9972[0m        [32m0.0192[0m       0.8155      0.8877        0.2001        138.5759
     12      0.9908        0.0280       0.8165      0.8862        0.2088        138.5751
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.9413514994910345
F1 Macro Score after query 8: 0.9278605687429297
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_7.pt

Iteration: 9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9674[0m        [32m0.0732[0m       [35m0.8280[0m      [31m0.8972[0m        [94m0.1586[0m     +  189.7217
      2      [36m0.9740[0m        [32m0.0593[0m       [35m0.8481[0m      [31m0.8996[0m        [94m0.1437[0m     +  190.0961
      3      [36m0.9799[0m        [32m0.0484[0m       0.8273      [31m0.9001[0m        0.1712        190.1317
      4      [36m0.9831[0m        [32m0.0435[0m       0.8224      0.8948        0.1794        190.2293
      5      [36m0.9843[0m        [32m0.0400[0m       0.8069      0.8829        0.1964        190.2047
      6      [36m0.9881[0m        [32m0.0350[0m       0.8302      0.8977        0.1714        190.0575
      7      [36m0.9884[0m        [32m0.0312[0m       0.8375      0.8984        0.1679        190.0885
      8      [36m0.9891[0m        [32m0.0296[0m       0.8288      0.8996        0.1778        190.0893
      9      [36m0.9923[0m        [32m0.0253[0m       0.8444      0.8945        0.1746        190.0837
     10      0.9922        [32m0.0234[0m       0.8141      0.8884        0.2124        190.7546
     11      [36m0.9940[0m        [32m0.0197[0m       0.8172      0.8940        0.2097        190.3048
     12      0.9910        0.0244       0.8205      0.8875        0.1979        190.1849
     13      0.9931        [32m0.0191[0m       0.8332      0.8957        0.1876        190.2371
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.9343212163309409
F1 Macro Score after query 9: 0.9233940586087224
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_8.pt

Iteration: 10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9721[0m        [32m0.0606[0m       [35m0.8184[0m      [31m0.8961[0m        [94m0.1648[0m     +  282.1188
      2      [36m0.9761[0m        [32m0.0500[0m       [35m0.8319[0m      [31m0.8966[0m        [94m0.1594[0m     +  282.6356
      3      [36m0.9803[0m        [32m0.0429[0m       [35m0.8385[0m      0.8914        [94m0.1558[0m     +  282.8553
      4      [36m0.9832[0m        [32m0.0382[0m       0.8283      0.8936        0.1708        282.8281
      5      [36m0.9856[0m        [32m0.0332[0m       0.8306      0.8877        0.1718        283.7075
      6      [36m0.9877[0m        [32m0.0284[0m       0.8339      [31m0.8977[0m        0.1822        282.5495
      7      [36m0.9892[0m        [32m0.0270[0m       0.8266      0.8942        0.1971        282.6563
      8      [36m0.9900[0m        [32m0.0232[0m       0.8354      0.8935        0.1861        282.3617
      9      0.9893        [32m0.0225[0m       0.8319      [31m0.8987[0m        0.1989        282.4269
     10      [36m0.9917[0m        [32m0.0196[0m       0.8215      0.8741        0.2029        282.5710
     11      [36m0.9922[0m        [32m0.0184[0m       0.8292      0.8927        0.2090        282.4258
     12      [36m0.9941[0m        [32m0.0177[0m       0.8240      0.8807        0.2124        282.9911
     13      [36m0.9945[0m        [32m0.0141[0m       0.8210      0.8879        0.2258        282.4395
     14      [36m0.9947[0m        [32m0.0138[0m       0.8141      0.8727        0.2384        282.3691
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.9260471204188482
F1 Macro Score after query 10: 0.9109635536695277
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_9.pt

Iteration: 11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9757[0m        [32m0.0479[0m       [35m0.8530[0m      [31m0.8989[0m        [94m0.1493[0m     +  445.6313
      2      [36m0.9801[0m        [32m0.0401[0m       0.8413      0.8896        0.1613        446.4655
      3      [36m0.9832[0m        [32m0.0333[0m       0.8483      [31m0.9029[0m        0.1605        446.6910
      4      [36m0.9847[0m        [32m0.0298[0m       0.8313      0.8920        0.1842        446.9568
      5      [36m0.9867[0m        [32m0.0269[0m       0.8375      0.8984        0.1764        446.2518
      6      [36m0.9892[0m        [32m0.0226[0m       0.8444      0.9013        0.1717        446.9037
      7      [36m0.9910[0m        [32m0.0200[0m       0.8484      [31m0.9056[0m        0.1773        446.2458
      8      [36m0.9919[0m        [32m0.0181[0m       0.8467      0.9029        0.1927        446.2467
      9      [36m0.9922[0m        [32m0.0165[0m       0.8342      0.8999        0.2025        446.4851
     10      [36m0.9932[0m        [32m0.0141[0m       0.8309      0.9010        0.2133        446.3325
     11      [36m0.9941[0m        [32m0.0138[0m       0.8396      [31m0.9058[0m        0.2126        446.8689
     12      [36m0.9954[0m        [32m0.0113[0m       0.8214      0.8940        0.2309        446.4070
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.9485705407085145
F1 Macro Score after query 11: 0.9375844524993214
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_10.pt

Iteration: 12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9764[0m        [32m0.0455[0m       [35m0.8326[0m      [31m0.9017[0m        [94m0.1659[0m     +  737.8481
      2      [36m0.9798[0m        [32m0.0378[0m       0.8125      0.8924        0.1740        739.5281
      3      [36m0.9825[0m        [32m0.0323[0m       0.8247      0.8976        0.1746        740.0053
      4      [36m0.9850[0m        [32m0.0276[0m       0.8182      0.8927        0.1970        738.3650
      5      [36m0.9868[0m        [32m0.0246[0m       0.8215      0.8677        0.2002        739.1411
      6      [36m0.9887[0m        [32m0.0215[0m       0.8102      0.8812        0.2157        738.4279
      7      [36m0.9907[0m        [32m0.0190[0m       0.8156      0.8866        0.2305        738.6725
      8      [36m0.9913[0m        [32m0.0167[0m       0.8127      0.8812        0.2336        738.6882
      9      [36m0.9922[0m        [32m0.0150[0m       0.8163      0.8894        0.2241        739.6916
     10      [36m0.9934[0m        [32m0.0127[0m       0.8243      0.8984        0.2375        738.9350
     11      [36m0.9941[0m        [32m0.0122[0m       0.8267      0.8956        0.2477        738.4220
     12      [36m0.9941[0m        [32m0.0118[0m       0.8111      0.8901        0.2687        738.3556
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.9374658443282066
F1 Macro Score after query 12: 0.9268317244803462
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_11.pt

Iteration: 13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9792[0m        [32m0.0402[0m       [35m0.8594[0m      [31m0.9107[0m        [94m0.1296[0m     +  854.9465
      2      [36m0.9820[0m        [32m0.0335[0m       [35m0.8604[0m      0.9019        0.1386        856.8591
      3      [36m0.9848[0m        [32m0.0283[0m       [35m0.8606[0m      0.9069        0.1445        858.2114
      4      [36m0.9865[0m        [32m0.0247[0m       0.8573      0.9011        0.1499        857.8883
      5      [36m0.9884[0m        [32m0.0222[0m       [35m0.8653[0m      0.9088        0.1412        856.9206
      6      [36m0.9906[0m        [32m0.0178[0m       0.8424      0.8957        0.1652        857.1324
      7      [36m0.9909[0m        [32m0.0168[0m       0.8349      0.8848        0.1928        856.9341
      8      [36m0.9922[0m        [32m0.0151[0m       0.8278      0.8701        0.2072        856.6411
      9      [36m0.9930[0m        [32m0.0135[0m       0.8361      0.8941        0.2008        856.4567
     10      [36m0.9944[0m        [32m0.0114[0m       0.8306      0.8772        0.2523        856.7539
     11      [36m0.9947[0m        [32m0.0108[0m       0.8236      0.8725        0.2291        856.4019
     12      [36m0.9953[0m        [32m0.0092[0m       0.8137      0.8605        0.2655        856.9948
/home/hpc/iwfa/iwfa044h/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).
  from pandas.core.computation.check import NUMEXPR_INSTALLED
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.9379256965944271
F1 Macro Score after query 13: 0.923092856993119
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/model_checkpoint_iteration_12.pt
Pickle file saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed43/AL_average_confidence_results_for_multilabel_classification:s43.pickle
=== JOB_STATISTICS ===
=== current date     : Thu 16 Jan 2025 05:06:22 PM CET
= Job-ID             : 974174 on tinygpu
= Job-Name           : MinicondaName
= Job-Command        : /home/woody/iwfa/iwfa044h/runner.sh
= Initial workdir    : /home/woody/iwfa/iwfa044h
= Queue/Partition    : a100
= Slurm account      : iwfa with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 18:10:06
= Total RAM usage    : 84.2 GiB of requested  GiB (%)   
= Node list          : tg090
= Subm/Elig/Start/End: 2025-01-15T19:57:12 / 2025-01-15T19:57:12 / 2025-01-15T22:56:16 / 2025-01-16T17:06:22
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           58.2G   104.9G   209.7G        N/A     147K     500K   1,000K        N/A    
!!! /home/woody       1035.3G  1000.0G  1500.0G      6days   1,813K   5,000K   7,500K        N/A !!!
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:41:00.0, 3616306, 95 %, 16 %, 6812 MiB, 65375481 ms
