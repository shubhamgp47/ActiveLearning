(26880, 5)
(5760, 5)
(5760, 5)
3
multiclass_train_df shape: (26880, 2)
multiclass_test_df shape: (5760, 2)
multiclass_val_df shape: (5760, 2)
Calculated train_size: 8
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.1250[0m        [32m2.0531[0m       [35m0.3391[0m      [31m0.3391[0m        [94m1.9715[0m     +  0.0000  187.0907
      2      [36m0.3750[0m        [32m2.0163[0m       [35m0.3510[0m      [31m0.3510[0m        1.9719        0.0000  185.4992
      3      0.2500        2.1357       0.3408      0.3408        1.9730        0.0000  177.7633
      4      0.2500        [32m1.9632[0m       0.3453      0.3453        1.9733        0.0000  186.6970
      5      0.2500        [32m1.9491[0m       [35m0.3526[0m      [31m0.3526[0m        1.9743        0.0000  186.7805
      6      0.1250        1.9898       [35m0.3547[0m      [31m0.3547[0m        1.9760        0.0000  186.5938
      7      0.3750        [32m1.8697[0m       [35m0.3571[0m      [31m0.3571[0m        1.9796        0.0000  186.4685
      8      0.3750        [32m1.8625[0m       0.3563      0.3563        1.9830        0.0000  186.5328
      9      [36m0.5000[0m        [32m1.8228[0m       0.3458      0.3458        1.9884        0.0000  186.3705
     10      0.3750        1.8804       0.3425      0.3425        1.9913        0.0000  186.5637
     11      [36m0.6250[0m        [32m1.7658[0m       0.3425      0.3425        1.9920        0.0000  175.3929
     12      [36m0.8750[0m        [32m1.6856[0m       0.3424      0.3424        1.9915        0.0000  186.7145
     13      0.7500        1.6971       0.3446      0.3446        1.9908        0.0000  186.7966
Stopping since valid_loss has not improved in the last 13 epochs.
Pre F1 micro score = 0.3365
Pre F1 macro score = 0.1408

Iteration:  1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.4583[0m        [32m1.8773[0m       [35m0.3708[0m      [31m0.3708[0m        [94m1.9176[0m     +  0.0000  187.4426
      2      [36m0.6667[0m        [32m1.7599[0m       0.3594      0.3594        [94m1.8429[0m     +  0.0000  187.2795
      3      0.5417        [32m1.6924[0m       [35m0.3760[0m      [31m0.3760[0m        [94m1.7993[0m     +  0.0000  187.7131
      4      0.5417        [32m1.6416[0m       [35m0.4021[0m      [31m0.4021[0m        [94m1.7661[0m     +  0.0000  187.8290
      5      0.5417        [32m1.5751[0m       [35m0.4108[0m      [31m0.4108[0m        [94m1.7325[0m     +  0.0000  187.6944
      6      0.5417        [32m1.5452[0m       [35m0.4191[0m      [31m0.4191[0m        [94m1.7002[0m     +  0.0000  187.8372
      7      0.6250        [32m1.5100[0m       [35m0.4318[0m      [31m0.4318[0m        [94m1.6728[0m     +  0.0000  187.6755
      8      0.6667        [32m1.4083[0m       [35m0.4460[0m      [31m0.4460[0m        [94m1.6585[0m     +  0.0000  176.8504
      9      0.6250        1.4250       [35m0.4585[0m      [31m0.4585[0m        [94m1.6537[0m     +  0.0000  187.3893
     10      0.6250        [32m1.3642[0m       [35m0.4694[0m      [31m0.4694[0m        [94m1.6397[0m     +  0.0000  187.6435
     11      [36m0.7083[0m        [32m1.2458[0m       [35m0.4792[0m      [31m0.4792[0m        [94m1.6190[0m     +  0.0000  187.6404
     12      [36m0.7500[0m        [32m1.1928[0m       [35m0.4948[0m      [31m0.4948[0m        [94m1.6024[0m     +  0.0000  187.8587
     13      [36m0.7917[0m        [32m1.1513[0m       [35m0.4990[0m      [31m0.4990[0m        [94m1.5999[0m     +  0.0000  187.8096
     14      [36m0.8333[0m        1.1635       [35m0.4997[0m      [31m0.4997[0m        [94m1.5991[0m     +  0.0000  187.1281
     15      0.8333        [32m1.0793[0m       0.4997      0.4997        1.5994        0.0000  187.6626
     16      [36m0.8750[0m        1.1101       [35m0.5024[0m      [31m0.5024[0m        [94m1.5970[0m     +  0.0000  176.7124
     17      0.7500        [32m1.0776[0m       [35m0.5035[0m      [31m0.5035[0m        [94m1.5931[0m     +  0.0000  187.9353
     18      0.8333        [32m1.0439[0m       [35m0.5038[0m      [31m0.5038[0m        [94m1.5878[0m     +  0.0000  187.8085
     19      0.8333        [32m1.0422[0m       [35m0.5068[0m      [31m0.5068[0m        [94m1.5876[0m     +  0.0000  187.7258
     20      0.8750        1.0514       0.5062      0.5062        [94m1.5874[0m     +  0.0000  187.7532
     21      0.8333        [32m0.9936[0m       0.5054      0.5054        [94m1.5833[0m     +  0.0000  187.2354
     22      [36m0.9167[0m        [32m0.9329[0m       0.5045      0.5045        [94m1.5781[0m     +  0.0000  187.3986
     23      0.8750        [32m0.9191[0m       [35m0.5108[0m      [31m0.5108[0m        [94m1.5726[0m     +  0.0000  187.6915
     24      0.9167        0.9482       0.5059      0.5059        1.5736        0.0000  181.8547
     25      0.8750        0.9249       0.5045      0.5045        1.5740        0.0000  182.9487
     26      0.9167        [32m0.9127[0m       0.5036      0.5036        1.5736        0.0000  187.8900
     27      0.9167        [32m0.9112[0m       0.5038      0.5038        1.5731        0.0000  188.0150
     28      0.8750        [32m0.8782[0m       0.5042      0.5042        [94m1.5722[0m     +  0.0000  188.1287
     29      0.9167        [32m0.8488[0m       0.5038      0.5038        [94m1.5707[0m     +  0.0000  187.6247
     30      [36m0.9583[0m        0.8493       0.5042      0.5042        [94m1.5696[0m     +  0.0000  187.7859
     31      0.9583        [32m0.8427[0m       0.5036      0.5036        [94m1.5679[0m     +  0.0000  187.7160
     32      0.9583        [32m0.8054[0m       0.5043      0.5043        [94m1.5666[0m     +  0.0000  187.2995
     33      0.9167        [32m0.7990[0m       0.5040      0.5040        [94m1.5659[0m     +  0.0000  164.8591
     34      0.9583        0.8384       0.5045      0.5045        1.5665        0.0000  179.2113
     35      0.9167        [32m0.7927[0m       0.5049      0.5049        1.5665        0.0000  187.7708
     36      [36m1.0000[0m        0.8027       0.5040      0.5040        1.5662        0.0000  187.4286
     37      0.9583        [32m0.7842[0m       0.5038      0.5038        1.5660        0.0000  187.8093
     38      0.8750        0.7962       0.5033      0.5033        [94m1.5657[0m     +  0.0000  187.8149
     39      1.0000        [32m0.7773[0m       0.5035      0.5035        [94m1.5649[0m     +  0.0000  187.7859
     40      0.9583        [32m0.7715[0m       0.5031      0.5031        [94m1.5647[0m     +  0.0000  187.2045
     41      1.0000        [32m0.7596[0m       0.5026      0.5026        1.5648        0.0000  187.1473
     42      0.9583        [32m0.7368[0m       0.5026      0.5026        [94m1.5644[0m     +  0.0000  187.5131
     43      0.9583        0.7987       0.5031      0.5031        [94m1.5634[0m     +  0.0000  187.7225
     44      0.9583        0.7702       0.5035      0.5035        [94m1.5625[0m     +  0.0000  187.5727
     45      1.0000        0.8205       0.5047      0.5047        [94m1.5614[0m     +  0.0000  187.6636
     46      0.9583        [32m0.7261[0m       0.5052      0.5052        [94m1.5607[0m     +  0.0000  187.5810
     47      1.0000        0.7508       0.5054      0.5054        [94m1.5603[0m     +  0.0000  187.7464
     48      1.0000        0.7608       0.5047      0.5047        1.5603        0.0000  171.7881
     49      0.9167        0.7601       0.5045      0.5045        1.5605        0.0000  187.9728
     50      0.9583        0.7376       0.5040      0.5040        1.5604        0.0000  187.3438
     51      0.9583        0.7645       0.5042      0.5042        1.5604        0.0000  187.7366
     52      0.9583        0.8003       0.5038      0.5038        [94m1.5603[0m     +  0.0000  187.7381
     53      1.0000        [32m0.7092[0m       0.5036      0.5036        [94m1.5602[0m     +  0.0000  187.7074
     54      0.9583        0.8022       0.5035      0.5035        1.5602        0.0000  187.5971
     55      0.9583        0.7823       0.5035      0.5035        1.5602        0.0000  187.8623
     56      1.0000        [32m0.7090[0m       0.5035      0.5035        1.5602        0.0000  187.6807
     57      1.0000        [32m0.7078[0m       0.5035      0.5035        [94m1.5601[0m     +  0.0000  187.6235
     58      1.0000        0.7380       0.5031      0.5031        [94m1.5599[0m     +  0.0000  187.7270
     59      1.0000        0.7085       0.5031      0.5031        [94m1.5597[0m     +  0.0000  187.5758
     60      0.9583        0.7335       0.5030      0.5030        [94m1.5596[0m     +  0.0000  187.6909
     61      1.0000        0.7426       0.5030      0.5030        [94m1.5596[0m     +  0.0000  187.6368
     62      1.0000        0.7254       0.5031      0.5031        [94m1.5595[0m     +  0.0000  187.7446
     63      1.0000        [32m0.6697[0m       0.5031      0.5031        [94m1.5595[0m     +  0.0000  173.9997
     64      1.0000        0.7514       0.5033      0.5033        [94m1.5594[0m     +  0.0000  187.7426
     65      1.0000        0.7024       0.5033      0.5033        [94m1.5593[0m     +  0.0000  187.8592
     66      0.9583        0.7432       0.5033      0.5033        [94m1.5593[0m     +  0.0000  187.3644
     67      0.9583        0.7351       0.5033      0.5033        1.5593        0.0000  187.6881
     68      0.9583        0.7172       0.5033      0.5033        1.5594        0.0000  187.6492
     69      1.0000        0.6902       0.5028      0.5028        1.5594        0.0000  187.6592
     70      1.0000        0.6780       0.5030      0.5030        1.5593        0.0000  187.6691
     71      0.9583        0.7339       0.5030      0.5030        [94m1.5593[0m     +  0.0000  187.6284
     72      1.0000        0.7080       0.5030      0.5030        [94m1.5593[0m     +  0.0000  187.2254
     73      0.9583        0.7131       0.5028      0.5028        1.5593        0.0000  187.6595
     74      1.0000        0.6979       0.5028      0.5028        1.5593        0.0000  187.6400
     75      0.9583        0.7232       0.5028      0.5028        1.5593        0.0000  187.6449
     76      0.9583        0.6757       0.5028      0.5028        1.5593        0.0000  187.5045
     77      1.0000        0.7161       0.5028      0.5028        [94m1.5592[0m     +  0.0000  187.6492
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24]
F1 Micro Score after query 1: 0.5543402777777777
F1 Macro Score after query 1: 0.21374246208034706
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_0.pt

Iteration:  2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.5714[0m        [32m1.2908[0m       [35m0.5536[0m      [31m0.5536[0m        [94m1.4571[0m     +  0.0000  189.9227
      2      [36m0.7679[0m        [32m1.1254[0m       [35m0.5929[0m      [31m0.5929[0m        [94m1.3827[0m     +  0.0000  190.2080
      3      [36m0.7857[0m        [32m1.0188[0m       [35m0.5974[0m      [31m0.5974[0m        [94m1.3430[0m     +  0.0000  189.8966
      4      0.7857        [32m0.9051[0m       0.5842      0.5842        [94m1.3295[0m     +  0.0000  190.3657
      5      [36m0.8214[0m        [32m0.8487[0m       0.5929      0.5929        [94m1.3078[0m     +  0.0000  190.2378
      6      [36m0.8571[0m        [32m0.7884[0m       0.5925      0.5925        [94m1.2964[0m     +  0.0000  190.3171
      7      [36m0.8929[0m        [32m0.6678[0m       0.5965      0.5965        [94m1.2772[0m     +  0.0000  190.3237
      8      [36m0.9643[0m        [32m0.6361[0m       0.5917      0.5917        [94m1.2700[0m     +  0.0000  176.2874
      9      0.9464        [32m0.5694[0m       [35m0.5983[0m      [31m0.5983[0m        [94m1.2639[0m     +  0.0000  190.1929
     10      0.9643        [32m0.5037[0m       0.5943      0.5943        [94m1.2614[0m     +  0.0000  190.2619
     11      [36m1.0000[0m        [32m0.4609[0m       0.5946      0.5946        1.2626        0.0000  190.2941
     12      0.9643        0.4676       [35m0.6014[0m      [31m0.6014[0m        [94m1.2425[0m     +  0.0000  190.3652
     13      1.0000        [32m0.4036[0m       0.5995      0.5995        1.2445        0.0000  190.2836
     14      0.9643        0.4063       0.6012      0.6012        1.2461        0.0000  190.2195
     15      1.0000        [32m0.3812[0m       0.6005      0.6005        1.2462        0.0000  190.1721
     16      1.0000        [32m0.3547[0m       [35m0.6017[0m      [31m0.6017[0m        [94m1.2384[0m     +  0.0000  190.3158
     17      1.0000        [32m0.3363[0m       0.6000      0.6000        [94m1.2372[0m     +  0.0000  190.2810
     18      1.0000        0.3507       0.5995      0.5995        1.2388        0.0000  189.8894
     19      1.0000        [32m0.3361[0m       0.5998      0.5998        1.2380        0.0000  190.2512
     20      1.0000        0.3443       0.6012      0.6012        [94m1.2345[0m     +  0.0000  190.2099
     21      1.0000        [32m0.3275[0m       [35m0.6035[0m      [31m0.6035[0m        [94m1.2318[0m     +  0.0000  189.7584
     22      1.0000        [32m0.2899[0m       0.6010      0.6010        [94m1.2311[0m     +  0.0000  187.6554
     23      1.0000        0.3175       0.6016      0.6016        [94m1.2298[0m     +  0.0000  178.9314
     24      1.0000        0.3447       0.6026      0.6026        [94m1.2286[0m     +  0.0000  190.4837
     25      1.0000        0.3131       0.6021      0.6021        1.2293        0.0000  190.3291
     26      1.0000        0.3029       0.6012      0.6012        1.2344        0.0000  190.3674
     27      1.0000        [32m0.2790[0m       0.6016      0.6016        1.2358        0.0000  190.1218
     28      1.0000        [32m0.2790[0m       0.6028      0.6028        1.2322        0.0000  190.1682
     29      1.0000        0.2854       0.6024      0.6024        1.2326        0.0000  190.3287
     30      1.0000        [32m0.2727[0m       0.6030      0.6030        1.2340        0.0000  190.1875
     31      1.0000        0.2929       0.6033      0.6033        1.2330        0.0000  190.2795
     32      1.0000        0.2916       [35m0.6057[0m      [31m0.6057[0m        [94m1.2284[0m     +  0.0000  189.9412
     33      1.0000        0.2928       0.6038      0.6038        1.2302        0.0000  190.1735
     34      1.0000        0.2964       0.6042      0.6042        1.2323        0.0000  189.8297
     35      1.0000        [32m0.2600[0m       0.6028      0.6028        1.2353        0.0000  189.9620
     36      1.0000        0.2739       0.6047      0.6047        1.2316        0.0000  190.2778
     37      1.0000        [32m0.2540[0m       0.6047      0.6047        1.2321        0.0000  181.2398
     38      1.0000        0.2819       0.6031      0.6031        1.2341        0.0000  184.8714
     39      1.0000        0.2604       0.6028      0.6028        1.2349        0.0000  190.3315
     40      1.0000        [32m0.2529[0m       0.6033      0.6033        1.2339        0.0000  190.2526
     41      1.0000        [32m0.2491[0m       0.6038      0.6038        1.2337        0.0000  190.2717
     42      1.0000        0.2641       0.6038      0.6038        1.2334        0.0000  190.3274
     43      1.0000        0.2537       0.6036      0.6036        1.2336        0.0000  190.3824
     44      1.0000        0.2586       0.6035      0.6035        1.2341        0.0000  190.2392
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24, 56]
F1 Micro Score after query 2: 0.6713541666666667
F1 Macro Score after query 2: 0.2541552536954906
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_1.pt

Iteration:  3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.7857[0m        [32m0.7826[0m       [35m0.6509[0m      [31m0.6509[0m        [94m1.1668[0m     +  0.0000  180.3923
      2      [36m0.8125[0m        [32m0.7321[0m       [35m0.6575[0m      [31m0.6575[0m        [94m1.0657[0m     +  0.0000  194.7025
      3      [36m0.9018[0m        [32m0.5743[0m       0.6483      0.6483        1.0938        0.0000  194.5005
      4      [36m0.9286[0m        [32m0.4567[0m       [35m0.6885[0m      [31m0.6885[0m        [94m1.0193[0m     +  0.0000  194.5900
      5      [36m0.9464[0m        [32m0.4219[0m       0.6665      0.6665        1.0457        0.0000  194.5556
      6      [36m0.9643[0m        [32m0.3632[0m       0.6774      0.6774        1.0300        0.0000  194.6141
      7      0.9643        [32m0.3257[0m       0.6832      0.6832        1.0319        0.0000  194.6138
      8      [36m0.9821[0m        [32m0.2923[0m       0.6863      0.6863        1.0259        0.0000  194.7132
      9      0.9821        [32m0.2526[0m       0.6792      0.6792        1.0481        0.0000  194.5081
     10      [36m1.0000[0m        0.2539       0.6828      0.6828        1.0495        0.0000  194.6063
     11      0.9821        [32m0.2368[0m       0.6833      0.6833        1.0585        0.0000  194.1801
     12      0.9911        [32m0.2195[0m       0.6677      0.6677        1.0899        0.0000  194.5806
     13      1.0000        [32m0.1946[0m       0.6828      0.6828        1.0622        0.0000  194.7891
     14      1.0000        [32m0.1867[0m       0.6795      0.6795        1.0705        0.0000  194.7535
     15      1.0000        0.1875       0.6851      0.6851        1.0615        0.0000  180.3727
     16      1.0000        0.1940       0.6823      0.6823        1.0704        0.0000  194.6545
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24, 56, 112]
F1 Micro Score after query 3: 0.7381944444444445
F1 Macro Score after query 3: 0.30067716598588234
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_2.pt

Iteration:  4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.7019[0m        [32m0.9284[0m       [35m0.7014[0m      [31m0.7014[0m        [94m0.9890[0m     +  0.0000  201.8364
      2      [36m0.8317[0m        [32m0.7458[0m       [35m0.7155[0m      [31m0.7155[0m        [94m0.9355[0m     +  0.0000  202.2495
      3      [36m0.8606[0m        [32m0.6176[0m       [35m0.7312[0m      [31m0.7312[0m        [94m0.9008[0m     +  0.0000  201.8400
      4      [36m0.8942[0m        [32m0.5480[0m       [35m0.7352[0m      [31m0.7352[0m        [94m0.8736[0m     +  0.0000  202.2578
      5      [36m0.9135[0m        [32m0.4913[0m       0.7146      0.7146        0.9005        0.0000  202.2125
      6      [36m0.9375[0m        [32m0.4146[0m       0.7123      0.7123        0.9143        0.0000  182.8162
      7      [36m0.9471[0m        [32m0.3923[0m       0.7278      0.7278        0.8740        0.0000  187.8858
      8      0.9375        [32m0.3681[0m       0.7234      0.7234        [94m0.8707[0m     +  0.0000  202.4277
      9      [36m0.9615[0m        [32m0.3324[0m       0.7094      0.7094        0.9118        0.0000  202.3593
     10      [36m0.9712[0m        [32m0.2874[0m       0.6559      0.6559        0.9916        0.0000  202.3949
     11      [36m0.9904[0m        [32m0.2685[0m       0.6745      0.6745        0.9630        0.0000  202.4767
     12      0.9808        0.2703       0.7092      0.7092        0.8876        0.0000  202.5503
     13      [36m1.0000[0m        [32m0.2269[0m       0.7286      0.7286        0.8981        0.0000  202.5481
     14      0.9952        0.2324       0.7238      0.7238        0.9238        0.0000  202.6893
     15      0.9952        [32m0.1894[0m       0.7257      0.7257        0.9134        0.0000  202.3476
     16      1.0000        [32m0.1820[0m       0.7271      0.7271        0.9051        0.0000  202.4664
     17      1.0000        [32m0.1773[0m       0.7306      0.7306        0.8993        0.0000  202.3515
     18      1.0000        [32m0.1674[0m       0.7292      0.7292        0.9052        0.0000  201.8525
     19      1.0000        [32m0.1631[0m       0.7299      0.7299        0.9036        0.0000  202.4352
     20      1.0000        [32m0.1601[0m       0.7314      0.7314        0.9047        0.0000  202.3173
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24, 56, 112, 208]
F1 Micro Score after query 4: 0.7795138888888887
F1 Macro Score after query 4: 0.36943410610864813
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_3.pt

Iteration:  5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.7630[0m        [32m0.7416[0m       [35m0.7431[0m      [31m0.7431[0m        [94m0.8154[0m     +  0.0000  215.5028
      2      [36m0.8672[0m        [32m0.5510[0m       [35m0.7625[0m      [31m0.7625[0m        [94m0.7459[0m     +  0.0000  215.9971
      3      [36m0.9271[0m        [32m0.4299[0m       [35m0.7861[0m      [31m0.7861[0m        [94m0.6957[0m     +  0.0000  216.0429
      4      [36m0.9427[0m        [32m0.3619[0m       [35m0.7964[0m      [31m0.7964[0m        [94m0.6630[0m     +  0.0000  216.0488
      5      [36m0.9661[0m        [32m0.3110[0m       0.7882      0.7882        0.6797        0.0000  215.3840
      6      0.9557        0.3131       0.7755      0.7755        0.7040        0.0000  216.0054
      7      0.9583        [32m0.2714[0m       0.7677      0.7677        0.7411        0.0000  216.1429
      8      [36m0.9844[0m        [32m0.2101[0m       0.7776      0.7776        0.7169        0.0000  215.9758
      9      0.9844        [32m0.1876[0m       0.7847      0.7847        0.7181        0.0000  216.0084
     10      [36m0.9922[0m        [32m0.1562[0m       0.7776      0.7776        0.7208        0.0000  216.0399
     11      [36m0.9948[0m        [32m0.1391[0m       0.7823      0.7823        0.7356        0.0000  215.3677
     12      0.9948        [32m0.1361[0m       0.7769      0.7769        0.7710        0.0000  198.3890
     13      [36m1.0000[0m        [32m0.1277[0m       0.7854      0.7854        0.7137        0.0000  216.1888
     14      0.9974        0.1282       0.7851      0.7851        0.7212        0.0000  216.1854
     15      0.9870        0.1299       0.7877      0.7877        0.7415        0.0000  216.0748
     16      0.9974        0.1280       0.7898      0.7898        0.7075        0.0000  216.0965
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.8388888888888889
F1 Macro Score after query 5: 0.5050963254297611
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_4.pt

Iteration:  6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.8366[0m        [32m0.6046[0m       [35m0.7488[0m      [31m0.7488[0m        [94m0.6945[0m     +  0.0000  241.2851
      2      [36m0.9020[0m        [32m0.4272[0m       [35m0.7936[0m      [31m0.7936[0m        [94m0.6163[0m     +  0.0000  241.3859
      3      [36m0.9446[0m        [32m0.3151[0m       [35m0.7984[0m      [31m0.7984[0m        [94m0.6086[0m     +  0.0000  240.8064
      4      [36m0.9588[0m        [32m0.2481[0m       [35m0.8082[0m      [31m0.8082[0m        [94m0.5959[0m     +  0.0000  225.2122
      5      [36m0.9631[0m        [32m0.2302[0m       0.7967      0.7967        0.6555        0.0000  241.0527
      6      [36m0.9744[0m        [32m0.1938[0m       0.7807      0.7807        0.7113        0.0000  240.8539
      7      [36m0.9759[0m        [32m0.1699[0m       0.7707      0.7707        0.7687        0.0000  241.5156
      8      [36m0.9929[0m        [32m0.1381[0m       0.7837      0.7837        0.7236        0.0000  241.3612
      9      0.9929        [32m0.1253[0m       0.7927      0.7927        0.7013        0.0000  242.1296
     10      0.9872        [32m0.1241[0m       0.7997      0.7997        0.6718        0.0000  242.2555
     11      0.9858        0.1284       0.8042      0.8042        0.6793        0.0000  241.3451
     12      0.9858        0.1308       0.7786      0.7786        0.7706        0.0000  241.4349
     13      0.9915        [32m0.1052[0m       0.7889      0.7889        0.7635        0.0000  241.5610
     14      [36m0.9972[0m        [32m0.0881[0m       0.7943      0.7943        0.7492        0.0000  241.5239
     15      0.9929        0.0889       0.7915      0.7915        0.7545        0.0000  241.4563
     16      [36m1.0000[0m        [32m0.0736[0m       0.7937      0.7937        0.7447        0.0000  241.3638
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.8598958333333333
F1 Macro Score after query 6: 0.5371856405730653
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_5.pt

Iteration:  7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.8434[0m        [32m0.5368[0m       [35m0.7764[0m      [31m0.7764[0m        [94m0.6875[0m     +  0.0000  285.6936
      2      [36m0.8948[0m        [32m0.4196[0m       0.6686      0.6686        0.9328        0.0000  285.1386
      3      [36m0.9233[0m        [32m0.3446[0m       0.7720      0.7720        0.7478        0.0000  285.7158
      4      [36m0.9422[0m        [32m0.2857[0m       0.7660      0.7660        0.8066        0.0000  285.7588
      5      [36m0.9533[0m        [32m0.2507[0m       [35m0.7911[0m      [31m0.7911[0m        0.6986        0.0000  285.6325
      6      0.9478        [32m0.2366[0m       0.7861      0.7861        0.7462        0.0000  285.2002
      7      [36m0.9541[0m        [32m0.2248[0m       0.7835      0.7835        0.7628        0.0000  285.4873
      8      [36m0.9597[0m        [32m0.1981[0m       [35m0.7998[0m      [31m0.7998[0m        0.7097        0.0000  269.8173
      9      [36m0.9628[0m        [32m0.1813[0m       [35m0.8106[0m      [31m0.8106[0m        [94m0.6290[0m     +  0.0000  285.0508
     10      [36m0.9668[0m        [32m0.1598[0m       0.7972      0.7972        0.6981        0.0000  285.6004
     11      [36m0.9794[0m        [32m0.1350[0m       0.7906      0.7906        0.7533        0.0000  285.5763
     12      [36m0.9866[0m        [32m0.1121[0m       0.7743      0.7743        0.8552        0.0000  285.6462
     13      [36m0.9881[0m        [32m0.1028[0m       [35m0.8217[0m      [31m0.8217[0m        0.6450        0.0000  285.7344
     14      0.9881        [32m0.0941[0m       [35m0.8330[0m      [31m0.8330[0m        [94m0.6245[0m     +  0.0000  285.7027
     15      [36m0.9929[0m        [32m0.0761[0m       [35m0.8332[0m      [31m0.8332[0m        [94m0.6118[0m     +  0.0000  286.3298
     16      [36m0.9968[0m        [32m0.0677[0m       0.8316      0.8316        0.6198        0.0000  287.5272
     17      0.9960        [32m0.0639[0m       0.8325      0.8325        0.6151        0.0000  285.3767
     18      0.9960        [32m0.0608[0m       [35m0.8337[0m      [31m0.8337[0m        0.6247        0.0000  285.5466
     19      0.9968        [32m0.0590[0m       0.8330      0.8330        0.6256        0.0000  285.4074
     20      0.9968        [32m0.0547[0m       0.8319      0.8319        0.6312        0.0000  266.9706
     21      [36m0.9976[0m        [32m0.0514[0m       0.8321      0.8321        0.6386        0.0000  286.8741
     22      [36m0.9984[0m        [32m0.0498[0m       0.8307      0.8307        0.6323        0.0000  286.1129
     23      0.9984        [32m0.0467[0m       0.8295      0.8295        0.6459        0.0000  285.8572
     24      0.9984        [32m0.0461[0m       0.8304      0.8304        0.6461        0.0000  285.8157
     25      0.9976        [32m0.0417[0m       0.8311      0.8311        0.6654        0.0000  285.9011
     26      [36m1.0000[0m        [32m0.0399[0m       0.8314      0.8314        0.6617        0.0000  285.7598
     27      1.0000        [32m0.0386[0m       0.8306      0.8306        0.6673        0.0000  286.3403
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.8630208333333335
F1 Macro Score after query 7: 0.5579788319959323
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_6.pt

Iteration:  8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.8551[0m        [32m0.4507[0m       [35m0.7986[0m      [31m0.7986[0m        [94m0.5889[0m     +  0.0000  364.6590
      2      [36m0.9077[0m        [32m0.3171[0m       [35m0.8130[0m      [31m0.8130[0m        [94m0.5779[0m     +  0.0000  365.0102
      3      [36m0.9368[0m        [32m0.2284[0m       [35m0.8243[0m      [31m0.8243[0m        0.6119        0.0000  364.7647
      4      [36m0.9536[0m        [32m0.1900[0m       0.8132      0.8132        0.6790        0.0000  364.6505
      5      [36m0.9541[0m        [32m0.1891[0m       0.8191      0.8191        0.6097        0.0000  364.7194
      6      [36m0.9642[0m        [32m0.1495[0m       0.8182      0.8182        0.6579        0.0000  364.6319
      7      [36m0.9660[0m        [32m0.1381[0m       0.8200      0.8200        0.6744        0.0000  364.5691
      8      [36m0.9739[0m        [32m0.1247[0m       0.8149      0.8149        0.6934        0.0000  364.6759
      9      0.9695        0.1389       [35m0.8267[0m      [31m0.8267[0m        0.6626        0.0000  345.7201
     10      [36m0.9766[0m        [32m0.1142[0m       [35m0.8349[0m      [31m0.8349[0m        0.6210        0.0000  364.7870
     11      0.9766        [32m0.1010[0m       0.8290      0.8290        0.6703        0.0000  364.8223
     12      [36m0.9845[0m        [32m0.0801[0m       0.8326      0.8326        0.6456        0.0000  364.7947
     13      [36m0.9938[0m        [32m0.0528[0m       [35m0.8425[0m      [31m0.8425[0m        0.6381        0.0000  365.0716
     14      [36m0.9982[0m        [32m0.0383[0m       0.8394      0.8394        0.6622        0.0000  364.7155
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.8715277777777778
F1 Macro Score after query 8: 0.5677717152565551
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_7.pt

Iteration:  9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.8812[0m        [32m0.3804[0m       [35m0.8005[0m      [31m0.8005[0m        [94m0.5828[0m     +  0.0000  504.8659
      2      [36m0.9146[0m        [32m0.2812[0m       [35m0.8319[0m      [31m0.8319[0m        [94m0.5065[0m     +  0.0000  505.2427
      3      [36m0.9297[0m        [32m0.2339[0m       0.8280      0.8280        0.5772        0.0000  504.8285
      4      [36m0.9446[0m        [32m0.1970[0m       0.8293      0.8293        0.5779        0.0000  504.5066
      5      [36m0.9510[0m        [32m0.1792[0m       [35m0.8457[0m      [31m0.8457[0m        [94m0.4905[0m     +  0.0000  504.5490
      6      [36m0.9636[0m        [32m0.1410[0m       0.8399      0.8399        0.5376        0.0000  504.2065
      7      [36m0.9683[0m        [32m0.1239[0m       0.8318      0.8318        0.5825        0.0000  486.7967
      8      [36m0.9715[0m        [32m0.1178[0m       [35m0.8608[0m      [31m0.8608[0m        0.5019        0.0000  504.5456
      9      [36m0.9760[0m        [32m0.1067[0m       0.8599      0.8599        0.5212        0.0000  504.4840
     10      [36m0.9770[0m        [32m0.0914[0m       0.8497      0.8497        0.5660        0.0000  504.4376
     11      [36m0.9814[0m        [32m0.0789[0m       0.8493      0.8493        0.6061        0.0000  504.5676
     12      [36m0.9824[0m        [32m0.0722[0m       0.8543      0.8543        0.5885        0.0000  506.0641
     13      [36m0.9894[0m        [32m0.0529[0m       0.8450      0.8450        0.6417        0.0000  485.7925
     14      [36m0.9946[0m        [32m0.0356[0m       0.8307      0.8307        0.7242        0.0000  505.2725
     15      [36m0.9983[0m        [32m0.0253[0m       0.8340      0.8340        0.7215        0.0000  504.9758
     16      [36m0.9985[0m        [32m0.0225[0m       0.8368      0.8368        0.7219        0.0000  504.9258
     17      [36m0.9993[0m        [32m0.0186[0m       0.8332      0.8332        0.7441        0.0000  504.7518
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.8699652777777778
F1 Macro Score after query 9: 0.5725904543741177
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_8.pt

Iteration:  10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9175[0m        [32m0.2656[0m       [35m0.8378[0m      [31m0.8378[0m        [94m0.5205[0m     +  0.0000  301.4871
      2      [36m0.9344[0m        [32m0.2149[0m       [35m0.8458[0m      [31m0.8458[0m        [94m0.4852[0m     +  0.0000  301.9077
      3      [36m0.9465[0m        [32m0.1722[0m       0.8177      0.8177        0.6574        0.0000  302.1138
      4      [36m0.9543[0m        [32m0.1465[0m       0.8347      0.8347        0.5628        0.0000  302.0133
      5      [36m0.9622[0m        [32m0.1265[0m       0.8370      0.8370        0.6462        0.0000  302.0878
      6      [36m0.9678[0m        [32m0.1171[0m       0.8304      0.8304        0.6543        0.0000  301.8647
      7      [36m0.9731[0m        [32m0.0943[0m       0.8187      0.8187        0.7423        0.0000  301.7838
      8      [36m0.9750[0m        [32m0.0908[0m       0.8236      0.8236        0.6794        0.0000  301.8895
      9      [36m0.9774[0m        [32m0.0812[0m       0.8234      0.8234        0.7353        0.0000  301.9735
     10      0.9774        [32m0.0750[0m       0.8184      0.8184        0.7774        0.0000  301.9863
     11      [36m0.9851[0m        [32m0.0600[0m       0.8009      0.8009        0.9479        0.0000  301.9860
     12      0.9819        0.0647       0.8120      0.8120        0.8364        0.0000  302.0067
     13      [36m0.9936[0m        [32m0.0313[0m       0.8158      0.8158        0.8543        0.0000  301.8634
     14      [36m0.9967[0m        [32m0.0220[0m       0.8229      0.8229        0.8096        0.0000  301.8382
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.8713541666666665
F1 Macro Score after query 10: 0.6332180879145662
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_9.pt

Iteration:  11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9551[0m        [32m0.1575[0m       [35m0.8069[0m      [31m0.8069[0m        [94m0.6957[0m     +  0.0000  477.4909
      2      [36m0.9610[0m        [32m0.1362[0m       [35m0.8509[0m      [31m0.8509[0m        [94m0.5023[0m     +  0.0000  478.2701
      3      [36m0.9633[0m        [32m0.1217[0m       0.8108      0.8108        0.6738        0.0000  478.1917
      4      [36m0.9701[0m        [32m0.0992[0m       0.8318      0.8318        0.6347        0.0000  478.4124
      5      [36m0.9708[0m        0.0992       0.8137      0.8137        0.7811        0.0000  478.2836
      6      [36m0.9768[0m        [32m0.0805[0m       0.8229      0.8229        0.6601        0.0000  478.1764
      7      [36m0.9832[0m        [32m0.0631[0m       0.8358      0.8358        0.6540        0.0000  478.3435
      8      0.9832        [32m0.0595[0m       0.8391      0.8391        0.6116        0.0000  478.3891
      9      [36m0.9870[0m        [32m0.0495[0m       0.8429      0.8429        0.5828        0.0000  478.3608
     10      0.9853        0.0504       0.8446      0.8446        0.6696        0.0000  478.4674
     11      [36m0.9882[0m        [32m0.0412[0m       0.8304      0.8304        0.7547        0.0000  478.6759
     12      [36m0.9884[0m        [32m0.0395[0m       0.8066      0.8066        0.8603        0.0000  478.5971
     13      [36m0.9936[0m        [32m0.0243[0m       0.8332      0.8332        0.7631        0.0000  478.4753
     14      [36m0.9973[0m        [32m0.0135[0m       0.8146      0.8146        0.8480        0.0000  478.5440
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.8602430555555556
F1 Macro Score after query 11: 0.6822506874342258
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_10.pt

Iteration:  12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9784[0m        [32m0.0811[0m       [35m0.8313[0m      [31m0.8313[0m        [94m0.5393[0m     +  0.0000  790.8054
      2      0.9772        [32m0.0806[0m       0.8174      0.8174        0.5981        0.0000  792.0732
      3      0.9783        [32m0.0747[0m       0.8280      0.8280        0.6271        0.0000  792.8820
      4      [36m0.9819[0m        [32m0.0626[0m       [35m0.8394[0m      [31m0.8394[0m        0.5760        0.0000  799.9641
      5      [36m0.9845[0m        [32m0.0519[0m       [35m0.8396[0m      [31m0.8396[0m        0.6206        0.0000  811.7850
      6      [36m0.9869[0m        [32m0.0462[0m       [35m0.8431[0m      [31m0.8431[0m        0.6283        0.0000  812.0776
      7      [36m0.9884[0m        [32m0.0405[0m       0.8021      0.8021        0.8271        0.0000  812.0975
      8      0.9884        [32m0.0384[0m       0.8410      0.8410        0.6791        0.0000  794.3903
      9      [36m0.9899[0m        [32m0.0350[0m       0.8370      0.8370        0.6949        0.0000  794.3285
     10      [36m0.9913[0m        [32m0.0303[0m       0.8418      0.8418        0.7453        0.0000  793.7814
     11      [36m0.9915[0m        0.0306       0.8354      0.8354        0.7571        0.0000  795.7986
     12      [36m0.9926[0m        [32m0.0239[0m       0.8323      0.8323        0.7536        0.0000  797.9785
     13      [36m0.9967[0m        [32m0.0123[0m       [35m0.8434[0m      [31m0.8434[0m        0.8034        0.0000  797.6058
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.8765625
F1 Macro Score after query 12: 0.5905623353572823
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_11.pt

Iteration:  13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9840[0m        [32m0.0560[0m       [35m0.8247[0m      [31m0.8247[0m        [94m0.6184[0m     +  0.0000  917.2625
      2      0.9815        0.0640       0.8193      0.8193        0.6502        0.0000  920.2680
      3      0.9831        0.0562       0.7566      0.7566        1.0753        0.0000  920.8151
      4      [36m0.9856[0m        [32m0.0499[0m       [35m0.8441[0m      [31m0.8441[0m        [94m0.6072[0m     +  0.0000  920.5637
      5      [36m0.9865[0m        [32m0.0450[0m       0.8382      0.8382        0.6377        0.0000  920.5008
      6      [36m0.9894[0m        [32m0.0404[0m       0.8408      0.8408        0.6206        0.0000  920.7360
      7      0.9893        [32m0.0361[0m       0.8309      0.8309        0.7169        0.0000  920.3643
      8      [36m0.9898[0m        [32m0.0345[0m       0.8403      0.8403        0.6999        0.0000  920.1647
      9      [36m0.9923[0m        [32m0.0278[0m       0.8365      0.8365        0.7516        0.0000  920.6484
     10      [36m0.9924[0m        [32m0.0278[0m       0.8358      0.8358        0.7283        0.0000  920.4093
     11      [36m0.9930[0m        [32m0.0235[0m       0.8361      0.8361        0.7674        0.0000  920.6702
     12      [36m0.9946[0m        [32m0.0193[0m       0.8073      0.8073        0.9662        0.0000  920.5321
     13      [36m0.9974[0m        [32m0.0100[0m       0.8260      0.8260        0.8484        0.0000  920.5424
     14      [36m0.9985[0m        [32m0.0065[0m       0.8333      0.8333        0.8683        0.0000  920.3892
     15      [36m0.9992[0m        [32m0.0046[0m       0.8441      0.8441        0.8196        0.0000  920.4234
     16      0.9989        0.0046       0.8401      0.8401        0.8533        0.0000  920.3293
Stopping since valid_loss has not improved in the last 13 epochs.
[8, 24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.8713541666666665
F1 Macro Score after query 13: 0.6311034762625782
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\model_checkpoint_iteration_12.pt
Pickle file saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multiclass/DinoL/margin_sampling_seed42_low_lr\AL_margin_sampling_results_for_multiclass_classification_s42.pickle
