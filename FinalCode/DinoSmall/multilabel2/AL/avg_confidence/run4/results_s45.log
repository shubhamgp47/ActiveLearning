Adjusted shapes after loading:
X_initial_np: (8, 224, 224, 3), y_initial_np: (8, 3)
X_pool_np: (26872, 224, 224, 3), y_pool_np: (26872, 3)
X_test_np: (5760, 224, 224, 3), y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3), y_val_np: (5760, 3)
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.5152[0m        [32m0.7299[0m       [35m0.0061[0m      [31m0.6298[0m        [94m0.7097[0m     +  0.0000  11.9660
      2      0.4924        [32m0.6640[0m       [35m0.0769[0m      0.5615        [94m0.6983[0m     +  0.0000  10.1412
      3      [36m0.5556[0m        [32m0.6529[0m       [35m0.2003[0m      0.2952        [94m0.6960[0m     +  0.0000  10.3291
      4      [36m0.7146[0m        [32m0.6185[0m       [35m0.2849[0m      0.3663        [94m0.6905[0m     +  0.0000  10.5303
      5      [36m0.7685[0m        [32m0.6058[0m       [35m0.2988[0m      0.3867        [94m0.6894[0m     +  0.0000  10.4804
      6      [36m0.8690[0m        [32m0.5623[0m       [35m0.3146[0m      0.3911        [94m0.6892[0m     +  0.0000  10.6500
      7      0.7024        0.5662       0.3095      0.3967        0.6905        0.0000  10.3691
      8      [36m0.9153[0m        0.5654       [35m0.3160[0m      0.3987        [94m0.6871[0m     +  0.0000  10.6002
      9      0.8519        [32m0.5385[0m       [35m0.3201[0m      0.4022        0.6875        0.0000  10.6733
     10      0.7500        0.5809       0.2981      0.3964        0.6919        0.0000  10.4123
     11      0.8056        [32m0.5338[0m       0.2977      0.3974        0.6923        0.0000  10.5594
     12      0.9153        0.5491       0.3028      0.3984        0.6925        0.0000  10.5499
     13      0.8889        [32m0.5282[0m       0.3064      0.3979        0.6915        0.0000  10.5610
     14      0.8333        0.5388       0.3099      0.3976        0.6910        0.0000  10.7319
     15      0.8413        0.5354       0.3071      0.3975        0.6921        0.0000  10.6190
     16      [36m0.9630[0m        [32m0.5129[0m       0.3071      0.3974        0.6922        0.0000  10.6228
     17      0.8571        0.5212       0.3083      0.3974        0.6920        0.0000  10.7855
     18      0.9524        0.5348       0.3036      0.3958        0.6930        0.0000  10.7786
Stopping since valid_loss has not improved in the last 11 epochs.
Pre F1 micro score = 0.4454
Pre F1 macro score = 0.4359
Pre Accuracy = 0.3345

Iteration: 1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.6675[0m        [32m0.6084[0m       [35m0.4028[0m      [31m0.1638[0m        [94m0.6278[0m     +  0.0000  10.7651
      2      0.2000        0.6094       [35m0.4148[0m      0.1468        [94m0.6216[0m     +  0.0000  10.7007
      3      0.3667        [32m0.5672[0m       [35m0.4290[0m      [31m0.1671[0m        [94m0.6129[0m     +  0.0000  10.4729
      4      0.4030        [32m0.5537[0m       0.4203      [31m0.2040[0m        [94m0.6088[0m     +  0.0000  10.8566
      5      0.6148        [32m0.4869[0m       0.4215      [31m0.2141[0m        [94m0.6009[0m     +  0.0000  10.7289
      6      0.5000        0.4994       0.4220      [31m0.2387[0m        0.6045        0.0000  10.7324
      7      0.6077        0.4890       0.4238      [31m0.2522[0m        0.6030        0.0000  10.8089
      8      0.5556        [32m0.4722[0m       0.4153      0.2511        0.6052        0.0000  10.8498
      9      [36m0.7037[0m        [32m0.4584[0m       0.4241      [31m0.2603[0m        [94m0.5981[0m     +  0.0000  10.7365
     10      0.6696        [32m0.4572[0m       0.4227      [31m0.2613[0m        0.5997        0.0000  10.6269
     11      [36m0.8000[0m        [32m0.4462[0m       0.4257      [31m0.2719[0m        [94m0.5952[0m     +  0.0000  10.5584
     12      0.6823        0.4676       0.4236      [31m0.2746[0m        [94m0.5949[0m     +  0.0000  10.8482
     13      0.6481        0.4492       0.4229      0.2639        0.5986        0.0000  10.8916
     14      0.7444        0.4485       0.4238      0.2702        0.5964        0.0000  10.5146
     15      [36m0.8480[0m        [32m0.4287[0m       0.4222      0.2703        0.5983        0.0000  10.8151
     16      0.6866        0.4455       0.4248      [31m0.2747[0m        0.5965        0.0000  10.8308
     17      0.7823        0.4417       0.4224      [31m0.2769[0m        [94m0.5947[0m     +  0.0000  10.7171
     18      [36m0.8524[0m        [32m0.4198[0m       0.4212      [31m0.2791[0m        [94m0.5946[0m     +  0.0000  10.7335
     19      0.7845        0.4231       0.4208      [31m0.2794[0m        [94m0.5940[0m     +  0.0000  10.7508
     20      0.6481        0.4447       0.4208      [31m0.2874[0m        [94m0.5925[0m     +  0.0000  10.6992
     21      0.7696        0.4396       0.4210      0.2873        [94m0.5924[0m     +  0.0000  10.7348
     22      0.8038        0.4380       0.4207      0.2863        [94m0.5922[0m     +  0.0000  10.8758
     23      0.7251        0.4468       0.4210      0.2850        0.5925        0.0000  10.6097
     24      0.8231        [32m0.4167[0m       0.4210      0.2848        0.5928        0.0000  10.6777
     25      0.7077        0.4300       0.4212      0.2851        0.5928        0.0000  10.9375
     26      0.8048        0.4374       0.4210      0.2847        0.5928        0.0000  10.7948
     27      0.7696        0.4382       0.4215      0.2845        0.5929        0.0000  10.9263
     28      0.7251        0.4479       0.4212      0.2847        0.5929        0.0000  10.7352
     29      0.8401        0.4177       0.4212      0.2847        0.5930        0.0000  10.9194
     30      [36m0.8694[0m        0.4309       0.4210      0.2852        0.5928        0.0000  10.7198
     31      0.7481        0.4258       0.4210      0.2857        0.5928        0.0000  10.6442
     32      0.8116        0.4271       0.4205      0.2857        0.5928        0.0000  10.8066
Stopping since valid_loss has not improved in the last 11 epochs.
[24]
F1 Micro Score after query 1: 0.36438234641827455
F1 Macro Score after query 1: 0.2623144450508808
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_0.pt

Iteration: 2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.5496[0m        [32m0.5264[0m       [35m0.3887[0m      [31m0.4769[0m        [94m0.5906[0m     +  0.0000  11.0067
      2      [36m0.7379[0m        [32m0.4446[0m       [35m0.4696[0m      [31m0.6428[0m        [94m0.5269[0m     +  0.0000  11.0468
      3      [36m0.7711[0m        [32m0.4109[0m       [35m0.4878[0m      0.6111        [94m0.5153[0m     +  0.0000  10.8578
      4      [36m0.7910[0m        [32m0.3993[0m       [35m0.5653[0m      [31m0.7259[0m        [94m0.4824[0m     +  0.0000  11.1249
      5      [36m0.7947[0m        [32m0.3768[0m       0.5580      0.6000        [94m0.4815[0m     +  0.0000  10.8221
      6      [36m0.8587[0m        [32m0.3442[0m       0.5630      0.6221        [94m0.4750[0m     +  0.0000  10.8939
      7      [36m0.8647[0m        [32m0.3407[0m       [35m0.5703[0m      0.6359        [94m0.4681[0m     +  0.0000  10.6721
      8      [36m0.8822[0m        0.3541       [35m0.5780[0m      0.6571        [94m0.4634[0m     +  0.0000  10.6904
      9      [36m0.8842[0m        [32m0.3275[0m       [35m0.5807[0m      0.6626        [94m0.4600[0m     +  0.0000  10.8099
     10      0.8647        0.3401       [35m0.5861[0m      0.6853        [94m0.4554[0m     +  0.0000  10.7035
     11      0.8822        0.3338       0.5823      0.6733        0.4566        0.0000  10.8251
     12      0.8822        0.3423       0.5845      0.6768        [94m0.4551[0m     +  0.0000  11.1387
     13      0.8695        0.3352       [35m0.5872[0m      0.6764        [94m0.4540[0m     +  0.0000  10.8439
     14      0.8687        0.3304       [35m0.5917[0m      0.6947        [94m0.4500[0m     +  0.0000  10.8628
     15      [36m0.9244[0m        0.3308       0.5892      0.6858        0.4511        0.0000  10.7033
     16      0.8862        [32m0.3226[0m       0.5901      0.6856        0.4507        0.0000  10.7185
     17      0.9049        [32m0.3188[0m       0.5889      0.6796        0.4516        0.0000  11.0154
     18      0.8950        0.3197       0.5901      0.6826        0.4508        0.0000  10.9983
     19      0.8950        0.3247       0.5899      0.6843        0.4502        0.0000  10.9431
     20      0.8792        0.3233       0.5908      0.6852        [94m0.4495[0m     +  0.0000  10.9066
     21      [36m0.9264[0m        [32m0.3146[0m       0.5908      0.6857        [94m0.4495[0m     +  0.0000  10.9192
     22      0.8647        0.3275       0.5913      0.6868        [94m0.4491[0m     +  0.0000  10.9495
     23      0.9118        0.3303       0.5913      0.6873        [94m0.4490[0m     +  0.0000  10.9001
     24      0.9208        0.3203       0.5906      0.6864        0.4491        0.0000  10.8680
     25      0.8949        [32m0.3049[0m       0.5906      0.6854        0.4493        0.0000  10.8958
     26      0.8375        0.3280       0.5910      0.6852        0.4494        0.0000  11.1106
     27      0.8693        0.3268       0.5906      0.6860        0.4491        0.0000  10.8589
     28      0.8822        0.3110       0.5910      0.6865        0.4490        0.0000  11.2302
     29      0.9149        0.3155       0.5906      0.6847        0.4493        0.0000  10.9983
     30      0.9149        0.3184       0.5906      0.6848        0.4491        0.0000  11.1535
     31      0.8851        0.3306       0.5908      0.6851        0.4491        0.0000  10.8537
     32      0.8822        0.3261       0.5910      0.6860        0.4490        0.0000  10.8864
     33      0.8667        0.3275       0.5906      0.6858        0.4490        0.0000  11.0657
     34      0.9044        0.3328       0.5910      0.6863        [94m0.4489[0m     +  0.0000  10.8853
     35      0.9027        0.3222       0.5913      0.6866        [94m0.4489[0m     +  0.0000  10.8413
     36      0.9073        0.3280       0.5910      0.6863        0.4489        0.0000  10.8401
     37      0.9121        0.3118       0.5906      0.6858        0.4489        0.0000  10.9039
     38      0.9073        0.3057       0.5908      0.6861        0.4489        0.0000  10.8554
     39      0.8949        0.3166       0.5911      0.6865        0.4489        0.0000  10.8491
     40      0.8944        0.3169       0.5915      0.6868        [94m0.4488[0m     +  0.0000  11.2385
     41      0.8869        0.3109       0.5915      0.6868        [94m0.4488[0m     +  0.0000  10.7713
     42      0.8844        0.3243       0.5915      0.6868        [94m0.4488[0m     +  0.0000  10.7047
     43      0.8869        0.3202       0.5915      0.6868        0.4488        0.0000  11.0303
     44      0.8950        0.3106       0.5915      0.6868        0.4488        0.0000  10.9797
     45      0.8773        0.3215       0.5915      0.6868        0.4488        0.0000  10.9246
     46      0.9088        0.3133       0.5915      0.6868        0.4488        0.0000  10.8696
     47      0.8988        0.3243       0.5915      0.6868        0.4488        0.0000  10.9072
     48      0.8805        0.3241       0.5915      0.6868        0.4488        0.0000  10.9077
     49      0.9149        0.3152       0.5915      0.6868        0.4488        0.0000  10.8561
     50      0.9050        0.3219       0.5915      0.6868        0.4488        0.0000  10.8143
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56]
F1 Micro Score after query 2: 0.7141073657927591
F1 Macro Score after query 2: 0.703185237680557
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_1.pt

Iteration: 3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.7254[0m        [32m0.4253[0m       [35m0.6161[0m      [31m0.8049[0m        [94m0.4336[0m     +  0.0000  11.1250
      2      [36m0.8191[0m        [32m0.3629[0m       [35m0.6852[0m      [31m0.8296[0m        [94m0.3934[0m     +  0.0000  11.2707
      3      [36m0.8469[0m        [32m0.3316[0m       [35m0.7193[0m      [31m0.8459[0m        [94m0.3689[0m     +  0.0000  11.0615
      4      0.8259        [32m0.3278[0m       [35m0.7234[0m      [31m0.8482[0m        [94m0.3632[0m     +  0.0000  11.1406
      5      [36m0.8623[0m        [32m0.3209[0m       0.7200      [31m0.8511[0m        [94m0.3474[0m     +  0.0000  11.3242
      6      [36m0.8802[0m        [32m0.3045[0m       0.6762      0.7975        [94m0.3458[0m     +  0.0000  11.1250
      7      0.8521        [32m0.3000[0m       0.6719      0.7991        [94m0.3436[0m     +  0.0000  11.1619
      8      0.8776        [32m0.2951[0m       0.6738      0.8023        [94m0.3415[0m     +  0.0000  11.0906
      9      [36m0.8963[0m        [32m0.2814[0m       0.6745      0.7916        0.3425        0.0000  11.2513
     10      0.8926        0.2836       0.6747      0.7979        [94m0.3392[0m     +  0.0000  11.1352
     11      [36m0.9086[0m        [32m0.2807[0m       0.6792      0.8080        [94m0.3364[0m     +  0.0000  11.3689
     12      [36m0.9287[0m        0.2852       0.6790      0.8078        [94m0.3357[0m     +  0.0000  11.1287
     13      [36m0.9307[0m        [32m0.2752[0m       0.6760      0.8023        0.3361        0.0000  11.3673
     14      0.9028        0.2763       0.6814      0.8106        [94m0.3340[0m     +  0.0000  11.3282
     15      0.9110        [32m0.2730[0m       0.6799      0.8075        [94m0.3339[0m     +  0.0000  11.0606
     16      0.9096        0.2753       0.6835      0.8120        [94m0.3330[0m     +  0.0000  11.1887
     17      0.9171        [32m0.2728[0m       0.6839      0.8127        [94m0.3327[0m     +  0.0000  11.5001
     18      0.9050        0.2831       0.6842      0.8142        [94m0.3324[0m     +  0.0000  11.0626
     19      0.9232        0.2735       0.6828      0.8118        0.3324        0.0000  11.0946
     20      0.9240        [32m0.2656[0m       0.6845      0.8135        [94m0.3320[0m     +  0.0000  11.0743
     21      0.9170        0.2700       0.6844      0.8134        [94m0.3319[0m     +  0.0000  11.1726
     22      0.9228        0.2722       0.6854      0.8146        [94m0.3317[0m     +  0.0000  11.1405
     23      [36m0.9376[0m        [32m0.2617[0m       0.6865      0.8158        [94m0.3315[0m     +  0.0000  11.0527
     24      0.9179        0.2773       0.6865      0.8160        [94m0.3313[0m     +  0.0000  11.0721
     25      0.8900        0.2866       0.6880      0.8182        [94m0.3311[0m     +  0.0000  11.2560
     26      0.8919        0.2745       0.6870      0.8174        0.3311        0.0000  11.2622
     27      0.9301        0.2676       0.6880      0.8182        [94m0.3310[0m     +  0.0000  11.2616
     28      0.9283        0.2719       0.6880      0.8182        [94m0.3309[0m     +  0.0000  11.1684
     29      0.9171        0.2772       0.6878      0.8181        [94m0.3309[0m     +  0.0000  10.9791
     30      0.9266        0.2730       0.6877      0.8179        [94m0.3309[0m     +  0.0000  11.2519
     31      0.9084        0.2718       0.6880      0.8181        [94m0.3309[0m     +  0.0000  11.2040
     32      0.9118        0.2747       0.6878      0.8181        [94m0.3309[0m     +  0.0000  11.0635
     33      0.9028        0.2754       0.6878      0.8181        [94m0.3308[0m     +  0.0000  11.2366
     34      0.9034        0.2705       0.6882      0.8184        [94m0.3308[0m     +  0.0000  11.0913
     35      0.9271        0.2774       0.6877      0.8181        0.3308        0.0000  10.8330
     36      0.9272        [32m0.2614[0m       0.6878      0.8181        0.3308        0.0000  11.3626
     37      0.9141        0.2730       0.6878      0.8181        0.3308        0.0000  11.2855
     38      0.9153        0.2620       0.6880      0.8182        [94m0.3308[0m     +  0.0000  11.1727
     39      0.9205        0.2703       0.6882      0.8184        [94m0.3308[0m     +  0.0000  11.1987
     40      0.9190        0.2754       0.6882      0.8184        [94m0.3308[0m     +  0.0000  11.0525
     41      0.9002        0.2757       0.6882      0.8184        [94m0.3308[0m     +  0.0000  11.2501
     42      0.9093        0.2801       0.6882      0.8184        0.3308        0.0000  11.0016
     43      0.9026        0.2800       0.6882      0.8184        0.3308        0.0000  11.2776
     44      0.9194        0.2747       0.6882      0.8184        [94m0.3308[0m     +  0.0000  11.0640
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112]
F1 Micro Score after query 3: 0.869629265897373
F1 Macro Score after query 3: 0.8484806833020535
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_2.pt

Iteration: 4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8710[0m        [32m0.3071[0m       [35m0.7290[0m      [31m0.8528[0m        [94m0.3138[0m     +  0.0000  11.7346
      2      [36m0.8939[0m        [32m0.2706[0m       [35m0.7519[0m      [31m0.8684[0m        [94m0.3041[0m     +  0.0000  11.7983
      3      [36m0.9076[0m        [32m0.2646[0m       [35m0.7613[0m      [31m0.8718[0m        [94m0.2987[0m     +  0.0000  11.5451
      4      [36m0.9082[0m        [32m0.2527[0m       0.7531      0.8696        [94m0.2970[0m     +  0.0000  11.7978
      5      [36m0.9243[0m        [32m0.2403[0m       0.7540      0.8695        [94m0.2927[0m     +  0.0000  11.6038
      6      [36m0.9397[0m        [32m0.2309[0m       0.7533      0.8675        [94m0.2920[0m     +  0.0000  11.7707
      7      0.9327        0.2321       0.7528      0.8673        [94m0.2917[0m     +  0.0000  11.5589
      8      0.9328        [32m0.2225[0m       0.7523      0.8671        [94m0.2908[0m     +  0.0000  11.5883
      9      0.9372        0.2250       0.7547      0.8672        [94m0.2881[0m     +  0.0000  11.6406
     10      [36m0.9497[0m        [32m0.2208[0m       0.7547      0.8673        [94m0.2868[0m     +  0.0000  11.7035
     11      0.9340        0.2209       0.7536      0.8666        [94m0.2863[0m     +  0.0000  11.7644
     12      [36m0.9550[0m        0.2234       0.7524      0.8659        [94m0.2861[0m     +  0.0000  11.4202
     13      0.9505        [32m0.2116[0m       0.7542      0.8656        [94m0.2852[0m     +  0.0000  11.7511
     14      [36m0.9589[0m        [32m0.2111[0m       0.7524      0.8653        0.2853        0.0000  11.8081
     15      0.9406        0.2187       0.7526      0.8660        [94m0.2849[0m     +  0.0000  11.6107
     16      0.9484        0.2164       0.7530      0.8657        [94m0.2846[0m     +  0.0000  11.6655
     17      0.9519        0.2115       0.7552      0.8652        [94m0.2839[0m     +  0.0000  11.7921
     18      0.9467        0.2152       0.7566      0.8661        [94m0.2837[0m     +  0.0000  11.6721
     19      0.9515        [32m0.2086[0m       0.7561      0.8661        0.2837        0.0000  11.8902
     20      0.9583        0.2134       0.7554      0.8660        0.2837        0.0000  11.5661
     21      0.9486        0.2136       0.7557      0.8659        [94m0.2835[0m     +  0.0000  11.6399
     22      [36m0.9592[0m        [32m0.2066[0m       0.7561      0.8663        [94m0.2833[0m     +  0.0000  11.8241
     23      [36m0.9636[0m        0.2146       0.7566      0.8664        [94m0.2833[0m     +  0.0000  11.9171
     24      0.9586        0.2081       0.7569      0.8660        [94m0.2831[0m     +  0.0000  11.7827
     25      0.9549        0.2086       0.7559      0.8662        0.2832        0.0000  11.6248
     26      0.9469        0.2106       0.7559      0.8662        0.2832        0.0000  11.6408
     27      0.9609        0.2097       0.7575      0.8664        [94m0.2831[0m     +  0.0000  11.7198
     28      0.9591        0.2079       0.7571      0.8664        [94m0.2831[0m     +  0.0000  11.7188
     29      0.9478        0.2118       0.7561      0.8661        0.2831        0.0000  11.7188
     30      0.9555        [32m0.2060[0m       0.7557      0.8664        0.2831        0.0000  11.5645
     31      0.9477        [32m0.2045[0m       0.7561      0.8666        [94m0.2831[0m     +  0.0000  11.5490
     32      0.9407        0.2130       0.7562      0.8667        [94m0.2831[0m     +  0.0000  11.5777
     33      0.9536        0.2056       0.7564      0.8666        [94m0.2830[0m     +  0.0000  11.7490
     34      0.9528        0.2140       0.7562      0.8664        [94m0.2830[0m     +  0.0000  11.8468
     35      0.9618        [32m0.2035[0m       0.7562      0.8662        [94m0.2830[0m     +  0.0000  11.8557
     36      0.9463        0.2139       0.7562      0.8662        0.2830        0.0000  11.7069
     37      0.9505        0.2096       0.7562      0.8662        [94m0.2830[0m     +  0.0000  11.8520
     38      0.9622        0.2081       0.7561      0.8661        [94m0.2830[0m     +  0.0000  11.6566
     39      0.9440        0.2119       0.7562      0.8662        [94m0.2830[0m     +  0.0000  11.5298
     40      0.9579        0.2150       0.7562      0.8662        0.2830        0.0000  11.7649
     41      0.9599        0.2111       0.7562      0.8662        0.2830        0.0000  11.7028
     42      0.9497        0.2090       0.7562      0.8662        0.2830        0.0000  11.4818
     43      0.9431        0.2136       0.7562      0.8662        0.2830        0.0000  11.7039
     44      0.9592        0.2083       0.7562      0.8662        0.2830        0.0000  11.7346
     45      0.9574        0.2105       0.7562      0.8662        0.2830        0.0000  11.7556
     46      0.9606        0.2120       0.7562      0.8662        0.2830        0.0000  11.7533
     47      0.9633        0.2114       0.7562      0.8662        0.2830        0.0000  11.5306
     48      [36m0.9692[0m        0.2063       0.7562      0.8662        0.2830        0.0000  11.7812
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208]
F1 Micro Score after query 4: 0.892081884897644
F1 Macro Score after query 4: 0.8784200538625542
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_3.pt

Iteration: 5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9017[0m        [32m0.2500[0m       [35m0.7767[0m      [31m0.8751[0m        [94m0.2760[0m     +  0.0000  12.6709
      2      [36m0.9223[0m        [32m0.2208[0m       [35m0.8003[0m      [31m0.8862[0m        [94m0.2627[0m     +  0.0000  12.3845
      3      [36m0.9468[0m        [32m0.2128[0m       [35m0.8038[0m      0.8845        [94m0.2585[0m     +  0.0000  12.6492
      4      0.9452        [32m0.1972[0m       0.8017      0.8862        [94m0.2556[0m     +  0.0000  12.2901
      5      [36m0.9550[0m        [32m0.1964[0m       0.8003      0.8861        [94m0.2507[0m     +  0.0000  12.1499
      6      [36m0.9589[0m        [32m0.1831[0m       0.7986      0.8823        [94m0.2473[0m     +  0.0000  12.2277
      7      [36m0.9655[0m        [32m0.1793[0m       0.7957      0.8807        [94m0.2466[0m     +  0.0000  12.4145
      8      [36m0.9699[0m        0.1828       0.8021      0.8827        [94m0.2464[0m     +  0.0000  12.4132
      9      [36m0.9714[0m        [32m0.1741[0m       0.8005      0.8814        [94m0.2460[0m     +  0.0000  12.1952
     10      [36m0.9723[0m        [32m0.1696[0m       0.8000      0.8809        [94m0.2452[0m     +  0.0000  12.2258
     11      0.9710        0.1733       0.7983      0.8807        [94m0.2439[0m     +  0.0000  12.2573
     12      [36m0.9789[0m        [32m0.1661[0m       0.7965      0.8793        0.2441        0.0000  12.3198
     13      0.9750        [32m0.1655[0m       0.7965      0.8796        0.2443        0.0000  12.4462
     14      [36m0.9835[0m        [32m0.1645[0m       0.7974      0.8806        [94m0.2432[0m     +  0.0000  12.4146
     15      0.9741        0.1657       0.7984      0.8808        [94m0.2428[0m     +  0.0000  12.3049
     16      0.9780        [32m0.1638[0m       0.7969      0.8806        [94m0.2427[0m     +  0.0000  12.4912
     17      0.9801        0.1651       0.7958      0.8805        [94m0.2426[0m     +  0.0000  12.4931
     18      [36m0.9865[0m        0.1656       0.7958      0.8800        0.2426        0.0000  12.2580
     19      0.9757        0.1642       0.7983      0.8810        [94m0.2421[0m     +  0.0000  12.3356
     20      0.9738        [32m0.1623[0m       0.7969      0.8802        0.2423        0.0000  12.4621
     21      0.9789        0.1637       0.7950      0.8795        0.2423        0.0000  12.3662
     22      0.9818        [32m0.1606[0m       0.7950      0.8798        0.2423        0.0000  12.5853
     23      0.9816        0.1615       0.7946      0.8795        0.2423        0.0000  12.2737
     24      0.9775        [32m0.1588[0m       0.7939      0.8794        0.2424        0.0000  12.4124
     25      0.9808        0.1651       0.7932      0.8791        0.2424        0.0000  12.3343
     26      0.9816        0.1617       0.7934      0.8789        0.2425        0.0000  12.5081
     27      0.9803        0.1611       0.7931      0.8791        0.2425        0.0000  12.4894
     28      0.9789        [32m0.1576[0m       0.7934      0.8790        0.2424        0.0000  12.4614
     29      0.9821        0.1637       0.7932      0.8790        0.2424        0.0000  12.3190
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.9119838559453587
F1 Macro Score after query 5: 0.9008075133293717
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_4.pt

Iteration: 6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9271[0m        [32m0.2095[0m       [35m0.8012[0m      [31m0.8828[0m        [94m0.2329[0m     +  0.0000  13.6814
      2      [36m0.9559[0m        [32m0.1817[0m       [35m0.8073[0m      [31m0.8837[0m        [94m0.2284[0m     +  0.0000  13.5105
      3      [36m0.9672[0m        [32m0.1705[0m       0.8038      [31m0.8849[0m        [94m0.2247[0m     +  0.0000  13.7750
      4      [36m0.9682[0m        [32m0.1681[0m       0.8057      0.8849        [94m0.2230[0m     +  0.0000  14.0248
      5      [36m0.9698[0m        [32m0.1575[0m       0.8059      0.8818        [94m0.2211[0m     +  0.0000  13.9123
      6      [36m0.9798[0m        [32m0.1519[0m       [35m0.8102[0m      [31m0.8859[0m        [94m0.2184[0m     +  0.0000  14.0814
      7      [36m0.9848[0m        [32m0.1457[0m       [35m0.8167[0m      [31m0.8873[0m        [94m0.2175[0m     +  0.0000  14.0604
      8      [36m0.9856[0m        [32m0.1437[0m       0.8151      [31m0.8886[0m        [94m0.2161[0m     +  0.0000  13.6938
      9      0.9802        [32m0.1431[0m       0.8111      [31m0.8889[0m        [94m0.2154[0m     +  0.0000  13.9662
     10      [36m0.9868[0m        [32m0.1369[0m       0.8141      [31m0.8902[0m        [94m0.2144[0m     +  0.0000  13.8906
     11      [36m0.9872[0m        [32m0.1345[0m       0.8142      0.8891        0.2151        0.0000  13.9889
     12      [36m0.9937[0m        0.1353       0.8139      0.8895        [94m0.2137[0m     +  0.0000  14.0006
     13      0.9915        [32m0.1335[0m       0.8137      0.8888        0.2140        0.0000  14.0273
     14      0.9937        0.1340       0.8149      [31m0.8923[0m        [94m0.2130[0m     +  0.0000  13.9001
     15      0.9897        [32m0.1314[0m       0.8132      0.8876        0.2139        0.0000  13.7291
     16      0.9919        [32m0.1299[0m       0.8156      [31m0.8929[0m        [94m0.2123[0m     +  0.0000  13.7622
     17      0.9929        0.1326       0.8160      [31m0.8939[0m        [94m0.2121[0m     +  0.0000  13.8352
     18      0.9914        0.1308       0.8149      0.8926        0.2125        0.0000  13.7309
     19      [36m0.9941[0m        [32m0.1294[0m       0.8155      0.8925        0.2124        0.0000  13.7893
     20      [36m0.9964[0m        [32m0.1289[0m       0.8160      0.8928        [94m0.2119[0m     +  0.0000  13.7593
     21      0.9941        [32m0.1277[0m       0.8153      0.8938        [94m0.2118[0m     +  0.0000  13.8693
     22      0.9946        0.1284       0.8153      [31m0.8948[0m        0.2120        0.0000  13.8376
     23      [36m0.9966[0m        0.1306       0.8160      0.8947        [94m0.2117[0m     +  0.0000  13.9479
     24      0.9952        0.1321       0.8148      0.8941        [94m0.2117[0m     +  0.0000  13.9005
     25      0.9926        [32m0.1273[0m       0.8148      0.8947        0.2118        0.0000  13.9936
     26      0.9910        0.1279       0.8155      [31m0.8954[0m        0.2119        0.0000  13.9004
     27      0.9943        0.1281       0.8158      [31m0.8955[0m        0.2119        0.0000  13.7443
     28      0.9943        [32m0.1269[0m       0.8155      0.8953        0.2118        0.0000  13.6331
     29      0.9961        0.1277       0.8155      0.8953        0.2118        0.0000  13.8529
     30      0.9952        0.1294       0.8158      [31m0.8955[0m        0.2118        0.0000  14.1022
     31      0.9957        [32m0.1265[0m       0.8158      [31m0.8955[0m        0.2118        0.0000  13.8685
     32      0.9941        0.1278       0.8156      0.8954        0.2118        0.0000  13.6175
     33      0.9961        0.1269       0.8163      [31m0.8959[0m        0.2118        0.0000  13.7128
     34      0.9939        [32m0.1249[0m       0.8156      0.8954        0.2117        0.0000  13.5750
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.913302034428795
F1 Macro Score after query 6: 0.8996831480081191
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_5.pt

Iteration: 7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9641[0m        [32m0.1597[0m       [35m0.8082[0m      [31m0.8935[0m        [94m0.2138[0m     +  0.0000  16.8546
      2      [36m0.9754[0m        [32m0.1406[0m       [35m0.8165[0m      [31m0.8963[0m        [94m0.2065[0m     +  0.0000  16.4951
      3      [36m0.9806[0m        [32m0.1322[0m       0.8043      0.8931        0.2131        0.0000  16.3245
      4      [36m0.9812[0m        [32m0.1256[0m       0.8120      0.8952        0.2068        0.0000  16.4643
      5      [36m0.9893[0m        [32m0.1177[0m       0.8113      0.8940        [94m0.2052[0m     +  0.0000  16.6845
      6      [36m0.9907[0m        [32m0.1099[0m       0.8082      0.8935        [94m0.2043[0m     +  0.0000  16.4641
      7      [36m0.9953[0m        [32m0.1063[0m       0.8128      0.8939        [94m0.2013[0m     +  0.0000  16.3694
      8      0.9953        [32m0.1053[0m       0.8146      0.8943        [94m0.1991[0m     +  0.0000  16.5291
      9      0.9947        [32m0.1026[0m       0.8148      0.8962        [94m0.1983[0m     +  0.0000  16.4023
     10      0.9948        [32m0.1009[0m       [35m0.8191[0m      [31m0.8967[0m        [94m0.1963[0m     +  0.0000  16.4642
     11      [36m0.9957[0m        [32m0.0978[0m       [35m0.8220[0m      0.8946        [94m0.1935[0m     +  0.0000  16.2155
     12      [36m0.9965[0m        0.0983       [35m0.8222[0m      0.8961        0.1938        0.0000  16.2764
     13      0.9965        [32m0.0958[0m       [35m0.8231[0m      0.8959        [94m0.1928[0m     +  0.0000  16.1344
     14      [36m0.9983[0m        [32m0.0946[0m       [35m0.8248[0m      0.8964        0.1929        0.0000  16.4864
     15      0.9978        0.0964       0.8245      0.8962        [94m0.1921[0m     +  0.0000  16.2452
     16      0.9980        0.0952       [35m0.8250[0m      0.8943        [94m0.1916[0m     +  0.0000  16.1979
     17      0.9977        [32m0.0933[0m       [35m0.8253[0m      0.8944        [94m0.1916[0m     +  0.0000  16.5747
     18      0.9983        0.0935       [35m0.8259[0m      0.8944        0.1918        0.0000  16.4009
     19      0.9978        0.0936       [35m0.8264[0m      0.8936        [94m0.1915[0m     +  0.0000  16.4020
     20      0.9975        [32m0.0914[0m       0.8259      0.8942        [94m0.1913[0m     +  0.0000  16.3869
     21      0.9980        0.0934       0.8250      0.8941        0.1918        0.0000  16.5427
     22      [36m0.9986[0m        0.0934       0.8255      0.8942        0.1917        0.0000  16.3391
     23      0.9978        [32m0.0907[0m       0.8255      0.8942        0.1918        0.0000  16.4629
     24      0.9983        0.0952       0.8255      0.8940        0.1917        0.0000  16.4631
     25      0.9970        0.0924       0.8257      0.8946        0.1915        0.0000  16.5255
     26      0.9980        0.0935       0.8247      0.8939        0.1915        0.0000  16.5119
     27      0.9970        0.0925       0.8247      0.8940        0.1916        0.0000  16.2458
     28      0.9983        0.0929       0.8255      0.8943        0.1917        0.0000  16.5412
     29      0.9975        [32m0.0906[0m       0.8245      0.8941        0.1918        0.0000  16.3366
     30      0.9980        0.0912       0.8247      0.8943        0.1917        0.0000  16.2135
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.9127870887647423
F1 Macro Score after query 7: 0.8981708217873061
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_6.pt

Iteration: 8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9697[0m        [32m0.1223[0m       [35m0.7573[0m      [31m0.8723[0m        [94m0.2425[0m     +  0.0000  20.7941
      2      [36m0.9807[0m        [32m0.1086[0m       [35m0.7929[0m      [31m0.8870[0m        [94m0.1999[0m     +  0.0000  20.9283
      3      [36m0.9832[0m        [32m0.1012[0m       0.7908      0.8852        0.2101        0.0000  21.3586
      4      [36m0.9856[0m        [32m0.0945[0m       [35m0.8071[0m      0.8865        [94m0.1874[0m     +  0.0000  20.6687
      5      [36m0.9873[0m        [32m0.0892[0m       0.8031      [31m0.8888[0m        0.1947        0.0000  21.1380
      6      [36m0.9896[0m        [32m0.0837[0m       [35m0.8148[0m      0.8860        [94m0.1822[0m     +  0.0000  20.5736
      7      [36m0.9912[0m        [32m0.0806[0m       0.8071      0.8809        0.1850        0.0000  20.8881
      8      0.9911        [32m0.0775[0m       0.8083      0.8827        0.1838        0.0000  20.8372
      9      [36m0.9925[0m        [32m0.0748[0m       0.8076      0.8816        0.1831        0.0000  20.8877
     10      0.9918        [32m0.0741[0m       0.8019      0.8777        0.1866        0.0000  20.8419
     11      [36m0.9948[0m        [32m0.0715[0m       0.8137      0.8842        0.1822        0.0000  20.8865
     12      0.9926        [32m0.0711[0m       0.8106      0.8833        0.1837        0.0000  21.0431
     13      0.9941        [32m0.0690[0m       0.8132      0.8833        0.1832        0.0000  20.7157
     14      0.9947        [32m0.0684[0m       0.8113      0.8841        [94m0.1821[0m     +  0.0000  20.8706
     15      0.9946        [32m0.0673[0m       0.8120      0.8830        0.1823        0.0000  20.8630
     16      [36m0.9954[0m        [32m0.0673[0m       [35m0.8163[0m      0.8883        [94m0.1810[0m     +  0.0000  20.7953
     17      0.9950        [32m0.0661[0m       [35m0.8186[0m      [31m0.8899[0m        [94m0.1801[0m     +  0.0000  20.9041
     18      0.9949        [32m0.0656[0m       [35m0.8198[0m      0.8897        [94m0.1798[0m     +  0.0000  21.3981
     19      [36m0.9955[0m        [32m0.0652[0m       0.8172      0.8897        [94m0.1796[0m     +  0.0000  20.9686
     20      0.9951        0.0653       0.8187      [31m0.8901[0m        0.1800        0.0000  20.8256
     21      0.9953        [32m0.0650[0m       0.8181      [31m0.8912[0m        0.1803        0.0000  20.6998
     22      [36m0.9956[0m        0.0651       0.8182      [31m0.8915[0m        0.1806        0.0000  20.7461
     23      0.9945        [32m0.0638[0m       0.8187      0.8913        0.1802        0.0000  21.0113
     24      0.9947        0.0642       0.8198      [31m0.8917[0m        0.1803        0.0000  21.1978
     25      0.9950        0.0641       0.8191      0.8914        0.1803        0.0000  20.8597
     26      [36m0.9964[0m        0.0640       0.8194      0.8914        0.1802        0.0000  20.6997
     27      0.9951        0.0639       0.8191      0.8913        0.1802        0.0000  21.0901
     28      0.9949        0.0644       [35m0.8205[0m      [31m0.8917[0m        0.1798        0.0000  20.9642
     29      0.9953        0.0640       0.8200      [31m0.8917[0m        0.1801        0.0000  21.0892
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.9124922696351269
F1 Macro Score after query 8: 0.8966207451959184
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_7.pt

Iteration: 9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9769[0m        [32m0.0862[0m       [35m0.8375[0m      [31m0.8928[0m        [94m0.1695[0m     +  0.0000  28.5760
      2      [36m0.9801[0m        [32m0.0765[0m       0.8300      0.8884        0.1719        0.0000  29.3154
      3      [36m0.9857[0m        [32m0.0689[0m       0.8042      0.8711        0.1881        0.0000  29.3605
      4      [36m0.9864[0m        [32m0.0638[0m       0.8057      0.8663        0.1912        0.0000  29.1908
      5      [36m0.9877[0m        [32m0.0578[0m       0.8262      0.8869        0.1756        0.0000  28.9080
      6      [36m0.9892[0m        [32m0.0530[0m       0.8174      0.8834        0.1820        0.0000  29.2335
      7      [36m0.9925[0m        [32m0.0491[0m       0.8174      0.8840        0.1814        0.0000  29.2507
      8      [36m0.9931[0m        [32m0.0465[0m       0.8007      0.8824        0.1981        0.0000  29.1878
      9      [36m0.9932[0m        [32m0.0454[0m       0.8130      0.8855        0.1893        0.0000  28.6710
     10      [36m0.9944[0m        [32m0.0439[0m       0.8174      0.8818        0.1835        0.0000  29.1880
     11      [36m0.9955[0m        [32m0.0401[0m       0.8172      0.8824        0.1840        0.0000  29.2518
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.9144588610053608
F1 Macro Score after query 9: 0.8964960098903401
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_8.pt

Iteration: 10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9714[0m        [32m0.0853[0m       [35m0.8106[0m      [31m0.8866[0m        [94m0.1746[0m     +  0.0000  43.4311
      2      [36m0.9778[0m        [32m0.0748[0m       [35m0.8108[0m      0.8838        [94m0.1721[0m     +  0.0000  43.4612
      3      [36m0.9800[0m        [32m0.0670[0m       [35m0.8193[0m      0.8831        0.1752        0.0000  43.6210
      4      [36m0.9834[0m        [32m0.0602[0m       0.8104      0.8812        0.1802        0.0000  43.3381
      5      [36m0.9837[0m        [32m0.0556[0m       0.8120      0.8842        0.1791        0.0000  43.6664
      6      [36m0.9861[0m        [32m0.0481[0m       [35m0.8276[0m      [31m0.8905[0m        [94m0.1679[0m     +  0.0000  43.5719
      7      [36m0.9889[0m        [32m0.0447[0m       [35m0.8300[0m      0.8894        0.1710        0.0000  42.9447
      8      [36m0.9900[0m        [32m0.0419[0m       0.8259      [31m0.8908[0m        0.1757        0.0000  43.6985
      9      [36m0.9905[0m        [32m0.0402[0m       0.8267      0.8898        0.1768        0.0000  43.7613
     10      0.9905        [32m0.0384[0m       0.8292      [31m0.8922[0m        0.1768        0.0000  43.4474
     11      [36m0.9909[0m        [32m0.0364[0m       0.8194      0.8909        0.1875        0.0000  43.4163
     12      [36m0.9930[0m        [32m0.0341[0m       0.8257      [31m0.8933[0m        0.1817        0.0000  43.9796
     13      [36m0.9933[0m        [32m0.0337[0m       0.8247      [31m0.8934[0m        0.1846        0.0000  43.5416
     14      0.9932        [32m0.0325[0m       0.8177      0.8908        0.1888        0.0000  43.4917
     15      [36m0.9941[0m        [32m0.0319[0m       0.8174      0.8904        0.1916        0.0000  43.6972
     16      [36m0.9951[0m        [32m0.0306[0m       0.8238      0.8919        0.1889        0.0000  43.7291
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.9177755557264826
F1 Macro Score after query 10: 0.9031407107671612
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_9.pt

Iteration: 11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9784[0m        [32m0.0574[0m       [35m0.8226[0m      [31m0.8870[0m        [94m0.1709[0m     +  0.0000  69.7270
      2      [36m0.9822[0m        [32m0.0492[0m       0.8212      0.8835        0.1735        0.0000  69.6399
      3      [36m0.9833[0m        [32m0.0445[0m       0.8063      0.8820        0.1906        0.0000  70.2451
      4      [36m0.9847[0m        [32m0.0405[0m       0.8127      0.8809        0.1934        0.0000  69.2523
      5      [36m0.9868[0m        [32m0.0362[0m       0.8165      0.8791        0.1994        0.0000  69.5750
      6      [36m0.9882[0m        [32m0.0310[0m       0.8092      0.8816        0.2082        0.0000  69.8398
      7      [36m0.9906[0m        [32m0.0283[0m       0.8184      0.8870        0.2007        0.0000  69.3562
      8      [36m0.9909[0m        [32m0.0262[0m       0.8113      0.8863        0.2088        0.0000  69.5234
      9      [36m0.9927[0m        [32m0.0243[0m       0.8125      [31m0.8878[0m        0.2090        0.0000  70.4146
     10      [36m0.9931[0m        [32m0.0227[0m       0.8118      0.8845        0.2155        0.0000  69.6377
     11      [36m0.9940[0m        [32m0.0209[0m       [35m0.8311[0m      [31m0.8921[0m        0.2035        0.0000  69.4001
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.9177468772095215
F1 Macro Score after query 11: 0.8994661191162431
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_10.pt

Iteration: 12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9768[0m        [32m0.0567[0m       [35m0.7694[0m      [31m0.8681[0m        [94m0.2285[0m     +  0.0000  115.7232
      2      [36m0.9795[0m        [32m0.0489[0m       [35m0.7811[0m      [31m0.8738[0m        [94m0.2213[0m     +  0.0000  115.6640
      3      [36m0.9810[0m        [32m0.0438[0m       [35m0.7885[0m      0.8723        [94m0.2180[0m     +  0.0000  115.8258
      4      [36m0.9819[0m        [32m0.0394[0m       0.7776      [31m0.8757[0m        0.2446        0.0000  116.4133
      5      [36m0.9836[0m        [32m0.0350[0m       [35m0.8179[0m      [31m0.8868[0m        [94m0.2013[0m     +  0.0000  116.1609
      6      [36m0.9874[0m        [32m0.0294[0m       0.8130      0.8828        [94m0.2010[0m     +  0.0000  115.0665
      7      [36m0.9891[0m        [32m0.0262[0m       0.8168      0.8817        [94m0.2006[0m     +  0.0000  116.1343
      8      [36m0.9899[0m        [32m0.0245[0m       0.8092      0.8780        0.2186        0.0000  115.4727
      9      [36m0.9905[0m        [32m0.0227[0m       0.8080      0.8722        0.2188        0.0000  116.3035
     10      [36m0.9921[0m        [32m0.0206[0m       0.8089      0.8753        0.2230        0.0000  116.1468
     11      [36m0.9926[0m        [32m0.0188[0m       [35m0.8236[0m      0.8865        0.2107        0.0000  116.0982
     12      [36m0.9939[0m        [32m0.0176[0m       0.8212      0.8857        0.2138        0.0000  115.9296
     13      [36m0.9945[0m        [32m0.0163[0m       0.8220      0.8867        0.2182        0.0000  115.8059
     14      [36m0.9946[0m        [32m0.0158[0m       0.8214      0.8862        0.2186        0.0000  115.7553
     15      [36m0.9957[0m        [32m0.0149[0m       0.8203      0.8854        0.2248        0.0000  116.0847
     16      [36m0.9957[0m        [32m0.0143[0m       0.8212      [31m0.8878[0m        0.2215        0.0000  115.9903
     17      [36m0.9965[0m        [32m0.0137[0m       0.8217      0.8867        0.2223        0.0000  115.9726
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.9174776485481446
F1 Macro Score after query 12: 0.8996450766035483
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_11.pt

Iteration: 13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9845[0m        [32m0.0330[0m       [35m0.8439[0m      [31m0.8946[0m        [94m0.1874[0m     +  0.0000  109.6750
      2      [36m0.9860[0m        [32m0.0296[0m       0.8337      0.8939        0.1914        0.0000  110.7942
      3      [36m0.9871[0m        [32m0.0265[0m       0.8250      0.8902        0.2069        0.0000  112.2513
      4      [36m0.9881[0m        [32m0.0244[0m       0.8207      0.8920        0.2055        0.0000  111.1242
      5      [36m0.9887[0m        [32m0.0226[0m       0.8189      0.8915        0.2197        0.0000  114.5421
      6      [36m0.9915[0m        [32m0.0177[0m       0.8182      0.8931        0.2297        0.0000  130.1272
      7      [36m0.9933[0m        [32m0.0150[0m       0.8226      0.8942        0.2249        0.0000  129.3733
      8      [36m0.9945[0m        [32m0.0134[0m       0.8238      [31m0.8964[0m        0.2345        0.0000  129.8567
      9      [36m0.9953[0m        [32m0.0120[0m       0.8187      0.8940        0.2469        0.0000  134.1308
     10      [36m0.9962[0m        [32m0.0106[0m       0.8215      0.8943        0.2501        0.0000  134.2948
     11      [36m0.9962[0m        [32m0.0103[0m       0.8245      0.8878        0.2485        0.0000  134.3216
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.9204284014280049
F1 Macro Score after query 13: 0.8991812234749702
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\model_checkpoint_iteration_12.pt
Pickle file saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_confidence_seed45_lowLR\AL_average_confidence_results_for_multilabel_classification_s45.pickle
