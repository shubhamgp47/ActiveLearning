Adjusted shapes after loading:
X_initial_np: (8, 224, 224, 3), y_initial_np: (8, 3)
X_pool_np: (26872, 224, 224, 3), y_pool_np: (26872, 3)
X_test_np: (5760, 224, 224, 3), y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3), y_val_np: (5760, 3)
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.5556[0m        [32m0.6869[0m       [35m0.1859[0m      [31m0.5403[0m        [94m0.6979[0m     +  0.0000  12.7029
      2      [36m0.6833[0m        [32m0.6858[0m       0.1476      0.4470        [94m0.6882[0m     +  0.0000  10.8441
      3      0.6722        [32m0.6446[0m       [35m0.1981[0m      0.4584        0.6915        0.0000  10.7905
      4      [36m0.8350[0m        [32m0.6036[0m       [35m0.2899[0m      0.4102        [94m0.6826[0m     +  0.0000  10.8283
      5      0.7222        0.6210       0.2344      0.4254        0.6899        0.0000  10.8451
      6      0.8350        [32m0.5965[0m       0.2651      0.4395        0.6858        0.0000  10.8478
      7      0.8148        [32m0.5547[0m       0.2773      0.4432        0.6834        0.0000  10.8140
      8      0.7593        [32m0.5228[0m       0.2642      0.4369        0.6838        0.0000  10.7967
      9      [36m0.9630[0m        0.5230       0.2616      0.4589        0.6835        0.0000  10.8006
     10      0.9259        [32m0.4998[0m       0.2655      0.4598        [94m0.6817[0m     +  0.0000  10.8846
     11      0.8426        0.5413       0.2724      0.4593        [94m0.6814[0m     +  0.0000  10.8028
     12      0.9153        0.5251       0.2658      0.4609        0.6814        0.0000  10.9931
     13      0.9153        0.5339       0.2634      0.4553        0.6827        0.0000  10.8013
     14      0.9259        0.5187       0.2644      0.4610        0.6825        0.0000  10.7811
     15      0.9630        [32m0.4953[0m       0.2726      0.4624        0.6815        0.0000  10.9082
     16      0.9333        [32m0.4932[0m       0.2736      0.4620        [94m0.6811[0m     +  0.0000  10.9033
     17      0.9630        [32m0.4929[0m       0.2734      0.4633        0.6813        0.0000  10.8103
     18      0.8487        0.5119       0.2719      0.4635        0.6816        0.0000  10.8425
     19      0.9630        0.4939       0.2708      0.4634        0.6817        0.0000  10.9104
     20      0.9630        [32m0.4892[0m       0.2694      0.4647        0.6819        0.0000  10.8749
     21      0.9167        0.5014       0.2681      0.4648        0.6820        0.0000  10.9944
     22      0.9630        [32m0.4857[0m       0.2677      0.4650        0.6819        0.0000  10.8441
     23      0.9259        0.4931       0.2681      0.4656        0.6819        0.0000  10.6569
     24      0.9630        0.5094       0.2682      0.4662        0.6819        0.0000  11.1082
     25      0.9630        0.4895       0.2684      0.4665        0.6819        0.0000  11.2530
     26      0.9259        [32m0.4809[0m       0.2684      0.4665        0.6819        0.0000  11.0784
Stopping since valid_loss has not improved in the last 11 epochs.
Pre F1 micro score = 0.5200
Pre F1 macro score = 0.5085
Pre Accuracy = 0.3510

Iteration: 1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8632[0m        [32m0.4900[0m       [35m0.1207[0m      [31m0.4088[0m        [94m0.6995[0m     +  0.0000  11.1405
      2      0.6117        [32m0.4638[0m       [35m0.1877[0m      [31m0.4635[0m        [94m0.6971[0m     +  0.0000  11.0091
      3      0.8085        [32m0.4212[0m       0.1800      [31m0.5113[0m        0.7103        0.0000  11.1235
      4      0.8008        [32m0.3873[0m       [35m0.1880[0m      0.4878        0.7067        0.0000  11.3905
      5      0.8167        [32m0.3852[0m       0.1755      [31m0.5177[0m        0.7199        0.0000  11.2546
      6      [36m0.8641[0m        [32m0.3663[0m       [35m0.1892[0m      [31m0.5219[0m        0.7176        0.0000  11.3149
      7      0.8085        0.3711       [35m0.1962[0m      [31m0.5295[0m        0.7187        0.0000  11.4826
      8      0.8641        [32m0.3498[0m       [35m0.2066[0m      0.5293        0.7155        0.0000  11.2479
      9      0.8641        0.3612       0.2030      [31m0.5305[0m        0.7163        0.0000  11.2512
     10      [36m0.9449[0m        [32m0.3450[0m       0.2003      [31m0.5321[0m        0.7165        0.0000  11.3456
     11      0.8564        [32m0.3355[0m       0.2012      0.5319        0.7166        0.0000  11.0136
     12      0.8641        [32m0.3352[0m       0.2005      [31m0.5331[0m        0.7178        0.0000  11.2776
Stopping since valid_loss has not improved in the last 11 epochs.
[24]
F1 Micro Score after query 1: 0.5571464774455144
F1 Macro Score after query 1: 0.5541149927245183
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_0.pt

Iteration: 2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8755[0m        [32m0.4250[0m       [35m0.1943[0m      [31m0.5374[0m        [94m0.7973[0m     +  0.0000  11.3436
      2      [36m0.9298[0m        [32m0.3479[0m       [35m0.2106[0m      [31m0.5463[0m        [94m0.7895[0m     +  0.0000  11.4962
      3      0.9245        0.3635       0.2089      0.5423        [94m0.7836[0m     +  0.0000  11.2354
      4      [36m0.9512[0m        [32m0.3345[0m       0.1958      0.5286        0.7851        0.0000  11.1407
      5      0.9469        [32m0.3327[0m       0.2095      0.5373        [94m0.7823[0m     +  0.0000  11.3484
      6      [36m0.9610[0m        [32m0.3071[0m       0.2104      0.5406        [94m0.7740[0m     +  0.0000  11.1593
      7      0.9525        [32m0.3042[0m       [35m0.2109[0m      0.5391        [94m0.7726[0m     +  0.0000  11.2532
      8      0.9610        [32m0.2941[0m       [35m0.2115[0m      0.5365        [94m0.7715[0m     +  0.0000  11.4257
      9      0.9512        0.3071       0.2082      0.5319        [94m0.7676[0m     +  0.0000  11.2553
     10      0.9512        0.3008       0.2089      0.5343        0.7682        0.0000  11.2984
     11      0.9524        [32m0.2832[0m       0.2109      0.5373        [94m0.7645[0m     +  0.0000  11.4511
     12      0.9556        0.2905       0.2083      0.5353        0.7650        0.0000  11.3003
     13      0.9535        0.2980       0.2049      0.5305        [94m0.7620[0m     +  0.0000  11.0934
     14      0.9481        0.2944       0.2069      0.5301        0.7622        0.0000  11.4844
     15      0.9566        0.2872       0.2083      0.5317        0.7626        0.0000  11.3509
     16      0.9566        0.2852       0.2059      0.5306        [94m0.7620[0m     +  0.0000  11.4220
     17      0.9556        [32m0.2797[0m       0.2064      0.5310        0.7620        0.0000  11.4977
     18      0.9566        0.2820       0.2066      0.5303        [94m0.7618[0m     +  0.0000  11.4791
     19      0.9512        0.2815       0.2047      0.5286        [94m0.7616[0m     +  0.0000  11.4538
     20      0.9556        [32m0.2758[0m       0.2052      0.5287        0.7619        0.0000  11.2488
     21      0.9610        0.2800       0.2040      0.5274        [94m0.7614[0m     +  0.0000  11.2983
     22      0.9610        0.2761       0.2043      0.5277        0.7616        0.0000  11.2525
     23      0.9566        [32m0.2746[0m       0.2040      0.5273        0.7615        0.0000  11.1408
     24      0.9610        0.2802       0.2038      0.5271        0.7614        0.0000  11.2665
     25      0.9610        0.2977       0.2035      0.5274        0.7616        0.0000  11.2142
     26      0.9512        0.2767       0.2035      0.5274        0.7616        0.0000  11.5294
     27      0.9512        0.2851       0.2042      0.5280        0.7617        0.0000  11.4655
     28      0.9610        0.2944       0.2045      0.5285        0.7617        0.0000  11.4061
     29      0.9610        0.2853       0.2047      0.5285        0.7617        0.0000  11.2548
     30      0.9512        0.2943       0.2047      0.5285        0.7617        0.0000  11.3584
     31      0.9556        0.2814       0.2047      0.5286        0.7617        0.0000  11.2992
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56]
F1 Micro Score after query 2: 0.5466489798496621
F1 Macro Score after query 2: 0.5424437137444614
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_1.pt

Iteration: 3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9502[0m        [32m0.3377[0m       [35m0.2017[0m      [31m0.5313[0m        [94m0.8549[0m     +  0.0000  11.4686
      2      0.9403        [32m0.3101[0m       0.1983      [31m0.5334[0m        [94m0.8063[0m     +  0.0000  11.4875
      3      0.9501        [32m0.2913[0m       0.1957      0.5307        [94m0.7766[0m     +  0.0000  11.6244
      4      0.9492        [32m0.2797[0m       [35m0.2038[0m      0.5227        [94m0.7427[0m     +  0.0000  11.5511
      5      [36m0.9654[0m        [32m0.2490[0m       [35m0.2040[0m      0.5172        [94m0.7178[0m     +  0.0000  11.6081
      6      [36m0.9673[0m        [32m0.2426[0m       [35m0.2102[0m      0.5168        [94m0.7136[0m     +  0.0000  11.5289
      7      [36m0.9695[0m        [32m0.2401[0m       [35m0.2113[0m      0.5183        [94m0.7099[0m     +  0.0000  11.4955
      8      0.9695        [32m0.2349[0m       [35m0.2139[0m      0.5188        [94m0.7053[0m     +  0.0000  11.5150
      9      0.9680        [32m0.2292[0m       [35m0.2167[0m      0.5223        [94m0.7009[0m     +  0.0000  11.5138
     10      0.9673        [32m0.2235[0m       [35m0.2238[0m      0.5306        0.7053        0.0000  11.5387
     11      [36m0.9710[0m        [32m0.2235[0m       [35m0.2250[0m      0.5302        0.7013        0.0000  11.5142
     12      0.9673        0.2285       [35m0.2253[0m      0.5265        [94m0.6982[0m     +  0.0000  11.7677
     13      0.9673        [32m0.2233[0m       [35m0.2262[0m      0.5301        0.6995        0.0000  11.6279
     14      0.9695        0.2255       [35m0.2273[0m      0.5306        [94m0.6975[0m     +  0.0000  11.4039
     15      [36m0.9710[0m        0.2237       0.2255      0.5290        [94m0.6966[0m     +  0.0000  11.5400
     16      0.9695        [32m0.2224[0m       0.2260      0.5306        0.6972        0.0000  11.7228
     17      0.9695        [32m0.2215[0m       [35m0.2276[0m      0.5303        0.6967        0.0000  11.4982
     18      0.9695        [32m0.2199[0m       0.2276      0.5317        0.6969        0.0000  11.7030
     19      0.9673        [32m0.2186[0m       [35m0.2280[0m      0.5324        0.6974        0.0000  11.6425
     20      0.9673        [32m0.2167[0m       0.2273      0.5320        [94m0.6962[0m     +  0.0000  11.5155
     21      0.9695        0.2189       [35m0.2281[0m      0.5332        0.6965        0.0000  11.7044
     22      0.9695        [32m0.2104[0m       [35m0.2288[0m      [31m0.5336[0m        0.6966        0.0000  11.6463
     23      0.9673        0.2149       0.2285      [31m0.5337[0m        0.6964        0.0000  11.3927
     24      0.9695        0.2214       0.2286      0.5335        0.6966        0.0000  11.3887
     25      0.9695        0.2262       0.2288      [31m0.5338[0m        0.6964        0.0000  11.4895
     26      0.9695        0.2131       0.2288      0.5337        0.6962        0.0000  11.8735
     27      0.9695        0.2186       0.2285      0.5335        [94m0.6961[0m     +  0.0000  11.4410
     28      0.9673        0.2183       [35m0.2290[0m      0.5338        [94m0.6961[0m     +  0.0000  11.5340
     29      0.9673        0.2154       [35m0.2292[0m      [31m0.5339[0m        [94m0.6960[0m     +  0.0000  11.4073
     30      0.9695        0.2158       0.2288      0.5338        [94m0.6958[0m     +  0.0000  11.6152
     31      0.9695        0.2164       0.2290      [31m0.5340[0m        0.6959        0.0000  11.4940
     32      0.9695        0.2235       0.2288      0.5338        [94m0.6958[0m     +  0.0000  11.5513
     33      0.9695        0.2154       0.2290      [31m0.5341[0m        0.6958        0.0000  11.6723
     34      0.9695        0.2190       0.2288      0.5338        [94m0.6957[0m     +  0.0000  11.6395
     35      0.9658        0.2243       0.2292      [31m0.5342[0m        0.6958        0.0000  11.3938
     36      0.9695        0.2112       0.2292      0.5341        0.6957        0.0000  11.6081
     37      0.9680        0.2200       0.2292      0.5341        0.6957        0.0000  11.6124
     38      0.9695        0.2234       0.2292      0.5341        0.6957        0.0000  11.6713
     39      0.9673        0.2193       0.2290      0.5339        [94m0.6957[0m     +  0.0000  11.8907
     40      0.9695        0.2138       0.2292      0.5342        0.6957        0.0000  11.7657
     41      0.9695        0.2191       0.2292      0.5342        0.6957        0.0000  11.6560
     42      0.9695        0.2211       0.2292      0.5342        0.6957        0.0000  11.7823
     43      0.9673        0.2244       0.2292      0.5342        0.6957        0.0000  11.7194
     44      0.9673        0.2216       0.2292      0.5342        0.6957        0.0000  11.7172
     45      0.9673        0.2165       0.2292      0.5342        0.6957        0.0000  11.4062
     46      0.9695        0.2189       0.2292      0.5342        0.6957        0.0000  11.7030
     47      [36m0.9715[0m        0.2118       0.2292      0.5342        0.6957        0.0000  11.5053
     48      0.9695        0.2133       0.2292      0.5342        0.6957        0.0000  11.4287
     49      0.9715        0.2139       0.2292      0.5342        0.6957        0.0000  11.5591
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112]
F1 Micro Score after query 3: 0.5705132396207911
F1 Macro Score after query 3: 0.5680015145150191
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_2.pt

Iteration: 4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9540[0m        [32m0.2670[0m       [35m0.2477[0m      [31m0.5901[0m        [94m0.7867[0m     +  0.0000  12.2762
      2      [36m0.9631[0m        [32m0.2261[0m       [35m0.2745[0m      [31m0.5991[0m        [94m0.7532[0m     +  0.0000  12.1714
      3      [36m0.9704[0m        [32m0.2143[0m       0.2698      [31m0.6013[0m        [94m0.7128[0m     +  0.0000  12.1086
      4      [36m0.9716[0m        [32m0.2055[0m       0.2720      [31m0.6029[0m        [94m0.7094[0m     +  0.0000  11.5381
      5      [36m0.9753[0m        [32m0.1932[0m       0.2712      0.6018        [94m0.6962[0m     +  0.0000  11.6743
      6      [36m0.9772[0m        [32m0.1880[0m       0.2672      0.5954        [94m0.6893[0m     +  0.0000  12.5670
      7      [36m0.9794[0m        [32m0.1851[0m       0.2674      0.5939        0.6900        0.0000  11.9255
      8      0.9762        [32m0.1821[0m       0.2672      0.5946        0.6896        0.0000  11.9902
      9      0.9794        0.1847       0.2658      0.5925        0.6909        0.0000  12.3047
     10      0.9783        [32m0.1772[0m       0.2660      0.5916        [94m0.6880[0m     +  0.0000  11.4752
     11      0.9780        [32m0.1743[0m       0.2597      0.5867        [94m0.6849[0m     +  0.0000  11.9297
     12      0.9784        0.1759       0.2589      0.5854        [94m0.6828[0m     +  0.0000  11.7712
     13      [36m0.9824[0m        [32m0.1713[0m       0.2571      0.5842        0.6859        0.0000  11.9123
     14      0.9805        0.1734       0.2556      0.5834        0.6850        0.0000  11.7082
     15      0.9823        [32m0.1700[0m       0.2559      0.5852        0.6868        0.0000  11.8340
     16      [36m0.9843[0m        0.1755       0.2550      0.5832        0.6845        0.0000  12.0537
     17      0.9821        [32m0.1691[0m       0.2538      0.5805        0.6838        0.0000  11.9600
     18      0.9821        0.1749       0.2538      0.5808        0.6839        0.0000  12.1654
     19      [36m0.9851[0m        [32m0.1685[0m       0.2538      0.5820        0.6862        0.0000  11.9922
     20      0.9816        0.1690       0.2536      0.5812        0.6846        0.0000  12.1325
     21      0.9821        0.1747       0.2536      0.5808        0.6849        0.0000  11.6330
     22      0.9816        0.1771       0.2533      0.5806        0.6854        0.0000  12.3790
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208]
F1 Micro Score after query 4: 0.5824526296041524
F1 Macro Score after query 4: 0.5834872276555659
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_3.pt

Iteration: 5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9510[0m        [32m0.2379[0m       [35m0.2837[0m      [31m0.5964[0m        [94m0.6243[0m     +  0.0000  12.3645
      2      [36m0.9734[0m        [32m0.1915[0m       [35m0.2894[0m      [31m0.5999[0m        [94m0.6046[0m     +  0.0000  12.7863
      3      [36m0.9811[0m        [32m0.1767[0m       0.2844      [31m0.6074[0m        [94m0.5986[0m     +  0.0000  12.5490
      4      [36m0.9841[0m        [32m0.1693[0m       0.2740      0.6032        [94m0.5954[0m     +  0.0000  12.8441
      5      0.9821        [32m0.1677[0m       0.2854      [31m0.6211[0m        0.5974        0.0000  12.4732
      6      [36m0.9852[0m        [32m0.1570[0m       0.2769      0.6182        0.6020        0.0000  12.7383
      7      [36m0.9882[0m        0.1581       0.2727      0.6133        0.6040        0.0000  12.8655
      8      0.9876        [32m0.1511[0m       0.2710      0.6113        0.6057        0.0000  12.7717
      9      0.9863        0.1525       0.2767      0.6178        0.6101        0.0000  12.5990
     10      [36m0.9884[0m        0.1515       0.2724      0.6131        0.6109        0.0000  12.4258
     11      [36m0.9895[0m        [32m0.1415[0m       0.2757      0.6166        0.6128        0.0000  12.7241
     12      [36m0.9895[0m        0.1461       0.2727      0.6141        0.6142        0.0000  12.6930
     13      [36m0.9901[0m        0.1474       0.2741      0.6152        0.6149        0.0000  12.5061
     14      0.9896        0.1428       0.2759      0.6165        0.6144        0.0000  12.5834
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.6118515737438208
F1 Macro Score after query 5: 0.6213208703171632
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_4.pt

Iteration: 6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9458[0m        [32m0.2194[0m       [35m0.2993[0m      [31m0.6447[0m        [94m0.6219[0m     +  0.0000  13.9274
      2      [36m0.9704[0m        [32m0.1796[0m       0.2811      0.6337        [94m0.6028[0m     +  0.0000  13.6182
      3      [36m0.9747[0m        [32m0.1690[0m       0.2663      0.6235        [94m0.5994[0m     +  0.0000  13.8027
      4      [36m0.9773[0m        [32m0.1601[0m       0.2819      0.6366        0.6033        0.0000  13.8194
      5      [36m0.9785[0m        [32m0.1547[0m       0.2682      0.6165        0.6082        0.0000  13.8820
      6      [36m0.9833[0m        [32m0.1467[0m       0.2821      0.6343        0.6040        0.0000  14.0071
      7      [36m0.9863[0m        [32m0.1409[0m       0.2873      0.6391        0.6009        0.0000  14.0199
      8      [36m0.9866[0m        0.1420       0.2922      0.6421        0.6061        0.0000  13.8483
      9      0.9861        [32m0.1374[0m       0.2953      0.6445        0.6060        0.0000  13.4590
     10      [36m0.9882[0m        [32m0.1354[0m       0.2990      [31m0.6476[0m        0.6072        0.0000  13.8667
     11      0.9875        [32m0.1327[0m       0.2981      [31m0.6481[0m        0.6063        0.0000  13.8820
     12      [36m0.9892[0m        [32m0.1281[0m       0.2984      [31m0.6482[0m        0.6075        0.0000  13.6938
     13      0.9882        0.1321       [35m0.3033[0m      [31m0.6516[0m        0.6100        0.0000  13.7850
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.6250858462883186
F1 Macro Score after query 6: 0.633036882663797
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_5.pt

Iteration: 7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9709[0m        [32m0.1715[0m       [35m0.3288[0m      [31m0.6573[0m        [94m0.6605[0m     +  0.0000  15.9593
      2      [36m0.9817[0m        [32m0.1495[0m       [35m0.3401[0m      [31m0.6680[0m        [94m0.6471[0m     +  0.0000  16.1939
      3      [36m0.9828[0m        [32m0.1400[0m       [35m0.3429[0m      [31m0.6728[0m        [94m0.6369[0m     +  0.0000  16.0858
      4      [36m0.9843[0m        [32m0.1336[0m       [35m0.3587[0m      [31m0.6831[0m        0.6562        0.0000  16.1017
      5      [36m0.9856[0m        [32m0.1265[0m       [35m0.3615[0m      [31m0.6878[0m        0.6620        0.0000  15.7804
      6      [36m0.9879[0m        [32m0.1213[0m       0.3590      0.6854        0.6380        0.0000  16.5398
      7      0.9878        [32m0.1172[0m       0.3599      0.6846        0.6388        0.0000  16.2411
      8      [36m0.9890[0m        [32m0.1136[0m       0.3578      0.6841        0.6409        0.0000  16.3397
      9      [36m0.9900[0m        [32m0.1109[0m       0.3530      0.6810        0.6417        0.0000  15.9440
     10      [36m0.9907[0m        [32m0.1090[0m       0.3568      0.6839        0.6436        0.0000  16.1511
     11      0.9903        [32m0.1051[0m       0.3453      0.6740        0.6466        0.0000  15.7101
     12      0.9907        [32m0.1031[0m       0.3455      0.6748        0.6465        0.0000  15.9582
     13      [36m0.9912[0m        0.1038       0.3429      0.6726        0.6465        0.0000  16.1638
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.6438339731285989
F1 Macro Score after query 7: 0.660835708711864
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_6.pt

Iteration: 8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9634[0m        [32m0.1699[0m       [35m0.3684[0m      [31m0.7002[0m        [94m0.5623[0m     +  0.0000  20.0683
      2      [36m0.9726[0m        [32m0.1428[0m       [35m0.3734[0m      [31m0.7112[0m        0.5835        0.0000  19.9018
      3      [36m0.9754[0m        [32m0.1341[0m       0.3712      [31m0.7138[0m        0.6035        0.0000  19.8265
      4      [36m0.9778[0m        [32m0.1246[0m       [35m0.3738[0m      0.7128        0.6281        0.0000  19.9015
      5      [36m0.9811[0m        [32m0.1185[0m       0.3628      0.7102        0.6355        0.0000  19.9788
      6      [36m0.9812[0m        [32m0.1105[0m       0.3686      0.7101        0.6521        0.0000  20.3219
      7      [36m0.9827[0m        [32m0.1069[0m       0.3727      0.7119        0.6484        0.0000  19.7901
      8      [36m0.9840[0m        [32m0.1031[0m       0.3738      0.7105        0.6509        0.0000  19.6646
      9      [36m0.9853[0m        [32m0.0995[0m       0.3731      0.7023        0.6598        0.0000  19.5106
     10      0.9850        [32m0.0966[0m       [35m0.3757[0m      0.7011        0.6766        0.0000  19.4797
     11      [36m0.9864[0m        [32m0.0929[0m       0.3727      0.6999        0.6953        0.0000  19.5414
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.674882287973028
F1 Macro Score after query 8: 0.6900981678394612
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_7.pt

Iteration: 9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9704[0m        [32m0.1512[0m       [35m0.3653[0m      [31m0.7050[0m        [94m0.5741[0m     +  0.0000  26.3098
      2      [36m0.9755[0m        [32m0.1314[0m       0.3606      [31m0.7158[0m        0.5873        0.0000  26.4240
      3      [36m0.9783[0m        [32m0.1205[0m       [35m0.3729[0m      [31m0.7277[0m        0.5784        0.0000  26.9367
      4      [36m0.9788[0m        [32m0.1117[0m       0.3571      0.7153        0.6123        0.0000  26.6243
      5      [36m0.9812[0m        [32m0.1034[0m       0.3618      0.7234        0.6150        0.0000  26.5887
      6      [36m0.9827[0m        [32m0.0953[0m       [35m0.3851[0m      [31m0.7293[0m        0.5921        0.0000  26.5432
      7      [36m0.9835[0m        [32m0.0916[0m       0.3724      0.7130        0.6130        0.0000  26.5271
      8      [36m0.9835[0m        [32m0.0887[0m       0.3753      0.7141        0.6197        0.0000  26.4353
      9      [36m0.9854[0m        [32m0.0851[0m       0.3644      0.7069        0.6320        0.0000  26.6414
     10      [36m0.9855[0m        [32m0.0827[0m       0.3635      0.7076        0.6424        0.0000  26.5449
     11      [36m0.9870[0m        [32m0.0787[0m       0.3741      0.7140        0.6338        0.0000  26.4984
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.7081074711289183
F1 Macro Score after query 9: 0.7205180799781781
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_8.pt

Iteration: 10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9808[0m        [32m0.1284[0m       [35m0.4141[0m      [31m0.7980[0m        [94m0.4622[0m     +  0.0000  38.4231
      2      [36m0.9844[0m        [32m0.1079[0m       0.4066      0.7903        0.4779        0.0000  38.7553
      3      [36m0.9859[0m        [32m0.0975[0m       0.3976      0.7907        0.4960        0.0000  38.9290
      4      [36m0.9871[0m        [32m0.0862[0m       0.4007      0.7896        0.4953        0.0000  39.0632
      5      [36m0.9873[0m        [32m0.0797[0m       0.3965      0.7881        0.5087        0.0000  39.0033
      6      [36m0.9894[0m        [32m0.0704[0m       0.3693      0.7707        0.5557        0.0000  38.6952
      7      [36m0.9896[0m        [32m0.0664[0m       0.3766      0.7707        0.5458        0.0000  38.7662
      8      [36m0.9902[0m        [32m0.0629[0m       0.3719      0.7673        0.5554        0.0000  38.7661
      9      [36m0.9912[0m        [32m0.0606[0m       0.3644      0.7587        0.5802        0.0000  38.8823
     10      [36m0.9914[0m        [32m0.0583[0m       0.3752      0.7659        0.5631        0.0000  38.7557
     11      [36m0.9929[0m        [32m0.0536[0m       0.3781      0.7688        0.5631        0.0000  38.6340
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.7432082404674416
F1 Macro Score after query 10: 0.7688300426478287
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_9.pt

Iteration: 11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9671[0m        [32m0.1552[0m       [35m0.7774[0m      [31m0.8846[0m        [94m0.2443[0m     +  0.0000  60.6951
      2      [36m0.9758[0m        [32m0.1224[0m       [35m0.7830[0m      [31m0.8883[0m        [94m0.2337[0m     +  0.0000  60.5555
      3      [36m0.9786[0m        [32m0.1057[0m       0.7785      0.8846        [94m0.2311[0m     +  0.0000  61.0642
      4      [36m0.9810[0m        [32m0.0943[0m       0.7812      [31m0.8885[0m        [94m0.2270[0m     +  0.0000  60.5148
      5      [36m0.9824[0m        [32m0.0852[0m       [35m0.7922[0m      [31m0.8905[0m        [94m0.2189[0m     +  0.0000  60.9569
      6      [36m0.9851[0m        [32m0.0741[0m       [35m0.7957[0m      [31m0.8920[0m        [94m0.2133[0m     +  0.0000  60.5448
      7      [36m0.9860[0m        [32m0.0701[0m       [35m0.8076[0m      [31m0.8950[0m        [94m0.2082[0m     +  0.0000  60.1812
      8      [36m0.9871[0m        [32m0.0665[0m       0.8064      0.8937        0.2101        0.0000  60.7041
      9      [36m0.9874[0m        [32m0.0630[0m       [35m0.8089[0m      0.8942        0.2094        0.0000  60.7331
     10      [36m0.9887[0m        [32m0.0600[0m       0.8082      0.8932        0.2092        0.0000  60.9863
     11      [36m0.9900[0m        [32m0.0554[0m       0.8043      0.8897        0.2134        0.0000  60.9213
     12      [36m0.9909[0m        [32m0.0537[0m       0.8059      0.8903        0.2110        0.0000  60.7824
     13      [36m0.9911[0m        [32m0.0520[0m       0.8035      0.8879        0.2145        0.0000  60.7508
     14      [36m0.9915[0m        [32m0.0501[0m       0.8030      0.8877        0.2171        0.0000  60.7198
     15      [36m0.9919[0m        [32m0.0490[0m       0.8035      0.8872        0.2164        0.0000  60.8141
     16      [36m0.9930[0m        [32m0.0470[0m       0.8059      0.8881        0.2155        0.0000  60.6714
     17      [36m0.9933[0m        [32m0.0457[0m       0.8047      0.8874        0.2171        0.0000  60.7009
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.9214964734743943
F1 Macro Score after query 11: 0.9108806741963887
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_10.pt

Iteration: 12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9760[0m        [32m0.0805[0m       [35m0.8321[0m      [31m0.8937[0m        [94m0.1788[0m     +  0.0000  98.8794
      2      [36m0.9804[0m        [32m0.0625[0m       [35m0.8358[0m      [31m0.9014[0m        [94m0.1720[0m     +  0.0000  99.3158
      3      [36m0.9824[0m        [32m0.0530[0m       0.8281      0.8961        0.1913        0.0000  99.9871
      4      [36m0.9843[0m        [32m0.0462[0m       0.8321      0.8984        0.1921        0.0000  99.4241
      5      [36m0.9851[0m        [32m0.0413[0m       [35m0.8365[0m      0.9006        0.1872        0.0000  98.4251
      6      [36m0.9892[0m        [32m0.0335[0m       [35m0.8398[0m      0.8987        0.1858        0.0000  99.9401
      7      [36m0.9908[0m        [32m0.0305[0m       0.8359      0.8908        0.1961        0.0000  99.8631
      8      [36m0.9915[0m        [32m0.0278[0m       0.8337      0.8920        0.2061        0.0000  100.0496
      9      [36m0.9927[0m        [32m0.0257[0m       0.8297      0.8923        0.2134        0.0000  99.5922
     10      [36m0.9933[0m        [32m0.0236[0m       0.8264      0.8839        0.2185        0.0000  99.6054
     11      [36m0.9950[0m        [32m0.0207[0m       0.8285      0.8956        0.2251        0.0000  99.6342
     12      [36m0.9958[0m        [32m0.0192[0m       0.8293      0.8953        0.2295        0.0000  99.6319
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.9298559750875828
F1 Macro Score after query 12: 0.9182888241029655
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_11.pt

Iteration: 13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9807[0m        [32m0.0517[0m       [35m0.8351[0m      [31m0.9057[0m        [94m0.1759[0m     +  0.0000  114.4392
      2      [36m0.9833[0m        [32m0.0435[0m       [35m0.8408[0m      0.9001        0.1762        0.0000  115.1407
      3      [36m0.9846[0m        [32m0.0386[0m       [35m0.8422[0m      0.9016        0.1784        0.0000  115.1408
      4      [36m0.9854[0m        [32m0.0347[0m       0.8389      0.8996        0.1854        0.0000  115.5666
      5      [36m0.9867[0m        [32m0.0306[0m       0.8347      0.8922        0.1978        0.0000  114.7893
      6      [36m0.9900[0m        [32m0.0252[0m       0.8318      0.8932        0.2006        0.0000  115.2231
      7      [36m0.9920[0m        [32m0.0224[0m       0.8380      0.8913        0.2136        0.0000  115.8693
      8      [36m0.9932[0m        [32m0.0201[0m       0.8351      0.8900        0.2217        0.0000  115.8785
      9      [36m0.9942[0m        [32m0.0182[0m       0.8380      0.8951        0.2203        0.0000  116.5155
     10      [36m0.9946[0m        [32m0.0168[0m       0.8366      0.8954        0.2326        0.0000  116.5468
     11      [36m0.9961[0m        [32m0.0144[0m       0.8365      0.8973        0.2368        0.0000  115.1828
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.9328485223580382
F1 Macro Score after query 13: 0.9207171737687294
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\model_checkpoint_iteration_12.pt
Pickle file saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed46_lowLR\AL_max_score_results_for_multilabel_classification_s46.pickle
