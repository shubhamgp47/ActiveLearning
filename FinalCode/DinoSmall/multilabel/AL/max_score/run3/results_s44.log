Adjusted shapes after loading:
X_initial_np: (8, 224, 224, 3), y_initial_np: (8, 3)
X_pool_np: (26872, 224, 224, 3), y_pool_np: (26872, 3)
X_test_np: (5760, 224, 224, 3), y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3), y_val_np: (5760, 3)
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.5460[0m        [32m0.7305[0m       [35m0.1863[0m      [31m0.4349[0m        [94m0.7025[0m     +  0.0000  12.1285
      2      [36m0.7190[0m        [32m0.6576[0m       [35m0.2182[0m      0.2878        [94m0.6934[0m     +  0.0000  10.1939
      3      0.6389        [32m0.6449[0m       [35m0.2687[0m      [31m0.4495[0m        [94m0.6918[0m     +  0.0000  10.2354
      4      0.5913        [32m0.6354[0m       [35m0.2983[0m      0.4066        [94m0.6754[0m     +  0.0000  10.2929
      5      [36m0.9048[0m        [32m0.5740[0m       0.2793      [31m0.4647[0m        0.6930        0.0000  10.3281
      6      [36m0.9259[0m        [32m0.5448[0m       0.2785      [31m0.4678[0m        0.6885        0.0000  10.2803
      7      0.8201        [32m0.5390[0m       0.2720      0.4648        0.6880        0.0000  10.3592
      8      0.8320        0.5509       0.2694      [31m0.4776[0m        0.6883        0.0000  10.3329
      9      0.8130        0.5648       0.2823      0.4742        0.6833        0.0000  10.6856
     10      [36m0.9630[0m        [32m0.5210[0m       0.2793      [31m0.4887[0m        0.6835        0.0000  10.4142
     11      0.8796        0.5431       0.2795      0.4866        0.6838        0.0000  10.3268
     12      0.8519        [32m0.5153[0m       0.2839      0.4838        0.6831        0.0000  10.5178
     13      0.9259        [32m0.4990[0m       0.2877      [31m0.4894[0m        0.6845        0.0000  10.3708
     14      0.9259        [32m0.4931[0m       0.2835      [31m0.4896[0m        0.6848        0.0000  10.4844
Stopping since valid_loss has not improved in the last 11 epochs.
Pre F1 micro score = 0.5246
Pre F1 macro score = 0.5115
Pre Accuracy = 0.3703

Iteration: 1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.6535[0m        [32m0.6215[0m       [35m0.1649[0m      [31m0.3144[0m        [94m0.6488[0m     +  0.0000  10.4858
      2      0.6008        [32m0.5982[0m       [35m0.3253[0m      [31m0.3953[0m        [94m0.6413[0m     +  0.0000  10.5270
      3      [36m0.6973[0m        [32m0.5489[0m       0.2924      [31m0.4569[0m        0.6421        0.0000  10.5937
      4      [36m0.7031[0m        [32m0.5347[0m       [35m0.3418[0m      0.4227        [94m0.6332[0m     +  0.0000  10.5914
      5      [36m0.7871[0m        [32m0.5066[0m       0.3045      0.4540        [94m0.6316[0m     +  0.0000  10.4415
      6      [36m0.8085[0m        [32m0.4887[0m       0.2951      [31m0.5045[0m        0.6317        0.0000  10.4881
      7      0.7740        [32m0.4612[0m       0.3069      [31m0.5054[0m        0.6319        0.0000  10.4999
      8      [36m0.8244[0m        [32m0.4379[0m       0.3181      0.5048        [94m0.6300[0m     +  0.0000  10.5120
      9      [36m0.8419[0m        [32m0.4311[0m       0.3219      [31m0.5074[0m        0.6301        0.0000  10.4699
     10      0.8329        0.4339       0.3236      0.5067        [94m0.6296[0m     +  0.0000  10.4689
     11      [36m0.8549[0m        [32m0.3985[0m       0.3243      0.5071        [94m0.6293[0m     +  0.0000  10.5126
     12      0.8230        0.4094       0.3300      [31m0.5091[0m        0.6294        0.0000  10.4818
     13      [36m0.9008[0m        0.4031       0.3340      0.5090        [94m0.6288[0m     +  0.0000  10.4566
     14      [36m0.9087[0m        0.4038       0.3314      [31m0.5113[0m        0.6292        0.0000  10.7836
     15      0.8982        0.4034       0.3345      0.5103        [94m0.6285[0m     +  0.0000  10.5274
     16      [36m0.9115[0m        [32m0.3954[0m       0.3361      0.5107        [94m0.6285[0m     +  0.0000  10.5157
     17      0.8943        0.3957       0.3368      0.5108        [94m0.6284[0m     +  0.0000  10.4871
     18      0.8857        [32m0.3717[0m       0.3373      0.5107        0.6285        0.0000  10.4994
     19      0.8973        0.3928       0.3356      0.5111        [94m0.6281[0m     +  0.0000  10.4999
     20      0.8848        0.3936       0.3354      0.5109        [94m0.6278[0m     +  0.0000  10.4337
     21      0.8704        0.4037       0.3372      0.5109        0.6279        0.0000  10.5007
     22      [36m0.9188[0m        0.3907       0.3375      0.5111        [94m0.6278[0m     +  0.0000  10.4532
     23      0.9087        0.3758       0.3382      0.5111        [94m0.6275[0m     +  0.0000  10.4340
     24      0.8373        0.3985       0.3385      0.5111        [94m0.6275[0m     +  0.0000  10.5569
     25      0.8848        0.3778       0.3398      [31m0.5114[0m        [94m0.6274[0m     +  0.0000  10.8441
     26      0.8419        0.3957       0.3392      [31m0.5115[0m        [94m0.6274[0m     +  0.0000  11.0678
     27      0.9087        0.3949       0.3398      [31m0.5116[0m        0.6274        0.0000  10.7037
     28      0.8620        0.3924       0.3385      [31m0.5117[0m        0.6274        0.0000  10.1605
     29      0.9188        0.3760       0.3391      [31m0.5118[0m        0.6274        0.0000  9.9547
     30      0.8867        0.3777       0.3392      [31m0.5121[0m        0.6274        0.0000  9.9297
     31      [36m0.9272[0m        0.3912       0.3387      0.5118        0.6274        0.0000  9.9731
     32      0.9066        0.3972       0.3387      0.5119        0.6274        0.0000  9.9405
     33      0.8638        0.3898       0.3387      0.5119        0.6274        0.0000  9.9248
     34      0.9243        0.3815       0.3385      0.5119        [94m0.6273[0m     +  0.0000  9.9830
     35      0.8712        0.4131       0.3384      0.5119        [94m0.6273[0m     +  0.0000  9.9408
Stopping since valid_loss has not improved in the last 11 epochs.
[24]
F1 Micro Score after query 1: 0.5823760944654189
F1 Macro Score after query 1: 0.5548739313607646
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_0.pt

Iteration: 2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8642[0m        [32m0.4212[0m       [35m0.2266[0m      [31m0.5674[0m        [94m0.6840[0m     +  0.0000  10.0769
      2      0.8610        [32m0.3702[0m       0.2149      0.5244        [94m0.6449[0m     +  0.0000  10.0218
      3      [36m0.9027[0m        [32m0.3207[0m       [35m0.2316[0m      0.5337        [94m0.6332[0m     +  0.0000  9.9802
      4      [36m0.9608[0m        [32m0.2902[0m       [35m0.2634[0m      0.5363        [94m0.6225[0m     +  0.0000  10.0966
      5      0.9237        0.2980       [35m0.2700[0m      0.5641        [94m0.6144[0m     +  0.0000  9.9937
      6      [36m0.9713[0m        [32m0.2664[0m       [35m0.2939[0m      0.5594        0.6169        0.0000  10.0637
      7      0.9636        [32m0.2630[0m       [35m0.2990[0m      0.5579        [94m0.6125[0m     +  0.0000  10.1777
      8      0.9610        [32m0.2536[0m       [35m0.3049[0m      0.5579        [94m0.6114[0m     +  0.0000  9.9970
      9      [36m0.9751[0m        [32m0.2496[0m       [35m0.3104[0m      0.5598        [94m0.6110[0m     +  0.0000  10.0314
     10      [36m0.9861[0m        [32m0.2435[0m       [35m0.3123[0m      0.5604        [94m0.6104[0m     +  0.0000  10.0464
     11      0.9750        [32m0.2390[0m       0.3120      0.5633        0.6119        0.0000  10.0146
     12      0.9861        [32m0.2340[0m       0.3102      0.5645        0.6134        0.0000  10.0045
     13      0.9750        0.2571       0.3118      0.5659        0.6133        0.0000  10.0613
     14      0.9787        0.2383       [35m0.3134[0m      0.5630        0.6115        0.0000  10.0471
     15      0.9677        0.2349       [35m0.3139[0m      0.5624        0.6117        0.0000  10.0269
     16      0.9758        [32m0.2292[0m       [35m0.3146[0m      0.5642        0.6127        0.0000  10.0361
     17      0.9824        0.2371       [35m0.3156[0m      0.5640        0.6127        0.0000  10.0085
     18      0.9824        0.2468       0.3151      0.5652        0.6132        0.0000  10.0385
     19      0.9861        0.2348       0.3149      0.5648        0.6126        0.0000  10.3596
     20      0.9787        0.2357       [35m0.3160[0m      0.5643        0.6122        0.0000  10.2914
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56]
F1 Micro Score after query 2: 0.586112708453134
F1 Macro Score after query 2: 0.572217828842782
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_1.pt

Iteration: 3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9000[0m        [32m0.2762[0m       [35m0.2217[0m      [31m0.5146[0m        [94m0.6294[0m     +  0.0000  10.7212
      2      [36m0.9313[0m        [32m0.2222[0m       [35m0.2448[0m      [31m0.5601[0m        [94m0.6028[0m     +  0.0000  10.7649
      3      [36m0.9532[0m        [32m0.2007[0m       [35m0.2679[0m      [31m0.5742[0m        [94m0.5929[0m     +  0.0000  10.7767
      4      [36m0.9709[0m        [32m0.1891[0m       [35m0.2799[0m      [31m0.5844[0m        [94m0.5903[0m     +  0.0000  10.8010
      5      0.9709        [32m0.1839[0m       0.2682      0.5793        0.5917        0.0000  10.6053
      6      0.9709        [32m0.1794[0m       [35m0.2965[0m      [31m0.5908[0m        [94m0.5870[0m     +  0.0000  10.7571
      7      [36m0.9760[0m        [32m0.1736[0m       [35m0.3068[0m      0.5885        [94m0.5849[0m     +  0.0000  10.8879
      8      [36m0.9777[0m        [32m0.1712[0m       0.3045      [31m0.5919[0m        0.5862        0.0000  10.9094
      9      [36m0.9793[0m        [32m0.1679[0m       [35m0.3151[0m      [31m0.5949[0m        [94m0.5848[0m     +  0.0000  10.8434
     10      0.9793        0.1703       [35m0.3170[0m      [31m0.5955[0m        [94m0.5839[0m     +  0.0000  10.7006
     11      0.9777        [32m0.1648[0m       [35m0.3186[0m      [31m0.5983[0m        [94m0.5838[0m     +  0.0000  10.7786
     12      0.9793        0.1684       0.3182      [31m0.5984[0m        0.5840        0.0000  10.7118
     13      [36m0.9838[0m        [32m0.1610[0m       0.3170      [31m0.6001[0m        0.5844        0.0000  10.7450
     14      [36m0.9855[0m        0.1622       [35m0.3198[0m      0.5988        0.5843        0.0000  10.7292
     15      0.9855        [32m0.1531[0m       [35m0.3201[0m      0.5991        0.5841        0.0000  10.7471
     16      0.9793        0.1587       [35m0.3219[0m      0.5995        0.5840        0.0000  10.7647
     17      0.9810        0.1588       0.3214      [31m0.6001[0m        [94m0.5838[0m     +  0.0000  10.7499
     18      [36m0.9902[0m        0.1592       [35m0.3238[0m      [31m0.6006[0m        [94m0.5836[0m     +  0.0000  10.6413
     19      0.9838        0.1563       0.3205      0.6001        0.5838        0.0000  10.7200
     20      0.9855        0.1589       0.3224      0.6003        [94m0.5831[0m     +  0.0000  10.6993
     21      0.9855        0.1595       0.3227      [31m0.6012[0m        0.5831        0.0000  10.7709
     22      0.9787        0.1572       0.3231      0.6011        0.5832        0.0000  10.7391
     23      0.9855        0.1543       0.3220      0.6008        0.5833        0.0000  10.7376
     24      0.9855        0.1570       0.3224      0.6008        0.5832        0.0000  10.7186
     25      0.9838        0.1557       [35m0.3241[0m      [31m0.6017[0m        0.5832        0.0000  10.7814
     26      0.9885        0.1608       0.3241      [31m0.6018[0m        0.5833        0.0000  10.6875
     27      0.9855        0.1538       0.3236      [31m0.6019[0m        0.5833        0.0000  10.6573
     28      0.9885        0.1533       0.3240      [31m0.6020[0m        0.5834        0.0000  10.8739
     29      0.9793        0.1611       [35m0.3247[0m      [31m0.6022[0m        0.5833        0.0000  10.8106
     30      0.9855        0.1534       [35m0.3248[0m      [31m0.6023[0m        0.5833        0.0000  10.8405
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112]
F1 Micro Score after query 3: 0.6195838084378563
F1 Macro Score after query 3: 0.6151792031905058
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_2.pt

Iteration: 4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9028[0m        [32m0.2124[0m       [35m0.2660[0m      [31m0.5767[0m        [94m0.6031[0m     +  0.0000  11.5515
      2      [36m0.9339[0m        [32m0.1816[0m       0.2535      0.5704        [94m0.5811[0m     +  0.0000  11.3713
      3      [36m0.9508[0m        [32m0.1665[0m       0.2516      0.5703        [94m0.5796[0m     +  0.0000  11.7130
      4      [36m0.9509[0m        [32m0.1635[0m       0.2547      [31m0.5811[0m        [94m0.5766[0m     +  0.0000  11.2677
      5      [36m0.9610[0m        [32m0.1521[0m       0.2630      0.5758        [94m0.5739[0m     +  0.0000  11.3135
      6      [36m0.9704[0m        [32m0.1464[0m       [35m0.2809[0m      [31m0.5824[0m        [94m0.5687[0m     +  0.0000  11.6722
      7      0.9677        0.1482       [35m0.2875[0m      [31m0.5834[0m        [94m0.5677[0m     +  0.0000  11.2500
      8      [36m0.9778[0m        [32m0.1383[0m       [35m0.2882[0m      0.5801        0.5685        0.0000  11.4862
      9      0.9683        0.1396       [35m0.2905[0m      0.5828        [94m0.5673[0m     +  0.0000  11.3629
     10      0.9756        [32m0.1381[0m       [35m0.2960[0m      0.5816        [94m0.5670[0m     +  0.0000  11.1349
     11      [36m0.9787[0m        [32m0.1372[0m       [35m0.3005[0m      [31m0.5911[0m        [94m0.5639[0m     +  0.0000  11.5026
     12      0.9752        [32m0.1330[0m       0.2995      [31m0.5931[0m        [94m0.5639[0m     +  0.0000  11.3640
     13      0.9752        0.1368       0.2990      0.5915        0.5639        0.0000  11.4249
     14      [36m0.9795[0m        0.1340       0.2976      0.5913        [94m0.5638[0m     +  0.0000  11.4250
     15      [36m0.9846[0m        0.1330       0.2986      0.5929        [94m0.5630[0m     +  0.0000  11.3298
     16      0.9760        [32m0.1284[0m       0.3003      [31m0.5958[0m        [94m0.5624[0m     +  0.0000  11.3629
     17      0.9787        0.1337       0.2995      0.5949        [94m0.5621[0m     +  0.0000  11.4050
     18      0.9752        0.1334       0.3005      [31m0.5972[0m        [94m0.5620[0m     +  0.0000  11.3223
     19      0.9795        0.1338       [35m0.3009[0m      0.5965        [94m0.5615[0m     +  0.0000  11.4257
     20      0.9752        0.1302       0.2998      [31m0.5974[0m        [94m0.5611[0m     +  0.0000  11.4504
     21      0.9821        0.1318       0.3003      [31m0.5979[0m        [94m0.5610[0m     +  0.0000  11.2830
     22      0.9787        0.1304       0.3003      [31m0.5985[0m        [94m0.5610[0m     +  0.0000  11.2944
     23      0.9821        0.1363       [35m0.3010[0m      0.5984        [94m0.5608[0m     +  0.0000  11.5295
     24      0.9743        0.1344       0.3010      0.5983        [94m0.5607[0m     +  0.0000  11.6027
     25      0.9778        0.1307       [35m0.3021[0m      [31m0.5987[0m        [94m0.5606[0m     +  0.0000  11.5323
     26      0.9824        [32m0.1273[0m       [35m0.3024[0m      0.5986        [94m0.5605[0m     +  0.0000  11.4185
     27      0.9787        0.1315       [35m0.3026[0m      0.5985        [94m0.5604[0m     +  0.0000  11.5032
     28      0.9808        0.1293       0.3026      0.5984        0.5605        0.0000  11.2531
     29      0.9846        0.1316       [35m0.3031[0m      0.5983        [94m0.5604[0m     +  0.0000  11.4635
     30      0.9795        0.1314       [35m0.3033[0m      [31m0.5987[0m        [94m0.5603[0m     +  0.0000  11.3607
     31      [36m0.9855[0m        0.1289       [35m0.3035[0m      [31m0.5988[0m        0.5603        0.0000  11.3052
     32      0.9846        [32m0.1260[0m       0.3033      0.5987        [94m0.5602[0m     +  0.0000  11.5469
     33      0.9833        0.1286       0.3033      0.5987        [94m0.5602[0m     +  0.0000  11.3349
     34      0.9855        0.1296       0.3033      0.5987        0.5603        0.0000  11.3265
     35      0.9830        0.1280       0.3033      0.5987        0.5602        0.0000  11.3942
     36      0.9787        0.1371       0.3031      0.5986        0.5602        0.0000  11.2814
     37      0.9804        0.1327       0.3031      0.5986        0.5602        0.0000  11.4232
     38      0.9787        0.1312       0.3031      0.5986        [94m0.5602[0m     +  0.0000  11.4292
     39      0.9830        0.1271       0.3031      0.5986        [94m0.5602[0m     +  0.0000  11.5309
     40      0.9846        0.1279       0.3031      0.5986        [94m0.5602[0m     +  0.0000  11.5141
     41      0.9778        0.1312       0.3031      0.5986        [94m0.5602[0m     +  0.0000  11.5038
     42      0.9803        0.1281       0.3031      0.5986        [94m0.5602[0m     +  0.0000  11.4219
     43      0.9778        [32m0.1236[0m       0.3031      0.5986        0.5602        0.0000  11.2675
     44      0.9804        0.1269       0.3031      0.5986        [94m0.5602[0m     +  0.0000  11.3145
     45      0.9791        0.1253       0.3031      0.5986        [94m0.5601[0m     +  0.0000  11.4846
     46      0.9787        0.1260       0.3030      0.5987        [94m0.5601[0m     +  0.0000  11.2606
     47      0.9787        0.1317       0.3030      0.5987        [94m0.5601[0m     +  0.0000  11.2663
     48      0.9830        0.1321       0.3030      0.5987        [94m0.5601[0m     +  0.0000  11.4070
     49      0.9812        0.1268       0.3030      [31m0.5989[0m        [94m0.5601[0m     +  0.0000  11.3538
     50      0.9838        0.1275       0.3030      0.5989        [94m0.5601[0m     +  0.0000  11.4220
     51      0.9743        0.1306       0.3030      0.5989        [94m0.5601[0m     +  0.0000  11.6559
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208]
F1 Micro Score after query 4: 0.6260099768144454
F1 Macro Score after query 4: 0.6234319243391157
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_3.pt

Iteration: 5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9135[0m        [32m0.1933[0m       [35m0.2733[0m      [31m0.5654[0m        [94m0.5635[0m     +  0.0000  12.0971
      2      [36m0.9501[0m        [32m0.1552[0m       [35m0.2873[0m      [31m0.5843[0m        [94m0.5594[0m     +  0.0000  12.0981
      3      [36m0.9563[0m        [32m0.1470[0m       [35m0.2991[0m      [31m0.5861[0m        [94m0.5559[0m     +  0.0000  11.9635
      4      [36m0.9655[0m        [32m0.1414[0m       [35m0.3116[0m      [31m0.6067[0m        [94m0.5441[0m     +  0.0000  11.9725
      5      [36m0.9731[0m        [32m0.1312[0m       0.2977      0.6014        0.5525        0.0000  11.9672
      6      [36m0.9768[0m        [32m0.1213[0m       0.3108      [31m0.6180[0m        [94m0.5429[0m     +  0.0000  12.0039
      7      [36m0.9808[0m        [32m0.1198[0m       [35m0.3182[0m      [31m0.6270[0m        [94m0.5390[0m     +  0.0000  12.2129
      8      [36m0.9830[0m        [32m0.1195[0m       0.3167      0.6204        0.5418        0.0000  12.0935
      9      0.9815        [32m0.1190[0m       [35m0.3217[0m      [31m0.6270[0m        0.5408        0.0000  12.3150
     10      [36m0.9890[0m        [32m0.1135[0m       [35m0.3236[0m      0.6243        [94m0.5383[0m     +  0.0000  12.1704
     11      0.9872        0.1154       [35m0.3295[0m      [31m0.6347[0m        [94m0.5356[0m     +  0.0000  12.1698
     12      0.9885        0.1155       [35m0.3299[0m      0.6328        0.5362        0.0000  12.0787
     13      [36m0.9904[0m        [32m0.1107[0m       [35m0.3312[0m      0.6330        [94m0.5355[0m     +  0.0000  12.0552
     14      [36m0.9918[0m        [32m0.1100[0m       0.3309      0.6342        [94m0.5350[0m     +  0.0000  12.0978
     15      [36m0.9936[0m        [32m0.1066[0m       0.3306      [31m0.6366[0m        [94m0.5345[0m     +  0.0000  12.1719
     16      0.9909        [32m0.1039[0m       [35m0.3339[0m      [31m0.6417[0m        [94m0.5330[0m     +  0.0000  12.2806
     17      0.9927        0.1074       0.3312      0.6416        0.5334        0.0000  11.9508
     18      [36m0.9964[0m        0.1077       0.3326      0.6416        [94m0.5328[0m     +  0.0000  12.0420
     19      0.9914        0.1074       0.3321      0.6409        0.5329        0.0000  12.1561
     20      0.9950        0.1041       0.3332      [31m0.6421[0m        0.5331        0.0000  12.1555
     21      0.9950        0.1040       [35m0.3344[0m      [31m0.6446[0m        [94m0.5323[0m     +  0.0000  12.0303
     22      0.9927        0.1059       [35m0.3345[0m      [31m0.6450[0m        [94m0.5322[0m     +  0.0000  12.1295
     23      0.9946        0.1060       0.3337      0.6428        0.5324        0.0000  11.9950
     24      0.9936        [32m0.1022[0m       0.3344      0.6447        [94m0.5316[0m     +  0.0000  12.0976
     25      0.9936        0.1071       0.3344      0.6438        [94m0.5314[0m     +  0.0000  12.0156
     26      [36m0.9973[0m        0.1040       [35m0.3347[0m      0.6447        [94m0.5312[0m     +  0.0000  12.1119
     27      0.9946        0.1034       [35m0.3359[0m      [31m0.6462[0m        [94m0.5311[0m     +  0.0000  12.1007
     28      0.9969        [32m0.1011[0m       [35m0.3361[0m      [31m0.6464[0m        0.5312        0.0000  12.2979
     29      0.9955        0.1034       [35m0.3363[0m      [31m0.6466[0m        0.5312        0.0000  12.3746
     30      0.9950        0.1039       0.3363      0.6461        0.5312        0.0000  12.4615
     31      0.9936        0.1033       [35m0.3366[0m      0.6465        0.5312        0.0000  12.6415
     32      0.9937        0.1021       0.3365      0.6461        0.5312        0.0000  12.6408
     33      0.9932        0.1052       0.3365      0.6465        0.5311        0.0000  12.5206
     34      0.9923        0.1033       0.3366      [31m0.6466[0m        [94m0.5310[0m     +  0.0000  12.5328
     35      0.9946        0.1066       0.3365      0.6464        0.5311        0.0000  12.6091
     36      0.9922        0.1025       0.3366      0.6464        [94m0.5310[0m     +  0.0000  12.6052
     37      0.9959        [32m0.1008[0m       0.3366      0.6464        [94m0.5310[0m     +  0.0000  12.6134
     38      0.9959        [32m0.1006[0m       [35m0.3368[0m      0.6464        0.5310        0.0000  12.4326
     39      0.9946        0.1053       0.3368      0.6464        [94m0.5310[0m     +  0.0000  12.6498
     40      0.9955        0.1021       0.3366      0.6463        0.5310        0.0000  12.6095
     41      [36m0.9977[0m        0.1021       0.3366      0.6463        0.5310        0.0000  12.5041
     42      0.9927        0.1091       0.3368      0.6464        [94m0.5310[0m     +  0.0000  12.6328
     43      0.9964        0.1056       0.3368      0.6464        [94m0.5310[0m     +  0.0000  12.3922
     44      0.9950        0.1041       0.3368      0.6464        [94m0.5310[0m     +  0.0000  12.5315
     45      0.9950        0.1043       0.3368      0.6464        0.5310        0.0000  12.5272
     46      0.9955        0.1049       0.3368      0.6464        [94m0.5310[0m     +  0.0000  12.3909
     47      0.9964        0.1040       0.3368      0.6464        [94m0.5310[0m     +  0.0000  12.2179
     48      0.9968        0.1021       0.3368      0.6464        [94m0.5309[0m     +  0.0000  12.3710
     49      0.9964        0.1038       0.3368      0.6464        [94m0.5309[0m     +  0.0000  12.5634
     50      0.9932        0.1056       0.3368      0.6464        0.5309        0.0000  12.7793
     51      0.9964        0.1038       0.3368      0.6464        0.5309        0.0000  12.6248
     52      0.9964        0.1065       0.3368      0.6464        0.5309        0.0000  12.5810
     53      0.9945        0.1059       0.3368      0.6464        0.5309        0.0000  12.6233
     54      0.9950        0.1016       0.3368      0.6464        0.5309        0.0000  12.6012
     55      0.9968        0.1050       0.3368      0.6464        0.5309        0.0000  12.4752
     56      0.9955        0.1049       0.3368      0.6464        0.5309        0.0000  12.3440
     57      0.9959        0.1041       0.3368      0.6464        0.5309        0.0000  12.3925
     58      0.9941        0.1043       0.3368      0.6464        [94m0.5309[0m     +  0.0000  12.5859
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.6565148685533689
F1 Macro Score after query 5: 0.657129704812156
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_4.pt

Iteration: 6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9707[0m        [32m0.1263[0m       [35m0.3240[0m      [31m0.6300[0m        [94m0.5557[0m     +  0.0000  13.8284
      2      [36m0.9821[0m        [32m0.1096[0m       [35m0.3319[0m      [31m0.6444[0m        [94m0.5409[0m     +  0.0000  13.6790
      3      [36m0.9855[0m        [32m0.1002[0m       [35m0.3410[0m      [31m0.6496[0m        [94m0.5372[0m     +  0.0000  13.9310
      4      [36m0.9892[0m        [32m0.0980[0m       [35m0.3458[0m      0.6490        0.5380        0.0000  13.8129
      5      0.9874        [32m0.0954[0m       [35m0.3576[0m      [31m0.6665[0m        [94m0.5366[0m     +  0.0000  13.9060
      6      [36m0.9922[0m        [32m0.0851[0m       [35m0.3582[0m      0.6633        [94m0.5336[0m     +  0.0000  13.8045
      7      [36m0.9947[0m        [32m0.0835[0m       [35m0.3608[0m      0.6657        [94m0.5299[0m     +  0.0000  13.9227
      8      [36m0.9957[0m        [32m0.0819[0m       [35m0.3682[0m      [31m0.6732[0m        [94m0.5278[0m     +  0.0000  13.6589
      9      [36m0.9978[0m        [32m0.0785[0m       0.3632      0.6684        0.5300        0.0000  14.0410
     10      0.9957        [32m0.0785[0m       0.3672      0.6715        0.5279        0.0000  13.9104
     11      0.9973        [32m0.0757[0m       [35m0.3710[0m      [31m0.6774[0m        [94m0.5224[0m     +  0.0000  13.8594
     12      [36m0.9984[0m        [32m0.0756[0m       [35m0.3722[0m      [31m0.6791[0m        [94m0.5212[0m     +  0.0000  13.8421
     13      0.9982        [32m0.0748[0m       [35m0.3747[0m      [31m0.6805[0m        0.5215        0.0000  13.7938
     14      [36m0.9991[0m        [32m0.0732[0m       0.3726      0.6782        0.5253        0.0000  13.9531
     15      0.9984        [32m0.0727[0m       [35m0.3750[0m      0.6803        0.5237        0.0000  13.7341
     16      0.9984        [32m0.0704[0m       [35m0.3812[0m      [31m0.6876[0m        [94m0.5181[0m     +  0.0000  13.7035
     17      [36m0.9993[0m        0.0707       0.3793      0.6828        0.5205        0.0000  13.8350
     18      0.9986        0.0720       0.3788      0.6839        0.5194        0.0000  13.8902
     19      0.9991        0.0714       [35m0.3818[0m      0.6857        0.5183        0.0000  13.7046
     20      0.9991        0.0708       0.3800      0.6858        0.5187        0.0000  13.8041
     21      0.9987        0.0707       [35m0.3832[0m      [31m0.6891[0m        [94m0.5174[0m     +  0.0000  13.9414
     22      0.9986        0.0704       0.3830      0.6885        0.5177        0.0000  13.8563
     23      0.9989        0.0706       [35m0.3835[0m      0.6885        0.5176        0.0000  13.7962
     24      0.9991        [32m0.0698[0m       0.3832      0.6876        0.5176        0.0000  13.7136
     25      0.9987        [32m0.0698[0m       [35m0.3847[0m      0.6889        [94m0.5173[0m     +  0.0000  13.7176
     26      [36m1.0000[0m        [32m0.0692[0m       0.3842      0.6891        0.5173        0.0000  13.3623
     27      0.9991        0.0714       0.3845      0.6891        0.5175        0.0000  13.7345
     28      0.9998        0.0695       0.3845      [31m0.6894[0m        [94m0.5168[0m     +  0.0000  13.8441
     29      1.0000        0.0695       [35m0.3851[0m      [31m0.6897[0m        0.5169        0.0000  13.7652
     30      0.9993        0.0719       0.3844      0.6891        0.5170        0.0000  13.8042
     31      0.9995        0.0719       [35m0.3852[0m      0.6897        [94m0.5168[0m     +  0.0000  13.7246
     32      0.9995        0.0709       [35m0.3854[0m      [31m0.6901[0m        [94m0.5165[0m     +  0.0000  13.7395
     33      1.0000        0.0702       [35m0.3856[0m      0.6897        0.5167        0.0000  13.8325
     34      0.9993        0.0707       0.3856      0.6899        [94m0.5165[0m     +  0.0000  13.6117
     35      0.9995        0.0697       [35m0.3861[0m      [31m0.6904[0m        [94m0.5162[0m     +  0.0000  13.8752
     36      0.9989        0.0693       [35m0.3863[0m      [31m0.6906[0m        [94m0.5161[0m     +  0.0000  13.8203
     37      0.9995        0.0694       [35m0.3865[0m      [31m0.6907[0m        [94m0.5161[0m     +  0.0000  13.7366
     38      0.9993        [32m0.0692[0m       [35m0.3866[0m      [31m0.6909[0m        [94m0.5160[0m     +  0.0000  13.6848
     39      0.9991        0.0699       0.3866      [31m0.6910[0m        [94m0.5160[0m     +  0.0000  13.8835
     40      0.9991        0.0705       0.3866      0.6910        [94m0.5159[0m     +  0.0000  13.7344
     41      1.0000        0.0710       0.3866      0.6910        [94m0.5159[0m     +  0.0000  13.7982
     42      0.9991        0.0694       0.3866      [31m0.6910[0m        [94m0.5159[0m     +  0.0000  13.9205
     43      0.9991        0.0709       0.3866      0.6910        0.5159        0.0000  13.7203
     44      0.9991        0.0703       0.3866      0.6910        0.5159        0.0000  13.7000
     45      1.0000        [32m0.0679[0m       [35m0.3868[0m      [31m0.6911[0m        [94m0.5159[0m     +  0.0000  13.6708
     46      0.9991        0.0698       0.3868      0.6911        0.5159        0.0000  13.8277
     47      0.9993        0.0702       0.3868      0.6911        [94m0.5159[0m     +  0.0000  13.7964
     48      0.9995        0.0705       0.3868      0.6911        [94m0.5159[0m     +  0.0000  13.6047
     49      0.9995        0.0707       0.3868      0.6911        [94m0.5159[0m     +  0.0000  13.7502
     50      0.9991        0.0713       0.3868      0.6911        [94m0.5158[0m     +  0.0000  13.7776
     51      0.9995        0.0693       0.3868      0.6911        [94m0.5158[0m     +  0.0000  13.9956
     52      0.9993        0.0690       0.3868      0.6911        [94m0.5158[0m     +  0.0000  13.6561
     53      0.9993        0.0706       0.3868      0.6911        [94m0.5158[0m     +  0.0000  13.9555
     54      0.9991        [32m0.0677[0m       0.3868      0.6911        [94m0.5158[0m     +  0.0000  13.7491
     55      0.9993        0.0699       0.3868      0.6911        [94m0.5158[0m     +  0.0000  13.7341
     56      0.9991        0.0692       0.3868      0.6911        0.5158        0.0000  13.7646
     57      0.9998        0.0713       0.3868      0.6911        0.5158        0.0000  13.8915
     58      0.9982        0.0712       0.3868      0.6911        [94m0.5158[0m     +  0.0000  13.9081
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.6719601641881434
F1 Macro Score after query 6: 0.6621752082034864
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_5.pt

Iteration: 7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9838[0m        [32m0.1018[0m       [35m0.3405[0m      [31m0.6732[0m        [94m0.6274[0m     +  0.0000  16.0448
      2      [36m0.9904[0m        [32m0.0876[0m       [35m0.3524[0m      [31m0.6784[0m        [94m0.6068[0m     +  0.0000  15.9521
      3      [36m0.9915[0m        [32m0.0818[0m       [35m0.3604[0m      0.6777        [94m0.6036[0m     +  0.0000  16.0793
      4      [36m0.9929[0m        [32m0.0743[0m       0.3503      0.6718        0.6153        0.0000  15.9304
      5      [36m0.9942[0m        [32m0.0721[0m       [35m0.3727[0m      [31m0.6837[0m        [94m0.5935[0m     +  0.0000  15.9132
      6      [36m0.9953[0m        [32m0.0653[0m       [35m0.3924[0m      [31m0.6920[0m        [94m0.5741[0m     +  0.0000  15.9321
      7      [36m0.9967[0m        [32m0.0625[0m       [35m0.3977[0m      [31m0.6959[0m        [94m0.5595[0m     +  0.0000  16.0166
      8      0.9964        [32m0.0613[0m       0.3899      0.6894        0.5677        0.0000  15.8586
      9      0.9963        [32m0.0604[0m       0.3929      0.6929        0.5603        0.0000  15.9189
     10      [36m0.9967[0m        [32m0.0580[0m       [35m0.3997[0m      [31m0.6973[0m        [94m0.5548[0m     +  0.0000  15.9907
     11      [36m0.9977[0m        [32m0.0532[0m       [35m0.4144[0m      [31m0.7094[0m        [94m0.5382[0m     +  0.0000  15.9762
     12      [36m0.9982[0m        0.0534       [35m0.4165[0m      [31m0.7122[0m        [94m0.5340[0m     +  0.0000  15.9190
     13      0.9977        [32m0.0528[0m       [35m0.4250[0m      [31m0.7160[0m        [94m0.5291[0m     +  0.0000  15.9929
     14      0.9977        [32m0.0526[0m       0.4208      0.7136        0.5307        0.0000  15.8154
     15      [36m0.9985[0m        [32m0.0514[0m       [35m0.4260[0m      [31m0.7176[0m        [94m0.5254[0m     +  0.0000  16.0738
     16      [36m0.9986[0m        [32m0.0508[0m       0.4233      0.7157        0.5288        0.0000  15.9211
     17      [36m0.9989[0m        [32m0.0508[0m       0.4259      0.7175        0.5269        0.0000  16.0000
     18      0.9985        [32m0.0500[0m       0.4240      0.7173        0.5267        0.0000  15.9077
     19      0.9986        [32m0.0498[0m       0.4238      0.7157        0.5295        0.0000  15.9418
     20      0.9986        0.0509       0.4212      0.7139        0.5319        0.0000  15.9588
     21      0.9987        0.0505       0.4222      0.7151        0.5304        0.0000  15.8814
     22      0.9987        [32m0.0488[0m       0.4238      0.7165        0.5289        0.0000  16.0694
     23      0.9989        [32m0.0488[0m       0.4219      0.7153        0.5309        0.0000  15.9894
     24      0.9989        [32m0.0479[0m       0.4207      0.7149        0.5319        0.0000  15.7777
     25      0.9987        0.0495       0.4236      0.7165        0.5286        0.0000  15.8119
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.696258161727775
F1 Macro Score after query 7: 0.692444868892939
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_6.pt

Iteration: 8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9828[0m        [32m0.0982[0m       [35m0.2469[0m      [31m0.6383[0m        [94m0.7578[0m     +  0.0000  19.6884
      2      [36m0.9888[0m        [32m0.0786[0m       [35m0.2911[0m      [31m0.6503[0m        [94m0.7529[0m     +  0.0000  19.4186
      3      [36m0.9909[0m        [32m0.0704[0m       [35m0.3108[0m      [31m0.6617[0m        0.7534        0.0000  19.5541
      4      [36m0.9910[0m        [32m0.0674[0m       [35m0.3311[0m      [31m0.6648[0m        [94m0.6874[0m     +  0.0000  19.4964
      5      [36m0.9917[0m        [32m0.0611[0m       [35m0.3345[0m      0.6622        0.7040        0.0000  19.4049
      6      [36m0.9944[0m        [32m0.0538[0m       [35m0.3424[0m      [31m0.6717[0m        [94m0.6550[0m     +  0.0000  19.4033
      7      [36m0.9949[0m        [32m0.0504[0m       [35m0.3462[0m      [31m0.6732[0m        0.6568        0.0000  19.7698
      8      [36m0.9951[0m        [32m0.0489[0m       [35m0.3493[0m      [31m0.6741[0m        [94m0.6501[0m     +  0.0000  19.4360
      9      [36m0.9954[0m        [32m0.0466[0m       [35m0.3542[0m      [31m0.6763[0m        [94m0.6395[0m     +  0.0000  19.4180
     10      [36m0.9958[0m        [32m0.0443[0m       0.3535      0.6761        0.6448        0.0000  19.3496
     11      [36m0.9965[0m        [32m0.0424[0m       [35m0.3556[0m      [31m0.6778[0m        [94m0.6358[0m     +  0.0000  19.4767
     12      [36m0.9967[0m        [32m0.0414[0m       [35m0.3660[0m      [31m0.6818[0m        [94m0.6175[0m     +  0.0000  19.5760
     13      [36m0.9970[0m        [32m0.0400[0m       0.3599      0.6792        0.6273        0.0000  19.4053
     14      0.9970        [32m0.0391[0m       0.3641      0.6806        0.6213        0.0000  19.3872
     15      [36m0.9976[0m        [32m0.0382[0m       [35m0.3667[0m      0.6813        0.6199        0.0000  19.3921
     16      [36m0.9977[0m        [32m0.0371[0m       0.3616      0.6790        0.6276        0.0000  19.3729
     17      0.9977        0.0372       [35m0.3677[0m      0.6814        0.6182        0.0000  19.4207
     18      [36m0.9981[0m        [32m0.0362[0m       [35m0.3682[0m      0.6815        0.6178        0.0000  19.6122
     19      0.9981        0.0367       [35m0.3727[0m      [31m0.6829[0m        [94m0.6134[0m     +  0.0000  19.5038
     20      0.9977        [32m0.0349[0m       0.3681      0.6819        0.6213        0.0000  19.3491
     21      [36m0.9983[0m        0.0355       0.3670      0.6811        0.6245        0.0000  19.3131
     22      [36m0.9984[0m        0.0353       0.3693      0.6817        0.6191        0.0000  19.6093
     23      [36m0.9986[0m        0.0352       0.3684      0.6815        0.6217        0.0000  19.4079
     24      0.9982        [32m0.0346[0m       0.3682      0.6813        0.6225        0.0000  19.5406
     25      0.9982        0.0347       0.3707      0.6824        0.6191        0.0000  19.3750
     26      0.9983        0.0347       0.3693      0.6817        0.6207        0.0000  19.4721
     27      [36m0.9987[0m        [32m0.0344[0m       0.3694      0.6816        0.6214        0.0000  19.6018
     28      0.9983        0.0350       0.3707      0.6822        0.6202        0.0000  19.6309
     29      0.9982        [32m0.0342[0m       0.3705      0.6823        0.6208        0.0000  19.4080
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.6848975252495423
F1 Macro Score after query 8: 0.6928688899431434
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_7.pt

Iteration: 9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9848[0m        [32m0.0828[0m       [35m0.3424[0m      [31m0.6799[0m        [94m0.6787[0m     +  0.0000  26.4369
      2      [36m0.9876[0m        [32m0.0678[0m       [35m0.3799[0m      [31m0.7138[0m        [94m0.5673[0m     +  0.0000  26.4229
      3      [36m0.9898[0m        [32m0.0605[0m       [35m0.3852[0m      [31m0.7149[0m        0.5968        0.0000  26.1838
      4      [36m0.9920[0m        [32m0.0537[0m       0.3505      0.7004        0.6655        0.0000  26.3696
      5      [36m0.9925[0m        [32m0.0492[0m       0.3507      0.6929        0.6858        0.0000  26.4163
      6      [36m0.9944[0m        [32m0.0417[0m       [35m0.4007[0m      [31m0.7205[0m        0.5871        0.0000  26.3567
      7      [36m0.9957[0m        [32m0.0376[0m       0.3997      0.7182        0.5871        0.0000  26.5213
      8      [36m0.9960[0m        [32m0.0351[0m       0.3929      0.7182        0.6204        0.0000  26.3597
      9      [36m0.9969[0m        [32m0.0329[0m       0.3903      0.7154        0.6263        0.0000  26.0848
     10      [36m0.9972[0m        [32m0.0300[0m       0.3976      [31m0.7218[0m        0.6176        0.0000  26.4388
     11      [36m0.9977[0m        [32m0.0282[0m       0.3983      0.7208        0.6165        0.0000  26.3289
     12      [36m0.9982[0m        [32m0.0264[0m       [35m0.4010[0m      [31m0.7229[0m        0.6044        0.0000  26.2154
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.7160449112640347
F1 Macro Score after query 9: 0.730814396298074
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_8.pt

Iteration: 10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9871[0m        [32m0.0724[0m       [35m0.2962[0m      [31m0.7285[0m        [94m0.6022[0m     +  0.0000  38.3748
      2      [36m0.9892[0m        [32m0.0615[0m       [35m0.3260[0m      [31m0.7325[0m        0.6066        0.0000  38.5330
      3      [36m0.9911[0m        [32m0.0532[0m       [35m0.3814[0m      [31m0.7566[0m        [94m0.5238[0m     +  0.0000  38.5838
      4      [36m0.9925[0m        [32m0.0476[0m       0.3727      0.7350        0.6047        0.0000  38.6202
      5      [36m0.9931[0m        [32m0.0422[0m       0.3573      0.7265        0.6523        0.0000  38.5038
      6      [36m0.9951[0m        [32m0.0349[0m       [35m0.4167[0m      0.7455        0.5385        0.0000  38.6102
      7      [36m0.9959[0m        [32m0.0313[0m       0.4109      0.7395        0.5507        0.0000  38.5028
      8      [36m0.9966[0m        [32m0.0284[0m       [35m0.4352[0m      0.7495        0.5369        0.0000  38.6549
      9      [36m0.9973[0m        [32m0.0257[0m       0.3882      0.7286        0.5934        0.0000  38.4842
     10      [36m0.9976[0m        [32m0.0238[0m       0.3974      0.7336        0.5844        0.0000  38.7498
     11      [36m0.9979[0m        [32m0.0220[0m       0.4181      0.7432        0.5877        0.0000  38.6744
     12      [36m0.9982[0m        [32m0.0204[0m       0.4280      0.7459        0.5825        0.0000  38.7700
     13      [36m0.9985[0m        [32m0.0193[0m       0.4167      0.7408        0.6070        0.0000  39.6887
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.7292396582457435
F1 Macro Score after query 10: 0.7374526440220629
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_9.pt

Iteration: 11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9672[0m        [32m0.1337[0m       [35m0.7472[0m      [31m0.8656[0m        [94m0.2781[0m     +  0.0000  59.7939
      2      [36m0.9792[0m        [32m0.1005[0m       [35m0.7661[0m      [31m0.8792[0m        [94m0.2737[0m     +  0.0000  60.7724
      3      [36m0.9827[0m        [32m0.0853[0m       0.7498      0.8713        [94m0.2711[0m     +  0.0000  60.3621
      4      [36m0.9851[0m        [32m0.0753[0m       0.7486      0.8699        0.2728        0.0000  60.0628
      5      [36m0.9872[0m        [32m0.0672[0m       0.7587      0.8740        [94m0.2664[0m     +  0.0000  60.2760
      6      [36m0.9909[0m        [32m0.0562[0m       0.7583      0.8686        [94m0.2521[0m     +  0.0000  60.3400
      7      [36m0.9921[0m        [32m0.0512[0m       0.7557      0.8679        0.2575        0.0000  60.5041
      8      [36m0.9928[0m        [32m0.0478[0m       0.7628      0.8728        0.2542        0.0000  60.3732
      9      [36m0.9936[0m        [32m0.0447[0m       0.7589      0.8723        0.2598        0.0000  60.6172
     10      [36m0.9943[0m        [32m0.0417[0m       0.7472      0.8680        0.2725        0.0000  60.5622
     11      [36m0.9954[0m        [32m0.0382[0m       0.7594      0.8736        0.2695        0.0000  60.5307
     12      [36m0.9961[0m        [32m0.0357[0m       0.7552      0.8725        0.2748        0.0000  60.5324
     13      [36m0.9967[0m        [32m0.0344[0m       0.7533      0.8709        0.2754        0.0000  60.0134
     14      [36m0.9972[0m        [32m0.0330[0m       0.7519      0.8703        0.2783        0.0000  60.2850
     15      [36m0.9973[0m        [32m0.0314[0m       0.7521      0.8704        0.2810        0.0000  60.1996
     16      [36m0.9977[0m        [32m0.0305[0m       0.7521      0.8699        0.2787        0.0000  60.4367
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.9032756489493202
F1 Macro Score after query 11: 0.8920895766350688
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_10.pt

Iteration: 12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9804[0m        [32m0.0788[0m       [35m0.7990[0m      [31m0.8913[0m        [94m0.2569[0m     +  0.0000  98.8447
      2      [36m0.9837[0m        [32m0.0605[0m       [35m0.8111[0m      [31m0.8979[0m        [94m0.2261[0m     +  0.0000  99.4377
      3      [36m0.9865[0m        [32m0.0500[0m       0.8087      0.8937        0.2433        0.0000  99.6087
      4      [36m0.9879[0m        [32m0.0422[0m       [35m0.8120[0m      0.8938        [94m0.2159[0m     +  0.0000  99.2357
      5      [36m0.9889[0m        [32m0.0369[0m       0.8049      0.8918        0.2566        0.0000  99.5652
      6      [36m0.9921[0m        [32m0.0296[0m       [35m0.8170[0m      [31m0.8983[0m        0.2204        0.0000  99.6813
      7      [36m0.9934[0m        [32m0.0259[0m       0.8111      0.8933        0.2356        0.0000  99.2413
      8      [36m0.9947[0m        [32m0.0232[0m       0.8057      0.8940        0.2555        0.0000  99.0359
      9      [36m0.9951[0m        [32m0.0211[0m       0.8151      0.8971        0.2373        0.0000  100.3435
     10      [36m0.9963[0m        [32m0.0187[0m       0.8158      0.8975        0.2357        0.0000  99.3748
     11      [36m0.9970[0m        [32m0.0165[0m       [35m0.8172[0m      0.8942        0.2393        0.0000  99.6455
     12      [36m0.9979[0m        [32m0.0150[0m       [35m0.8220[0m      0.8956        0.2365        0.0000  99.1952
     13      [36m0.9983[0m        [32m0.0140[0m       0.8215      [31m0.8989[0m        0.2432        0.0000  99.3208
     14      [36m0.9986[0m        [32m0.0131[0m       [35m0.8238[0m      [31m0.8990[0m        0.2464        0.0000  99.4356
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.9158806839966259
F1 Macro Score after query 12: 0.9049017635220068
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_11.pt

Iteration: 13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9891[0m        [32m0.0342[0m       [35m0.7922[0m      [31m0.8900[0m        [94m0.2596[0m     +  0.0000  114.9727
      2      [36m0.9896[0m        [32m0.0299[0m       [35m0.7936[0m      0.8777        0.2632        0.0000  116.1807
      3      [36m0.9906[0m        [32m0.0262[0m       [35m0.8068[0m      0.8879        [94m0.2429[0m     +  0.0000  115.1742
      4      [36m0.9917[0m        [32m0.0230[0m       0.7842      0.8821        0.2776        0.0000  115.7814
      5      [36m0.9924[0m        [32m0.0209[0m       [35m0.8198[0m      [31m0.9009[0m        [94m0.2416[0m     +  0.0000  115.5453
      6      [36m0.9952[0m        [32m0.0157[0m       [35m0.8347[0m      [31m0.9026[0m        [94m0.2213[0m     +  0.0000  115.1872
      7      [36m0.9968[0m        [32m0.0131[0m       0.8304      0.9002        0.2270        0.0000  115.7472
      8      [36m0.9971[0m        [32m0.0114[0m       0.8250      0.8989        0.2423        0.0000  115.7451
      9      [36m0.9975[0m        [32m0.0099[0m       0.8299      0.8995        0.2391        0.0000  115.9219
     10      [36m0.9981[0m        [32m0.0091[0m       0.8304      0.9004        0.2496        0.0000  115.3590
     11      [36m0.9986[0m        [32m0.0077[0m       0.8311      0.8978        0.2510        0.0000  115.4151
     12      [36m0.9990[0m        [32m0.0069[0m       0.8274      0.8988        0.2589        0.0000  115.8780
     13      [36m0.9992[0m        [32m0.0064[0m       0.8309      0.8973        0.2582        0.0000  115.3411
     14      [36m0.9994[0m        [32m0.0059[0m       0.8234      0.8977        0.2745        0.0000  115.9324
     15      0.9994        [32m0.0056[0m       0.8300      0.8995        0.2650        0.0000  115.9916
     16      [36m0.9996[0m        [32m0.0051[0m       0.8234      0.8988        0.2799        0.0000  116.1309
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.9185474424053106
F1 Macro Score after query 13: 0.9062307455287838
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\model_checkpoint_iteration_12.pt
Pickle file saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_lowLR\AL_max_score_results_for_multilabel_classification_s44.pickle
