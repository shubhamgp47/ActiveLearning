Adjusted shapes after loading:
X_initial_np: (8, 224, 224, 3), y_initial_np: (8, 3)
X_pool_np: (26872, 224, 224, 3), y_pool_np: (26872, 3)
X_test_np: (5760, 224, 224, 3), y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3), y_val_np: (5760, 3)
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.5460[0m        [32m0.7305[0m       [35m0.0606[0m      [31m0.1748[0m        [94m0.6982[0m     +  0.0001  12.2398
      2      0.5000        [32m0.6546[0m       [35m0.3118[0m      [31m0.5036[0m        [94m0.6765[0m     +  0.0001  10.4804
      3      [36m0.6984[0m        [32m0.6165[0m       0.3059      [31m0.5187[0m        [94m0.6743[0m     +  0.0001  10.6540
      4      0.5675        0.6462       [35m0.3158[0m      0.4720        0.6783        0.0001  10.7052
      5      [36m0.8214[0m        [32m0.5715[0m       0.2073      0.4845        0.6887        0.0001  10.5655
      6      0.7667        [32m0.5593[0m       0.2906      0.4912        0.6802        0.0000  10.9149
      7      [36m0.8320[0m        [32m0.5395[0m       0.2708      0.4806        0.6846        0.0000  10.6886
      8      0.7667        0.5659       0.2970      0.4869        0.6792        0.0000  10.7916
      9      0.7667        0.5557       0.3035      0.4821        0.6774        0.0000  10.8291
     10      0.7963        [32m0.5302[0m       0.3089      0.4837        0.6771        0.0000  10.7623
     11      0.7857        [32m0.5224[0m       0.3080      0.4812        0.6779        0.0000  10.7038
     12      [36m0.8519[0m        [32m0.5205[0m       0.3073      0.4780        0.6762        0.0000  10.8411
     13      0.8333        [32m0.5047[0m       0.3082      0.4801        0.6753        0.0000  10.8555
Stopping since valid_loss has not improved in the last 11 epochs.
Pre F1 micro score = 0.4935
Pre F1 macro score = 0.5063
Pre Accuracy = 0.3436

Iteration: 1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.8556[0m        [32m0.4527[0m       [35m0.2151[0m      [31m0.5457[0m        [94m0.9101[0m     +  0.0001  10.7826
      2      [36m0.8727[0m        [32m0.4390[0m       0.2062      0.5321        [94m0.8605[0m     +  0.0001  10.9083
      3      [36m0.9068[0m        [32m0.3841[0m       [35m0.2693[0m      0.5346        [94m0.8587[0m     +  0.0001  10.8720
      4      0.9036        [32m0.3607[0m       [35m0.2856[0m      0.5402        0.8618        0.0001  10.9321
      5      [36m0.9085[0m        [32m0.3508[0m       0.2632      0.5129        [94m0.8491[0m     +  0.0001  10.8928
      6      0.9036        [32m0.3119[0m       0.2651      0.5144        0.8589        0.0000  10.9026
      7      [36m0.9121[0m        0.3269       0.2696      0.5140        0.8619        0.0000  10.7778
      8      [36m0.9196[0m        [32m0.2900[0m       0.2703      0.5106        0.8582        0.0000  10.8090
      9      [36m0.9302[0m        0.3075       0.2703      0.5112        0.8557        0.0000  11.1409
     10      0.9302        [32m0.2884[0m       0.2759      0.5194        0.8558        0.0000  10.6731
     11      0.9226        0.3097       0.2703      0.5140        0.8529        0.0000  10.7674
     12      0.9226        0.2958       0.2696      0.5128        [94m0.8485[0m     +  0.0000  11.0763
     13      0.9216        [32m0.2884[0m       0.2670      0.5126        [94m0.8484[0m     +  0.0000  10.7942
     14      0.9216        0.2996       0.2665      0.5128        [94m0.8465[0m     +  0.0000  10.8282
     15      0.9302        [32m0.2804[0m       0.2642      0.5120        0.8480        0.0000  10.7521
     16      0.9216        0.2981       0.2637      0.5121        0.8466        0.0000  10.8563
     17      0.9302        [32m0.2775[0m       0.2630      0.5113        0.8470        0.0000  10.7930
     18      0.9140        0.3096       0.2644      0.5124        0.8466        0.0000  10.9274
     19      0.9226        0.2797       0.2635      0.5131        [94m0.8460[0m     +  0.0000  10.8253
     20      0.9216        [32m0.2767[0m       0.2634      0.5124        0.8469        0.0000  10.9073
     21      0.9216        [32m0.2737[0m       0.2632      0.5123        0.8469        0.0000  10.8635
     22      0.9302        0.3019       0.2632      0.5121        0.8465        0.0000  10.7541
     23      0.9140        0.2981       0.2628      0.5122        0.8465        0.0000  10.9412
     24      0.9196        0.2912       0.2628      0.5119        0.8462        0.0000  10.9570
     25      0.9226        0.2822       0.2637      0.5120        [94m0.8454[0m     +  0.0000  10.6726
     26      0.9296        0.2752       0.2637      0.5120        0.8455        0.0000  10.8438
     27      0.9216        0.2746       0.2635      0.5120        0.8455        0.0000  10.7351
     28      0.9121        0.2856       0.2632      0.5120        [94m0.8454[0m     +  0.0000  10.8881
     29      0.9296        0.2814       0.2628      0.5119        0.8454        0.0000  10.7519
     30      0.9216        [32m0.2730[0m       0.2630      0.5119        [94m0.8453[0m     +  0.0000  10.9664
     31      [36m0.9311[0m        0.2935       0.2632      0.5121        [94m0.8453[0m     +  0.0000  10.8181
     32      0.9140        0.2995       0.2632      0.5121        [94m0.8452[0m     +  0.0000  10.6306
     33      0.9216        0.2739       0.2632      0.5121        0.8452        0.0000  10.7380
     34      0.9296        0.2989       0.2632      0.5121        [94m0.8452[0m     +  0.0000  10.7158
     35      0.9216        0.2888       0.2632      0.5121        [94m0.8452[0m     +  0.0000  10.8513
     36      0.9302        [32m0.2616[0m       0.2632      0.5121        0.8452        0.0000  11.0318
     37      0.9216        0.2764       0.2632      0.5121        [94m0.8452[0m     +  0.0000  10.8126
     38      [36m0.9382[0m        0.2623       0.2632      0.5121        [94m0.8452[0m     +  0.0000  10.8617
     39      0.9226        0.2797       0.2632      0.5121        0.8452        0.0000  11.0464
     40      0.9302        0.2640       0.2632      0.5121        0.8452        0.0000  11.0144
     41      0.9226        0.2762       0.2632      0.5121        0.8452        0.0000  10.7044
     42      0.9140        0.2768       0.2632      0.5121        0.8452        0.0000  10.8385
Stopping since valid_loss has not improved in the last 11 epochs.
[24]
F1 Micro Score after query 1: 0.4975765384379411
F1 Macro Score after query 1: 0.48689435675819065
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_0.pt

Iteration: 2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9222[0m        [32m0.2707[0m       [35m0.2302[0m      [31m0.5579[0m        [94m0.9993[0m     +  0.0001  10.7203
      2      [36m0.9559[0m        [32m0.2319[0m       [35m0.2576[0m      0.5023        [94m0.9264[0m     +  0.0001  11.0437
      3      0.9519        [32m0.1998[0m       0.2517      0.5203        0.9459        0.0001  10.6977
      4      0.9558        0.2015       0.2311      0.4740        [94m0.9043[0m     +  0.0001  10.7864
      5      0.9556        0.2006       0.2243      0.4993        [94m0.8872[0m     +  0.0001  10.7865
      6      [36m0.9652[0m        [32m0.1784[0m       0.2281      0.5022        [94m0.8740[0m     +  0.0000  11.0167
      7      0.9652        [32m0.1632[0m       0.2227      0.4992        [94m0.8722[0m     +  0.0000  10.9996
      8      0.9621        0.1652       0.2210      0.4953        [94m0.8609[0m     +  0.0000  10.8625
      9      [36m0.9685[0m        [32m0.1551[0m       0.2160      0.4920        [94m0.8562[0m     +  0.0000  10.8959
     10      0.9685        0.1713       0.2205      0.4874        [94m0.8335[0m     +  0.0000  10.9663
     11      [36m0.9716[0m        [32m0.1441[0m       0.2196      0.4886        [94m0.8334[0m     +  0.0000  11.0036
     12      0.9685        [32m0.1432[0m       0.2165      0.4868        [94m0.8329[0m     +  0.0000  11.0059
     13      0.9684        0.1551       0.2200      0.4873        [94m0.8244[0m     +  0.0000  11.0261
     14      [36m0.9749[0m        [32m0.1405[0m       0.2175      0.4835        [94m0.8214[0m     +  0.0000  10.8597
     15      0.9652        0.1618       0.2165      0.4817        [94m0.8209[0m     +  0.0000  10.7971
     16      0.9749        [32m0.1334[0m       0.2163      0.4814        [94m0.8193[0m     +  0.0000  10.9492
     17      0.9685        [32m0.1314[0m       0.2163      0.4819        0.8204        0.0000  10.9888
     18      0.9749        0.1315       0.2161      0.4816        0.8193        0.0000  10.7431
     19      0.9685        0.1321       0.2148      0.4796        [94m0.8184[0m     +  0.0000  10.8229
     20      0.9749        0.1325       0.2156      0.4800        [94m0.8175[0m     +  0.0000  11.0311
     21      0.9684        0.1359       0.2148      0.4799        [94m0.8169[0m     +  0.0000  10.9669
     22      0.9716        [32m0.1284[0m       0.2146      0.4799        0.8169        0.0000  10.7380
     23      0.9717        0.1349       0.2134      0.4788        [94m0.8163[0m     +  0.0000  10.7289
     24      0.9684        0.1428       0.2132      0.4790        [94m0.8153[0m     +  0.0000  11.0477
     25      0.9652        0.1386       0.2130      0.4782        [94m0.8146[0m     +  0.0000  11.0737
     26      0.9749        0.1285       0.2128      0.4779        0.8146        0.0000  10.9902
     27      0.9716        [32m0.1267[0m       0.2128      0.4779        0.8148        0.0000  10.9509
     28      0.9716        0.1488       0.2128      0.4778        0.8147        0.0000  10.9196
     29      [36m0.9783[0m        0.1317       0.2123      0.4776        0.8148        0.0000  10.9553
     30      0.9716        0.1309       0.2127      0.4775        [94m0.8144[0m     +  0.0000  10.9486
     31      0.9716        0.1461       0.2125      0.4775        [94m0.8143[0m     +  0.0000  10.8578
     32      0.9749        [32m0.1245[0m       0.2127      0.4776        [94m0.8142[0m     +  0.0000  10.8652
     33      0.9716        0.1441       0.2123      0.4774        [94m0.8140[0m     +  0.0000  10.6308
     34      0.9717        0.1378       0.2122      0.4773        [94m0.8139[0m     +  0.0000  10.8746
     35      0.9716        0.1474       0.2122      0.4772        [94m0.8139[0m     +  0.0000  11.1228
     36      0.9716        0.1308       0.2122      0.4772        [94m0.8139[0m     +  0.0000  10.8645
     37      0.9716        0.1334       0.2122      0.4773        [94m0.8138[0m     +  0.0000  10.9667
     38      0.9749        0.1312       0.2120      0.4772        [94m0.8138[0m     +  0.0000  10.9416
     39      0.9749        0.1285       0.2122      0.4774        0.8138        0.0000  11.0991
     40      0.9749        0.1246       0.2123      0.4774        0.8138        0.0000  10.8151
     41      0.9716        0.1341       0.2122      0.4773        [94m0.8137[0m     +  0.0000  10.9632
     42      0.9716        0.1461       0.2122      0.4773        [94m0.8137[0m     +  0.0000  10.8768
     43      0.9716        0.1258       0.2122      0.4773        [94m0.8137[0m     +  0.0000  11.1088
     44      0.9716        0.1468       0.2122      0.4773        [94m0.8137[0m     +  0.0000  10.8802
     45      0.9678        0.1381       0.2122      0.4773        0.8137        0.0000  10.9770
     46      0.9684        0.1434       0.2122      0.4773        0.8137        0.0000  10.8310
     47      0.9749        0.1356       0.2122      0.4773        0.8137        0.0000  10.9518
     48      0.9684        0.1460       0.2122      0.4773        [94m0.8137[0m     +  0.0000  10.9900
     49      0.9684        0.1415       0.2123      0.4773        [94m0.8137[0m     +  0.0000  10.8719
     50      0.9749        0.1344       0.2123      0.4773        [94m0.8137[0m     +  0.0000  10.9859
     51      0.9783        0.1257       0.2123      0.4773        [94m0.8137[0m     +  0.0000  10.8938
     52      0.9749        0.1349       0.2123      0.4773        [94m0.8137[0m     +  0.0000  10.9637
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56]
F1 Micro Score after query 2: 0.5073566717402334
F1 Macro Score after query 2: 0.4890977903959089
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_1.pt

Iteration: 3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9634[0m        [32m0.1747[0m       [35m0.1962[0m      [31m0.5383[0m        [94m1.4467[0m     +  0.0001  10.9345
      2      0.9569        [32m0.1723[0m       [35m0.2142[0m      0.5027        [94m0.9440[0m     +  0.0001  11.1299
      3      [36m0.9726[0m        [32m0.1266[0m       0.2113      0.4842        [94m0.9011[0m     +  0.0001  10.9700
      4      [36m0.9754[0m        [32m0.1122[0m       0.2059      0.4770        [94m0.8990[0m     +  0.0001  11.1451
      5      0.9738        [32m0.1019[0m       [35m0.2207[0m      0.4571        [94m0.8366[0m     +  0.0001  11.2335
      6      [36m0.9860[0m        [32m0.0757[0m       [35m0.2233[0m      0.4515        [94m0.8242[0m     +  0.0000  10.8897
      7      [36m0.9894[0m        [32m0.0602[0m       [35m0.2247[0m      0.4493        [94m0.8106[0m     +  0.0000  11.2039
      8      [36m0.9911[0m        [32m0.0591[0m       [35m0.2330[0m      0.4458        [94m0.7913[0m     +  0.0000  11.0817
      9      0.9910        0.0602       0.2285      0.4449        0.7919        0.0000  10.9190
     10      0.9911        [32m0.0550[0m       0.2274      0.4400        [94m0.7866[0m     +  0.0000  11.1517
     11      [36m0.9928[0m        [32m0.0496[0m       0.2266      0.4435        [94m0.7804[0m     +  0.0000  11.0780
     12      0.9911        [32m0.0494[0m       0.2259      0.4455        [94m0.7782[0m     +  0.0000  11.1534
     13      0.9910        [32m0.0489[0m       0.2274      0.4457        [94m0.7706[0m     +  0.0000  10.9531
     14      [36m0.9946[0m        [32m0.0482[0m       0.2276      0.4451        0.7708        0.0000  10.9062
     15      0.9930        [32m0.0465[0m       0.2266      0.4478        [94m0.7694[0m     +  0.0000  11.0512
     16      0.9928        0.0491       0.2269      0.4480        [94m0.7663[0m     +  0.0000  11.1568
     17      [36m0.9982[0m        [32m0.0439[0m       0.2273      0.4487        [94m0.7650[0m     +  0.0000  11.0404
     18      0.9982        [32m0.0427[0m       0.2259      0.4493        0.7653        0.0000  10.9738
     19      0.9928        0.0526       0.2260      0.4488        [94m0.7628[0m     +  0.0000  11.1044
     20      0.9964        0.0443       0.2260      0.4494        [94m0.7623[0m     +  0.0000  11.1305
     21      0.9964        [32m0.0426[0m       0.2248      0.4490        0.7641        0.0000  10.9979
     22      0.9982        0.0430       0.2253      0.4499        0.7644        0.0000  10.9671
     23      0.9982        0.0438       0.2247      0.4495        0.7634        0.0000  11.0624
     24      0.9946        [32m0.0423[0m       0.2248      0.4500        0.7640        0.0000  10.9242
     25      0.9946        0.0461       0.2248      0.4498        0.7639        0.0000  11.0798
     26      0.9964        0.0452       0.2253      0.4502        0.7637        0.0000  11.0736
     27      0.9982        [32m0.0421[0m       0.2253      0.4498        0.7636        0.0000  10.9088
     28      0.9946        [32m0.0410[0m       0.2247      0.4494        0.7631        0.0000  11.0795
     29      0.9963        0.0428       0.2245      0.4494        0.7630        0.0000  10.8907
     30      0.9982        [32m0.0398[0m       0.2250      0.4496        0.7631        0.0000  11.1428
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112]
F1 Micro Score after query 3: 0.5020737146881602
F1 Macro Score after query 3: 0.4838881119483213
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_2.pt

Iteration: 4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9712[0m        [32m0.1350[0m       [35m0.2040[0m      [31m0.5444[0m        [94m1.4929[0m     +  0.0001  11.0617
      2      [36m0.9740[0m        [32m0.1049[0m       0.1955      0.4741        [94m0.8826[0m     +  0.0001  11.2655
      3      [36m0.9857[0m        [32m0.0759[0m       0.1927      0.4704        0.9077        0.0001  11.3145
      4      [36m0.9875[0m        [32m0.0548[0m       0.2002      0.4693        [94m0.8461[0m     +  0.0001  11.1613
      5      [36m0.9904[0m        [32m0.0540[0m       0.1934      0.4737        0.8462        0.0001  11.2203
      6      [36m0.9942[0m        [32m0.0438[0m       0.1943      0.4761        0.8516        0.0000  11.1570
      7      0.9922        [32m0.0385[0m       0.1955      0.4755        [94m0.8433[0m     +  0.0000  11.1436
      8      [36m0.9952[0m        [32m0.0380[0m       0.1977      0.4725        [94m0.8219[0m     +  0.0000  11.2524
      9      0.9913        [32m0.0380[0m       0.1969      0.4779        0.8354        0.0000  11.0450
     10      0.9942        [32m0.0358[0m       0.2010      0.4777        0.8230        0.0000  11.0930
     11      0.9913        0.0372       0.2003      0.4773        [94m0.8175[0m     +  0.0000  11.4960
     12      0.9942        [32m0.0325[0m       0.2014      0.4764        [94m0.8125[0m     +  0.0000  11.1880
     13      0.9932        [32m0.0324[0m       0.2017      0.4782        [94m0.8119[0m     +  0.0000  11.2581
     14      [36m0.9961[0m        [32m0.0303[0m       0.2009      0.4785        [94m0.8075[0m     +  0.0000  11.2817
     15      0.9951        0.0338       0.2035      0.4821        0.8101        0.0000  11.2334
     16      0.9932        0.0341       0.2023      0.4798        [94m0.8055[0m     +  0.0000  11.2508
     17      0.9942        [32m0.0297[0m       0.2024      0.4805        [94m0.8052[0m     +  0.0000  11.0778
     18      0.9952        [32m0.0282[0m       0.2019      0.4803        [94m0.8046[0m     +  0.0000  11.1668
     19      0.9932        0.0286       0.2023      0.4817        0.8056        0.0000  11.2607
     20      0.9932        0.0303       0.2024      0.4817        [94m0.8046[0m     +  0.0000  11.4714
     21      0.9952        0.0294       0.2021      0.4809        [94m0.8043[0m     +  0.0000  11.1300
     22      0.9952        [32m0.0272[0m       0.2014      0.4800        [94m0.8038[0m     +  0.0000  11.0723
     23      0.9942        [32m0.0271[0m       0.2017      0.4793        [94m0.8025[0m     +  0.0000  11.0050
     24      0.9952        0.0282       0.2003      0.4772        [94m0.8012[0m     +  0.0000  11.2923
     25      0.9951        0.0285       0.2017      0.4805        0.8027        0.0000  11.3208
     26      0.9952        0.0272       0.2021      0.4813        0.8032        0.0000  11.4243
     27      0.9942        0.0280       0.2021      0.4811        0.8025        0.0000  11.0933
     28      0.9942        0.0298       0.2023      0.4814        0.8024        0.0000  11.0516
     29      0.9942        0.0308       0.2014      0.4804        0.8017        0.0000  11.1535
     30      0.9942        0.0290       0.2019      0.4808        0.8018        0.0000  11.1931
     31      0.9952        0.0288       0.2017      0.4808        0.8018        0.0000  11.1722
     32      0.9952        0.0282       0.2017      0.4807        0.8017        0.0000  11.1404
     33      0.9942        0.0281       0.2016      0.4804        0.8015        0.0000  11.3148
     34      [36m0.9971[0m        [32m0.0268[0m       0.2016      0.4804        0.8014        0.0000  11.1543
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208]
F1 Micro Score after query 4: 0.5245588235294117
F1 Macro Score after query 4: 0.5095292185598927
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_3.pt

Iteration: 5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9694[0m        [32m0.1372[0m       [35m0.2608[0m      [31m0.6001[0m        [94m1.1575[0m     +  0.0001  11.6790
      2      [36m0.9783[0m        [32m0.1185[0m       0.2384      0.5670        [94m0.8219[0m     +  0.0001  11.5298
      3      [36m0.9833[0m        [32m0.0941[0m       0.2172      0.5513        [94m0.7745[0m     +  0.0001  11.6875
      4      0.9832        [32m0.0870[0m       0.2347      0.5620        [94m0.7675[0m     +  0.0001  11.5315
      5      0.9831        [32m0.0802[0m       0.2437      0.5595        [94m0.7606[0m     +  0.0001  11.5196
      6      [36m0.9882[0m        [32m0.0666[0m       0.2392      0.5556        0.7635        0.0000  11.8083
      7      [36m0.9887[0m        [32m0.0587[0m       0.2396      0.5531        [94m0.7528[0m     +  0.0000  11.6729
      8      0.9887        0.0599       0.2431      0.5460        [94m0.7355[0m     +  0.0000  11.4664
      9      [36m0.9897[0m        [32m0.0534[0m       0.2422      0.5439        [94m0.7284[0m     +  0.0000  11.5877
     10      0.9887        0.0556       0.2474      0.5532        [94m0.7251[0m     +  0.0000  11.7037
     11      [36m0.9913[0m        [32m0.0478[0m       0.2493      0.5536        [94m0.7237[0m     +  0.0000  11.5276
     12      0.9913        [32m0.0471[0m       0.2457      0.5455        [94m0.7184[0m     +  0.0000  11.5046
     13      0.9907        0.0506       0.2464      0.5441        [94m0.7118[0m     +  0.0000  11.6949
     14      [36m0.9923[0m        [32m0.0440[0m       0.2521      0.5503        [94m0.7115[0m     +  0.0000  11.6156
     15      0.9913        0.0504       0.2552      0.5535        [94m0.7057[0m     +  0.0000  11.6615
     16      0.9913        0.0444       0.2569      0.5514        [94m0.7026[0m     +  0.0000  11.4457
     17      0.9923        [32m0.0438[0m       0.2580      0.5544        0.7038        0.0000  11.7428
     18      0.9917        [32m0.0424[0m       0.2583      0.5543        0.7037        0.0000  11.4943
     19      [36m0.9928[0m        0.0427       0.2580      0.5522        [94m0.7006[0m     +  0.0000  11.6330
     20      0.9917        0.0426       0.2587      0.5538        [94m0.7001[0m     +  0.0000  11.4441
     21      0.9918        0.0444       0.2589      0.5508        [94m0.6988[0m     +  0.0000  11.4474
     22      0.9913        0.0425       0.2580      0.5493        0.6994        0.0000  11.5552
     23      0.9928        [32m0.0423[0m       0.2576      0.5490        0.6988        0.0000  11.5083
     24      [36m0.9933[0m        [32m0.0412[0m       0.2587      0.5503        [94m0.6986[0m     +  0.0000  11.4938
     25      0.9928        0.0419       0.2590      0.5510        0.6987        0.0000  11.7129
     26      0.9928        0.0414       0.2592      0.5509        [94m0.6985[0m     +  0.0000  11.4436
     27      0.9928        [32m0.0405[0m       0.2594      0.5510        0.6987        0.0000  11.6984
     28      0.9923        0.0413       0.2595      0.5512        0.6987        0.0000  11.5237
     29      0.9933        0.0410       0.2590      0.5510        0.6986        0.0000  11.3850
     30      0.9918        0.0427       0.2595      0.5510        [94m0.6983[0m     +  0.0000  11.7411
     31      0.9933        0.0426       0.2592      0.5509        [94m0.6983[0m     +  0.0000  11.4888
     32      0.9933        0.0408       0.2594      0.5509        [94m0.6981[0m     +  0.0000  11.4672
     33      0.9928        0.0407       0.2590      0.5512        0.6983        0.0000  11.3986
     34      0.9933        0.0417       0.2590      0.5511        0.6983        0.0000  11.5103
     35      0.9933        0.0412       0.2595      0.5514        0.6983        0.0000  11.2824
     36      0.9923        0.0438       0.2594      0.5512        0.6983        0.0000  11.3833
     37      0.9913        0.0415       0.2594      0.5512        0.6983        0.0000  11.7912
     38      0.9917        0.0443       0.2592      0.5511        0.6983        0.0000  11.6577
     39      0.9923        0.0417       0.2597      0.5513        0.6983        0.0000  11.6036
     40      0.9928        0.0417       0.2595      0.5512        0.6982        0.0000  11.3538
     41      0.9923        0.0417       0.2595      0.5512        0.6983        0.0000  11.6202
     42      0.9928        0.0430       0.2595      0.5512        0.6982        0.0000  11.6036
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.5778116660312619
F1 Macro Score after query 5: 0.5597007508763358
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_4.pt

Iteration: 6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9744[0m        [32m0.1173[0m       [35m0.2771[0m      [31m0.6448[0m        [94m0.9412[0m     +  0.0001  12.2609
      2      [36m0.9806[0m        [32m0.0871[0m       [35m0.2988[0m      [31m0.6579[0m        [94m0.7580[0m     +  0.0001  12.3726
      3      [36m0.9812[0m        [32m0.0778[0m       [35m0.3224[0m      [31m0.6609[0m        [94m0.7270[0m     +  0.0001  12.0759
      4      [36m0.9837[0m        [32m0.0658[0m       [35m0.3464[0m      [31m0.6728[0m        [94m0.6932[0m     +  0.0001  12.0279
      5      0.9837        0.0674       0.3382      [31m0.6761[0m        [94m0.6819[0m     +  0.0001  12.1455
      6      [36m0.9871[0m        [32m0.0523[0m       0.3193      0.6448        [94m0.6609[0m     +  0.0000  12.1397
      7      [36m0.9883[0m        [32m0.0502[0m       0.3274      0.6552        0.6703        0.0000  12.1070
      8      [36m0.9890[0m        [32m0.0478[0m       0.3269      0.6547        0.6624        0.0000  12.3231
      9      0.9889        [32m0.0465[0m       0.3208      0.6484        0.6740        0.0000  12.4209
     10      [36m0.9899[0m        [32m0.0451[0m       0.3352      0.6586        0.6642        0.0000  12.4340
     11      0.9896        [32m0.0418[0m       0.3102      0.6332        0.6771        0.0000  12.1066
     12      [36m0.9904[0m        0.0436       0.3033      0.6201        0.6835        0.0000  12.4978
     13      [36m0.9907[0m        0.0425       0.3111      0.6286        0.6781        0.0000  12.2627
     14      0.9902        [32m0.0407[0m       0.3130      0.6328        0.6737        0.0000  12.3698
     15      0.9907        0.0422       0.3101      0.6253        0.6773        0.0000  12.3403
     16      0.9904        [32m0.0406[0m       0.3050      0.6180        0.6746        0.0000  12.3369
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.6433505416999927
F1 Macro Score after query 6: 0.6286504965015317
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_5.pt

Iteration: 7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9699[0m        [32m0.0973[0m       [35m0.3019[0m      [31m0.6745[0m        [94m0.8464[0m     +  0.0001  13.3061
      2      [36m0.9783[0m        [32m0.0745[0m       [35m0.3259[0m      [31m0.6764[0m        [94m0.7726[0m     +  0.0001  13.3693
      3      [36m0.9812[0m        [32m0.0688[0m       [35m0.3363[0m      [31m0.6767[0m        [94m0.7545[0m     +  0.0001  13.4315
      4      0.9812        [32m0.0655[0m       [35m0.3448[0m      0.6700        [94m0.7030[0m     +  0.0001  13.3683
      5      [36m0.9813[0m        [32m0.0640[0m       [35m0.3477[0m      0.6706        [94m0.6712[0m     +  0.0001  13.4952
      6      [36m0.9859[0m        [32m0.0571[0m       0.3455      0.6634        0.6838        0.0000  13.4688
      7      0.9859        [32m0.0545[0m       [35m0.3538[0m      0.6688        [94m0.6711[0m     +  0.0000  13.5444
      8      [36m0.9870[0m        [32m0.0511[0m       [35m0.3561[0m      0.6704        0.6722        0.0000  13.4352
      9      0.9863        0.0540       [35m0.3571[0m      0.6696        0.6731        0.0000  13.4800
     10      [36m0.9870[0m        0.0515       0.3540      0.6664        [94m0.6648[0m     +  0.0000  13.6089
     11      [36m0.9872[0m        [32m0.0491[0m       [35m0.3585[0m      0.6744        [94m0.6576[0m     +  0.0000  13.3725
     12      [36m0.9884[0m        [32m0.0477[0m       0.3568      0.6685        0.6615        0.0000  13.4024
     13      0.9884        [32m0.0469[0m       [35m0.3589[0m      0.6703        0.6627        0.0000  13.6369
     14      0.9877        0.0475       0.3573      0.6682        0.6669        0.0000  13.6857
     15      0.9871        0.0473       [35m0.3627[0m      0.6672        [94m0.6523[0m     +  0.0000  13.3576
     16      0.9877        [32m0.0467[0m       0.3590      0.6706        0.6588        0.0000  13.4678
     17      0.9884        [32m0.0455[0m       0.3556      0.6712        0.6636        0.0000  13.2946
     18      0.9882        [32m0.0447[0m       0.3569      0.6707        0.6649        0.0000  13.4805
     19      0.9868        0.0460       0.3580      0.6709        0.6614        0.0000  13.5292
     20      0.9882        [32m0.0438[0m       0.3608      0.6738        0.6553        0.0000  13.2953
     21      0.9880        [32m0.0437[0m       0.3585      0.6714        0.6607        0.0000  13.6858
     22      [36m0.9886[0m        [32m0.0423[0m       0.3575      0.6704        0.6623        0.0000  13.4499
     23      [36m0.9886[0m        0.0440       0.3566      0.6697        0.6643        0.0000  13.2417
     24      0.9881        0.0436       0.3595      0.6727        0.6606        0.0000  13.2305
     25      0.9884        0.0445       0.3583      0.6698        0.6620        0.0000  13.1863
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.697714042311515
F1 Macro Score after query 7: 0.6851236456429213
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_6.pt

Iteration: 8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9812[0m        [32m0.0837[0m       [35m0.5222[0m      [31m0.7374[0m        [94m0.5128[0m     +  0.0001  15.4994
      2      [36m0.9846[0m        [32m0.0625[0m       [35m0.5646[0m      [31m0.7608[0m        [94m0.4569[0m     +  0.0001  15.5305
      3      [36m0.9866[0m        [32m0.0545[0m       [35m0.5943[0m      [31m0.7628[0m        [94m0.4343[0m     +  0.0001  15.6703
      4      [36m0.9883[0m        [32m0.0526[0m       [35m0.6045[0m      0.7601        0.4424        0.0001  15.2500
      5      0.9881        [32m0.0493[0m       [35m0.6061[0m      0.7600        0.4431        0.0001  15.8433
      6      [36m0.9894[0m        [32m0.0443[0m       0.6010      0.7507        0.4520        0.0000  15.6727
      7      0.9893        [32m0.0418[0m       0.6009      0.7483        0.4597        0.0000  15.4716
      8      [36m0.9898[0m        0.0419       [35m0.6125[0m      0.7600        0.4513        0.0000  15.8133
      9      [36m0.9912[0m        [32m0.0399[0m       0.6068      0.7587        0.4627        0.0000  15.3745
     10      0.9903        [32m0.0390[0m       0.6089      0.7600        0.4551        0.0000  15.5930
     11      0.9897        [32m0.0372[0m       [35m0.6130[0m      0.7560        0.4594        0.0000  15.7202
     12      0.9907        [32m0.0369[0m       [35m0.6137[0m      0.7544        0.4581        0.0000  15.3436
     13      0.9905        [32m0.0362[0m       0.6104      0.7547        0.4608        0.0000  15.4067
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.7615220600295226
F1 Macro Score after query 8: 0.7574489958865712
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_7.pt

Iteration: 9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9824[0m        [32m0.0811[0m       [35m0.5104[0m      [31m0.7373[0m        [94m0.4754[0m     +  0.0001  19.3091
      2      [36m0.9837[0m        [32m0.0710[0m       [35m0.5391[0m      [31m0.7517[0m        [94m0.4651[0m     +  0.0001  19.2794
      3      [36m0.9842[0m        [32m0.0657[0m       [35m0.5465[0m      0.7486        0.4809        0.0001  19.1717
      4      [36m0.9847[0m        [32m0.0647[0m       [35m0.5701[0m      [31m0.7546[0m        0.4767        0.0001  19.5058
      5      [36m0.9859[0m        [32m0.0619[0m       [35m0.5849[0m      [31m0.7646[0m        [94m0.4420[0m     +  0.0001  19.3175
      6      [36m0.9872[0m        [32m0.0547[0m       [35m0.5939[0m      0.7562        0.4505        0.0000  19.3694
      7      [36m0.9883[0m        [32m0.0525[0m       [35m0.5991[0m      0.7515        0.4696        0.0000  19.0620
      8      [36m0.9883[0m        [32m0.0518[0m       0.5972      0.7385        0.4627        0.0000  19.3409
      9      [36m0.9888[0m        [32m0.0497[0m       [35m0.6042[0m      0.7504        0.4605        0.0000  19.1556
     10      0.9882        [32m0.0493[0m       [35m0.6104[0m      0.7554        0.4546        0.0000  19.4673
     11      [36m0.9891[0m        [32m0.0462[0m       0.6097      0.7539        0.4641        0.0000  19.0657
     12      [36m0.9893[0m        [32m0.0456[0m       0.6089      0.7506        0.4607        0.0000  19.3250
     13      0.9892        0.0467       [35m0.6116[0m      0.7521        0.4606        0.0000  18.9881
     14      0.9890        [32m0.0453[0m       [35m0.6125[0m      0.7508        0.4649        0.0000  19.3846
     15      0.9893        0.0454       0.6104      0.7499        0.4664        0.0000  18.9204
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.7879387048937223
F1 Macro Score after query 9: 0.7846612310492653
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_8.pt

Iteration: 10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9727[0m        [32m0.1183[0m       [35m0.6549[0m      [31m0.7947[0m        [94m0.3270[0m     +  0.0001  25.0442
      2      [36m0.9759[0m        [32m0.1030[0m       0.6467      [31m0.8026[0m        0.3460        0.0001  25.9374
      3      [36m0.9771[0m        [32m0.0960[0m       0.6503      0.7891        0.3478        0.0001  25.8558
      4      [36m0.9785[0m        [32m0.0916[0m       0.6523      [31m0.8074[0m        0.3457        0.0001  25.7690
      5      [36m0.9792[0m        [32m0.0874[0m       0.6403      0.7906        0.3520        0.0001  25.9672
      6      [36m0.9823[0m        [32m0.0774[0m       0.6457      0.7898        0.3607        0.0000  25.5890
      7      [36m0.9826[0m        [32m0.0749[0m       0.6422      0.7937        0.3706        0.0000  25.9375
      8      [36m0.9830[0m        [32m0.0730[0m       0.6446      0.7900        0.3701        0.0000  25.5822
      9      [36m0.9841[0m        [32m0.0710[0m       0.6363      0.7872        0.3757        0.0000  26.0656
     10      0.9837        0.0711       0.6309      0.7867        0.3731        0.0000  25.6741
     11      [36m0.9847[0m        [32m0.0667[0m       0.6316      0.7852        0.3773        0.0000  25.9765
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.8411732152739347
F1 Macro Score after query 10: 0.836690560126487
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_9.pt

Iteration: 11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9362[0m        [32m0.1943[0m       [35m0.6854[0m      [31m0.8196[0m        [94m0.2993[0m     +  0.0001  37.4873
      2      [36m0.9471[0m        [32m0.1622[0m       [35m0.6932[0m      [31m0.8256[0m        0.3193        0.0001  37.6487
      3      [36m0.9527[0m        [32m0.1434[0m       [35m0.7050[0m      [31m0.8380[0m        0.3196        0.0001  37.9988
      4      [36m0.9578[0m        [32m0.1333[0m       [35m0.7210[0m      0.8313        [94m0.2982[0m     +  0.0001  37.3606
      5      [36m0.9600[0m        [32m0.1264[0m       0.7184      [31m0.8410[0m        0.3183        0.0001  37.9377
      6      [36m0.9655[0m        [32m0.1102[0m       0.7174      0.8359        0.3080        0.0000  38.0145
      7      [36m0.9674[0m        [32m0.1052[0m       0.7181      0.8319        0.3160        0.0000  37.3252
      8      [36m0.9676[0m        [32m0.1034[0m       0.7144      0.8343        0.3139        0.0000  37.7621
      9      [36m0.9682[0m        [32m0.1018[0m       0.7203      0.8365        0.3139        0.0000  37.9955
     10      [36m0.9686[0m        [32m0.0992[0m       0.7161      0.8359        0.3210        0.0000  37.3089
     11      [36m0.9705[0m        [32m0.0935[0m       [35m0.7219[0m      0.8336        0.3130        0.0000  37.8483
     12      0.9705        [32m0.0934[0m       0.7198      0.8348        0.3132        0.0000  37.7790
     13      [36m0.9718[0m        [32m0.0917[0m       [35m0.7227[0m      0.8374        0.3110        0.0000  37.4899
     14      0.9715        0.0921       0.7189      0.8350        0.3149        0.0000  38.0213
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.8930053804765565
F1 Macro Score after query 11: 0.8819559843610345
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_10.pt

Iteration: 12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9434[0m        [32m0.1177[0m       [35m0.7326[0m      [31m0.8135[0m        [94m0.3403[0m     +  0.0001  59.0469
      2      [36m0.9486[0m        [32m0.1046[0m       [35m0.7469[0m      [31m0.8183[0m        [94m0.2990[0m     +  0.0001  59.0362
      3      [36m0.9515[0m        [32m0.0984[0m       0.7377      0.7987        0.3263        0.0001  58.8767
      4      [36m0.9542[0m        [32m0.0912[0m       [35m0.7550[0m      [31m0.8217[0m        0.3119        0.0001  58.8007
      5      [36m0.9554[0m        [32m0.0867[0m       0.7424      0.8176        0.3320        0.0001  58.7302
      6      [36m0.9610[0m        [32m0.0766[0m       0.7474      [31m0.8296[0m        0.3583        0.0000  58.5218
      7      [36m0.9634[0m        [32m0.0728[0m       0.7464      [31m0.8315[0m        0.3530        0.0000  58.5152
      8      [36m0.9641[0m        [32m0.0707[0m       0.7410      0.8279        0.3468        0.0000  58.4727
      9      [36m0.9651[0m        [32m0.0700[0m       0.7425      [31m0.8334[0m        0.3435        0.0000  58.7347
     10      [36m0.9652[0m        [32m0.0681[0m       0.7387      0.8297        0.3590        0.0000  59.0812
     11      [36m0.9676[0m        [32m0.0642[0m       0.7396      0.8329        0.3452        0.0000  59.1395
     12      [36m0.9676[0m        [32m0.0634[0m       0.7403      [31m0.8352[0m        0.3435        0.0000  58.5015
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.8839044009590843
F1 Macro Score after query 12: 0.865563256650455
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_11.pt

Iteration: 13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9440[0m        [32m0.1007[0m       [35m0.7175[0m      [31m0.7850[0m        [94m0.3610[0m     +  0.0001  67.1852
      2      [36m0.9469[0m        [32m0.0931[0m       [35m0.7184[0m      0.7570        0.3702        0.0001  67.2782
      3      [36m0.9507[0m        [32m0.0860[0m       [35m0.7250[0m      [31m0.7864[0m        [94m0.3574[0m     +  0.0001  66.7054
      4      [36m0.9540[0m        [32m0.0815[0m       [35m0.7267[0m      0.7661        [94m0.3495[0m     +  0.0001  62.8266
      5      [36m0.9550[0m        [32m0.0780[0m       0.7267      0.7670        0.3882        0.0001  62.5463
      6      [36m0.9609[0m        [32m0.0684[0m       [35m0.7382[0m      [31m0.8073[0m        [94m0.3455[0m     +  0.0000  62.5106
      7      [36m0.9631[0m        [32m0.0656[0m       [35m0.7387[0m      0.8009        0.3540        0.0000  62.6057
      8      [36m0.9637[0m        [32m0.0636[0m       [35m0.7410[0m      [31m0.8142[0m        [94m0.3383[0m     +  0.0000  62.6691
      9      [36m0.9637[0m        [32m0.0628[0m       0.7384      0.8069        [94m0.3358[0m     +  0.0000  62.5470
     10      [36m0.9652[0m        [32m0.0612[0m       [35m0.7425[0m      [31m0.8146[0m        [94m0.3299[0m     +  0.0000  62.4376
     11      [36m0.9670[0m        [32m0.0575[0m       0.7403      [31m0.8324[0m        0.3445        0.0000  62.4852
     12      [36m0.9682[0m        [32m0.0564[0m       [35m0.7444[0m      [31m0.8328[0m        0.3311        0.0000  62.4819
     13      0.9672        [32m0.0558[0m       0.7424      0.8305        [94m0.3278[0m     +  0.0000  62.3843
     14      0.9675        0.0560       0.7425      0.8321        [94m0.3235[0m     +  0.0000  62.5751
     15      [36m0.9687[0m        [32m0.0550[0m       0.7425      0.8304        0.3310        0.0000  62.5465
     16      [36m0.9688[0m        [32m0.0546[0m       0.7382      [31m0.8346[0m        [94m0.3191[0m     +  0.0000  62.5033
     17      [36m0.9690[0m        [32m0.0540[0m       0.7354      [31m0.8366[0m        0.3231        0.0000  62.4836
     18      0.9687        [32m0.0536[0m       0.7403      0.8361        [94m0.3160[0m     +  0.0000  62.4346
     19      [36m0.9695[0m        [32m0.0533[0m       0.7418      [31m0.8398[0m        0.3214        0.0000  62.4422
     20      0.9688        [32m0.0532[0m       0.7384      0.8361        0.3174        0.0000  62.4247
     21      0.9690        0.0539       0.7392      0.8365        [94m0.3020[0m     +  0.0000  62.4421
     22      [36m0.9696[0m        [32m0.0531[0m       0.7406      0.8383        0.3053        0.0000  62.4680
     23      0.9691        [32m0.0526[0m       0.7382      0.8367        0.3038        0.0000  62.2931
     24      [36m0.9696[0m        0.0533       0.7396      0.8371        0.3026        0.0000  62.5828
     25      0.9695        [32m0.0524[0m       0.7405      0.8371        0.3041        0.0000  62.4554
     26      0.9693        0.0532       0.7406      [31m0.8419[0m        [94m0.3014[0m     +  0.0000  62.3415
     27      [36m0.9698[0m        [32m0.0524[0m       0.7438      [31m0.8442[0m        [94m0.2981[0m     +  0.0000  62.2875
     28      0.9698        [32m0.0522[0m       0.7418      0.8429        0.2996        0.0000  62.5785
     29      0.9696        0.0525       0.7420      0.8421        0.2983        0.0000  62.3894
     30      [36m0.9700[0m        0.0523       0.7401      0.8418        0.2997        0.0000  62.4855
     31      0.9696        0.0525       0.7418      0.8441        [94m0.2968[0m     +  0.0000  62.4236
     32      0.9688        0.0527       0.7417      0.8439        [94m0.2945[0m     +  0.0000  62.2519
     33      0.9693        [32m0.0521[0m       0.7418      [31m0.8444[0m        [94m0.2935[0m     +  0.0000  62.3274
     34      [36m0.9703[0m        0.0521       0.7427      0.8440        [94m0.2918[0m     +  0.0000  62.2819
     35      0.9693        0.0527       0.7425      0.8441        0.2920        0.0000  62.2547
     36      0.9697        [32m0.0519[0m       0.7434      0.8443        [94m0.2894[0m     +  0.0000  62.2208
     37      0.9693        0.0525       0.7424      0.8442        0.2905        0.0000  62.3402
     38      0.9693        0.0521       0.7417      0.8440        0.2902        0.0000  63.2825
     39      0.9698        0.0523       0.7420      0.8440        0.2898        0.0000  62.9486
     40      0.9701        0.0521       0.7422      0.8441        0.2898        0.0000  63.1408
     41      0.9692        0.0521       0.7417      0.8437        [94m0.2894[0m     +  0.0000  62.4532
     42      0.9690        0.0528       0.7417      0.8439        0.2895        0.0000  62.2945
     43      0.9695        0.0529       0.7415      0.8437        [94m0.2891[0m     +  0.0000  62.3256
     44      0.9702        0.0520       0.7415      0.8434        [94m0.2885[0m     +  0.0000  62.5163
     45      0.9693        0.0523       0.7415      0.8438        0.2885        0.0000  62.8030
     46      [36m0.9705[0m        [32m0.0517[0m       0.7415      0.8437        0.2886        0.0000  62.7790
     47      0.9691        0.0523       0.7417      0.8438        [94m0.2884[0m     +  0.0000  62.7178
     48      0.9696        0.0521       0.7418      0.8438        [94m0.2884[0m     +  0.0000  63.2555
     49      0.9691        0.0526       0.7417      0.8434        [94m0.2880[0m     +  0.0000  63.8098
     50      0.9698        [32m0.0517[0m       0.7418      0.8435        [94m0.2879[0m     +  0.0000  63.1725
     51      0.9693        0.0522       0.7417      0.8434        0.2880        0.0000  63.2969
     52      0.9694        0.0518       0.7417      0.8434        0.2879        0.0000  63.6577
     53      0.9694        0.0521       0.7417      0.8434        [94m0.2879[0m     +  0.0000  63.4680
     54      [36m0.9709[0m        0.0519       0.7415      0.8433        [94m0.2879[0m     +  0.0000  64.4841
     55      0.9691        0.0523       0.7415      0.8433        [94m0.2878[0m     +  0.0000  64.8279
     56      0.9698        0.0521       0.7415      0.8433        0.2878        0.0000  64.9415
     57      0.9695        0.0519       0.7417      0.8434        [94m0.2878[0m     +  0.0000  64.9707
     58      0.9697        0.0522       0.7417      0.8434        [94m0.2877[0m     +  0.0000  64.8694
     59      0.9698        0.0520       0.7417      0.8434        0.2877        0.0000  64.8015
     60      0.9695        0.0522       0.7417      0.8434        [94m0.2877[0m     +  0.0000  64.7791
     61      0.9697        0.0523       0.7415      0.8433        [94m0.2877[0m     +  0.0000  64.5036
     62      0.9694        0.0519       0.7415      0.8433        [94m0.2876[0m     +  0.0000  64.9551
     63      0.9696        0.0520       0.7415      0.8433        [94m0.2876[0m     +  0.0000  64.9828
     64      0.9697        0.0519       0.7415      0.8433        [94m0.2876[0m     +  0.0000  65.0180
     65      0.9696        0.0526       0.7415      0.8434        [94m0.2876[0m     +  0.0000  64.6917
     66      0.9697        [32m0.0516[0m       0.7415      0.8433        [94m0.2875[0m     +  0.0000  64.8083
     67      0.9704        0.0522       0.7415      0.8433        [94m0.2875[0m     +  0.0000  64.8459
     68      0.9699        0.0522       0.7415      0.8433        [94m0.2875[0m     +  0.0000  64.7850
     69      0.9702        0.0523       0.7415      0.8433        [94m0.2875[0m     +  0.0000  64.8254
     70      0.9701        0.0526       0.7415      0.8433        [94m0.2875[0m     +  0.0000  64.8541
     71      0.9693        0.0525       0.7415      0.8433        [94m0.2875[0m     +  0.0000  64.7861
     72      0.9693        0.0523       0.7415      0.8433        [94m0.2875[0m     +  0.0000  64.6098
     73      0.9702        0.0523       0.7415      0.8433        [94m0.2875[0m     +  0.0000  65.1267
     74      0.9695        0.0520       0.7415      0.8433        [94m0.2875[0m     +  0.0000  64.6248
     75      0.9695        0.0523       0.7415      0.8433        [94m0.2875[0m     +  0.0000  64.5806
     76      0.9700        0.0523       0.7415      0.8433        [94m0.2875[0m     +  0.0000  65.0003
     77      0.9698        0.0518       0.7415      0.8433        [94m0.2875[0m     +  0.0000  64.8786
     78      0.9698        0.0520       0.7415      0.8433        [94m0.2875[0m     +  0.0000  64.8697
     79      0.9696        0.0520       0.7415      0.8433        [94m0.2875[0m     +  0.0000  65.0540
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.8975438059530186
F1 Macro Score after query 13: 0.8855439704289184
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\model_checkpoint_iteration_12.pt
Pickle file saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/max_score_seed44_config2\AL_max_score_results_for_multilabel_classification.pickle
