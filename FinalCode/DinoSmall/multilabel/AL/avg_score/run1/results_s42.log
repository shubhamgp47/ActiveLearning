Adjusted shapes after loading:
X_initial_np: (8, 224, 224, 3), y_initial_np: (8, 3)
X_pool_np: (26872, 224, 224, 3), y_pool_np: (26872, 3)
X_test_np: (5760, 224, 224, 3), y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3), y_val_np: (5760, 3)
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.6963[0m        [32m0.6694[0m       [35m0.0210[0m      [31m0.2447[0m        [94m0.7251[0m     +  0.0000  12.5604
      2      0.6035        [32m0.6429[0m       [35m0.1413[0m      [31m0.4842[0m        [94m0.7204[0m     +  0.0000  10.6663
      3      [36m0.7090[0m        0.6429       [35m0.2264[0m      0.4404        [94m0.6974[0m     +  0.0000  10.8679
      4      [36m0.8320[0m        [32m0.6066[0m       0.1587      0.4439        0.7094        0.0000  10.7604
      5      0.7389        [32m0.5923[0m       [35m0.2753[0m      0.4530        0.6996        0.0000  10.7816
      6      0.8320        [32m0.5545[0m       0.2490      0.4541        0.7084        0.0000  10.9406
      7      [36m0.8889[0m        [32m0.5539[0m       0.2568      0.4521        0.7083        0.0000  10.9827
      8      0.8333        [32m0.5391[0m       0.2441      0.4534        0.7119        0.0000  10.8968
      9      0.8333        [32m0.5271[0m       0.2625      0.4558        0.7121        0.0000  10.7707
     10      0.8333        [32m0.5047[0m       0.2462      0.4523        0.7132        0.0000  10.9835
     11      0.8333        0.5390       0.2391      0.4512        0.7137        0.0000  11.0274
     12      0.8796        0.5228       0.2403      0.4510        0.7143        0.0000  11.1217
     13      0.8796        0.5083       0.2385      0.4500        0.7141        0.0000  10.9377
Stopping since valid_loss has not improved in the last 11 epochs.
Pre F1 micro score = 0.4609
Pre F1 macro score = 0.4592
Pre Accuracy = 0.2559

Iteration: 1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9356[0m        [32m0.4735[0m       [35m0.2615[0m      [31m0.5761[0m        [94m0.7461[0m     +  0.0000  11.0285
      2      0.9171        [32m0.4367[0m       0.1976      0.5461        0.7574        0.0000  10.9843
      3      [36m0.9445[0m        [32m0.4135[0m       0.1851      0.5305        0.7733        0.0000  11.0362
      4      0.9162        [32m0.3826[0m       0.2403      0.5026        0.7630        0.0000  11.0210
      5      0.9276        0.3924       0.1731      0.5436        0.7583        0.0000  10.9701
      6      0.9045        0.3877       0.2009      0.5283        0.7707        0.0000  10.9195
      7      [36m0.9534[0m        [32m0.3712[0m       0.2116      0.5201        0.7692        0.0000  11.0313
      8      0.9445        [32m0.3521[0m       0.2207      0.5187        0.7671        0.0000  10.9190
      9      0.9534        0.3639       0.2175      0.5227        0.7663        0.0000  10.9639
     10      0.9534        [32m0.3503[0m       0.2215      0.5240        0.7646        0.0000  11.0514
     11      0.9360        0.3614       0.2207      0.5228        0.7669        0.0000  11.0033
Stopping since valid_loss has not improved in the last 11 epochs.
[24]
F1 Micro Score after query 1: 0.5111212575826757
F1 Macro Score after query 1: 0.5052884386449668
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_0.pt

Iteration: 2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9062[0m        [32m0.4812[0m       [35m0.1057[0m      [31m0.4717[0m        [94m0.8047[0m     +  0.0000  11.1720
      2      [36m0.9144[0m        [32m0.4251[0m       [35m0.1457[0m      [31m0.4793[0m        [94m0.7583[0m     +  0.0000  11.0774
      3      [36m0.9378[0m        [32m0.3721[0m       [35m0.1708[0m      [31m0.4883[0m        [94m0.7415[0m     +  0.0000  11.0799
      4      0.9342        [32m0.3670[0m       [35m0.1882[0m      0.4880        [94m0.6964[0m     +  0.0000  11.2131
      5      0.9341        [32m0.3319[0m       [35m0.2217[0m      [31m0.5002[0m        [94m0.6841[0m     +  0.0000  11.0352
      6      [36m0.9546[0m        [32m0.3126[0m       0.2214      0.4955        [94m0.6756[0m     +  0.0000  11.1692
      7      [36m0.9580[0m        [32m0.3019[0m       [35m0.2288[0m      0.4990        [94m0.6676[0m     +  0.0000  11.0944
      8      0.9513        0.3063       0.2243      0.4999        [94m0.6620[0m     +  0.0000  11.2757
      9      [36m0.9679[0m        [32m0.2970[0m       0.2222      [31m0.5022[0m        [94m0.6620[0m     +  0.0000  11.2519
     10      0.9647        [32m0.2874[0m       0.2276      [31m0.5045[0m        [94m0.6554[0m     +  0.0000  11.1045
     11      0.9647        0.2921       [35m0.2313[0m      [31m0.5065[0m        [94m0.6523[0m     +  0.0000  11.1138
     12      [36m0.9717[0m        [32m0.2850[0m       [35m0.2339[0m      [31m0.5080[0m        [94m0.6509[0m     +  0.0000  11.0901
     13      0.9646        [32m0.2781[0m       [35m0.2340[0m      [31m0.5095[0m        [94m0.6497[0m     +  0.0000  11.0511
     14      0.9680        0.2787       0.2321      [31m0.5114[0m        [94m0.6489[0m     +  0.0000  11.0827
     15      0.9578        0.2923       0.2318      [31m0.5127[0m        [94m0.6488[0m     +  0.0000  11.0776
     16      0.9682        [32m0.2777[0m       0.2333      [31m0.5143[0m        [94m0.6479[0m     +  0.0000  11.1390
     17      0.9711        0.2793       0.2337      [31m0.5143[0m        [94m0.6465[0m     +  0.0000  11.0596
     18      [36m0.9781[0m        0.2790       [35m0.2342[0m      [31m0.5148[0m        [94m0.6465[0m     +  0.0000  11.1670
     19      0.9747        [32m0.2648[0m       [35m0.2345[0m      [31m0.5161[0m        [94m0.6457[0m     +  0.0000  11.2655
     20      0.9643        0.2810       [35m0.2377[0m      [31m0.5169[0m        [94m0.6449[0m     +  0.0000  11.0039
     21      0.9711        0.2760       0.2373      0.5169        0.6449        0.0000  11.2185
     22      0.9713        0.2766       [35m0.2380[0m      [31m0.5175[0m        [94m0.6447[0m     +  0.0000  11.1304
     23      [36m0.9816[0m        0.2715       [35m0.2382[0m      0.5167        [94m0.6446[0m     +  0.0000  11.1878
     24      [36m0.9850[0m        0.2654       [35m0.2384[0m      0.5165        [94m0.6444[0m     +  0.0000  11.2133
     25      0.9747        0.2760       0.2373      0.5167        [94m0.6443[0m     +  0.0000  11.1866
     26      0.9713        0.2739       0.2375      0.5168        [94m0.6442[0m     +  0.0000  11.1574
     27      0.9714        0.2662       0.2375      0.5169        [94m0.6441[0m     +  0.0000  11.0988
     28      0.9714        [32m0.2616[0m       0.2378      0.5171        [94m0.6441[0m     +  0.0000  11.0609
     29      0.9714        0.2638       0.2380      0.5173        [94m0.6440[0m     +  0.0000  11.0888
     30      0.9749        0.2712       0.2378      [31m0.5175[0m        [94m0.6439[0m     +  0.0000  11.0727
     31      0.9747        0.2737       0.2382      [31m0.5178[0m        [94m0.6439[0m     +  0.0000  11.0155
     32      0.9714        0.2714       0.2377      0.5178        0.6439        0.0000  11.1836
     33      0.9714        0.2703       0.2384      0.5178        [94m0.6439[0m     +  0.0000  11.2436
     34      0.9781        0.2724       0.2384      [31m0.5180[0m        0.6439        0.0000  11.1253
     35      0.9816        0.2784       0.2382      [31m0.5181[0m        0.6439        0.0000  11.2185
     36      0.9747        0.2853       0.2382      [31m0.5181[0m        0.6439        0.0000  11.3880
     37      0.9749        0.2642       0.2382      0.5181        0.6439        0.0000  11.0756
     38      0.9681        0.2699       0.2382      0.5180        0.6439        0.0000  11.4969
     39      0.9714        0.2664       0.2382      0.5180        0.6439        0.0000  11.1245
     40      0.9674        0.2712       0.2384      [31m0.5182[0m        [94m0.6439[0m     +  0.0000  11.1587
     41      0.9816        [32m0.2599[0m       0.2382      0.5181        [94m0.6439[0m     +  0.0000  11.2981
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56]
F1 Micro Score after query 2: 0.5197618535255721
F1 Macro Score after query 2: 0.5216468466449256
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_1.pt

Iteration: 3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9805[0m        [32m0.2484[0m       [35m0.2747[0m      [31m0.6097[0m        [94m0.6821[0m     +  0.0000  11.3303
      2      [36m0.9869[0m        [32m0.2060[0m       [35m0.3172[0m      [31m0.6132[0m        [94m0.6290[0m     +  0.0000  11.3412
      3      [36m0.9901[0m        [32m0.1965[0m       [35m0.3589[0m      0.6106        [94m0.6143[0m     +  0.0000  11.3760
      4      [36m0.9934[0m        [32m0.1846[0m       [35m0.3618[0m      [31m0.6170[0m        [94m0.6099[0m     +  0.0000  11.4330
      5      0.9934        0.1893       [35m0.3736[0m      [31m0.6183[0m        [94m0.6081[0m     +  0.0000  11.2830
      6      0.9934        [32m0.1770[0m       0.3661      0.6146        0.6112        0.0000  11.3545
      7      0.9934        0.1793       [35m0.3769[0m      0.6124        [94m0.6069[0m     +  0.0000  11.4564
      8      0.9934        0.1875       0.3724      0.6144        0.6093        0.0000  11.3459
      9      0.9934        [32m0.1723[0m       [35m0.3847[0m      [31m0.6226[0m        [94m0.6036[0m     +  0.0000  11.1621
     10      0.9934        [32m0.1706[0m       0.3760      0.6172        0.6052        0.0000  11.2826
     11      0.9934        0.1712       0.3797      0.6179        [94m0.6023[0m     +  0.0000  11.3558
     12      0.9934        [32m0.1677[0m       0.3783      0.6173        [94m0.6022[0m     +  0.0000  11.4238
     13      0.9934        0.1718       0.3778      0.6138        [94m0.6021[0m     +  0.0000  11.3307
     14      0.9934        0.1704       0.3722      0.6114        0.6035        0.0000  11.2853
     15      0.9934        0.1699       0.3783      0.6108        [94m0.6008[0m     +  0.0000  11.3534
     16      0.9934        0.1709       0.3800      0.6119        [94m0.6004[0m     +  0.0000  11.2686
     17      0.9934        0.1744       0.3795      0.6112        0.6008        0.0000  11.2456
     18      0.9934        0.1712       0.3788      0.6112        0.6010        0.0000  11.2995
     19      0.9934        [32m0.1615[0m       0.3825      0.6120        [94m0.5998[0m     +  0.0000  11.4173
     20      0.9934        0.1694       0.3830      0.6118        [94m0.5990[0m     +  0.0000  11.3368
     21      0.9934        0.1710       0.3830      0.6116        0.5994        0.0000  11.2200
     22      0.9934        0.1722       0.3830      0.6115        0.5995        0.0000  11.2767
     23      0.9934        0.1674       0.3825      0.6113        [94m0.5989[0m     +  0.0000  11.2979
     24      0.9934        0.1728       0.3816      0.6108        0.5991        0.0000  11.3073
     25      0.9934        0.1661       0.3814      0.6105        0.5993        0.0000  11.3105
     26      0.9934        0.1683       0.3812      0.6105        0.5990        0.0000  11.2664
     27      0.9934        0.1640       0.3811      0.6104        0.5992        0.0000  11.2771
     28      0.9934        0.1732       0.3816      0.6107        0.5990        0.0000  11.3300
     29      0.9934        0.1665       0.3812      0.6104        0.5992        0.0000  11.3797
     30      0.9934        0.1696       0.3812      0.6101        0.5992        0.0000  11.5678
     31      0.9934        0.1696       0.3809      0.6099        0.5992        0.0000  11.3717
     32      0.9918        0.1670       0.3809      0.6101        0.5992        0.0000  11.2792
     33      0.9934        0.1662       0.3814      0.6105        0.5992        0.0000  11.3772
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112]
F1 Micro Score after query 3: 0.6087146286462501
F1 Macro Score after query 3: 0.5970723170643863
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_2.pt

Iteration: 4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9824[0m        [32m0.2081[0m       [35m0.2812[0m      [31m0.5892[0m        [94m0.6395[0m     +  0.0000  11.9199
      2      0.9815        [32m0.2044[0m       [35m0.3606[0m      [31m0.6299[0m        [94m0.6054[0m     +  0.0000  11.7196
      3      [36m0.9832[0m        [32m0.1852[0m       [35m0.3764[0m      0.6199        0.6087        0.0000  11.6880
      4      0.9832        [32m0.1824[0m       [35m0.3932[0m      0.6116        [94m0.5984[0m     +  0.0000  11.7762
      5      0.9832        0.1870       [35m0.4002[0m      0.6187        0.6011        0.0000  11.6722
      6      0.9824        0.1827       [35m0.4016[0m      0.6112        [94m0.5908[0m     +  0.0000  11.8317
      7      0.9832        0.1869       0.3993      0.6174        0.5934        0.0000  11.7169
      8      0.9832        [32m0.1823[0m       0.3990      0.6058        [94m0.5874[0m     +  0.0000  11.7888
      9      0.9832        [32m0.1789[0m       0.4012      0.6191        0.5890        0.0000  11.7979
     10      0.9832        [32m0.1774[0m       0.3885      0.6046        0.5901        0.0000  11.5455
     11      0.9832        [32m0.1745[0m       0.3962      0.6172        0.5906        0.0000  11.8298
     12      [36m0.9840[0m        0.1777       0.3937      0.6148        0.5932        0.0000  11.7475
     13      0.9832        0.1760       0.3955      0.6152        0.5897        0.0000  11.6819
     14      0.9832        0.1791       0.3936      0.6116        0.5887        0.0000  11.7811
     15      0.9832        0.1768       0.3906      0.6091        0.5924        0.0000  11.5837
     16      0.9832        0.1765       0.3929      0.6106        0.5913        0.0000  11.4720
     17      0.9840        0.1755       0.3941      0.6127        0.5905        0.0000  11.7670
     18      0.9832        [32m0.1728[0m       0.3939      0.6117        0.5899        0.0000  11.7295
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208]
F1 Micro Score after query 4: 0.6151701877934271
F1 Macro Score after query 4: 0.602815745901762
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_3.pt

Iteration: 5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9773[0m        [32m0.2024[0m       [35m0.3172[0m      [31m0.6346[0m        [94m0.6327[0m     +  0.0000  12.4375
      2      [36m0.9786[0m        [32m0.1851[0m       [35m0.3540[0m      0.6070        [94m0.5886[0m     +  0.0000  12.4085
      3      [36m0.9803[0m        [32m0.1772[0m       [35m0.4045[0m      [31m0.6435[0m        [94m0.5786[0m     +  0.0000  12.4457
      4      [36m0.9817[0m        0.1778       0.3727      0.6089        0.5857        0.0000  12.4376
      5      [36m0.9826[0m        [32m0.1718[0m       0.3870      0.6333        0.5819        0.0000  12.5928
      6      [36m0.9836[0m        [32m0.1678[0m       0.3955      0.6249        0.5838        0.0000  12.2790
      7      [36m0.9840[0m        [32m0.1635[0m       0.4007      0.6322        0.5819        0.0000  12.4245
      8      0.9840        0.1640       [35m0.4083[0m      0.6262        [94m0.5784[0m     +  0.0000  12.3396
      9      0.9830        [32m0.1610[0m       0.4012      0.6232        0.5827        0.0000  12.4185
     10      0.9840        0.1630       0.3927      0.6196        0.5845        0.0000  12.4637
     11      0.9840        0.1614       0.3958      0.6229        0.5849        0.0000  12.3290
     12      [36m0.9863[0m        [32m0.1585[0m       0.3932      0.6218        0.5860        0.0000  12.5052
     13      0.9854        [32m0.1584[0m       0.4033      0.6271        0.5815        0.0000  12.1720
     14      0.9844        [32m0.1569[0m       0.4012      0.6301        0.5830        0.0000  12.4023
     15      0.9854        0.1580       0.3979      0.6259        0.5844        0.0000  12.3650
     16      0.9849        [32m0.1550[0m       0.3970      0.6253        0.5832        0.0000  12.3013
     17      0.9859        0.1551       0.3990      0.6243        0.5813        0.0000  12.5702
     18      0.9854        0.1570       0.4009      0.6273        0.5821        0.0000  12.3369
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.6201776095501529
F1 Macro Score after query 5: 0.6053299215217701
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_4.pt

Iteration: 6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9819[0m        [32m0.1717[0m       [35m0.3609[0m      [31m0.6779[0m        [94m0.6594[0m     +  0.0000  13.5736
      2      [36m0.9823[0m        [32m0.1577[0m       0.3550      0.6717        [94m0.6202[0m     +  0.0000  13.7135
      3      [36m0.9845[0m        [32m0.1524[0m       [35m0.3611[0m      0.6655        [94m0.5933[0m     +  0.0000  13.6206
      4      [36m0.9863[0m        [32m0.1471[0m       0.3585      0.6638        [94m0.5865[0m     +  0.0000  13.5870
      5      [36m0.9870[0m        [32m0.1447[0m       [35m0.3658[0m      0.6684        [94m0.5808[0m     +  0.0000  13.4506
      6      [36m0.9886[0m        [32m0.1370[0m       [35m0.4182[0m      [31m0.6846[0m        [94m0.5439[0m     +  0.0000  13.6200
      7      [36m0.9889[0m        0.1372       0.4148      0.6835        0.5456        0.0000  13.6371
      8      [36m0.9891[0m        [32m0.1355[0m       0.4123      0.6841        0.5445        0.0000  13.8723
      9      [36m0.9896[0m        [32m0.1345[0m       0.4174      0.6846        [94m0.5413[0m     +  0.0000  13.7296
     10      [36m0.9896[0m        [32m0.1308[0m       0.4128      0.6805        [94m0.5411[0m     +  0.0000  13.5419
     11      0.9891        [32m0.1295[0m       [35m0.4243[0m      [31m0.6871[0m        [94m0.5365[0m     +  0.0000  13.7465
     12      0.9894        0.1310       0.4238      0.6863        [94m0.5363[0m     +  0.0000  13.7155
     13      0.9894        [32m0.1279[0m       [35m0.4306[0m      [31m0.6884[0m        [94m0.5352[0m     +  0.0000  13.6055
     14      0.9889        0.1296       0.4299      0.6878        0.5354        0.0000  13.6039
     15      0.9896        [32m0.1262[0m       0.4259      0.6866        0.5358        0.0000  13.7307
     16      [36m0.9902[0m        [32m0.1254[0m       [35m0.4340[0m      [31m0.6901[0m        [94m0.5349[0m     +  0.0000  13.7165
     17      0.9888        [32m0.1243[0m       [35m0.4345[0m      [31m0.6905[0m        [94m0.5345[0m     +  0.0000  13.6683
     18      0.9891        0.1253       [35m0.4358[0m      0.6905        [94m0.5336[0m     +  0.0000  13.6825
     19      0.9894        [32m0.1233[0m       0.4354      [31m0.6906[0m        0.5343        0.0000  13.6048
     20      0.9896        0.1267       [35m0.4359[0m      [31m0.6908[0m        0.5346        0.0000  13.6371
     21      0.9891        0.1248       [35m0.4368[0m      [31m0.6908[0m        0.5339        0.0000  13.7611
     22      0.9896        [32m0.1226[0m       0.4366      [31m0.6909[0m        0.5336        0.0000  13.5091
     23      0.9899        [32m0.1225[0m       [35m0.4377[0m      [31m0.6913[0m        [94m0.5334[0m     +  0.0000  13.5750
     24      0.9897        0.1247       0.4370      0.6910        [94m0.5328[0m     +  0.0000  13.6539
     25      0.9896        0.1227       [35m0.4385[0m      [31m0.6913[0m        [94m0.5326[0m     +  0.0000  13.6980
     26      0.9894        0.1228       0.4384      0.6911        [94m0.5325[0m     +  0.0000  13.8092
     27      0.9894        [32m0.1220[0m       [35m0.4387[0m      0.6912        [94m0.5324[0m     +  0.0000  13.5415
     28      0.9891        [32m0.1219[0m       [35m0.4396[0m      [31m0.6917[0m        0.5326        0.0000  13.5726
     29      0.9902        0.1233       0.4396      0.6916        0.5327        0.0000  13.5129
     30      0.9894        0.1226       0.4392      0.6915        0.5329        0.0000  13.5264
     31      0.9899        0.1240       0.4394      0.6915        0.5329        0.0000  13.7119
     32      0.9894        0.1232       0.4396      0.6915        0.5327        0.0000  13.9315
     33      0.9896        0.1240       0.4394      0.6914        0.5327        0.0000  13.5121
     34      0.9894        0.1235       0.4394      0.6914        0.5326        0.0000  13.8072
     35      0.9894        0.1223       0.4394      0.6915        0.5326        0.0000  13.6828
     36      0.9902        0.1241       0.4394      0.6915        0.5326        0.0000  13.5900
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.6862342638204708
F1 Macro Score after query 6: 0.6917347207370499
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_5.pt

Iteration: 7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9890[0m        [32m0.1266[0m       [35m0.3550[0m      [31m0.6781[0m        [94m0.6296[0m     +  0.0000  15.7008
      2      [36m0.9913[0m        [32m0.1125[0m       [35m0.3731[0m      [31m0.6886[0m        [94m0.5651[0m     +  0.0000  15.7771
      3      [36m0.9920[0m        [32m0.1085[0m       0.3674      0.6838        0.5653        0.0000  15.7843
      4      [36m0.9923[0m        [32m0.1016[0m       [35m0.3823[0m      [31m0.6950[0m        [94m0.5463[0m     +  0.0000  15.9355
      5      [36m0.9930[0m        [32m0.0985[0m       0.3766      [31m0.6957[0m        [94m0.5456[0m     +  0.0000  15.9334
      6      [36m0.9936[0m        [32m0.0948[0m       [35m0.4056[0m      [31m0.7045[0m        [94m0.5215[0m     +  0.0000  15.7160
      7      [36m0.9939[0m        [32m0.0920[0m       [35m0.4128[0m      [31m0.7065[0m        [94m0.5203[0m     +  0.0000  15.7450
      8      [36m0.9944[0m        [32m0.0895[0m       [35m0.4212[0m      [31m0.7070[0m        [94m0.5129[0m     +  0.0000  15.8883
      9      0.9944        [32m0.0873[0m       [35m0.4219[0m      0.7069        [94m0.5122[0m     +  0.0000  15.7941
     10      [36m0.9947[0m        0.0876       [35m0.4252[0m      0.7068        [94m0.5103[0m     +  0.0000  15.7461
     11      0.9944        [32m0.0849[0m       [35m0.4354[0m      [31m0.7104[0m        [94m0.5027[0m     +  0.0000  15.7312
     12      [36m0.9954[0m        [32m0.0840[0m       [35m0.4410[0m      [31m0.7122[0m        [94m0.5012[0m     +  0.0000  15.7456
     13      0.9946        [32m0.0833[0m       0.4387      0.7105        [94m0.5004[0m     +  0.0000  15.6201
     14      [36m0.9954[0m        [32m0.0813[0m       [35m0.4455[0m      [31m0.7127[0m        [94m0.4973[0m     +  0.0000  15.8711
     15      0.9954        [32m0.0804[0m       [35m0.4464[0m      0.7126        [94m0.4968[0m     +  0.0000  15.7624
     16      0.9953        [32m0.0800[0m       [35m0.4477[0m      0.7118        [94m0.4957[0m     +  0.0000  15.6374
     17      [36m0.9961[0m        0.0800       [35m0.4490[0m      0.7119        [94m0.4950[0m     +  0.0000  15.7153
     18      0.9954        [32m0.0795[0m       0.4486      0.7112        0.4954        0.0000  15.7302
     19      [36m0.9964[0m        [32m0.0771[0m       [35m0.4495[0m      0.7115        [94m0.4941[0m     +  0.0000  15.7924
     20      0.9960        0.0782       [35m0.4503[0m      [31m0.7129[0m        [94m0.4940[0m     +  0.0000  15.6999
     21      0.9964        0.0779       [35m0.4514[0m      [31m0.7141[0m        [94m0.4939[0m     +  0.0000  15.5195
     22      0.9957        0.0777       [35m0.4524[0m      0.7133        [94m0.4928[0m     +  0.0000  15.7928
     23      0.9964        [32m0.0758[0m       0.4521      0.7122        [94m0.4925[0m     +  0.0000  15.8226
     24      [36m0.9967[0m        0.0780       [35m0.4536[0m      0.7138        0.4928        0.0000  15.8249
     25      0.9960        0.0763       0.4516      0.7125        0.4931        0.0000  15.7582
     26      0.9958        0.0785       [35m0.4540[0m      [31m0.7147[0m        0.4930        0.0000  15.7276
     27      0.9954        0.0790       [35m0.4552[0m      [31m0.7150[0m        0.4926        0.0000  15.7099
     28      0.9963        0.0769       [35m0.4557[0m      [31m0.7152[0m        [94m0.4924[0m     +  0.0000  15.7451
     29      0.9964        0.0766       [35m0.4561[0m      [31m0.7153[0m        0.4925        0.0000  15.7450
     30      0.9964        0.0759       0.4559      0.7151        [94m0.4923[0m     +  0.0000  15.9044
     31      [36m0.9973[0m        [32m0.0751[0m       0.4559      [31m0.7155[0m        0.4924        0.0000  15.7773
     32      0.9963        0.0769       0.4559      0.7153        0.4924        0.0000  15.7458
     33      0.9966        0.0767       [35m0.4562[0m      [31m0.7155[0m        0.4923        0.0000  15.8542
     34      0.9966        0.0772       [35m0.4571[0m      [31m0.7158[0m        [94m0.4921[0m     +  0.0000  15.5423
     35      0.9960        0.0788       0.4571      0.7157        [94m0.4921[0m     +  0.0000  15.6049
     36      0.9961        0.0772       0.4571      0.7157        [94m0.4921[0m     +  0.0000  15.7452
     37      0.9963        0.0778       [35m0.4575[0m      [31m0.7158[0m        [94m0.4920[0m     +  0.0000  15.7615
     38      0.9970        0.0768       0.4571      0.7156        [94m0.4919[0m     +  0.0000  15.9958
     39      0.9969        0.0766       0.4569      0.7155        [94m0.4918[0m     +  0.0000  15.5759
     40      0.9964        0.0768       0.4566      0.7152        [94m0.4918[0m     +  0.0000  15.3647
     41      0.9964        0.0757       0.4566      0.7152        [94m0.4917[0m     +  0.0000  15.4659
     42      0.9971        0.0774       0.4566      0.7151        [94m0.4917[0m     +  0.0000  15.8394
     43      0.9967        [32m0.0751[0m       0.4566      0.7150        [94m0.4917[0m     +  0.0000  15.8100
     44      0.9961        [32m0.0750[0m       0.4566      0.7151        [94m0.4917[0m     +  0.0000  16.0761
     45      0.9967        0.0767       0.4568      0.7151        [94m0.4916[0m     +  0.0000  15.8231
     46      0.9960        0.0771       0.4568      0.7151        [94m0.4916[0m     +  0.0000  15.7059
     47      0.9970        0.0760       0.4568      0.7151        [94m0.4916[0m     +  0.0000  15.6369
     48      0.9960        0.0766       0.4566      0.7150        [94m0.4916[0m     +  0.0000  15.7152
     49      0.9966        0.0761       0.4564      0.7148        [94m0.4916[0m     +  0.0000  15.7771
     50      0.9964        0.0758       0.4566      0.7147        [94m0.4916[0m     +  0.0000  15.4651
     51      0.9963        0.0775       0.4566      0.7147        [94m0.4916[0m     +  0.0000  15.8855
     52      0.9967        0.0752       0.4566      0.7147        [94m0.4915[0m     +  0.0000  15.9193
     53      0.9968        0.0766       0.4568      0.7148        [94m0.4915[0m     +  0.0000  15.7755
     54      0.9960        0.0765       0.4568      0.7147        [94m0.4915[0m     +  0.0000  15.8929
     55      0.9966        0.0766       0.4568      0.7147        [94m0.4915[0m     +  0.0000  15.8261
     56      0.9968        0.0762       0.4568      0.7147        [94m0.4915[0m     +  0.0000  15.8880
     57      0.9967        0.0760       0.4568      0.7147        [94m0.4915[0m     +  0.0000  15.9180
     58      0.9964        0.0764       0.4568      0.7147        [94m0.4915[0m     +  0.0000  15.7765
     59      0.9967        0.0762       0.4569      0.7148        [94m0.4915[0m     +  0.0000  15.6991
     60      0.9964        0.0769       0.4569      0.7148        [94m0.4915[0m     +  0.0000  15.9178
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.6931363913544277
F1 Macro Score after query 7: 0.7090541662502826
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_6.pt

Iteration: 8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9869[0m        [32m0.1110[0m       [35m0.4406[0m      [31m0.7373[0m        [94m0.4702[0m     +  0.0000  19.5144
      2      [36m0.9904[0m        [32m0.0943[0m       0.4061      0.7241        0.4944        0.0000  19.8299
      3      [36m0.9920[0m        [32m0.0853[0m       0.3946      0.7199        0.5012        0.0000  19.6392
      4      [36m0.9924[0m        [32m0.0821[0m       0.4003      0.7258        0.4937        0.0000  19.7482
      5      [36m0.9934[0m        [32m0.0743[0m       0.3899      0.7175        0.5051        0.0000  19.6230
      6      [36m0.9948[0m        [32m0.0675[0m       0.3962      0.7208        0.4879        0.0000  19.7205
      7      [36m0.9953[0m        [32m0.0661[0m       0.4014      0.7210        0.4776        0.0000  19.7154
      8      [36m0.9956[0m        [32m0.0633[0m       0.4057      0.7223        0.4720        0.0000  19.3579
      9      [36m0.9960[0m        [32m0.0613[0m       0.4042      0.7216        0.4725        0.0000  19.4976
     10      0.9959        [32m0.0596[0m       0.4023      0.7217        0.4775        0.0000  19.5435
     11      [36m0.9961[0m        [32m0.0572[0m       0.4052      0.7232        0.4722        0.0000  19.5565
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.7357749334685084
F1 Macro Score after query 8: 0.7590515753448385
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_7.pt

Iteration: 9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9886[0m        [32m0.0987[0m       [35m0.2790[0m      [31m0.6895[0m        [94m0.6414[0m     +  0.0000  26.5179
      2      [36m0.9906[0m        [32m0.0842[0m       0.2769      [31m0.6970[0m        0.6471        0.0000  26.3099
      3      [36m0.9919[0m        [32m0.0763[0m       0.2767      [31m0.6981[0m        0.6522        0.0000  26.6631
      4      [36m0.9923[0m        [32m0.0710[0m       [35m0.2899[0m      [31m0.7012[0m        0.6732        0.0000  26.5338
      5      [36m0.9934[0m        [32m0.0639[0m       [35m0.2932[0m      0.6996        0.6859        0.0000  26.4720
      6      [36m0.9946[0m        [32m0.0574[0m       [35m0.3368[0m      [31m0.7133[0m        [94m0.6033[0m     +  0.0000  26.6428
      7      [36m0.9947[0m        [32m0.0556[0m       [35m0.3686[0m      [31m0.7163[0m        [94m0.5779[0m     +  0.0000  26.6614
      8      [36m0.9951[0m        [32m0.0530[0m       [35m0.3818[0m      [31m0.7177[0m        [94m0.5562[0m     +  0.0000  26.3671
      9      [36m0.9952[0m        [32m0.0503[0m       [35m0.3872[0m      [31m0.7201[0m        0.5568        0.0000  26.3957
     10      [36m0.9958[0m        [32m0.0482[0m       [35m0.3976[0m      [31m0.7230[0m        [94m0.5477[0m     +  0.0000  26.4883
     11      [36m0.9962[0m        [32m0.0451[0m       [35m0.4012[0m      0.7200        [94m0.5285[0m     +  0.0000  26.4280
     12      [36m0.9968[0m        [32m0.0437[0m       0.3986      0.7177        [94m0.5280[0m     +  0.0000  26.3457
     13      [36m0.9970[0m        [32m0.0426[0m       [35m0.4040[0m      0.7205        [94m0.5247[0m     +  0.0000  26.4302
     14      [36m0.9971[0m        [32m0.0416[0m       [35m0.4049[0m      0.7203        0.5253        0.0000  26.4758
     15      [36m0.9974[0m        [32m0.0403[0m       0.4030      0.7189        0.5282        0.0000  26.4689
     16      [36m0.9976[0m        [32m0.0391[0m       [35m0.4071[0m      0.7207        [94m0.5198[0m     +  0.0000  26.6616
     17      [36m0.9977[0m        0.0394       [35m0.4128[0m      0.7226        [94m0.5156[0m     +  0.0000  26.6485
     18      0.9977        [32m0.0388[0m       0.4104      0.7213        [94m0.5154[0m     +  0.0000  26.6818
     19      [36m0.9978[0m        [32m0.0378[0m       0.4092      0.7209        0.5216        0.0000  26.7187
     20      [36m0.9978[0m        [32m0.0373[0m       0.4116      0.7211        0.5172        0.0000  26.4635
     21      [36m0.9978[0m        [32m0.0367[0m       0.4118      0.7214        0.5170        0.0000  26.2697
     22      [36m0.9980[0m        0.0369       0.4127      0.7215        0.5158        0.0000  26.7253
     23      [36m0.9981[0m        [32m0.0359[0m       0.4123      0.7213        0.5173        0.0000  26.9092
     24      [36m0.9983[0m        0.0365       0.4128      0.7214        0.5174        0.0000  26.5197
     25      [36m0.9983[0m        [32m0.0356[0m       [35m0.4132[0m      0.7213        0.5173        0.0000  26.7861
     26      0.9983        [32m0.0355[0m       [35m0.4134[0m      0.7216        0.5163        0.0000  26.7066
     27      0.9981        [32m0.0354[0m       [35m0.4139[0m      0.7216        0.5172        0.0000  26.4387
     28      0.9982        0.0357       0.4137      0.7214        0.5165        0.0000  26.4714
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.7211521152115212
F1 Macro Score after query 9: 0.7444990452070589
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_8.pt

Iteration: 10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9801[0m        [32m0.1028[0m       [35m0.4095[0m      [31m0.7326[0m        [94m0.5066[0m     +  0.0000  38.6537
      2      [36m0.9856[0m        [32m0.0816[0m       0.3679      0.7138        0.5899        0.0000  38.6063
      3      [36m0.9877[0m        [32m0.0726[0m       0.3700      0.7156        0.6019        0.0000  38.7916
      4      [36m0.9886[0m        [32m0.0664[0m       0.3655      0.7167        0.6199        0.0000  38.5724
      5      [36m0.9897[0m        [32m0.0618[0m       0.3587      0.7139        0.6548        0.0000  38.5274
      6      [36m0.9924[0m        [32m0.0520[0m       0.3861      0.7321        0.5792        0.0000  38.5922
      7      [36m0.9934[0m        [32m0.0488[0m       0.3811      0.7295        0.5954        0.0000  38.6688
      8      [36m0.9943[0m        [32m0.0457[0m       0.3851      0.7276        0.5910        0.0000  38.6289
      9      [36m0.9946[0m        [32m0.0428[0m       0.3943      0.7321        0.5870        0.0000  38.7013
     10      [36m0.9951[0m        [32m0.0413[0m       0.3910      0.7309        0.5969        0.0000  38.5906
     11      [36m0.9958[0m        [32m0.0372[0m       0.4036      [31m0.7342[0m        0.5853        0.0000  38.6435
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.7146308408461494
F1 Macro Score after query 10: 0.741536255384354
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_9.pt

Iteration: 11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9691[0m        [32m0.1382[0m       [35m0.7740[0m      [31m0.8740[0m        [94m0.2423[0m     +  0.0000  60.7030
      2      [36m0.9784[0m        [32m0.1082[0m       [35m0.7849[0m      [31m0.8827[0m        [94m0.2287[0m     +  0.0000  60.2966
      3      [36m0.9813[0m        [32m0.0946[0m       [35m0.7880[0m      0.8730        [94m0.2234[0m     +  0.0000  60.3618
      4      [36m0.9833[0m        [32m0.0835[0m       [35m0.7967[0m      0.8819        [94m0.2153[0m     +  0.0000  60.5272
      5      [36m0.9853[0m        [32m0.0746[0m       [35m0.7990[0m      0.8771        [94m0.2131[0m     +  0.0000  60.7799
      6      [36m0.9888[0m        [32m0.0639[0m       [35m0.8035[0m      0.8797        [94m0.2083[0m     +  0.0000  60.7490
      7      [36m0.9899[0m        [32m0.0599[0m       [35m0.8075[0m      0.8817        [94m0.2067[0m     +  0.0000  60.5451
      8      [36m0.9906[0m        [32m0.0562[0m       0.8043      [31m0.8832[0m        [94m0.2059[0m     +  0.0000  60.5592
      9      [36m0.9913[0m        [32m0.0526[0m       0.8075      [31m0.8860[0m        0.2084        0.0000  60.4209
     10      [36m0.9919[0m        [32m0.0501[0m       0.8030      0.8838        0.2127        0.0000  60.2012
     11      [36m0.9937[0m        [32m0.0458[0m       0.8012      [31m0.8896[0m        0.2151        0.0000  60.4661
     12      [36m0.9941[0m        [32m0.0442[0m       0.8024      [31m0.8904[0m        0.2180        0.0000  60.7171
     13      [36m0.9944[0m        [32m0.0432[0m       0.8038      [31m0.8909[0m        0.2170        0.0000  60.3843
     14      [36m0.9949[0m        [32m0.0410[0m       0.8016      0.8900        0.2189        0.0000  60.4186
     15      [36m0.9955[0m        [32m0.0394[0m       0.8000      0.8893        0.2239        0.0000  60.3871
     16      [36m0.9956[0m        [32m0.0382[0m       0.7957      0.8897        0.2308        0.0000  60.0571
     17      [36m0.9962[0m        [32m0.0373[0m       0.7950      0.8893        0.2319        0.0000  60.1830
     18      [36m0.9962[0m        [32m0.0365[0m       0.7965      0.8900        0.2308        0.0000  60.5329
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.9233414081425334
F1 Macro Score after query 11: 0.9149966861177524
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_10.pt

Iteration: 12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  -------
      1      [36m0.9785[0m        [32m0.0735[0m       [35m0.8217[0m      [31m0.8896[0m        [94m0.1876[0m     +  0.0000  98.7139
      2      [36m0.9828[0m        [32m0.0572[0m       0.8161      0.8759        0.1918        0.0000  99.2101
      3      [36m0.9858[0m        [32m0.0471[0m       0.8094      0.8824        0.2019        0.0000  99.4951
      4      [36m0.9870[0m        [32m0.0399[0m       0.8021      0.8793        0.2190        0.0000  100.1969
      5      [36m0.9881[0m        [32m0.0356[0m       0.7984      0.8882        0.2493        0.0000  98.7531
      6      [36m0.9916[0m        [32m0.0288[0m       0.8182      0.8876        0.2082        0.0000  100.4922
      7      [36m0.9929[0m        [32m0.0257[0m       0.8168      [31m0.8904[0m        0.2156        0.0000  99.7252
      8      [36m0.9936[0m        [32m0.0231[0m       [35m0.8222[0m      [31m0.8909[0m        0.2141        0.0000  99.4606
      9      [36m0.9948[0m        [32m0.0205[0m       0.8212      0.8891        0.2305        0.0000  100.2560
     10      [36m0.9954[0m        [32m0.0187[0m       0.8163      0.8852        0.2359        0.0000  99.5088
     11      [36m0.9966[0m        [32m0.0164[0m       0.8182      [31m0.8917[0m        0.2374        0.0000  99.8376
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.9292196007259528
F1 Macro Score after query 12: 0.9174822724386583
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_11.pt

Iteration: 13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      lr       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------  --------
      1      [36m0.9809[0m        [32m0.0588[0m       [35m0.8184[0m      [31m0.8876[0m        [94m0.2079[0m     +  0.0000  114.7035
      2      [36m0.9835[0m        [32m0.0472[0m       0.8005      0.8814        0.2213        0.0000  115.2846
      3      [36m0.9852[0m        [32m0.0401[0m       0.8003      0.8859        0.2478        0.0000  115.1256
      4      [36m0.9869[0m        [32m0.0348[0m       0.7993      0.8854        0.2305        0.0000  115.3779
      5      [36m0.9877[0m        [32m0.0308[0m       0.8080      0.8863        0.2291        0.0000  114.9397
      6      [36m0.9914[0m        [32m0.0239[0m       0.8163      0.8851        0.2252        0.0000  115.3263
      7      [36m0.9929[0m        [32m0.0208[0m       0.8161      0.8845        0.2335        0.0000  115.4941
      8      [36m0.9940[0m        [32m0.0186[0m       [35m0.8196[0m      0.8852        0.2327        0.0000  115.8711
      9      [36m0.9951[0m        [32m0.0169[0m       0.8090      0.8821        0.2499        0.0000  115.2554
     10      [36m0.9955[0m        [32m0.0152[0m       0.8196      0.8847        0.2520        0.0000  115.9629
     11      [36m0.9968[0m        [32m0.0130[0m       [35m0.8267[0m      [31m0.8892[0m        0.2410        0.0000  115.7383
Stopping since valid_loss has not improved in the last 11 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.9318705779473727
F1 Macro Score after query 13: 0.9224913581756997
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\model_checkpoint_iteration_12.pt
Pickle file saved to C:\Users\localuserSKSG\Desktop\Shubham\ActiveLearning/Multilabel/DinoS/average_score_seed42_lowLR\AL_average_score_results_for_multilabel_classification_s42.pickle
