Adjusted shapes after loading:
X_initial_np: (8, 224, 224, 3), y_initial_np: (8, 3)
X_pool_np: (26872, 224, 224, 3), y_pool_np: (26872, 3)
X_test_np: (5760, 224, 224, 3), y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3), y_val_np: (5760, 3)
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.7374[0m        [32m0.7129[0m       [35m0.1227[0m      [31m0.4134[0m        [94m0.7027[0m     +  74.9841
      2      0.5778        [32m0.6839[0m       [35m0.1615[0m      [31m0.5167[0m        [94m0.7008[0m     +  76.0077
      3      0.5599        [32m0.6747[0m       [35m0.1842[0m      [31m0.5803[0m        [94m0.7003[0m     +  76.7232
      4      0.7374        0.7233       0.1731      [31m0.5815[0m        [94m0.6994[0m     +  76.6136
      5      [36m0.8500[0m        [32m0.6678[0m       0.1769      [31m0.5864[0m        [94m0.6984[0m     +  76.3638
      6      0.6985        [32m0.6630[0m       0.1679      0.5812        [94m0.6975[0m     +  76.4066
      7      0.7389        [32m0.6438[0m       0.1538      0.5632        [94m0.6965[0m     +  76.5052
      8      0.4563        0.6506       0.1651      0.5503        [94m0.6951[0m     +  76.3761
      9      0.5889        0.6506       [35m0.1910[0m      0.5477        [94m0.6936[0m     +  76.4175
     10      0.8426        [32m0.6197[0m       [35m0.2071[0m      0.5474        [94m0.6917[0m     +  76.4175
     11      0.7963        [32m0.6028[0m       [35m0.2108[0m      0.5387        [94m0.6899[0m     +  76.2915
     12      0.8500        [32m0.5811[0m       [35m0.2118[0m      0.5305        [94m0.6886[0m     +  76.2534
     13      0.7831        0.6123       0.2104      0.5216        [94m0.6872[0m     +  76.3349
     14      [36m0.8796[0m        0.5836       0.2102      0.5137        [94m0.6860[0m     +  76.2984
     15      0.7963        [32m0.5787[0m       0.2083      0.5082        [94m0.6854[0m     +  76.4461
     16      [36m0.8963[0m        [32m0.5655[0m       0.2102      0.5082        0.6856        76.4287
     17      0.8593        [32m0.5378[0m       0.2062      0.5079        0.6859        76.3608
     18      [36m0.9259[0m        [32m0.5290[0m       0.2033      0.5095        0.6867        76.3851
     19      0.8426        0.5480       0.2017      0.5115        0.6873        76.3167
     20      0.8426        0.5492       0.2024      0.5149        0.6878        76.3048
     21      0.8889        [32m0.4998[0m       0.2052      0.5175        0.6881        76.3279
     22      0.8426        0.5206       0.2064      0.5192        0.6881        76.3275
     23      0.8783        [32m0.4940[0m       0.2062      0.5211        0.6878        76.3292
     24      0.9259        0.4962       0.2059      0.5224        0.6868        76.3146
     25      [36m0.9630[0m        [32m0.4526[0m       0.2057      0.5233        0.6859        76.3055
     26      0.9259        0.4611       0.2071      0.5248        [94m0.6850[0m     +  76.3015
     27      0.9259        [32m0.4416[0m       0.2118      0.5284        [94m0.6844[0m     +  76.1471
     28      0.9630        0.4612       [35m0.2220[0m      0.5335        [94m0.6843[0m     +  76.2953
     29      0.9630        [32m0.4090[0m       [35m0.2304[0m      0.5382        0.6845        76.3702
     30      0.9259        0.4365       [35m0.2342[0m      0.5396        0.6852        76.3832
     31      0.9630        0.4153       0.2314      0.5378        0.6861        76.3790
     32      0.9630        [32m0.4075[0m       0.2280      0.5363        0.6869        76.3184
     33      [36m1.0000[0m        [32m0.3851[0m       0.2260      0.5364        0.6876        76.3611
     34      1.0000        0.3881       0.2257      0.5363        0.6877        76.3760
     35      1.0000        [32m0.3717[0m       0.2271      0.5386        0.6880        76.3245
     36      0.9630        [32m0.3667[0m       0.2286      0.5386        0.6879        76.3808
     37      0.9630        [32m0.3452[0m       0.2316      0.5385        0.6875        76.3608
     38      0.9630        0.3690       0.2309      0.5364        0.6868        76.3467
     39      1.0000        [32m0.3433[0m       0.2332      0.5358        0.6861        76.3381
     40      1.0000        [32m0.3202[0m       0.2330      0.5334        0.6852        76.3113
     41      1.0000        0.3325       [35m0.2352[0m      0.5318        [94m0.6840[0m     +  76.3670
     42      1.0000        0.3362       [35m0.2358[0m      0.5299        [94m0.6834[0m     +  76.1737
     43      0.9630        [32m0.3101[0m       0.2354      0.5294        0.6837        76.3050
     44      0.9630        0.3166       0.2344      0.5282        0.6836        76.3631
     45      1.0000        0.3374       0.2319      0.5277        0.6835        76.3711
     46      1.0000        [32m0.2820[0m       0.2304      0.5279        0.6834        76.3994
     47      1.0000        [32m0.2686[0m       0.2306      0.5283        [94m0.6833[0m     +  76.3566
     48      1.0000        [32m0.2663[0m       0.2304      0.5288        [94m0.6831[0m     +  76.1594
     49      1.0000        0.2788       0.2293      0.5286        0.6832        76.2391
     50      1.0000        [32m0.2567[0m       0.2262      0.5269        0.6839        76.3799
     51      1.0000        [32m0.2470[0m       0.2238      0.5257        0.6852        76.4040
     52      1.0000        0.2729       0.2220      0.5250        0.6860        76.3333
     53      1.0000        0.2541       0.2212      0.5239        0.6867        76.3580
     54      1.0000        [32m0.2367[0m       0.2194      0.5218        0.6878        76.3086
     55      1.0000        0.2562       0.2188      0.5216        0.6889        76.3484
     56      1.0000        0.2419       0.2186      0.5227        0.6899        76.3088
     57      1.0000        0.2475       0.2181      0.5250        0.6912        76.3225
     58      1.0000        [32m0.2179[0m       0.2151      0.5245        0.6928        76.3120
     59      1.0000        [32m0.2167[0m       0.2142      0.5243        0.6939        76.3309
     60      1.0000        [32m0.2079[0m       0.2102      0.5223        0.6957        76.3920
     61      1.0000        0.2102       0.2089      0.5210        0.6973        76.3084
     62      1.0000        [32m0.1970[0m       0.2085      0.5208        0.6989        76.3789
     63      1.0000        [32m0.1930[0m       0.2094      0.5212        0.7005        76.3691
     64      1.0000        0.2013       0.2118      0.5218        0.7010        76.5153
     65      1.0000        0.2077       0.2132      0.5230        0.7008        76.3896
     66      1.0000        0.2010       0.2128      0.5220        0.7006        76.3757
     67      1.0000        0.2010       0.2097      0.5198        0.7013        76.4007
Stopping since valid_loss has not improved in the last 20 epochs.
Pre F1 micro score = 0.5597
Pre F1 macro score = 0.5504
Pre Accuracy = 0.2854

Iteration: 1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.6962[0m        [32m0.5365[0m       [35m0.2321[0m      [31m0.5581[0m        [94m0.6760[0m     +  76.9145
      2      [36m0.7885[0m        [32m0.4362[0m       0.2193      0.5578        [94m0.6526[0m     +  76.7812
      3      [36m0.8283[0m        0.4474       [35m0.2451[0m      [31m0.5808[0m        [94m0.6318[0m     +  76.8287
      4      [36m0.8917[0m        [32m0.3734[0m       [35m0.2660[0m      [31m0.5981[0m        [94m0.6076[0m     +  76.8454
      5      [36m0.9032[0m        [32m0.3543[0m       [35m0.2783[0m      [31m0.6062[0m        [94m0.5819[0m     +  76.8702
      6      [36m0.9258[0m        [32m0.3388[0m       [35m0.3063[0m      [31m0.6362[0m        [94m0.5634[0m     +  76.8594
      7      [36m0.9335[0m        [32m0.3164[0m       [35m0.3391[0m      [31m0.6624[0m        [94m0.5505[0m     +  76.8454
      8      [36m0.9373[0m        [32m0.2951[0m       [35m0.3668[0m      [31m0.6779[0m        [94m0.5368[0m     +  76.8010
      9      [36m0.9804[0m        0.2991       [35m0.3920[0m      [31m0.6821[0m        [94m0.5216[0m     +  76.9392
     10      [36m0.9867[0m        [32m0.2660[0m       [35m0.4045[0m      [31m0.6832[0m        [94m0.5088[0m     +  76.8292
     11      [36m1.0000[0m        [32m0.2575[0m       [35m0.4115[0m      [31m0.6871[0m        [94m0.4968[0m     +  76.9216
     12      1.0000        [32m0.2417[0m       [35m0.4378[0m      [31m0.7067[0m        [94m0.4878[0m     +  76.8641
     13      1.0000        [32m0.2360[0m       [35m0.4582[0m      [31m0.7206[0m        [94m0.4810[0m     +  76.9429
     14      1.0000        [32m0.2345[0m       [35m0.4700[0m      [31m0.7319[0m        [94m0.4761[0m     +  76.8762
     15      1.0000        [32m0.2184[0m       [35m0.4731[0m      [31m0.7344[0m        [94m0.4710[0m     +  76.9058
     16      1.0000        0.2225       [35m0.4733[0m      0.7246        [94m0.4645[0m     +  76.8689
     17      1.0000        [32m0.1967[0m       [35m0.4833[0m      0.7266        [94m0.4610[0m     +  76.8889
     18      1.0000        0.2059       [35m0.4948[0m      [31m0.7376[0m        [94m0.4592[0m     +  76.8641
     19      1.0000        [32m0.1870[0m       [35m0.4976[0m      [31m0.7453[0m        [94m0.4582[0m     +  76.8892
     20      1.0000        [32m0.1774[0m       [35m0.4993[0m      0.7445        [94m0.4542[0m     +  76.8442
     21      1.0000        0.1780       [35m0.5005[0m      0.7362        [94m0.4452[0m     +  76.8599
     22      1.0000        [32m0.1653[0m       [35m0.5040[0m      0.7380        [94m0.4413[0m     +  76.8521
     23      1.0000        [32m0.1605[0m       [35m0.5071[0m      [31m0.7474[0m        0.4427        76.8330
     24      1.0000        0.1653       [35m0.5097[0m      0.7439        [94m0.4394[0m     +  76.8882
     25      1.0000        [32m0.1551[0m       0.5080      0.7326        [94m0.4344[0m     +  76.8619
     26      1.0000        0.1684       [35m0.5123[0m      0.7362        [94m0.4320[0m     +  76.8166
     27      1.0000        0.1581       [35m0.5205[0m      [31m0.7534[0m        0.4325        76.9065
     28      1.0000        [32m0.1501[0m       [35m0.5240[0m      [31m0.7568[0m        [94m0.4279[0m     +  76.9086
     29      1.0000        0.1510       0.5231      0.7524        [94m0.4227[0m     +  76.8294
     30      1.0000        [32m0.1473[0m       [35m0.5266[0m      0.7527        [94m0.4208[0m     +  76.9053
     31      1.0000        [32m0.1381[0m       [35m0.5295[0m      [31m0.7601[0m        0.4226        76.8843
     32      1.0000        0.1468       [35m0.5299[0m      0.7539        [94m0.4200[0m     +  76.8738
     33      1.0000        0.1397       [35m0.5307[0m      0.7478        [94m0.4158[0m     +  76.6814
     34      1.0000        0.1530       [35m0.5309[0m      0.7486        [94m0.4139[0m     +  76.7379
     35      1.0000        [32m0.1335[0m       [35m0.5366[0m      0.7581        0.4140        76.9470
     36      1.0000        0.1475       0.5339      0.7519        0.4140        76.9469
     37      1.0000        [32m0.1282[0m       0.5309      0.7454        [94m0.4128[0m     +  76.8783
     38      1.0000        0.1293       0.5342      0.7541        [94m0.4118[0m     +  76.7482
     39      1.0000        [32m0.1278[0m       [35m0.5378[0m      0.7545        [94m0.4086[0m     +  76.7220
     40      1.0000        0.1316       0.5339      0.7512        [94m0.4056[0m     +  76.7582
     41      1.0000        [32m0.1223[0m       [35m0.5434[0m      0.7569        [94m0.4045[0m     +  76.8347
     42      1.0000        [32m0.1158[0m       [35m0.5493[0m      0.7559        [94m0.4042[0m     +  76.8406
     43      1.0000        0.1187       0.5424      0.7561        [94m0.4031[0m     +  76.7577
     44      1.0000        [32m0.1098[0m       0.5443      0.7543        [94m0.3992[0m     +  76.8561
     45      1.0000        [32m0.1064[0m       0.5434      [31m0.7637[0m        [94m0.3991[0m     +  76.7573
     46      1.0000        0.1166       0.5476      0.7628        [94m0.3979[0m     +  76.8150
     47      1.0000        0.1070       [35m0.5495[0m      0.7576        [94m0.3967[0m     +  76.8036
     48      1.0000        0.1187       0.5465      0.7575        [94m0.3962[0m     +  76.8244
     49      1.0000        0.1085       [35m0.5524[0m      0.7561        [94m0.3945[0m     +  76.8406
     50      1.0000        0.1165       [35m0.5568[0m      0.7585        0.3945        76.8110
     51      1.0000        0.1224       0.5436      0.7624        0.3982        76.8740
     52      1.0000        0.1080       0.5446      0.7578        0.3956        76.8250
     53      1.0000        0.1118       [35m0.5771[0m      0.7498        [94m0.3933[0m     +  76.8074
     54      1.0000        0.1070       0.5681      0.7581        [94m0.3898[0m     +  76.6700
     55      1.0000        0.1089       0.5510      [31m0.7658[0m        0.3934        76.7335
     56      1.0000        [32m0.0995[0m       0.5557      0.7641        0.3918        76.8361
     57      1.0000        0.1063       0.5712      0.7635        [94m0.3882[0m     +  76.8695
     58      1.0000        [32m0.0977[0m       0.5599      [31m0.7661[0m        [94m0.3865[0m     +  76.7797
     59      1.0000        [32m0.0964[0m       0.5516      0.7644        0.3890        76.7611
     60      1.0000        0.1025       0.5585      0.7563        0.3880        76.8935
     61      1.0000        0.0980       0.5576      0.7626        0.3881        76.8890
     62      1.0000        0.0977       0.5545      [31m0.7690[0m        0.3889        76.8621
     63      1.0000        0.0979       0.5693      0.7640        [94m0.3832[0m     +  76.8940
     64      1.0000        [32m0.0959[0m       [35m0.5797[0m      0.7659        [94m0.3822[0m     +  76.8528
     65      1.0000        0.0980       0.5649      [31m0.7738[0m        0.3847        76.7951
     66      1.0000        [32m0.0954[0m       0.5641      0.7708        0.3845        76.9738
     67      1.0000        0.1011       [35m0.5806[0m      0.7636        [94m0.3819[0m     +  76.8710
     68      1.0000        0.1007       0.5740      [31m0.7741[0m        [94m0.3800[0m     +  76.7691
     69      1.0000        [32m0.0915[0m       0.5580      [31m0.7751[0m        0.3837        76.8628
     70      1.0000        [32m0.0878[0m       0.5731      0.7650        0.3812        76.9178
     71      1.0000        0.0980       [35m0.5819[0m      0.7664        0.3804        76.9067
     72      1.0000        [32m0.0845[0m       0.5684      0.7646        0.3804        76.8413
     73      1.0000        0.0889       0.5653      0.7686        0.3807        76.8568
     74      1.0000        0.0908       0.5642      0.7688        0.3805        76.8302
     75      1.0000        0.0869       0.5753      0.7679        [94m0.3786[0m     +  76.9182
     76      1.0000        [32m0.0842[0m       0.5807      0.7644        [94m0.3768[0m     +  76.7299
     77      1.0000        0.0923       0.5705      0.7699        0.3780        76.7987
     78      1.0000        0.0876       0.5628      0.7677        0.3806        76.9386
     79      1.0000        [32m0.0794[0m       0.5686      0.7626        0.3800        77.1128
     80      1.0000        [32m0.0748[0m       0.5781      0.7657        0.3770        78.4063
     81      1.0000        0.0783       0.5748      [31m0.7770[0m        [94m0.3753[0m     +  78.7508
     82      1.0000        0.0793       0.5741      0.7696        0.3755        77.6556
     83      1.0000        0.0841       0.5752      0.7658        [94m0.3750[0m     +  77.7162
     84      1.0000        0.0851       0.5762      0.7752        [94m0.3741[0m     +  77.7311
     85      1.0000        0.0762       0.5769      0.7734        0.3744        77.7257
     86      1.0000        0.0846       0.5745      0.7700        [94m0.3740[0m     +  78.3898
     87      1.0000        0.0811       0.5707      0.7708        [94m0.3732[0m     +  77.5815
     88      1.0000        0.0792       [35m0.5825[0m      0.7725        0.3736        77.5688
     89      1.0000        0.0770       0.5753      0.7747        0.3733        77.6073
     90      1.0000        0.0793       0.5705      [31m0.7794[0m        [94m0.3729[0m     +  77.8243
     91      1.0000        0.0794       0.5760      0.7685        [94m0.3715[0m     +  78.0880
     92      1.0000        0.0906       0.5821      0.7719        0.3724        77.6441
     93      1.0000        [32m0.0718[0m       0.5661      0.7782        0.3745        77.6214
     94      1.0000        0.0754       0.5781      0.7743        [94m0.3700[0m     +  77.7580
     95      1.0000        0.0725       [35m0.5910[0m      0.7700        0.3717        77.5842
     96      1.0000        0.0754       0.5663      0.7730        0.3767        77.6350
     97      1.0000        0.0786       0.5655      0.7715        0.3766        77.6495
     98      1.0000        0.0739       0.5845      0.7654        0.3732        77.8181
     99      1.0000        [32m0.0697[0m       0.5856      0.7781        0.3721        77.7908
    100      1.0000        0.0834       0.5726      0.7770        0.3737        77.5356
[24]
F1 Micro Score after query 1: 0.7902794292508917
F1 Macro Score after query 1: 0.7782632968739591
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_0.pt

Iteration: 2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.8944[0m        [32m0.2173[0m       [35m0.5934[0m      [31m0.7596[0m        [94m0.3500[0m     +  78.9763
      2      [36m0.9711[0m        [32m0.1598[0m       [35m0.6462[0m      [31m0.8098[0m        [94m0.3175[0m     +  78.1117
      3      0.9711        [32m0.1443[0m       [35m0.6668[0m      0.7804        [94m0.3134[0m     +  77.9965
      4      [36m0.9834[0m        [32m0.1322[0m       0.6634      0.7976        [94m0.3042[0m     +  78.2800
      5      [36m0.9885[0m        [32m0.1164[0m       [35m0.6675[0m      0.7872        0.3069        78.3562
      6      0.9877        [32m0.1080[0m       [35m0.6677[0m      0.7887        0.3057        78.2004
      7      [36m1.0000[0m        [32m0.1021[0m       0.6667      0.7922        [94m0.3020[0m     +  78.0625
      8      1.0000        0.1046       [35m0.6679[0m      0.7932        [94m0.3013[0m     +  77.9444
      9      1.0000        [32m0.0983[0m       [35m0.6707[0m      0.7892        0.3014        78.0468
     10      1.0000        [32m0.0929[0m       [35m0.6710[0m      0.7984        [94m0.2978[0m     +  78.0803
     11      1.0000        [32m0.0907[0m       [35m0.6734[0m      0.7791        0.3065        78.2827
     12      1.0000        [32m0.0890[0m       [35m0.6740[0m      0.8025        [94m0.2947[0m     +  78.9585
     13      1.0000        0.0896       [35m0.6741[0m      0.7763        0.3048        78.5644
     14      1.0000        [32m0.0815[0m       0.6719      [31m0.8139[0m        0.2952        78.0361
     15      1.0000        0.0865       [35m0.6847[0m      0.7797        0.3054        78.0787
     16      1.0000        0.0818       0.6653      0.7887        0.2981        78.0161
     17      1.0000        0.0837       0.6691      0.8045        0.2971        77.9988
     18      1.0000        [32m0.0765[0m       0.6847      0.7651        0.3121        77.9560
     19      1.0000        0.0777       0.6710      [31m0.8178[0m        [94m0.2930[0m     +  77.9726
     20      1.0000        0.0793       0.6793      0.7597        0.3150        77.9340
     21      1.0000        0.0773       0.6738      [31m0.8180[0m        [94m0.2920[0m     +  78.0043
     22      1.0000        [32m0.0753[0m       0.6691      0.7703        0.3064        77.9880
     23      1.0000        [32m0.0720[0m       0.6708      0.8022        0.2943        77.9846
     24      1.0000        0.0729       0.6707      0.7651        0.3109        77.9680
     25      1.0000        [32m0.0716[0m       0.6740      0.8053        0.2921        77.9440
     26      1.0000        0.0718       0.6727      0.7543        0.3130        77.9588
     27      1.0000        0.0735       0.6734      [31m0.8201[0m        [94m0.2902[0m     +  77.9987
     28      1.0000        [32m0.0709[0m       0.6641      0.7344        0.3266        77.9862
     29      1.0000        0.0732       0.6615      0.8177        0.2970        78.0626
     30      1.0000        0.0741       0.6604      0.7293        0.3307        78.5015
     31      1.0000        [32m0.0679[0m       0.6701      0.7852        0.2954        78.0314
     32      1.0000        0.0690       0.6670      0.7779        0.2993        78.0422
     33      1.0000        0.0712       0.6592      0.7508        0.3123        78.4454
     34      1.0000        0.0699       0.6776      0.7982        0.2926        78.7644
     35      1.0000        [32m0.0639[0m       0.6644      0.7556        0.3094        78.6718
     36      1.0000        0.0648       0.6781      0.8061        0.2902        78.7499
     37      1.0000        0.0655       0.6700      0.7649        0.3034        78.9205
     38      1.0000        [32m0.0587[0m       0.6720      0.7867        0.2936        78.7030
     39      1.0000        [32m0.0571[0m       0.6684      0.7825        0.2969        78.6251
     40      1.0000        0.0643       0.6759      0.7828        0.2971        78.8815
     41      1.0000        0.0600       0.6698      0.7732        0.2996        78.8616
     42      1.0000        0.0613       0.6717      0.7866        0.2929        78.6658
     43      1.0000        0.0595       0.6703      0.7792        0.2966        78.9224
     44      1.0000        [32m0.0556[0m       0.6740      0.7807        0.2939        78.7206
     45      1.0000        0.0556       0.6691      0.7869        0.2927        78.5663
     46      1.0000        0.0577       0.6722      0.7767        0.2962        78.6856
Stopping since valid_loss has not improved in the last 20 epochs.
[24, 56]
F1 Micro Score after query 2: 0.8192127004697878
F1 Macro Score after query 2: 0.7917100154050493
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_1.pt

Iteration: 3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.8848[0m        [32m0.2133[0m       [35m0.6332[0m      [31m0.6079[0m        [94m0.3902[0m     +  80.5171
      2      [36m0.9331[0m        [32m0.1661[0m       [35m0.6965[0m      [31m0.8122[0m        [94m0.2668[0m     +  80.3433
      3      [36m0.9578[0m        [32m0.1416[0m       [35m0.7146[0m      [31m0.8501[0m        [94m0.2507[0m     +  80.4054
      4      [36m0.9791[0m        [32m0.1146[0m       [35m0.7260[0m      0.8431        [94m0.2490[0m     +  80.4842
      5      [36m0.9877[0m        [32m0.1002[0m       0.7241      0.8363        0.2545        80.3751
      6      0.9877        [32m0.0970[0m       [35m0.7295[0m      0.8449        0.2505        80.5107
      7      0.9877        [32m0.0909[0m       [35m0.7344[0m      0.8489        [94m0.2458[0m     +  80.4794
      8      0.9877        [32m0.0893[0m       [35m0.7378[0m      [31m0.8565[0m        [94m0.2407[0m     +  80.3498
      9      0.9877        [32m0.0802[0m       0.7378      0.8382        0.2502        80.4901
     10      0.9877        [32m0.0767[0m       [35m0.7470[0m      [31m0.8608[0m        [94m0.2381[0m     +  80.3772
     11      [36m0.9939[0m        0.0785       [35m0.7476[0m      [31m0.8640[0m        [94m0.2359[0m     +  80.4943
     12      0.9939        [32m0.0742[0m       0.7418      0.8547        0.2372        80.4114
     13      [36m1.0000[0m        [32m0.0713[0m       0.7462      0.8426        0.2435        80.4591
     14      1.0000        [32m0.0704[0m       0.7411      0.8447        0.2475        80.4525
     15      1.0000        [32m0.0700[0m       0.7464      0.8596        0.2392        80.2764
     16      1.0000        [32m0.0691[0m       0.7470      [31m0.8688[0m        [94m0.2321[0m     +  80.3775
     17      1.0000        [32m0.0622[0m       [35m0.7497[0m      0.8648        0.2338        80.2806
     18      1.0000        0.0649       0.7422      0.8456        0.2425        80.0207
     19      1.0000        0.0627       0.7470      0.8604        [94m0.2308[0m     +  79.8858
     20      1.0000        0.0649       0.7476      0.8601        0.2315        79.7359
     21      1.0000        [32m0.0599[0m       0.7434      0.8511        0.2381        80.1726
     22      1.0000        0.0605       0.7396      0.8414        0.2513        80.3076
     23      1.0000        0.0607       0.7438      0.8559        0.2401        80.9364
     24      1.0000        0.0629       0.7378      0.8663        0.2313        80.5243
     25      1.0000        [32m0.0572[0m       0.7431      0.8687        [94m0.2304[0m     +  80.2306
     26      1.0000        0.0595       0.7377      0.8339        0.2406        80.4911
     27      1.0000        0.0583       0.7201      0.7981        0.2707        80.3066
     28      1.0000        0.0594       0.7352      0.8266        0.2465        79.9246
     29      1.0000        [32m0.0565[0m       0.7458      [31m0.8730[0m        0.2305        79.8741
     30      1.0000        [32m0.0563[0m       0.7422      0.8599        0.2320        79.9649
     31      1.0000        [32m0.0520[0m       0.7323      0.8347        0.2452        80.0400
     32      1.0000        0.0537       [35m0.7503[0m      0.8596        [94m0.2277[0m     +  79.7341
     33      1.0000        0.0528       [35m0.7519[0m      0.8644        [94m0.2242[0m     +  80.0021
     34      1.0000        [32m0.0516[0m       0.7491      0.8552        0.2277        80.6818
     35      1.0000        0.0539       0.7491      0.8535        0.2304        80.8133
     36      1.0000        0.0539       0.7514      0.8560        0.2267        80.9200
     37      1.0000        [32m0.0494[0m       0.7505      0.8652        0.2308        80.6427
     38      1.0000        0.0495       0.7385      0.8609        0.2382        80.9007
     39      1.0000        0.0494       0.7304      0.8575        0.2428        80.9218
     40      1.0000        [32m0.0474[0m       0.7519      0.8632        0.2270        82.0164
     41      1.0000        0.0514       0.7479      0.8519        0.2257        80.7903
     42      1.0000        0.0483       0.7358      0.8290        0.2334        80.4445
     43      1.0000        0.0477       0.7229      0.8058        0.2432        80.8318
     44      1.0000        0.0486       0.7467      0.8472        0.2304        81.0935
     45      1.0000        0.0519       0.7347      0.8594        0.2393        80.5351
     46      1.0000        0.0487       0.7234      0.8616        0.2623        80.9584
     47      1.0000        0.0513       0.7215      0.8591        0.2712        81.0328
     48      1.0000        0.0556       0.7325      0.8559        0.2468        80.7629
     49      0.9818        0.0677       0.6825      0.8304        0.3330        80.9671
     50      0.9455        0.1002       0.6240      0.6759        0.3458        80.7811
     51      0.9706        0.0971       0.6658      0.7030        0.3371        80.8432
     52      0.9606        0.1085       [35m0.7811[0m      0.8569        0.2250        80.9548
Stopping since valid_loss has not improved in the last 20 epochs.
[24, 56, 112]
F1 Micro Score after query 3: 0.8315063520871143
F1 Macro Score after query 3: 0.81012397710857
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_2.pt

Iteration: 4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9264[0m        [32m0.1615[0m       [35m0.6102[0m      [31m0.6493[0m        [94m0.3408[0m     +  84.0063
      2      [36m0.9513[0m        [32m0.1373[0m       [35m0.7245[0m      [31m0.8209[0m        [94m0.2349[0m     +  83.3526
      3      [36m0.9700[0m        [32m0.1024[0m       0.7245      [31m0.8246[0m        0.2359        83.8072
      4      [36m0.9824[0m        [32m0.0821[0m       [35m0.7342[0m      [31m0.8443[0m        [94m0.2347[0m     +  83.8780
      5      [36m0.9912[0m        [32m0.0774[0m       0.7276      0.8373        0.2419        83.4054
      6      0.9891        [32m0.0718[0m       [35m0.7384[0m      [31m0.8641[0m        0.2365        83.9845
      7      [36m0.9985[0m        [32m0.0683[0m       0.7325      0.8527        0.2487        83.6051
      8      0.9964        [32m0.0645[0m       0.7299      0.8609        0.2548        83.6573
      9      0.9985        [32m0.0615[0m       0.7175      0.8501        0.2635        83.9583
     10      0.9964        [32m0.0607[0m       0.7311      0.8578        0.2484        83.3525
     11      0.9970        [32m0.0586[0m       0.7281      0.8500        0.2432        83.8497
     12      0.9963        [32m0.0581[0m       0.7196      0.8410        0.2566        83.6816
     13      0.9963        [32m0.0555[0m       [35m0.7385[0m      0.8262        0.2429        83.5613
     14      0.9928        0.0586       0.7231      0.8047        0.2543        83.9522
     15      0.9928        0.0630       0.6578      0.6624        0.3285        83.4698
     16      0.9763        0.0716       0.6708      0.7185        0.2999        83.8789
     17      0.9825        0.0702       0.6616      0.6424        0.3553        83.4072
     18      0.9826        0.0752       0.6840      0.7205        0.3053        83.5539
     19      0.9648        0.0800       0.6734      0.7801        0.2872        83.4370
     20      0.9963        0.0556       0.7160      0.8432        0.2662        83.1721
     21      0.9964        [32m0.0491[0m       0.7297      0.8348        0.2401        83.0850
     22      [36m1.0000[0m        [32m0.0430[0m       0.7248      0.8399        0.2451        82.8438
     23      1.0000        0.0430       0.7278      0.8392        0.2436        82.7972
Stopping since valid_loss has not improved in the last 20 epochs.
[24, 56, 112, 208]
F1 Micro Score after query 4: 0.8892601431980907
F1 Macro Score after query 4: 0.8771978734052638
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_3.pt

Iteration: 5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9218[0m        [32m0.1539[0m       [35m0.7141[0m      [31m0.8516[0m        [94m0.2905[0m     +  88.2456
      2      [36m0.9348[0m        [32m0.1380[0m       [35m0.7458[0m      [31m0.8691[0m        [94m0.2440[0m     +  88.1571
      3      [36m0.9553[0m        [32m0.1071[0m       [35m0.7604[0m      [31m0.8774[0m        [94m0.2321[0m     +  88.7072
      4      [36m0.9681[0m        [32m0.0916[0m       0.7533      0.8740        0.2397        88.2450
      5      [36m0.9772[0m        [32m0.0787[0m       0.7490      0.8719        0.2447        88.2828
      6      [36m0.9867[0m        [32m0.0738[0m       [35m0.7655[0m      [31m0.8798[0m        [94m0.2286[0m     +  88.2583
      7      [36m0.9951[0m        [32m0.0685[0m       0.7594      0.8548        [94m0.2204[0m     +  88.2294
      8      0.9902        [32m0.0682[0m       0.7491      0.8465        0.2219        88.2668
      9      0.9889        [32m0.0673[0m       0.7516      0.8435        0.2312        88.2681
     10      0.9877        0.0705       0.7460      0.8676        0.2411        88.2028
     11      [36m0.9951[0m        [32m0.0592[0m       0.7089      0.8247        0.2837        88.1591
     12      0.9945        0.0597       0.6984      0.8251        0.3046        88.2377
     13      0.9930        [32m0.0551[0m       0.7616      0.8562        0.2230        88.1410
     14      [36m1.0000[0m        [32m0.0493[0m       0.7521      0.8633        0.2239        88.1457
     15      1.0000        [32m0.0438[0m       0.7545      0.8591        [94m0.2203[0m     +  88.1701
     16      1.0000        [32m0.0426[0m       0.7535      0.8550        0.2225        88.1264
     17      1.0000        [32m0.0404[0m       0.7530      0.8547        0.2213        88.2374
     18      1.0000        [32m0.0401[0m       0.7523      0.8522        0.2238        88.1930
     19      1.0000        [32m0.0382[0m       0.7458      0.8434        0.2293        88.1853
     20      1.0000        [32m0.0374[0m       0.7448      0.8466        0.2291        88.1413
     21      1.0000        [32m0.0370[0m       0.7481      0.8442        0.2309        88.2483
     22      1.0000        [32m0.0349[0m       0.7455      0.8436        0.2316        88.2190
     23      1.0000        [32m0.0348[0m       0.7434      0.8373        0.2365        88.1744
     24      1.0000        [32m0.0335[0m       0.7441      0.8396        0.2337        88.2200
     25      1.0000        0.0336       0.7399      0.8386        0.2374        88.1772
     26      1.0000        [32m0.0312[0m       0.7444      0.8341        0.2413        88.1727
     27      1.0000        0.0322       0.7429      0.8388        0.2387        88.1990
     28      1.0000        0.0315       0.7398      0.8440        0.2341        88.2509
     29      1.0000        [32m0.0294[0m       0.7441      0.8413        0.2362        88.2335
     30      1.0000        0.0301       0.7392      0.8373        0.2421        88.2501
     31      1.0000        [32m0.0287[0m       0.7405      0.8386        0.2366        88.2193
     32      1.0000        [32m0.0282[0m       0.7380      0.8447        0.2383        88.2117
     33      1.0000        [32m0.0277[0m       0.7380      0.8410        0.2370        88.2380
     34      1.0000        0.0277       0.7439      0.8471        0.2337        88.1559
Stopping since valid_loss has not improved in the last 20 epochs.
[24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.9006078787400332
F1 Macro Score after query 5: 0.8875623725201166
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_4.pt

Iteration: 6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9564[0m        [32m0.0974[0m       [35m0.7679[0m      [31m0.8743[0m        [94m0.2039[0m     +  98.2458
      2      [36m0.9828[0m        [32m0.0684[0m       0.7556      0.8579        0.2100        98.2203
      3      [36m0.9900[0m        [32m0.0554[0m       0.7519      0.8543        0.2298        98.2975
      4      [36m0.9947[0m        [32m0.0478[0m       [35m0.7816[0m      [31m0.8830[0m        0.2076        98.2150
      5      [36m0.9964[0m        [32m0.0440[0m       [35m0.7925[0m      [31m0.8869[0m        [94m0.2033[0m     +  98.1760
      6      0.9927        [32m0.0434[0m       0.7771      0.8734        0.2073        98.2193
      7      0.9912        0.0453       [35m0.7939[0m      0.8814        [94m0.1995[0m     +  98.3263
      8      0.9922        0.0448       0.7741      0.8762        0.2406        98.2355
      9      0.9906        0.0462       0.7724      0.8737        0.2080        98.3105
     10      [36m0.9965[0m        [32m0.0391[0m       0.7457      0.8520        0.2231        98.2786
     11      0.9959        [32m0.0357[0m       0.7747      0.8685        0.2033        98.2778
     12      [36m0.9995[0m        [32m0.0302[0m       0.7701      0.8584        0.2092        98.2537
     13      [36m1.0000[0m        0.0302       0.7743      0.8636        0.2149        98.2479
     14      0.9967        0.0313       [35m0.7955[0m      0.8716        0.2192        98.2962
     15      0.9983        [32m0.0291[0m       0.7670      0.8527        0.2162        98.2482
     16      0.9962        0.0303       0.7347      0.8366        0.2470        98.2057
     17      1.0000        [32m0.0249[0m       0.7403      0.8449        0.2328        98.2506
     18      0.9969        0.0266       0.7366      0.8348        0.2459        98.2331
     19      0.9958        0.0269       0.7260      0.8245        0.2662        98.1885
     20      0.9990        [32m0.0225[0m       0.7431      0.8581        0.2414        98.2186
     21      1.0000        [32m0.0206[0m       0.7444      0.8616        0.2453        98.2052
     22      1.0000        [32m0.0201[0m       0.7431      0.8584        0.2427        98.2195
     23      1.0000        [32m0.0194[0m       0.7410      0.8602        0.2485        98.2480
     24      1.0000        [32m0.0183[0m       0.7425      0.8588        0.2446        98.1737
     25      1.0000        [32m0.0175[0m       0.7436      0.8611        0.2483        98.7219
     26      1.0000        [32m0.0170[0m       0.7444      0.8617        0.2498        98.8903
Stopping since valid_loss has not improved in the last 20 epochs.
[24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.9115071439545245
F1 Macro Score after query 6: 0.9001555915599342
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_5.pt

Iteration: 7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9639[0m        [32m0.0865[0m       [35m0.7720[0m      [31m0.8704[0m        [94m0.1958[0m     +  115.7696
      2      [36m0.9758[0m        [32m0.0661[0m       [35m0.7899[0m      0.8681        [94m0.1842[0m     +  116.3935
      3      [36m0.9841[0m        [32m0.0536[0m       0.7686      [31m0.8765[0m        0.2130        116.3125
      4      [36m0.9891[0m        [32m0.0462[0m       0.7780      0.8750        0.2068        116.5621
      5      [36m0.9904[0m        [32m0.0450[0m       [35m0.7962[0m      [31m0.8908[0m        0.1977        116.6803
      6      0.9889        [32m0.0408[0m       0.7681      0.8793        0.2184        117.3020
      7      [36m0.9959[0m        [32m0.0334[0m       [35m0.8082[0m      0.8899        0.1917        117.0233
      8      0.9932        [32m0.0328[0m       0.7851      0.8849        0.2111        117.2966
      9      0.9952        [32m0.0302[0m       0.7531      0.8724        0.2528        117.0664
     10      0.9944        0.0335       0.7898      0.8801        0.2002        116.9302
     11      0.9938        0.0330       0.7561      0.8732        0.2293        116.8263
     12      0.9939        0.0304       0.7957      0.8765        0.2102        116.8310
     13      0.9952        [32m0.0267[0m       0.7290      0.8552        0.2779        117.1028
     14      [36m0.9963[0m        [32m0.0255[0m       0.7738      0.8809        0.2344        116.9518
     15      0.9944        [32m0.0244[0m       0.7951      0.8776        0.1967        116.9294
     16      [36m0.9995[0m        [32m0.0183[0m       [35m0.8113[0m      0.8894        0.1936        116.2575
     17      0.9995        [32m0.0170[0m       0.7981      0.8685        0.2006        115.8279
     18      0.9995        [32m0.0155[0m       0.8085      0.8871        0.1992        115.8402
     19      0.9995        [32m0.0147[0m       0.8016      0.8799        0.2029        115.8838
     20      0.9995        [32m0.0141[0m       0.8101      0.8832        0.1991        115.8008
     21      0.9995        [32m0.0131[0m       0.8028      0.8825        0.2066        115.7495
Stopping since valid_loss has not improved in the last 20 epochs.
[24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.9215549887935698
F1 Macro Score after query 7: 0.9046839971123525
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_6.pt

Iteration: 8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9690[0m        [32m0.0727[0m       [35m0.8052[0m      [31m0.8897[0m        [94m0.1814[0m     +  146.9313
      2      [36m0.9759[0m        [32m0.0625[0m       0.8040      0.8892        0.1836        147.0710
      3      [36m0.9809[0m        [32m0.0489[0m       0.7979      0.8872        0.1925        147.1413
      4      [36m0.9894[0m        [32m0.0404[0m       0.7951      0.8818        0.1962        147.0058
      5      0.9877        [32m0.0377[0m       0.7984      0.8856        0.2016        146.9219
      6      0.9886        [32m0.0365[0m       [35m0.8420[0m      [31m0.9015[0m        [94m0.1789[0m     +  146.7193
      7      0.9881        [32m0.0347[0m       0.7877      0.8843        0.2287        146.7989
      8      [36m0.9918[0m        [32m0.0299[0m       0.8075      0.8900        0.1987        146.8447
      9      0.9913        [32m0.0274[0m       0.8090      0.8933        0.2147        146.7532
     10      [36m0.9933[0m        [32m0.0240[0m       0.8142      0.8932        0.2004        146.7187
     11      [36m0.9944[0m        [32m0.0225[0m       0.7911      0.8814        0.2248        146.7026
     12      0.9942        [32m0.0215[0m       0.7870      0.8865        0.2383        146.8043
     13      [36m0.9976[0m        [32m0.0162[0m       0.8210      0.8934        0.2076        146.7559
     14      0.9969        0.0170       0.8319      0.8840        0.2087        146.7375
     15      0.9911        0.0262       0.7984      0.8413        0.2599        146.7381
     16      0.9956        0.0188       0.7903      0.8867        0.2415        146.7812
     17      0.9946        0.0166       0.7965      0.8883        0.2152        146.7459
     18      [36m0.9981[0m        [32m0.0120[0m       0.7844      0.8855        0.2666        146.7235
     19      0.9981        [32m0.0109[0m       0.7962      0.8883        0.2392        146.7627
     20      [36m0.9981[0m        [32m0.0104[0m       0.7925      0.8899        0.2611        146.7587
     21      [36m0.9994[0m        [32m0.0087[0m       0.8014      0.8885        0.2351        146.7195
     22      [36m0.9997[0m        [32m0.0076[0m       0.8160      0.8946        0.2300        146.7907
     23      [36m0.9997[0m        [32m0.0072[0m       0.8014      0.8851        0.2484        146.8323
     24      0.9950        0.0137       0.8372      [31m0.9035[0m        0.2291        146.8592
     25      0.9921        0.0211       0.7905      0.8886        0.2685        146.9140
Stopping since valid_loss has not improved in the last 20 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.9302006255244489
F1 Macro Score after query 8: 0.9159313357234037
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_7.pt

Iteration: 9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9788[0m        [32m0.0552[0m       [35m0.7861[0m      [31m0.8791[0m        [94m0.2042[0m     +  202.0878
      2      [36m0.9813[0m        [32m0.0445[0m       [35m0.8354[0m      [31m0.9000[0m        [94m0.1763[0m     +  202.3330
      3      [36m0.9876[0m        [32m0.0348[0m       0.8026      0.8903        0.2168        202.3442
      4      [36m0.9904[0m        [32m0.0302[0m       0.8080      0.8847        0.2127        202.3450
      5      [36m0.9909[0m        [32m0.0286[0m       0.8061      0.8881        0.2237        202.3463
      6      0.9897        [32m0.0274[0m       0.8354      [31m0.9010[0m        0.1949        202.2747
      7      [36m0.9930[0m        [32m0.0224[0m       0.8248      0.8981        0.2027        202.2504
      8      0.9921        0.0227       0.8252      0.8986        0.2097        202.0645
      9      [36m0.9944[0m        [32m0.0201[0m       0.8201      0.8937        0.2196        202.1935
     10      [36m0.9951[0m        [32m0.0182[0m       0.8260      0.8987        0.2325        202.1773
     11      [36m0.9955[0m        [32m0.0159[0m       0.8328      0.8828        0.2073        202.2060
     12      0.9920        0.0200       0.8113      0.8956        0.2275        202.1532
     13      [36m0.9965[0m        [32m0.0135[0m       0.8113      0.8877        0.2396        202.2447
     14      0.9958        [32m0.0130[0m       [35m0.8365[0m      0.8975        0.2164        202.2357
     15      [36m0.9971[0m        [32m0.0111[0m       0.8316      0.8979        0.2127        202.2031
     16      [36m0.9974[0m        [32m0.0104[0m       [35m0.8469[0m      [31m0.9054[0m        0.2289        202.1913
     17      0.9930        0.0168       0.8354      0.8993        0.2392        202.1465
     18      0.9961        0.0135       0.8122      0.8903        0.2564        202.1306
     19      0.9948        0.0144       0.8226      0.8945        0.2307        202.1110
     20      [36m0.9993[0m        [32m0.0062[0m       0.8207      0.8869        0.2461        202.7722
     21      [36m0.9998[0m        [32m0.0050[0m       0.8219      0.8918        0.2519        202.8545
Stopping since valid_loss has not improved in the last 20 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.9407212505804056
F1 Macro Score after query 9: 0.927725059393436
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_8.pt

Iteration: 10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9760[0m        [32m0.0522[0m       [35m0.8174[0m      [31m0.8939[0m        [94m0.1684[0m     +  301.4794
      2      [36m0.9816[0m        [32m0.0418[0m       0.8139      0.8933        0.1884        301.4254
      3      [36m0.9861[0m        [32m0.0340[0m       0.7955      0.8654        0.2134        301.5323
      4      [36m0.9881[0m        [32m0.0294[0m       0.7990      0.8755        0.2093        301.6228
      5      0.9878        [32m0.0277[0m       0.7825      0.8640        0.2270        301.5000
      6      [36m0.9904[0m        [32m0.0248[0m       0.8120      0.8854        0.2100        301.4527
      7      [36m0.9909[0m        [32m0.0218[0m       [35m0.8295[0m      0.8934        0.1943        301.3822
      8      [36m0.9927[0m        [32m0.0185[0m       0.7806      0.8794        0.2617        301.3488
      9      [36m0.9933[0m        [32m0.0169[0m       0.8177      0.8853        0.2205        301.5561
     10      [36m0.9942[0m        [32m0.0149[0m       0.7738      0.8664        0.2729        301.2359
     11      [36m0.9947[0m        [32m0.0137[0m       0.8181      0.8820        0.2240        301.0909
     12      [36m0.9951[0m        [32m0.0135[0m       0.8134      0.8792        0.2369        301.1126
     13      [36m0.9959[0m        [32m0.0119[0m       0.8193      [31m0.8959[0m        0.2474        300.9784
     14      0.9954        [32m0.0108[0m       0.8023      0.8872        0.2700        300.9001
     15      [36m0.9965[0m        0.0111       0.8057      0.8846        0.2728        300.7271
     16      0.9964        [32m0.0089[0m       0.8153      0.8825        0.2699        300.7041
     17      [36m0.9970[0m        [32m0.0080[0m       0.8123      0.8871        0.2670        300.8697
     18      0.9962        0.0094       0.8203      0.8691        0.2455        300.8863
     19      0.9964        0.0102       0.8111      0.8778        0.2730        301.0152
     20      [36m0.9985[0m        [32m0.0057[0m       0.8217      0.8799        0.2651        300.8965
Stopping since valid_loss has not improved in the last 20 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.9390557939914164
F1 Macro Score after query 10: 0.9247298855575868
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_9.pt

Iteration: 11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9768[0m        [32m0.0497[0m       [35m0.8191[0m      [31m0.8621[0m        [94m0.1920[0m     +  476.3581
      2      [36m0.9811[0m        [32m0.0399[0m       [35m0.8451[0m      [31m0.9025[0m        [94m0.1634[0m     +  476.8732
      3      [36m0.9839[0m        [32m0.0342[0m       0.8281      0.8994        0.1870        476.4527
      4      [36m0.9861[0m        [32m0.0294[0m       0.8363      [31m0.9027[0m        0.1809        476.4106
      5      [36m0.9890[0m        [32m0.0250[0m       0.8418      [31m0.9061[0m        0.1938        476.5446
      6      [36m0.9890[0m        [32m0.0233[0m       [35m0.8464[0m      [31m0.9112[0m        0.1839        476.4432
      7      [36m0.9917[0m        [32m0.0183[0m       [35m0.8510[0m      [31m0.9137[0m        0.1910        476.4515
      8      [36m0.9928[0m        [32m0.0164[0m       [35m0.8557[0m      0.9071        0.1735        476.2416
      9      [36m0.9930[0m        [32m0.0157[0m       0.8175      0.8883        0.2166        476.2526
     10      [36m0.9934[0m        [32m0.0149[0m       0.8042      0.8893        0.2517        476.4267
     11      [36m0.9955[0m        [32m0.0122[0m       0.8363      0.8996        0.2220        476.5729
     12      0.9951        [32m0.0119[0m       0.8217      0.8914        0.2279        476.5182
     13      [36m0.9956[0m        [32m0.0111[0m       0.8198      0.8889        0.2357        476.4458
     14      0.9944        0.0121       0.7955      0.8753        0.2740        477.0255
     15      [36m0.9972[0m        [32m0.0075[0m       0.7974      0.8867        0.2932        477.5007
     16      0.9959        0.0090       0.8028      0.8906        0.2825        477.4479
     17      0.9964        0.0081       0.8132      0.8855        0.2563        477.3844
     18      [36m0.9974[0m        [32m0.0070[0m       0.8146      0.8909        0.2810        476.6014
     19      0.9965        0.0074       0.8024      0.8796        0.2910        476.2890
     20      0.9972        [32m0.0065[0m       0.8240      0.8866        0.2859        476.2866
     21      [36m0.9980[0m        [32m0.0057[0m       0.8066      0.8682        0.3025        476.2447
Stopping since valid_loss has not improved in the last 20 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.9386498345008082
F1 Macro Score after query 11: 0.9255942357054492
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_10.pt

Iteration: 12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9787[0m        [32m0.0424[0m       [35m0.8241[0m      [31m0.8847[0m        [94m0.1543[0m     +  788.5629
      2      [36m0.9817[0m        [32m0.0356[0m       [35m0.8250[0m      [31m0.8927[0m        0.1558        789.2902
      3      [36m0.9838[0m        [32m0.0308[0m       [35m0.8356[0m      0.8832        0.1591        788.7062
      4      [36m0.9865[0m        [32m0.0266[0m       0.8172      0.8830        0.1861        789.1632
      5      [36m0.9875[0m        [32m0.0227[0m       0.8276      [31m0.8935[0m        0.1973        789.0708
      6      [36m0.9889[0m        [32m0.0206[0m       0.7906      0.8423        0.2312        788.6638
      7      [36m0.9911[0m        [32m0.0179[0m       0.8227      0.8915        0.2243        792.9170
      8      [36m0.9919[0m        [32m0.0157[0m       0.8123      0.8833        0.2245        792.8428
      9      [36m0.9935[0m        [32m0.0131[0m       0.8226      0.8848        0.2038        791.7842
     10      [36m0.9939[0m        [32m0.0122[0m       0.8097      0.8670        0.2628        790.6134
     11      [36m0.9948[0m        [32m0.0110[0m       0.8210      0.8894        0.2404        790.7783
     12      [36m0.9951[0m        [32m0.0101[0m       0.7925      0.8744        0.2760        789.7980
     13      [36m0.9956[0m        [32m0.0094[0m       0.8141      0.8886        0.2488        789.5276
     14      [36m0.9961[0m        [32m0.0078[0m       0.8040      0.8679        0.2909        789.5498
     15      [36m0.9964[0m        0.0079       0.8208      0.8854        0.2586        788.9246
     16      [36m0.9966[0m        [32m0.0069[0m       0.8174      0.8855        0.2857        789.3197
     17      [36m0.9968[0m        [32m0.0066[0m       0.8278      [31m0.8999[0m        0.2681        789.0998
     18      [36m0.9971[0m        [32m0.0064[0m       0.8017      0.8654        0.3067        794.0616
     19      [36m0.9977[0m        [32m0.0054[0m       0.8220      0.8867        0.2934        793.7700
     20      0.9971        0.0061       0.8224      0.8991        0.2818        791.3236
Stopping since valid_loss has not improved in the last 20 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.9437763548336717
F1 Macro Score after query 12: 0.9311401299249685
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_11.pt

Iteration: 13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9806[0m        [32m0.0371[0m       [35m0.8175[0m      [31m0.8733[0m        [94m0.1801[0m     +  917.8007
      2      [36m0.9828[0m        [32m0.0319[0m       [35m0.8196[0m      0.8709        0.1916        917.3700
      3      [36m0.9849[0m        [32m0.0274[0m       [35m0.8260[0m      [31m0.8859[0m        0.1854        918.1852
      4      [36m0.9876[0m        [32m0.0234[0m       [35m0.8313[0m      [31m0.8940[0m        0.2067        918.5178
      5      [36m0.9887[0m        [32m0.0201[0m       0.8151      0.8783        0.2430        918.2817
      6      [36m0.9908[0m        [32m0.0171[0m       0.7962      0.8819        0.2682        925.3734
      7      [36m0.9916[0m        [32m0.0158[0m       0.8191      [31m0.8991[0m        0.2648        924.5207
      8      [36m0.9933[0m        [32m0.0134[0m       [35m0.8366[0m      [31m0.9033[0m        0.2461        925.6328
      9      [36m0.9936[0m        [32m0.0121[0m       0.8182      0.8959        0.2871        927.2948
     10      [36m0.9942[0m        [32m0.0110[0m       0.7750      0.8643        0.3113        924.4996
     11      [36m0.9950[0m        [32m0.0097[0m       0.8231      0.9002        0.2820        925.7497
     12      [36m0.9957[0m        [32m0.0086[0m       [35m0.8436[0m      [31m0.9066[0m        0.2392        923.6390
     13      [36m0.9963[0m        [32m0.0079[0m       0.8292      0.8955        0.2764        921.5595
     14      0.9957        0.0085       0.8302      0.9041        0.2979        921.2400
     15      [36m0.9969[0m        [32m0.0065[0m       0.8396      0.9039        0.2548        921.3464
     16      0.9968        0.0070       0.8234      0.9007        0.2875        922.2067
     17      0.9968        [32m0.0064[0m       0.8352      0.9054        0.2900        921.6842
     18      [36m0.9972[0m        [32m0.0054[0m       0.8245      0.9000        0.3035        921.5629
     19      [36m0.9976[0m        [32m0.0051[0m       0.8335      0.8985        0.2650        921.3950
     20      0.9976        0.0053       0.8408      0.9030        0.2773        921.1785
Stopping since valid_loss has not improved in the last 20 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.9367959949937422
F1 Macro Score after query 13: 0.9201133597330466
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to C:\Users\localuserSKSG\Desktop\Shubham\Results\ActiveLearning/Multilabel/DinoL/average_confidence_seed42\model_checkpoint_iteration_12.pt
