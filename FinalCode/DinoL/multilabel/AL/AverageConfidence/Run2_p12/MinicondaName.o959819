### Starting TaskPrologue of job 959819 on tg093 at Sat 21 Dec 2024 04:36:23 AM CET
Running on cores 96-127 with governor ondemand
Sat Dec 21 04:36:23 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:C1:00.0 Off |                    0 |
| N/A   33C    P0             52W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

Adjusted shapes after loading:
X_initial_np: (8, 224, 224, 3), y_initial_np: (8, 3)
X_pool_np: (26872, 224, 224, 3), y_pool_np: (26872, 3)
X_test_np: (5760, 224, 224, 3), y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3), y_val_np: (5760, 3)
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.7374[0m        [32m0.7129[0m       [35m0.1229[0m      [31m0.4139[0m        [94m0.7027[0m     +  74.9045
      2      0.5778        [32m0.6840[0m       [35m0.1613[0m      [31m0.5163[0m        [94m0.7008[0m     +  72.7275
      3      0.5599        [32m0.6747[0m       [35m0.1842[0m      [31m0.5807[0m        [94m0.7003[0m     +  72.5666
      4      0.7374        0.7233       0.1741      [31m0.5815[0m        [94m0.6993[0m     +  72.6904
      5      [36m0.8500[0m        [32m0.6679[0m       0.1774      [31m0.5868[0m        [94m0.6984[0m     +  72.7224
      6      0.6985        [32m0.6630[0m       0.1686      0.5813        [94m0.6974[0m     +  72.7067
      7      0.7389        [32m0.6438[0m       0.1549      0.5635        [94m0.6964[0m     +  72.7626
      8      0.4563        0.6507       0.1663      0.5508        [94m0.6950[0m     +  72.7276
      9      0.5889        0.6507       [35m0.1913[0m      0.5477        [94m0.6936[0m     +  72.7748
     10      0.8426        [32m0.6197[0m       [35m0.2075[0m      0.5468        [94m0.6916[0m     +  72.7690
     11      0.7963        [32m0.6029[0m       [35m0.2115[0m      0.5375        [94m0.6899[0m     +  72.6989
     12      0.8500        [32m0.5811[0m       [35m0.2116[0m      0.5286        [94m0.6885[0m     +  72.7532
     13      0.7831        0.6123       0.2102      0.5195        [94m0.6871[0m     +  72.7374
     14      [36m0.8796[0m        0.5836       0.2104      0.5118        [94m0.6859[0m     +  72.7354
     15      0.7963        [32m0.5788[0m       0.2085      0.5064        [94m0.6853[0m     +  72.7507
     16      [36m0.8963[0m        [32m0.5658[0m       0.2095      0.5066        0.6855        72.7477
     17      0.8593        [32m0.5379[0m       0.2068      0.5072        0.6858        72.6967
     18      [36m0.9259[0m        [32m0.5293[0m       0.2035      0.5083        0.6865        72.7019
     19      0.8426        0.5480       0.2007      0.5105        0.6871        72.7088
     20      0.8426        0.5492       0.2021      0.5136        0.6876        72.6999
     21      0.8889        [32m0.4996[0m       0.2038      0.5157        0.6880        72.7041
     22      0.8426        0.5205       0.2062      0.5183        0.6880        72.7159
     23      0.8783        [32m0.4947[0m       0.2062      0.5200        0.6878        72.7472
     24      0.9259        0.4966       0.2050      0.5219        0.6869        72.7500
     25      [36m0.9630[0m        [32m0.4531[0m       0.2040      0.5236        0.6861        72.7342
     26      0.9259        0.4612       0.2052      0.5248        [94m0.6850[0m     +  72.7016
     27      0.9259        [32m0.4418[0m       0.2099      0.5273        [94m0.6843[0m     +  72.7405
     28      0.9630        0.4615       [35m0.2194[0m      0.5325        [94m0.6840[0m     +  72.7049
     29      0.9630        [32m0.4089[0m       [35m0.2286[0m      0.5372        0.6840        72.7182
     30      0.9259        0.4365       [35m0.2340[0m      0.5391        0.6846        72.6768
     31      0.9630        0.4158       0.2319      0.5382        0.6855        72.6964
     32      0.9630        [32m0.4080[0m       0.2295      0.5369        0.6863        72.6723
     33      [36m1.0000[0m        [32m0.3855[0m       0.2285      0.5370        0.6871        72.6905
     34      1.0000        0.3884       0.2273      0.5358        0.6872        72.7684
     35      1.0000        [32m0.3719[0m       0.2285      0.5371        0.6875        72.6891
     36      0.9630        [32m0.3666[0m       0.2290      0.5376        0.6873        72.6906
     37      1.0000        [32m0.3452[0m       0.2299      0.5364        0.6867        72.6795
     38      0.9630        0.3692       0.2323      0.5359        0.6857        72.6787
     39      1.0000        [32m0.3429[0m       0.2340      0.5352        0.6848        72.7601
Stopping since valid_loss has not improved in the last 12 epochs.
Pre F1 micro score = 0.5643
Pre F1 macro score = 0.5558
Pre Accuracy = 0.2991

Iteration: 1
Selecting 16 informative samples: 

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.7046[0m        [32m0.5760[0m       [35m0.2130[0m      [31m0.5210[0m        [94m0.6614[0m     +  73.0648
      2      [36m0.7764[0m        [32m0.5317[0m       0.1894      0.5132        [94m0.6475[0m     +  73.0496
      3      [36m0.7786[0m        [32m0.5019[0m       [35m0.2181[0m      [31m0.5559[0m        [94m0.6368[0m     +  73.0583
      4      [36m0.8423[0m        [32m0.4672[0m       [35m0.2497[0m      [31m0.5915[0m        [94m0.6224[0m     +  73.1363
      5      [36m0.8774[0m        [32m0.4426[0m       [35m0.2707[0m      [31m0.6053[0m        [94m0.6065[0m     +  73.2035
      6      [36m0.9115[0m        [32m0.4229[0m       [35m0.2866[0m      0.5964        [94m0.5895[0m     +  73.1642
      7      0.8918        [32m0.4012[0m       [35m0.3108[0m      [31m0.6063[0m        [94m0.5735[0m     +  73.2073
      8      [36m0.9515[0m        [32m0.3744[0m       [35m0.3429[0m      [31m0.6333[0m        [94m0.5610[0m     +  73.1595
      9      [36m0.9689[0m        [32m0.3526[0m       [35m0.3734[0m      [31m0.6593[0m        [94m0.5477[0m     +  73.2039
     10      0.9689        [32m0.3296[0m       [35m0.3932[0m      [31m0.6754[0m        [94m0.5337[0m     +  73.2324
     11      [36m1.0000[0m        0.3312       [35m0.4099[0m      [31m0.6873[0m        [94m0.5209[0m     +  73.2122
     12      1.0000        [32m0.3156[0m       [35m0.4285[0m      [31m0.7026[0m        [94m0.5116[0m     +  73.1986
     13      1.0000        [32m0.2954[0m       [35m0.4469[0m      [31m0.7184[0m        [94m0.5060[0m     +  73.2048
     14      1.0000        [32m0.2763[0m       [35m0.4552[0m      [31m0.7211[0m        [94m0.4981[0m     +  73.1955
     15      1.0000        [32m0.2652[0m       [35m0.4618[0m      0.7208        [94m0.4881[0m     +  73.2327
     16      1.0000        0.2653       [35m0.4830[0m      [31m0.7325[0m        [94m0.4805[0m     +  73.1829
     17      1.0000        [32m0.2451[0m       [35m0.4974[0m      [31m0.7450[0m        [94m0.4760[0m     +  73.2207
     18      0.9825        [32m0.2292[0m       [35m0.5062[0m      [31m0.7489[0m        [94m0.4709[0m     +  73.2059
     19      1.0000        0.2359       0.5056      0.7436        [94m0.4658[0m     +  73.2539
     20      1.0000        0.2309       0.5024      0.7419        [94m0.4627[0m     +  73.2354
     21      1.0000        [32m0.2185[0m       [35m0.5151[0m      [31m0.7498[0m        [94m0.4580[0m     +  73.2390
     22      1.0000        [32m0.2142[0m       [35m0.5255[0m      [31m0.7578[0m        [94m0.4535[0m     +  73.2396
     23      1.0000        [32m0.2052[0m       [35m0.5306[0m      0.7544        [94m0.4462[0m     +  73.2336
     24      1.0000        [32m0.1962[0m       [35m0.5309[0m      [31m0.7586[0m        [94m0.4433[0m     +  73.2277
     25      1.0000        [32m0.1891[0m       [35m0.5316[0m      0.7581        [94m0.4399[0m     +  73.2446
     26      1.0000        [32m0.1831[0m       [35m0.5340[0m      0.7577        [94m0.4369[0m     +  73.2312
     27      1.0000        [32m0.1798[0m       [35m0.5373[0m      [31m0.7662[0m        [94m0.4365[0m     +  73.2572
     28      1.0000        [32m0.1667[0m       [35m0.5387[0m      0.7653        [94m0.4326[0m     +  73.2200
     29      1.0000        0.1744       [35m0.5405[0m      0.7615        [94m0.4277[0m     +  73.2204
     30      1.0000        0.1768       [35m0.5443[0m      0.7649        [94m0.4262[0m     +  73.2466
     31      1.0000        [32m0.1666[0m       0.5392      [31m0.7699[0m        0.4278        73.1927
     32      1.0000        0.1697       0.5382      [31m0.7718[0m        [94m0.4259[0m     +  73.1462
     33      1.0000        [32m0.1647[0m       [35m0.5474[0m      0.7681        [94m0.4190[0m     +  73.1911
     34      1.0000        [32m0.1616[0m       [35m0.5589[0m      0.7664        [94m0.4131[0m     +  73.2242
     35      1.0000        [32m0.1545[0m       0.5528      [31m0.7724[0m        0.4132        73.1891
     36      1.0000        [32m0.1402[0m       0.5528      [31m0.7740[0m        0.4133        73.1834
     37      1.0000        0.1474       0.5530      [31m0.7754[0m        [94m0.4123[0m     +  73.3001
     38      1.0000        0.1471       [35m0.5595[0m      0.7734        [94m0.4084[0m     +  73.1793
     39      1.0000        [32m0.1399[0m       [35m0.5658[0m      0.7721        [94m0.4051[0m     +  73.1740
     40      1.0000        0.1429       0.5571      [31m0.7812[0m        0.4079        73.2020
     41      1.0000        [32m0.1344[0m       0.5589      0.7784        [94m0.4042[0m     +  73.1561
     42      1.0000        0.1488       [35m0.5712[0m      0.7678        [94m0.4007[0m     +  73.1864
     43      1.0000        [32m0.1332[0m       0.5648      0.7784        0.4011        73.1737
     44      1.0000        [32m0.1295[0m       0.5672      0.7804        [94m0.3988[0m     +  73.1776
     45      1.0000        [32m0.1231[0m       0.5684      0.7777        [94m0.3966[0m     +  73.2171
     46      1.0000        0.1274       [35m0.5731[0m      0.7768        [94m0.3951[0m     +  73.1956
     47      1.0000        0.1270       0.5696      [31m0.7829[0m        0.3956        73.2205
     48      1.0000        [32m0.1105[0m       0.5691      [31m0.7831[0m        [94m0.3934[0m     +  73.1366
     49      1.0000        0.1252       [35m0.5773[0m      0.7761        [94m0.3901[0m     +  73.2273
     50      1.0000        0.1112       [35m0.5797[0m      0.7785        [94m0.3898[0m     +  73.2216
     51      1.0000        0.1140       0.5714      0.7821        0.3910        73.2072
     52      1.0000        0.1162       0.5766      0.7777        [94m0.3892[0m     +  73.1669
     53      1.0000        0.1149       0.5741      0.7713        [94m0.3887[0m     +  73.2178
     54      1.0000        0.1150       0.5743      0.7829        0.3896        73.2104
     55      1.0000        0.1206       0.5705      [31m0.7862[0m        [94m0.3882[0m     +  73.1681
     56      1.0000        [32m0.0987[0m       0.5780      0.7807        [94m0.3834[0m     +  73.2037
     57      1.0000        0.1137       [35m0.5799[0m      0.7797        0.3840        73.1967
     58      1.0000        0.1041       0.5747      0.7833        0.3843        73.1678
     59      1.0000        [32m0.0955[0m       0.5786      0.7796        [94m0.3806[0m     +  73.1567
     60      1.0000        0.1022       0.5797      0.7735        0.3816        73.2164
     61      1.0000        0.1129       0.5786      [31m0.7866[0m        0.3836        73.1517
     62      1.0000        0.0955       0.5793      0.7837        [94m0.3800[0m     +  73.1504
     63      1.0000        0.1069       [35m0.5865[0m      0.7746        [94m0.3761[0m     +  73.1854
     64      1.0000        0.1008       0.5795      [31m0.7889[0m        0.3814        73.1723
     65      1.0000        0.1004       0.5771      0.7822        0.3810        73.1544
     66      1.0000        0.1049       0.5816      0.7746        0.3776        73.1795
     67      1.0000        [32m0.0906[0m       0.5814      0.7875        0.3771        73.1838
     68      1.0000        0.0940       0.5818      0.7884        0.3780        73.1773
     69      1.0000        0.0956       0.5863      0.7812        [94m0.3752[0m     +  73.1860
     70      1.0000        [32m0.0905[0m       0.5807      0.7818        [94m0.3734[0m     +  73.1970
     71      1.0000        0.0930       0.5852      [31m0.7898[0m        [94m0.3718[0m     +  73.2524
     72      1.0000        0.1016       [35m0.5901[0m      [31m0.7911[0m        [94m0.3702[0m     +  73.2352
     73      1.0000        0.0959       0.5863      0.7901        [94m0.3690[0m     +  73.2091
     74      1.0000        [32m0.0863[0m       0.5844      [31m0.7960[0m        0.3714        73.2324
     75      1.0000        0.0870       0.5889      0.7894        0.3713        73.1661
     76      1.0000        0.0882       0.5845      0.7796        0.3715        73.1657
     77      1.0000        0.0924       0.5844      0.7920        0.3721        73.1804
     78      1.0000        0.0903       0.5896      0.7870        [94m0.3675[0m     +  73.1816
     79      1.0000        0.0865       0.5887      0.7890        0.3691        73.2296
     80      1.0000        [32m0.0855[0m       0.5863      0.7841        0.3690        73.1578
     81      1.0000        0.0905       [35m0.5906[0m      0.7803        [94m0.3669[0m     +  73.1907
     82      1.0000        [32m0.0831[0m       0.5847      0.7903        0.3692        73.2137
     83      1.0000        0.0932       0.5880      0.7833        [94m0.3654[0m     +  73.1838
     84      1.0000        [32m0.0777[0m       0.5896      0.7791        0.3661        73.2163
     85      1.0000        0.0862       0.5892      0.7882        0.3690        73.1807
     86      1.0000        0.1024       0.5835      0.7792        0.3685        73.1692
     87      1.0000        0.0825       0.5748      0.7790        0.3728        73.1777
     88      1.0000        0.0785       0.5863      0.7818        0.3693        73.1622
     89      1.0000        0.0814       [35m0.5925[0m      [31m0.7968[0m        0.3674        73.1931
     90      1.0000        0.0858       0.5865      0.7882        0.3657        73.1893
     91      1.0000        0.0787       0.5885      0.7902        0.3668        73.1789
     92      1.0000        [32m0.0755[0m       0.5896      0.7918        [94m0.3645[0m     +  73.2603
     93      1.0000        0.0817       0.5870      0.7861        [94m0.3622[0m     +  73.2556
     94      1.0000        0.0830       0.5878      0.7885        0.3642        73.2342
     95      1.0000        0.0780       [35m0.5934[0m      0.7874        0.3650        73.2026
     96      1.0000        0.0849       0.5899      0.7873        0.3634        73.1656
     97      1.0000        0.0803       0.5865      0.7881        0.3646        73.1697
     98      1.0000        [32m0.0708[0m       [35m0.5955[0m      0.7827        0.3643        73.1642
     99      1.0000        0.0728       [35m0.5967[0m      0.7945        0.3623        73.1811
    100      1.0000        0.0796       0.5901      0.7938        [94m0.3611[0m     +  73.1751
[24]
F1 Micro Score after query 1: 0.8036366346225502
F1 Macro Score after query 1: 0.7949247431015918
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_0.pt

Iteration: 2
Selecting 32 informative samples: 

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9239[0m        [32m0.1993[0m       [35m0.5892[0m      [31m0.7292[0m        [94m0.3557[0m     +  74.0846
      2      [36m0.9532[0m        [32m0.1737[0m       [35m0.6269[0m      [31m0.8193[0m        [94m0.3210[0m     +  74.1455
      3      [36m0.9604[0m        [32m0.1492[0m       [35m0.6623[0m      0.7834        [94m0.3162[0m     +  74.1112
      4      [36m0.9658[0m        [32m0.1345[0m       0.6575      0.8183        [94m0.3058[0m     +  74.1823
      5      [36m0.9885[0m        0.1353       [35m0.6757[0m      0.7968        [94m0.3041[0m     +  74.1555
      6      0.9885        [32m0.1219[0m       0.6663      0.7970        [94m0.3014[0m     +  74.1724
      7      0.9885        [32m0.1180[0m       0.6740      0.7990        [94m0.2964[0m     +  74.1708
      8      0.9885        [32m0.1118[0m       0.6733      0.7870        0.3014        74.1396
      9      0.9885        [32m0.1028[0m       0.6726      0.7977        0.2974        74.1209
     10      [36m1.0000[0m        [32m0.0966[0m       [35m0.6780[0m      0.7870        0.3008        74.1328
     11      1.0000        [32m0.0944[0m       0.6769      0.7965        [94m0.2951[0m     +  74.1297
     12      1.0000        0.0971       [35m0.6788[0m      0.8046        [94m0.2926[0m     +  74.1640
     13      1.0000        [32m0.0919[0m       0.6764      0.7905        0.2967        74.1200
     14      1.0000        [32m0.0888[0m       0.6774      0.8103        [94m0.2899[0m     +  74.1010
     15      1.0000        [32m0.0886[0m       [35m0.6790[0m      0.7838        0.2996        74.1390
     16      1.0000        [32m0.0872[0m       0.6764      0.8089        [94m0.2874[0m     +  74.0957
     17      1.0000        0.0895       0.6755      0.7705        0.3013        74.1459
     18      1.0000        [32m0.0831[0m       0.6726      0.8023        0.2940        74.0971
     19      1.0000        0.0884       [35m0.6849[0m      0.7991        0.2896        74.1012
     20      1.0000        [32m0.0808[0m       0.6700      0.7768        0.2975        74.1015
     21      1.0000        0.0832       0.6762      0.8046        0.2900        74.1133
     22      1.0000        [32m0.0748[0m       0.6764      0.7797        0.2995        74.1079
     23      1.0000        0.0753       0.6778      0.8076        0.2881        74.0856
     24      1.0000        0.0769       0.6792      0.7976        0.2909        74.1187
     25      1.0000        0.0750       0.6774      0.7981        0.2908        74.0982
     26      1.0000        [32m0.0737[0m       0.6757      0.7975        0.2919        74.1083
     27      1.0000        [32m0.0707[0m       0.6760      0.7897        0.2949        74.0820
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56]
F1 Micro Score after query 2: 0.8240667821704205
F1 Macro Score after query 2: 0.7995351958879745
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_1.pt

Iteration: 3
Selecting 56 informative samples: 

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.8966[0m        [32m0.2158[0m       [35m0.6495[0m      [31m0.6495[0m        [94m0.3655[0m     +  75.7145
      2      [36m0.9330[0m        [32m0.1766[0m       [35m0.6649[0m      [31m0.7470[0m        [94m0.3219[0m     +  75.7731
      3      0.9178        0.1866       [35m0.7021[0m      [31m0.8357[0m        [94m0.2575[0m     +  75.7929
      4      [36m0.9640[0m        [32m0.1392[0m       [35m0.7155[0m      [31m0.8364[0m        [94m0.2521[0m     +  75.7744
      5      [36m0.9726[0m        [32m0.1265[0m       [35m0.7201[0m      0.8361        0.2608        75.8010
      6      [36m0.9811[0m        [32m0.1221[0m       [35m0.7227[0m      [31m0.8446[0m        [94m0.2454[0m     +  75.7565
      7      [36m0.9877[0m        [32m0.1038[0m       [35m0.7359[0m      0.8392        0.2497        75.7962
      8      0.9877        [32m0.0946[0m       0.7292      [31m0.8457[0m        0.2492        75.7334
      9      0.9877        0.0973       0.7267      [31m0.8518[0m        [94m0.2424[0m     +  75.7465
     10      0.9877        [32m0.0901[0m       0.7273      0.8404        0.2459        75.7972
     11      0.9877        [32m0.0854[0m       0.7333      0.8299        0.2609        75.7449
     12      [36m0.9939[0m        [32m0.0844[0m       0.7354      0.8499        [94m0.2395[0m     +  75.7323
     13      0.9939        [32m0.0829[0m       0.7352      [31m0.8575[0m        [94m0.2368[0m     +  75.7804
     14      0.9939        [32m0.0789[0m       [35m0.7418[0m      0.8559        0.2397        75.8142
     15      [36m1.0000[0m        [32m0.0748[0m       0.7405      0.8490        0.2419        75.7346
     16      1.0000        [32m0.0741[0m       0.7377      0.8390        0.2426        75.7490
     17      1.0000        0.0765       0.7368      0.8330        0.2465        75.7435
     18      1.0000        0.0745       0.7368      0.8572        [94m0.2358[0m     +  75.7602
     19      1.0000        [32m0.0720[0m       0.7339      [31m0.8598[0m        0.2362        75.8035
     20      1.0000        [32m0.0676[0m       0.7372      0.8449        0.2402        75.7495
     21      1.0000        [32m0.0674[0m       0.7349      0.8262        0.2486        75.7466
     22      1.0000        [32m0.0651[0m       0.7267      0.8105        0.2587        75.7528
     23      1.0000        0.0660       0.7347      0.8592        [94m0.2329[0m     +  75.7539
     24      1.0000        0.0667       0.7332      0.8592        0.2359        75.8101
     25      1.0000        [32m0.0638[0m       0.7351      0.8446        0.2449        75.7714
     26      1.0000        0.0642       0.7245      0.8073        0.2639        75.7441
     27      1.0000        [32m0.0637[0m       0.7398      0.8393        0.2380        75.7020
     28      1.0000        [32m0.0610[0m       0.7378      [31m0.8648[0m        [94m0.2302[0m     +  75.9882
     29      1.0000        0.0632       0.7319      0.8352        0.2382        76.0710
     30      1.0000        [32m0.0586[0m       0.7326      0.8286        0.2469        75.7564
     31      1.0000        [32m0.0568[0m       [35m0.7427[0m      0.8512        0.2341        75.6890
     32      1.0000        0.0574       [35m0.7443[0m      0.8628        [94m0.2288[0m     +  75.6784
     33      1.0000        0.0573       0.7431      0.8574        0.2322        75.7774
     34      1.0000        0.0580       0.7420      0.8486        0.2374        75.7026
     35      1.0000        [32m0.0552[0m       [35m0.7490[0m      0.8526        0.2298        75.6773
     36      1.0000        0.0561       0.7444      0.8515        [94m0.2265[0m     +  75.6810
     37      1.0000        [32m0.0540[0m       0.7406      0.8415        0.2318        75.7207
     38      1.0000        0.0540       0.7444      0.8434        0.2389        75.6272
     39      1.0000        0.0561       0.7370      0.8352        0.2560        75.5977
     40      1.0000        0.0546       0.7425      0.8623        0.2308        75.5868
     41      1.0000        0.0555       0.7389      0.8600        0.2287        75.5713
     42      1.0000        [32m0.0525[0m       0.7347      0.8285        0.2477        75.4924
     43      1.0000        0.0554       0.7208      0.7975        0.2600        75.5423
     44      1.0000        [32m0.0516[0m       0.7490      0.8567        [94m0.2236[0m     +  75.5484
     45      1.0000        0.0542       [35m0.7491[0m      0.8562        0.2243        75.5489
     46      1.0000        [32m0.0510[0m       0.7340      0.8257        0.2453        75.5132
     47      1.0000        [32m0.0491[0m       0.7418      0.8536        0.2328        75.4783
     48      1.0000        [32m0.0470[0m       0.7424      0.8638        0.2291        75.5058
     49      1.0000        0.0479       0.7432      0.8515        0.2323        75.4760
     50      1.0000        0.0480       0.7469      0.8530        0.2263        75.4891
     51      1.0000        0.0471       0.7420      0.8448        0.2288        75.4641
     52      1.0000        [32m0.0451[0m       0.7481      0.8564        [94m0.2232[0m     +  75.4640
     53      1.0000        0.0475       0.7432      0.8580        0.2253        75.5213
     54      1.0000        0.0480       0.7453      0.8487        0.2353        75.4841
     55      1.0000        0.0452       0.7469      0.8551        0.2272        75.4918
     56      1.0000        0.0453       0.7455      0.8589        [94m0.2229[0m     +  75.4401
     57      1.0000        0.0452       0.7476      0.8533        0.2263        75.4963
     58      1.0000        [32m0.0439[0m       0.7406      0.8378        0.2328        75.4525
     59      1.0000        [32m0.0429[0m       0.7467      0.8551        0.2241        75.4584
     60      1.0000        0.0457       0.7486      0.8588        [94m0.2223[0m     +  75.4582
     61      1.0000        0.0450       0.7488      0.8498        0.2298        75.5179
     62      1.0000        0.0440       0.7474      0.8524        0.2274        75.4587
     63      1.0000        [32m0.0406[0m       0.7425      0.8499        0.2324        75.5081
     64      1.0000        0.0431       0.7483      0.8596        [94m0.2214[0m     +  75.5201
     65      1.0000        0.0419       0.7465      0.8509        0.2246        75.5552
     66      1.0000        0.0429       0.7457      0.8443        0.2305        75.5017
     67      1.0000        [32m0.0402[0m       0.7469      0.8514        0.2327        75.5144
     68      1.0000        0.0436       0.7446      0.8559        0.2308        75.4992
     69      1.0000        0.0420       0.7476      0.8639        0.2230        75.4910
     70      1.0000        0.0424       [35m0.7498[0m      [31m0.8649[0m        [94m0.2189[0m     +  75.5008
     71      1.0000        0.0434       0.7243      0.8039        0.2531        75.5748
     72      1.0000        0.0413       0.7314      0.8162        0.2496        75.4824
     73      1.0000        [32m0.0399[0m       0.7413      0.8463        0.2285        75.4969
     74      1.0000        0.0442       0.7462      0.8580        0.2280        75.4401
     75      1.0000        0.0426       0.7422      0.8366        0.2459        75.5863
     76      1.0000        0.0440       0.7363      0.8447        0.2595        75.6016
     77      1.0000        0.0399       0.7335      [31m0.8650[0m        0.2333        75.4924
     78      1.0000        0.0425       0.7431      0.8577        0.2259        75.4981
     79      1.0000        [32m0.0392[0m       0.7373      0.8211        0.2476        75.4990
     80      1.0000        0.0406       0.7417      0.8342        0.2294        75.5042
     81      1.0000        0.0411       [35m0.7562[0m      0.8586        [94m0.2158[0m     +  75.5021
     82      1.0000        0.0413       0.7453      0.8497        0.2327        75.5324
     83      1.0000        [32m0.0379[0m       0.7495      0.8590        0.2231        75.4919
     84      1.0000        [32m0.0356[0m       0.7512      0.8556        0.2187        75.5096
     85      1.0000        [32m0.0355[0m       0.7474      0.8458        0.2237        75.5021
     86      1.0000        0.0360       0.7512      0.8593        0.2184        75.5222
     87      1.0000        0.0364       0.7495      0.8547        0.2232        75.5101
     88      1.0000        0.0369       0.7512      0.8571        0.2235        75.4941
     89      1.0000        [32m0.0344[0m       0.7533      0.8553        0.2172        75.5199
     90      1.0000        0.0356       0.7498      0.8417        0.2254        75.4833
     91      1.0000        0.0368       0.7491      0.8532        0.2248        75.4921
     92      1.0000        0.0377       0.7493      0.8643        0.2177        75.4974
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112]
F1 Micro Score after query 3: 0.8591149005278116
F1 Macro Score after query 3: 0.8479768872249799
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_2.pt

Iteration: 4
Selecting 96 informative samples: 

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9386[0m        [32m0.1510[0m       [35m0.6509[0m      [31m0.7226[0m        [94m0.3101[0m     +  78.2652
      2      [36m0.9619[0m        [32m0.1193[0m       [35m0.7288[0m      [31m0.7984[0m        [94m0.2494[0m     +  78.3147
      3      [36m0.9786[0m        [32m0.0832[0m       0.7264      [31m0.8237[0m        [94m0.2451[0m     +  78.3193
      4      [36m0.9856[0m        [32m0.0674[0m       0.7158      0.8233        0.2550        78.3077
      5      [36m0.9857[0m        [32m0.0623[0m       [35m0.7349[0m      [31m0.8424[0m        [94m0.2441[0m     +  78.2626
      6      0.9839        0.0651       0.7345      [31m0.8622[0m        0.2517        78.2957
      7      [36m0.9933[0m        [32m0.0598[0m       [35m0.7411[0m      [31m0.8711[0m        0.2592        78.2821
      8      0.9928        [32m0.0595[0m       [35m0.7578[0m      [31m0.8743[0m        [94m0.2413[0m     +  78.2782
      9      0.9933        [32m0.0533[0m       0.7533      0.8438        0.2416        78.3042
     10      [36m0.9963[0m        [32m0.0480[0m       0.7094      0.7983        0.2850        78.2748
     11      [36m0.9964[0m        [32m0.0459[0m       0.7003      0.7634        0.2806        78.2614
     12      [36m1.0000[0m        [32m0.0435[0m       0.7427      0.8279        0.2415        78.3028
     13      1.0000        [32m0.0410[0m       0.7285      0.8191        0.2489        78.2560
     14      1.0000        0.0428       0.7153      0.7916        0.2682        78.2677
     15      1.0000        [32m0.0387[0m       0.7227      0.8054        0.2589        78.2815
     16      1.0000        [32m0.0381[0m       0.7226      0.8254        0.2501        78.3029
     17      1.0000        [32m0.0353[0m       0.7196      0.8330        0.2525        78.3182
     18      1.0000        [32m0.0337[0m       0.7345      0.8389        [94m0.2406[0m     +  78.2685
     19      1.0000        0.0343       0.7323      0.8379        [94m0.2389[0m     +  78.3011
     20      1.0000        0.0341       0.7260      0.8232        0.2486        78.3112
     21      1.0000        [32m0.0329[0m       0.7271      0.8252        0.2497        78.2683
     22      1.0000        [32m0.0322[0m       0.7299      0.8368        0.2445        78.2696
     23      1.0000        0.0323       0.7330      0.8377        0.2419        78.2841
     24      1.0000        [32m0.0321[0m       0.7245      0.8264        0.2533        78.2634
     25      1.0000        [32m0.0314[0m       0.7318      0.8364        0.2434        78.2374
     26      1.0000        0.0318       0.7319      0.8368        0.2443        78.2417
     27      1.0000        [32m0.0305[0m       0.7339      0.8366        0.2445        78.2563
     28      1.0000        [32m0.0304[0m       0.7299      0.8345        0.2456        78.2309
     29      1.0000        [32m0.0298[0m       0.7276      0.8341        0.2485        78.2862
     30      1.0000        0.0305       0.7337      0.8323        0.2466        78.2673
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208]
F1 Micro Score after query 4: 0.896261608064132
F1 Macro Score after query 4: 0.8864081755442155
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_3.pt

Iteration: 5
Selecting 176 informative samples: 

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9394[0m        [32m0.1195[0m       [35m0.7590[0m      [31m0.8746[0m        [94m0.2363[0m     +  83.4308
      2      [36m0.9618[0m        [32m0.1010[0m       0.7347      0.8680        0.2687        83.4574
      3      0.9593        [32m0.0868[0m       0.7512      0.8689        0.2520        83.4737
      4      [36m0.9818[0m        [32m0.0662[0m       0.7516      0.8671        0.2531        83.4283
      5      [36m0.9938[0m        [32m0.0498[0m       0.7590      0.8685        [94m0.2355[0m     +  83.4309
      6      [36m0.9938[0m        [32m0.0467[0m       0.7307      0.8458        0.2639        83.4882
      7      [36m0.9958[0m        0.0481       0.7556      [31m0.8754[0m        0.2375        83.4188
      8      [36m0.9988[0m        [32m0.0403[0m       0.7535      0.8687        0.2418        83.4242
      9      0.9979        [32m0.0396[0m       0.7286      0.8337        0.2648        83.4439
     10      0.9980        [32m0.0385[0m       [35m0.7656[0m      [31m0.8814[0m        0.2385        83.3773
     11      0.9971        [32m0.0370[0m       0.7446      0.8589        0.2464        83.4396
     12      [36m1.0000[0m        [32m0.0353[0m       0.7389      0.8396        0.2594        83.4165
     13      0.9915        0.0381       0.7531      0.8492        0.2432        83.4309
     14      0.9980        [32m0.0338[0m       0.7337      0.8298        0.2681        83.3982
     15      1.0000        [32m0.0334[0m       0.7158      0.8243        0.2814        83.4328
     16      1.0000        [32m0.0300[0m       0.7153      0.8177        0.2642        83.4158
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384]
F1 Micro Score after query 5: 0.8930262418746489
F1 Macro Score after query 5: 0.877664839152681
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_4.pt

Iteration: 6
Selecting 320 informative samples: 

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9529[0m        [32m0.1001[0m       [35m0.7910[0m      [31m0.8706[0m        [94m0.1953[0m     +  92.6859
      2      [36m0.9741[0m        [32m0.0741[0m       0.7595      [31m0.8742[0m        0.2315        92.7627
      3      [36m0.9861[0m        [32m0.0562[0m       0.7125      0.8309        0.2772        92.6949
      4      [36m0.9916[0m        [32m0.0475[0m       0.7175      0.8512        0.2903        92.7162
      5      [36m0.9953[0m        [32m0.0416[0m       0.7519      0.8556        0.2210        92.7422
      6      0.9929        0.0424       0.7729      [31m0.8757[0m        0.2119        92.7603
      7      [36m0.9954[0m        [32m0.0385[0m       0.7799      [31m0.8830[0m        0.2221        92.7426
      8      0.9845        0.0481       0.7552      0.8674        0.2819        92.7505
      9      0.9894        0.0441       0.7462      0.8720        0.2518        92.7513
     10      0.9912        0.0404       0.7644      0.8537        0.2071        92.7534
     11      0.9933        [32m0.0371[0m       0.7639      0.8195        0.2342        92.7274
     12      0.9953        [32m0.0319[0m       0.7604      0.8568        0.2151        92.7574
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704]
F1 Micro Score after query 6: 0.900269115086275
F1 Macro Score after query 6: 0.8709310447386587
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_5.pt

Iteration: 7
Selecting 560 informative samples: 

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9548[0m        [32m0.1022[0m       [35m0.8085[0m      [31m0.8742[0m        [94m0.1843[0m     +  109.0164
      2      [36m0.9653[0m        [32m0.0822[0m       0.7781      0.8372        0.2097        109.1009
      3      [36m0.9794[0m        [32m0.0642[0m       0.7427      0.8361        0.2297        109.0158
      4      [36m0.9815[0m        [32m0.0576[0m       0.7384      0.8379        0.2477        109.0412
      5      [36m0.9841[0m        [32m0.0555[0m       0.7649      [31m0.8757[0m        0.2174        109.0219
      6      0.9784        0.0586       0.7646      [31m0.8814[0m        0.2389        109.0730
      7      [36m0.9854[0m        [32m0.0472[0m       0.7806      0.8767        0.1955        109.0478
      8      [36m0.9883[0m        [32m0.0411[0m       0.7918      0.8743        0.1985        109.0662
      9      [36m0.9909[0m        [32m0.0370[0m       0.7873      [31m0.8845[0m        0.2027        109.0209
     10      [36m0.9926[0m        [32m0.0338[0m       0.7806      0.8815        0.2212        109.0668
     11      [36m0.9929[0m        [32m0.0315[0m       0.7745      0.8835        0.2384        109.0441
     12      [36m0.9955[0m        [32m0.0279[0m       0.7984      [31m0.8895[0m        0.2030        109.0566
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264]
F1 Micro Score after query 7: 0.9153794989283867
F1 Macro Score after query 7: 0.9014287750851078
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_6.pt

Iteration: 8
Selecting 1000 informative samples: 

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9593[0m        [32m0.0917[0m       [35m0.7875[0m      [31m0.8789[0m        [94m0.1936[0m     +  137.9764
      2      [36m0.9729[0m        [32m0.0688[0m       0.7847      [31m0.8800[0m        0.2000        138.2308
      3      [36m0.9785[0m        [32m0.0569[0m       0.7776      0.8794        0.2194        138.1804
      4      [36m0.9846[0m        [32m0.0474[0m       [35m0.7922[0m      [31m0.8843[0m        0.2109        138.3161
      5      [36m0.9858[0m        [32m0.0455[0m       0.7852      0.8792        0.2016        138.3119
      6      [36m0.9871[0m        [32m0.0410[0m       [35m0.8097[0m      [31m0.8851[0m        0.1957        138.2818
      7      [36m0.9903[0m        [32m0.0364[0m       0.8021      0.8640        0.2042        138.3218
      8      0.9867        0.0392       0.7905      0.8811        0.2079        138.2458
      9      [36m0.9917[0m        [32m0.0299[0m       0.7847      0.8810        0.2314        138.4968
     10      0.9916        [32m0.0276[0m       0.7839      0.8758        0.2296        138.1616
     11      0.9884        0.0325       0.7738      0.8713        0.2352        138.1629
     12      [36m0.9939[0m        [32m0.0253[0m       [35m0.8212[0m      0.8809        0.1962        138.1757
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264]
F1 Micro Score after query 8: 0.920891875381796
F1 Macro Score after query 8: 0.9038877476835968
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_7.pt

Iteration: 9
Selecting 1776 informative samples: 

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9612[0m        [32m0.0835[0m       [35m0.8045[0m      [31m0.8628[0m        [94m0.1889[0m     +  189.7937
      2      [36m0.9765[0m        [32m0.0612[0m       0.7830      [31m0.8786[0m        0.2082        190.1250
      3      [36m0.9800[0m        [32m0.0517[0m       0.7797      [31m0.8787[0m        0.2196        190.0276
      4      [36m0.9816[0m        [32m0.0484[0m       0.7839      [31m0.8819[0m        0.2258        189.9948
      5      [36m0.9853[0m        [32m0.0420[0m       [35m0.8095[0m      [31m0.8903[0m        0.2066        189.9833
      6      [36m0.9880[0m        [32m0.0353[0m       0.8028      0.8861        0.2154        189.9576
      7      0.9867        0.0359       0.8057      0.8883        0.2110        189.9889
      8      [36m0.9920[0m        [32m0.0278[0m       0.8007      0.8853        0.2299        190.0658
      9      0.9917        [32m0.0270[0m       0.8089      0.8864        0.2161        190.0067
     10      [36m0.9921[0m        [32m0.0244[0m       0.7830      0.8820        0.2712        189.9898
     11      [36m0.9921[0m        [32m0.0236[0m       [35m0.8120[0m      0.8870        0.2150        190.0102
     12      0.9905        0.0246       [35m0.8168[0m      0.8870        0.2126        189.9935
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040]
F1 Micro Score after query 9: 0.930863827234553
F1 Macro Score after query 9: 0.9186282253729697
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_8.pt

Iteration: 10
Selecting 3160 informative samples: 

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9679[0m        [32m0.0715[0m       [35m0.7965[0m      [31m0.8844[0m        [94m0.1896[0m     +  281.7071
      2      [36m0.9778[0m        [32m0.0532[0m       0.7934      0.8841        0.1958        282.1711
      3      [36m0.9803[0m        [32m0.0460[0m       [35m0.7969[0m      0.8606        0.1956        282.1306
      4      [36m0.9833[0m        [32m0.0411[0m       [35m0.8024[0m      0.8774        0.1966        282.0957
      5      [36m0.9854[0m        [32m0.0348[0m       0.7934      0.8720        0.2092        282.0908
      6      [36m0.9868[0m        [32m0.0320[0m       0.7814      0.8597        0.2384        282.0885
      7      [36m0.9876[0m        [32m0.0301[0m       0.7977      0.8668        0.2223        282.1037
      8      [36m0.9886[0m        [32m0.0273[0m       0.7960      0.8832        0.2348        282.1363
      9      [36m0.9908[0m        [32m0.0235[0m       0.7807      0.8697        0.2467        282.1203
     10      [36m0.9918[0m        [32m0.0208[0m       0.7828      0.8844        0.2520        282.7257
     11      [36m0.9921[0m        [32m0.0191[0m       [35m0.8063[0m      0.8780        0.2375        282.3474
     12      [36m0.9935[0m        [32m0.0165[0m       [35m0.8267[0m      [31m0.8877[0m        0.2227        282.3390
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200]
F1 Micro Score after query 10: 0.930565167243368
F1 Macro Score after query 10: 0.9148819663312381
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_9.pt

Iteration: 11
Selecting 5624 informative samples: 

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9734[0m        [32m0.0586[0m       [35m0.8349[0m      [31m0.8950[0m        [94m0.1602[0m     +  445.1152
      2      [36m0.9788[0m        [32m0.0471[0m       [35m0.8443[0m      [31m0.9013[0m        [94m0.1498[0m     +  446.1201
      3      [36m0.9826[0m        [32m0.0399[0m       [35m0.8451[0m      [31m0.9030[0m        0.1706        446.3326
      4      [36m0.9834[0m        [32m0.0354[0m       0.8382      0.8967        0.1533        446.5459
      5      [36m0.9862[0m        [32m0.0296[0m       0.8418      [31m0.9054[0m        0.1632        446.8555
      6      [36m0.9872[0m        [32m0.0274[0m       0.8382      0.8961        0.1667        446.8758
      7      [36m0.9889[0m        [32m0.0239[0m       0.8247      0.8969        0.1946        446.0448
      8      [36m0.9903[0m        [32m0.0212[0m       0.8380      0.9010        0.1961        446.1161
      9      [36m0.9912[0m        [32m0.0196[0m       0.8300      0.8980        0.2198        446.7294
     10      [36m0.9928[0m        [32m0.0160[0m       0.8050      0.8875        0.2581        445.7940
     11      [36m0.9929[0m        [32m0.0158[0m       0.8344      0.9034        0.2151        445.9629
     12      [36m0.9938[0m        [32m0.0148[0m       0.8101      0.8890        0.2428        446.2042
     13      [36m0.9955[0m        [32m0.0118[0m       0.8123      0.8920        0.2498        446.4582
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824]
F1 Micro Score after query 11: 0.928386802493538
F1 Macro Score after query 11: 0.9154717750793425
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_10.pt

Iteration: 12
Selecting 10000 informative samples: 

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9767[0m        [32m0.0476[0m       [35m0.8323[0m      [31m0.8823[0m        [94m0.1613[0m     +  736.0815
      2      [36m0.9798[0m        [32m0.0391[0m       0.8139      0.8715        0.1797        738.3991
      3      [36m0.9820[0m        [32m0.0346[0m       0.8222      [31m0.8883[0m        0.1652        738.3259
      4      [36m0.9846[0m        [32m0.0296[0m       0.8168      0.8640        0.1740        737.7868
      5      [36m0.9854[0m        [32m0.0273[0m       0.8214      0.8872        0.1801        737.5198
      6      [36m0.9873[0m        [32m0.0235[0m       0.8118      0.8828        0.2073        738.3358
      7      [36m0.9890[0m        [32m0.0205[0m       0.8026      0.8639        0.2252        737.5791
      8      [36m0.9898[0m        [32m0.0188[0m       0.8203      0.8828        0.2226        737.7814
      9      [36m0.9916[0m        [32m0.0155[0m       0.7986      0.8522        0.2622        737.5128
     10      0.9915        0.0155       [35m0.8359[0m      [31m0.8943[0m        0.2188        737.9266
     11      [36m0.9937[0m        [32m0.0134[0m       0.8076      0.8658        0.2536        737.6046
     12      [36m0.9938[0m        [32m0.0116[0m       0.8012      0.8798        0.2591        738.3538
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824]
F1 Micro Score after query 12: 0.952836742570436
F1 Macro Score after query 12: 0.942478261305329
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_11.pt

Iteration: 13
Selecting 4056 informative samples: 

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9784[0m        [32m0.0410[0m       [35m0.8007[0m      [31m0.8694[0m        [94m0.1934[0m     +  854.4660
      2      [36m0.9818[0m        [32m0.0342[0m       [35m0.8339[0m      [31m0.8996[0m        [94m0.1775[0m     +  855.8363
      3      [36m0.9835[0m        [32m0.0302[0m       0.8233      0.8900        0.1951        855.9923
      4      [36m0.9857[0m        [32m0.0261[0m       0.8168      0.8980        0.2249        856.6216
      5      [36m0.9866[0m        [32m0.0237[0m       0.8212      0.8980        0.2249        855.8845
      6      [36m0.9882[0m        [32m0.0201[0m       0.8023      0.8959        0.2666        856.3875
      7      [36m0.9903[0m        [32m0.0177[0m       0.8264      0.8970        0.2256        855.8259
      8      [36m0.9911[0m        [32m0.0161[0m       0.8181      0.8948        0.2674        856.0177
      9      [36m0.9923[0m        [32m0.0145[0m       0.8304      [31m0.9002[0m        0.2491        855.8745
     10      [36m0.9932[0m        [32m0.0127[0m       0.8288      [31m0.9079[0m        0.2834        856.0796
     11      [36m0.9935[0m        [32m0.0118[0m       0.8285      0.9020        0.2659        856.1791
     12      [36m0.9953[0m        [32m0.0098[0m       [35m0.8349[0m      [31m0.9091[0m        0.2653        856.1700
     13      [36m0.9954[0m        [32m0.0092[0m       [35m0.8403[0m      [31m0.9103[0m        0.2581        856.1399
/home/hpc/iwfa/iwfa044h/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).
  from pandas.core.computation.check import NUMEXPR_INSTALLED
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Stopping since valid_loss has not improved in the last 12 epochs.
[24, 56, 112, 208, 384, 704, 1264, 2264, 4040, 7200, 12824, 22824, 26880]
F1 Micro Score after query 13: 0.9450992555831266
F1 Macro Score after query 13: 0.93290298008778
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/model_checkpoint_iteration_12.pt
Pickle file saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/EOD/Results/ActiveLearning/Multilabel/DinoL/average_confidence_seed42/AL_average_confidence_results_for_multilabel_classification.pickle
=== JOB_STATISTICS ===
=== current date     : Sat 21 Dec 2024 11:41:57 PM CET
= Job-ID             : 959819 on tinygpu
= Job-Name           : MinicondaName
= Job-Command        : /home/woody/iwfa/iwfa044h/CleanLab_Test/runner4.sh
= Initial workdir    : /home/woody/iwfa/iwfa044h/CleanLab_Test
= Queue/Partition    : a100
= Slurm account      : iwfa with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 19:05:37
= Total RAM usage    : 84.1 GiB of requested  GiB (%)   
= Node list          : tg093
= Subm/Elig/Start/End: 2024-12-21T04:36:03 / 2024-12-21T04:36:03 / 2024-12-21T04:36:20 / 2024-12-21T23:41:57
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           57.7G   104.9G   209.7G        N/A     152K     500K   1,000K        N/A    
    /home/vault          0.0K  1048.6G  2097.2G        N/A       1      200K     400K        N/A    
    /home/woody        987.6G  1000.0G  1500.0G        N/A   1,812K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:C1:00.0, 2886196, 95 %, 15 %, 6812 MiB, 68721284 ms
