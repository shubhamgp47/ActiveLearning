### Starting TaskPrologue of job 889597 on tg091 at Tue 10 Sep 2024 06:30:16 PM CEST
Running on cores 0-31 with governor ondemand
Tue Sep 10 18:30:16 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:01:00.0 Off |                    0 |
| N/A   39C    P0             55W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

/home/hpc/iwfa/iwfa044h/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).
  from pandas.core.computation.check import NUMEXPR_INSTALLED
X_train_initial shape: torch.Size([26880, 3, 224, 224])
y_train_initial shape: torch.Size([26880, 3])
Adjusted shapes after loading:
X_train_np: (10, 224, 224, 3) y_train_np: (10, 3)
X_pool: (26870, 224, 224, 3) y_pool: (26870, 3)
X_test_np: (5760, 224, 224, 3) y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3) y_val_np: (5760, 3)

Iteration: 1
Number of samples in training set: 8
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.5522[0m        [32m0.7367[0m       [35m0.0948[0m      [31m0.4361[0m        [94m0.7252[0m     +  62.4309
      2      0.5522        [32m0.7160[0m       0.0615      0.3992        [94m0.7156[0m     +  59.1404
      3      0.4993        [32m0.6977[0m       0.0875      0.3944        [94m0.7069[0m     +  59.1376
      4      [36m0.6453[0m        [32m0.6813[0m       [35m0.1109[0m      0.3970        [94m0.6990[0m     +  59.2537
      5      0.4988        [32m0.6654[0m       [35m0.1286[0m      [31m0.7403[0m        [94m0.6917[0m     +  59.2477
      6      0.5202        [32m0.6553[0m       [35m0.1394[0m      [31m0.7459[0m        [94m0.6850[0m     +  59.2024
      7      0.5202        [32m0.6420[0m       [35m0.1451[0m      0.4151        [94m0.6789[0m     +  59.2108
      8      0.5455        [32m0.6281[0m       [35m0.1488[0m      0.4167        [94m0.6731[0m     +  59.1907
      9      0.5455        [32m0.6167[0m       [35m0.1549[0m      0.4187        [94m0.6678[0m     +  59.2247
     10      0.5455        [32m0.6047[0m       [35m0.1590[0m      0.4210        [94m0.6628[0m     +  59.1986
     11      0.5455        [32m0.5926[0m       [35m0.1641[0m      0.4228        [94m0.6582[0m     +  59.2116
     12      0.5697        [32m0.5836[0m       [35m0.1738[0m      0.4240        [94m0.6540[0m     +  59.2044
     13      0.5697        [32m0.5745[0m       [35m0.1889[0m      0.4277        [94m0.6502[0m     +  59.1794
     14      0.6364        [32m0.5625[0m       [35m0.2000[0m      0.4302        [94m0.6467[0m     +  59.2700
     15      0.6364        [32m0.5523[0m       [35m0.2120[0m      0.4282        [94m0.6436[0m     +  59.2536
     16      0.6364        [32m0.5426[0m       [35m0.2208[0m      0.4252        [94m0.6407[0m     +  59.2796
     17      0.6364        [32m0.5327[0m       [35m0.2309[0m      0.4248        [94m0.6382[0m     +  59.1905
     18      0.6364        [32m0.5217[0m       [35m0.2373[0m      0.4234        [94m0.6360[0m     +  59.2769
     19      0.6364        [32m0.5132[0m       [35m0.2422[0m      0.4224        [94m0.6342[0m     +  59.2041
     20      0.6364        [32m0.5013[0m       [35m0.2462[0m      0.4221        [94m0.6326[0m     +  59.2346
     21      0.6364        [32m0.4931[0m       [35m0.2481[0m      0.4215        [94m0.6313[0m     +  59.1985
     22      0.6364        [32m0.4828[0m       [35m0.2507[0m      0.4213        [94m0.6302[0m     +  59.1944
     23      0.6364        [32m0.4769[0m       [35m0.2545[0m      0.4216        [94m0.6291[0m     +  59.1761
     24      0.6364        [32m0.4651[0m       [35m0.2594[0m      0.4213        [94m0.6281[0m     +  59.1983
     25      0.6364        [32m0.4580[0m       [35m0.2641[0m      0.4202        [94m0.6270[0m     +  59.2108
     26      0.6364        [32m0.4465[0m       [35m0.2696[0m      0.4197        [94m0.6258[0m     +  59.1989
     27      0.6364        [32m0.4375[0m       [35m0.2757[0m      0.4181        [94m0.6246[0m     +  59.1800
     28      0.6364        [32m0.4268[0m       [35m0.2790[0m      0.4161        [94m0.6233[0m     +  59.2287
     29      [36m0.6667[0m        [32m0.4186[0m       [35m0.2826[0m      0.4145        [94m0.6221[0m     +  59.1825
     30      [36m0.8889[0m        [32m0.4083[0m       [35m0.2835[0m      0.4134        [94m0.6212[0m     +  59.2041
     31      0.8889        [32m0.4007[0m       [35m0.2868[0m      0.4144        [94m0.6208[0m     +  59.2302
     32      0.8889        [32m0.3891[0m       [35m0.2870[0m      0.4141        [94m0.6205[0m     +  59.1847
     33      0.8889        [32m0.3809[0m       [35m0.2878[0m      0.4154        0.6206        59.1621
     34      0.8889        [32m0.3723[0m       0.2875      0.4161        0.6208        59.1213
     35      0.8889        [32m0.3623[0m       0.2856      0.4168        0.6212        59.1071
     36      0.8889        [32m0.3549[0m       0.2868      0.4179        0.6215        59.1524
     37      0.8889        [32m0.3475[0m       0.2849      0.4173        0.6216        59.1272
     38      0.8889        [32m0.3413[0m       0.2832      0.4170        0.6217        59.0915
     39      0.8889        [32m0.3322[0m       0.2837      0.4172        0.6217        59.1081
     40      [36m1.0000[0m        [32m0.3251[0m       0.2832      [31m0.7506[0m        0.6218        59.1102
     41      1.0000        [32m0.3183[0m       0.2830      [31m0.7514[0m        0.6222        59.1354
     42      1.0000        [32m0.3108[0m       0.2823      0.4191        0.6229        59.0963
     43      1.0000        [32m0.3070[0m       0.2806      0.4198        0.6241        59.1654
     44      1.0000        [32m0.2989[0m       0.2786      0.4200        0.6252        59.1293
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.43270698259611784
Test Accuracy: 0.24774305555555556
Number of samples added in this iteration: 8

Iteration: 2
Number of samples in training set: 16
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.7974[0m        [32m0.4491[0m       [35m0.2814[0m      [31m0.7527[0m        [94m0.6228[0m     +  59.3795
      2      0.7974        [32m0.4284[0m       0.2799      [31m0.7537[0m        [94m0.6226[0m     +  59.3783
      3      0.7974        [32m0.4125[0m       [35m0.2863[0m      [31m0.7569[0m        0.6238        59.3768
      4      0.7974        [32m0.4000[0m       [35m0.2882[0m      [31m0.7590[0m        0.6256        59.3266
      5      [36m0.8333[0m        [32m0.3862[0m       [35m0.2901[0m      [31m0.7597[0m        0.6269        59.3307
      6      [36m0.8804[0m        [32m0.3749[0m       [35m0.2938[0m      [31m0.7609[0m        0.6270        59.2990
      7      [36m0.8962[0m        [32m0.3642[0m       [35m0.2969[0m      0.7601        0.6261        59.3401
      8      [36m0.9158[0m        [32m0.3545[0m       [35m0.3043[0m      [31m0.7618[0m        0.6248        59.3052
      9      0.9158        [32m0.3453[0m       [35m0.3085[0m      [31m0.7621[0m        0.6237        59.3012
     10      [36m0.9333[0m        [32m0.3363[0m       [35m0.3092[0m      [31m0.7635[0m        0.6228        59.3326
     11      0.9333        [32m0.3288[0m       [35m0.3099[0m      [31m0.7645[0m        [94m0.6218[0m     +  59.2829
     12      0.9333        [32m0.3213[0m       [35m0.3148[0m      [31m0.7654[0m        [94m0.6204[0m     +  59.2862
     13      [36m1.0000[0m        [32m0.3142[0m       [35m0.3219[0m      [31m0.7656[0m        [94m0.6185[0m     +  59.2833
     14      1.0000        [32m0.3088[0m       [35m0.3266[0m      [31m0.7657[0m        [94m0.6166[0m     +  59.2163
     15      1.0000        [32m0.3018[0m       [35m0.3302[0m      [31m0.7667[0m        [94m0.6149[0m     +  59.2271
     16      1.0000        [32m0.2965[0m       [35m0.3314[0m      [31m0.7675[0m        [94m0.6138[0m     +  59.2283
     17      1.0000        [32m0.2900[0m       [35m0.3335[0m      [31m0.7681[0m        [94m0.6130[0m     +  59.2162
     18      1.0000        [32m0.2849[0m       [35m0.3342[0m      [31m0.7688[0m        [94m0.6125[0m     +  59.2161
     19      1.0000        [32m0.2802[0m       [35m0.3345[0m      [31m0.7692[0m        [94m0.6121[0m     +  59.2315
     20      1.0000        [32m0.2756[0m       [35m0.3349[0m      [31m0.7693[0m        [94m0.6121[0m     +  59.2063
     21      1.0000        [32m0.2708[0m       [35m0.3359[0m      [31m0.7697[0m        0.6123        59.2058
     22      1.0000        [32m0.2657[0m       [35m0.3375[0m      0.4375        0.6125        59.1324
     23      1.0000        [32m0.2608[0m       0.3375      0.4363        0.6124        59.1024
     24      1.0000        [32m0.2563[0m       [35m0.3378[0m      0.4349        0.6127        59.0855
     25      1.0000        [32m0.2518[0m       [35m0.3394[0m      0.4336        0.6129        59.0859
     26      1.0000        [32m0.2484[0m       [35m0.3405[0m      0.4337        0.6137        59.0421
     27      1.0000        [32m0.2443[0m       [35m0.3408[0m      0.4336        0.6146        58.9968
     28      1.0000        [32m0.2406[0m       0.3408      0.4336        0.6156        59.0175
     29      1.0000        [32m0.2364[0m       0.3396      0.4342        0.6164        59.0588
     30      1.0000        [32m0.2330[0m       0.3394      0.4341        0.6166        59.0376
     31      1.0000        [32m0.2294[0m       0.3391      0.4339        0.6171        59.0232
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.4701134727541653
Test Accuracy: 0.30451388888888886
Number of samples added in this iteration: 16

Iteration: 3
Number of samples in training set: 32
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.7675[0m        [32m0.4112[0m       [35m0.2837[0m      [31m0.4733[0m        [94m0.6405[0m     +  59.3963
      2      [36m0.7956[0m        [32m0.3822[0m       0.2766      [31m0.4907[0m        0.6415        59.3795
      3      [36m0.8208[0m        [32m0.3681[0m       [35m0.2977[0m      [31m0.5019[0m        [94m0.6216[0m     +  59.3162
      4      [36m0.8289[0m        [32m0.3518[0m       [35m0.3312[0m      [31m0.5103[0m        [94m0.5924[0m     +  59.3981
      5      [36m0.8374[0m        [32m0.3355[0m       [35m0.3477[0m      [31m0.5162[0m        [94m0.5658[0m     +  59.2909
      6      [36m0.8444[0m        [32m0.3259[0m       [35m0.3580[0m      [31m0.5225[0m        [94m0.5486[0m     +  59.2770
      7      0.8415        [32m0.3168[0m       [35m0.3627[0m      [31m0.5350[0m        [94m0.5402[0m     +  59.4234
      8      [36m0.8693[0m        [32m0.3075[0m       [35m0.3701[0m      [31m0.5573[0m        [94m0.5382[0m     +  59.3591
      9      [36m0.9582[0m        [32m0.2967[0m       [35m0.3734[0m      [31m0.5797[0m        0.5411        59.3563
     10      [36m0.9683[0m        [32m0.2884[0m       [35m0.3793[0m      [31m0.5964[0m        0.5451        59.2905
     11      0.9683        [32m0.2823[0m       [35m0.3854[0m      [31m0.6062[0m        0.5449        59.3495
     12      [36m0.9905[0m        [32m0.2770[0m       [35m0.3899[0m      0.6046        0.5383        59.2600
     13      0.9905        [32m0.2702[0m       [35m0.3970[0m      0.5978        [94m0.5276[0m     +  59.2787
     14      0.9905        [32m0.2644[0m       [35m0.4094[0m      0.5887        [94m0.5168[0m     +  59.3091
     15      0.9905        [32m0.2583[0m       [35m0.4188[0m      0.5788        [94m0.5095[0m     +  59.3279
     16      0.9905        [32m0.2546[0m       [35m0.4245[0m      0.5734        [94m0.5061[0m     +  59.4525
     17      [36m1.0000[0m        [32m0.2510[0m       [35m0.4260[0m      0.5752        [94m0.5056[0m     +  59.5316
     18      1.0000        [32m0.2467[0m       0.4236      0.5815        0.5075        59.5308
     19      1.0000        [32m0.2424[0m       0.4174      0.5896        0.5104        59.5155
     20      1.0000        [32m0.2380[0m       0.4146      0.5990        0.5125        59.4973
     21      1.0000        [32m0.2352[0m       0.4158      0.6040        0.5114        59.5102
     22      1.0000        [32m0.2309[0m       0.4217      [31m0.6082[0m        0.5068        59.4258
     23      1.0000        [32m0.2282[0m       [35m0.4309[0m      [31m0.6118[0m        [94m0.5000[0m     +  59.4090
     24      1.0000        [32m0.2245[0m       [35m0.4380[0m      [31m0.6119[0m        [94m0.4931[0m     +  59.3165
     25      1.0000        [32m0.2211[0m       [35m0.4434[0m      [31m0.6141[0m        [94m0.4876[0m     +  59.2887
     26      1.0000        [32m0.2181[0m       [35m0.4493[0m      [31m0.6173[0m        [94m0.4841[0m     +  59.2709
     27      1.0000        [32m0.2158[0m       [35m0.4509[0m      [31m0.6217[0m        [94m0.4828[0m     +  59.3023
     28      1.0000        [32m0.2126[0m       [35m0.4510[0m      [31m0.6277[0m        0.4833        59.3144
     29      1.0000        [32m0.2099[0m       0.4495      [31m0.6318[0m        0.4843        59.3006
     30      1.0000        [32m0.2072[0m       0.4491      [31m0.6339[0m        0.4847        59.2584
     31      1.0000        [32m0.2050[0m       0.4505      0.6338        0.4833        59.2618
     32      1.0000        [32m0.2015[0m       [35m0.4530[0m      0.6308        [94m0.4803[0m     +  59.2882
     33      1.0000        [32m0.1994[0m       [35m0.4583[0m      0.6284        [94m0.4771[0m     +  59.3446
     34      1.0000        [32m0.1972[0m       [35m0.4592[0m      0.6235        [94m0.4747[0m     +  59.3297
     35      1.0000        [32m0.1951[0m       [35m0.4622[0m      0.6239        [94m0.4737[0m     +  59.3356
     36      1.0000        [32m0.1936[0m       0.4616      0.6258        0.4741        59.3512
     37      1.0000        [32m0.1905[0m       0.4606      0.6303        0.4753        59.3037
     38      1.0000        [32m0.1879[0m       0.4582      0.6311        0.4763        59.2824
     39      1.0000        [32m0.1863[0m       0.4602      0.6333        0.4761        59.3162
     40      1.0000        [32m0.1839[0m       [35m0.4632[0m      [31m0.6355[0m        0.4745        59.3153
     41      1.0000        [32m0.1819[0m       [35m0.4675[0m      [31m0.6358[0m        [94m0.4721[0m     +  59.2808
     42      1.0000        [32m0.1801[0m       [35m0.4708[0m      [31m0.6375[0m        [94m0.4704[0m     +  59.3468
     43      1.0000        [32m0.1787[0m       [35m0.4747[0m      [31m0.6388[0m        [94m0.4697[0m     +  59.3206
     44      1.0000        [32m0.1767[0m       [35m0.4755[0m      [31m0.6409[0m        0.4701        59.3114
     45      1.0000        [32m0.1749[0m       0.4750      [31m0.6430[0m        0.4713        59.2872
     46      1.0000        [32m0.1732[0m       0.4736      [31m0.6437[0m        0.4725        59.2890
     47      1.0000        [32m0.1714[0m       0.4717      0.6432        0.4732        59.2870
     48      1.0000        [32m0.1700[0m       0.4745      0.6434        0.4728        59.2968
     49      1.0000        [32m0.1683[0m       [35m0.4769[0m      0.6420        0.4717        59.3260
     50      1.0000        [32m0.1668[0m       [35m0.4783[0m      0.6417        0.4708        59.3995
     51      1.0000        [32m0.1654[0m       0.4783      0.6416        0.4707        59.3146
     52      1.0000        [32m0.1642[0m       0.4776      0.6427        0.4712        59.3402
     53      1.0000        [32m0.1628[0m       0.4774      [31m0.6452[0m        0.4718        59.3484
     54      1.0000        [32m0.1617[0m       0.4764      [31m0.6458[0m        0.4719        59.3125
     55      1.0000        [32m0.1603[0m       0.4780      [31m0.6467[0m        0.4712        59.3072
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.6851277013792432
Test Accuracy: 0.5230902777777777
Number of samples added in this iteration: 32

Iteration: 4
Number of samples in training set: 64
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.8322[0m        [32m0.3087[0m       [35m0.5295[0m      [31m0.5392[0m        [94m0.4813[0m     +  60.0831
      2      [36m0.8383[0m        [32m0.3020[0m       [35m0.5344[0m      [31m0.5714[0m        [94m0.4643[0m     +  59.9387
      3      [36m0.8818[0m        [32m0.2836[0m       0.5238      [31m0.6327[0m        [94m0.4444[0m     +  59.9626
      4      0.8790        [32m0.2689[0m       0.5165      [31m0.6603[0m        [94m0.4390[0m     +  60.0019
      5      0.8800        [32m0.2656[0m       0.5219      [31m0.6657[0m        [94m0.4355[0m     +  59.9951
      6      [36m0.8963[0m        [32m0.2603[0m       [35m0.5347[0m      0.6586        [94m0.4316[0m     +  59.9737
      7      [36m0.9115[0m        [32m0.2504[0m       [35m0.5538[0m      0.6428        0.4328        60.0276
      8      [36m0.9294[0m        [32m0.2437[0m       [35m0.5590[0m      0.6212        0.4375        59.9783
      9      0.9267        [32m0.2416[0m       0.5589      0.6151        0.4371        59.9386
     10      [36m0.9327[0m        [32m0.2383[0m       [35m0.5620[0m      0.6307        [94m0.4297[0m     +  59.9533
     11      [36m0.9385[0m        [32m0.2322[0m       [35m0.5651[0m      0.6513        [94m0.4202[0m     +  60.0191
     12      0.9344        [32m0.2278[0m       0.5628      [31m0.6659[0m        [94m0.4134[0m     +  60.0212
     13      0.9344        [32m0.2256[0m       [35m0.5670[0m      [31m0.6757[0m        [94m0.4096[0m     +  59.9891
     14      0.9344        [32m0.2236[0m       [35m0.5700[0m      0.6737        [94m0.4083[0m     +  60.0148
     15      0.9385        [32m0.2194[0m       0.5656      0.6585        0.4102        60.0035
     16      0.9385        [32m0.2147[0m       0.5613      0.6426        0.4146        59.9592
     17      [36m0.9507[0m        [32m0.2121[0m       0.5571      0.6311        0.4175        59.9995
     18      [36m0.9516[0m        [32m0.2100[0m       0.5559      0.6323        0.4154        60.0860
     19      0.9516        [32m0.2074[0m       0.5623      0.6489        0.4091        59.9782
     20      [36m0.9558[0m        [32m0.2031[0m       0.5682      0.6677        [94m0.4021[0m     +  59.9741
     21      0.9507        [32m0.2005[0m       [35m0.5741[0m      [31m0.6785[0m        [94m0.3969[0m     +  60.0077
     22      [36m0.9652[0m        [32m0.1981[0m       [35m0.5769[0m      [31m0.6823[0m        [94m0.3942[0m     +  59.9695
     23      0.9652        [32m0.1960[0m       0.5764      0.6789        [94m0.3940[0m     +  59.9967
     24      [36m0.9703[0m        [32m0.1933[0m       0.5747      0.6693        0.3956        60.0168
     25      [36m0.9756[0m        [32m0.1903[0m       0.5734      0.6596        0.3974        59.9416
     26      [36m0.9810[0m        [32m0.1886[0m       0.5764      0.6582        0.3968        59.9659
     27      0.9810        [32m0.1868[0m       [35m0.5781[0m      0.6639        [94m0.3928[0m     +  60.0086
     28      0.9810        [32m0.1843[0m       [35m0.5839[0m      0.6766        [94m0.3874[0m     +  60.0541
     29      0.9810        [32m0.1823[0m       [35m0.5866[0m      [31m0.6850[0m        [94m0.3830[0m     +  60.0238
     30      0.9810        [32m0.1805[0m       [35m0.5884[0m      [31m0.6893[0m        [94m0.3808[0m     +  60.0097
     31      [36m0.9944[0m        [32m0.1789[0m       0.5872      0.6860        0.3811        60.0122
     32      0.9944        [32m0.1768[0m       0.5868      0.6803        0.3831        59.9670
     33      0.9944        [32m0.1747[0m       0.5849      0.6765        0.3852        59.9562
     34      0.9944        [32m0.1731[0m       0.5845      0.6756        0.3855        59.9480
     35      0.9944        [32m0.1714[0m       0.5840      0.6777        0.3838        59.9335
     36      0.9944        [32m0.1697[0m       0.5866      0.6838        0.3810        59.9380
     37      0.9944        [32m0.1682[0m       0.5875      [31m0.6894[0m        [94m0.3788[0m     +  59.9346
     38      0.9944        [32m0.1667[0m       0.5877      [31m0.6902[0m        [94m0.3782[0m     +  60.0170
     39      0.9944        [32m0.1651[0m       0.5873      0.6877        0.3791        60.0058
     40      0.9944        [32m0.1636[0m       0.5830      0.6811        0.3811        59.9522
     41      0.9944        [32m0.1617[0m       0.5818      0.6772        0.3824        59.9303
     42      0.9944        [32m0.1605[0m       0.5825      0.6778        0.3820        59.9246
     43      0.9944        [32m0.1587[0m       0.5849      0.6823        0.3799        59.9222
     44      0.9944        [32m0.1578[0m       0.5877      0.6877        [94m0.3778[0m     +  59.9548
     45      0.9944        [32m0.1562[0m       [35m0.5901[0m      [31m0.6904[0m        [94m0.3766[0m     +  59.9868
     46      0.9944        [32m0.1551[0m       0.5894      0.6898        0.3769        59.9920
     47      0.9944        [32m0.1535[0m       0.5865      0.6845        0.3786        60.1335
     48      0.9944        [32m0.1524[0m       0.5847      0.6808        0.3803        59.9711
     49      0.9944        [32m0.1510[0m       0.5842      0.6798        0.3806        59.9482
     50      0.9944        [32m0.1501[0m       0.5866      0.6836        0.3791        59.9619
     51      0.9944        [32m0.1486[0m       0.5891      0.6878        0.3770        59.9671
     52      0.9944        [32m0.1478[0m       [35m0.5910[0m      0.6901        [94m0.3756[0m     +  59.9689
     53      0.9944        [32m0.1467[0m       0.5903      0.6892        [94m0.3755[0m     +  60.2779
     54      [36m1.0000[0m        [32m0.1456[0m       0.5885      0.6858        0.3766        60.0645
     55      1.0000        [32m0.1443[0m       0.5887      0.6838        0.3777        59.9841
     56      1.0000        [32m0.1433[0m       0.5889      0.6828        0.3777        60.0070
     57      1.0000        [32m0.1424[0m       0.5889      0.6845        0.3765        60.0238
     58      1.0000        [32m0.1412[0m       0.5906      0.6878        [94m0.3748[0m     +  59.9506
     59      1.0000        [32m0.1405[0m       [35m0.5920[0m      0.6895        [94m0.3740[0m     +  60.0199
     60      1.0000        [32m0.1394[0m       0.5910      0.6883        0.3744        60.0067
     61      1.0000        [32m0.1386[0m       0.5896      0.6857        0.3756        59.9704
     62      1.0000        [32m0.1377[0m       0.5894      0.6841        0.3763        59.9858
     63      1.0000        [32m0.1370[0m       0.5894      0.6844        0.3761        59.9787
     64      1.0000        [32m0.1358[0m       0.5905      0.6865        0.3751        60.0295
     65      1.0000        [32m0.1348[0m       0.5915      0.6885        0.3742        59.9589
     66      1.0000        [32m0.1343[0m       [35m0.5924[0m      0.6894        [94m0.3738[0m     +  59.9663
     67      1.0000        [32m0.1333[0m       0.5917      0.6868        0.3742        60.0441
     68      1.0000        [32m0.1324[0m       0.5913      0.6861        0.3748        59.9661
     69      1.0000        [32m0.1317[0m       0.5917      0.6861        0.3747        59.9614
     70      1.0000        [32m0.1310[0m       0.5918      0.6862        0.3744        59.9647
     71      1.0000        [32m0.1300[0m       [35m0.5925[0m      0.6872        [94m0.3736[0m     +  59.9413
     72      1.0000        [32m0.1294[0m       [35m0.5934[0m      0.6883        [94m0.3731[0m     +  59.9817
     73      1.0000        [32m0.1287[0m       0.5932      0.6881        0.3731        60.0029
     74      1.0000        [32m0.1279[0m       0.5929      0.6875        0.3735        59.9456
     75      1.0000        [32m0.1272[0m       0.5922      0.6861        0.3737        59.9672
     76      1.0000        [32m0.1266[0m       0.5932      0.6879        [94m0.3730[0m     +  59.9909
     77      1.0000        [32m0.1259[0m       [35m0.5938[0m      0.6887        [94m0.3723[0m     +  60.1347
     78      1.0000        [32m0.1251[0m       [35m0.5943[0m      0.6896        [94m0.3720[0m     +  60.0195
     79      1.0000        [32m0.1244[0m       0.5939      0.6889        0.3721        60.0010
     80      1.0000        [32m0.1238[0m       0.5931      0.6877        0.3723        59.9199
     81      1.0000        [32m0.1230[0m       0.5922      0.6866        0.3725        59.9380
     82      1.0000        [32m0.1224[0m       0.5929      0.6875        0.3722        59.9296
     83      1.0000        [32m0.1217[0m       0.5936      0.6884        [94m0.3718[0m     +  59.9465
     84      1.0000        [32m0.1212[0m       [35m0.5950[0m      0.6903        [94m0.3714[0m     +  60.0292
     85      1.0000        [32m0.1206[0m       0.5943      0.6896        0.3714        59.9967
     86      1.0000        [32m0.1201[0m       0.5939      0.6885        0.3715        59.9507
     87      1.0000        [32m0.1196[0m       0.5943      0.6890        0.3714        59.9637
     88      1.0000        [32m0.1188[0m       0.5948      0.6899        [94m0.3711[0m     +  59.9755
     89      1.0000        [32m0.1184[0m       [35m0.5957[0m      [31m0.6905[0m        [94m0.3708[0m     +  60.0538
     90      1.0000        [32m0.1179[0m       0.5955      0.6900        0.3709        60.0042
     91      1.0000        [32m0.1173[0m       0.5953      0.6897        0.3710        59.9789
     92      1.0000        [32m0.1167[0m       0.5955      0.6900        0.3710        59.9615
     93      1.0000        [32m0.1159[0m       0.5955      [31m0.6907[0m        0.3708        59.9673
     94      1.0000        [32m0.1156[0m       [35m0.5972[0m      [31m0.6934[0m        [94m0.3704[0m     +  59.9644
     95      1.0000        [32m0.1149[0m       0.5969      0.6932        0.3705        60.0176
     96      1.0000        [32m0.1145[0m       0.5972      0.6928        0.3708        59.9677
     97      1.0000        [32m0.1142[0m       0.5970      0.6920        0.3709        59.9655
     98      1.0000        [32m0.1136[0m       [35m0.5977[0m      [31m0.6934[0m        0.3705        59.9704
     99      1.0000        [32m0.1129[0m       0.5977      [31m0.6943[0m        [94m0.3699[0m     +  59.9710
    100      1.0000        [32m0.1124[0m       [35m0.5981[0m      [31m0.6945[0m        [94m0.3698[0m     +  60.0059
Test F1 Score: 0.7481159988636651
Test Accuracy: 0.6524305555555555
Number of samples added in this iteration: 56

Iteration: 5
Number of samples in training set: 120
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.8875[0m        [32m0.2088[0m       [35m0.6358[0m      [31m0.7888[0m        [94m0.3634[0m     +  61.1538
      2      [36m0.9214[0m        0.2422       0.6339      0.7587        [94m0.3485[0m     +  61.1626
      3      [36m0.9564[0m        [32m0.1890[0m       0.5918      0.6761        0.3891        61.1779
      4      0.9050        0.2004       0.5934      0.6698        0.3963        61.1272
      5      0.9080        0.2031       0.6146      0.7146        0.3672        61.1224
      6      0.9466        [32m0.1805[0m       [35m0.6488[0m      0.7673        [94m0.3427[0m     +  61.1268
      7      [36m0.9702[0m        [32m0.1732[0m       [35m0.6691[0m      [31m0.7956[0m        [94m0.3351[0m     +  61.1528
      8      0.9533        0.1817       [35m0.6743[0m      [31m0.8008[0m        [94m0.3325[0m     +  61.1589
      9      0.9533        0.1810       0.6707      0.7921        [94m0.3306[0m     +  61.1962
     10      0.9656        [32m0.1712[0m       0.6562      0.7688        0.3348        61.1659
     11      [36m0.9784[0m        [32m0.1632[0m       0.6401      0.7387        0.3451        61.1414
     12      0.9674        0.1637       0.6347      0.7244        0.3533        61.1245
     13      0.9649        0.1666       0.6352      0.7256        0.3516        61.1405
     14      0.9674        0.1643       0.6450      0.7446        0.3415        61.1202
     15      0.9763        [32m0.1579[0m       0.6580      0.7686        [94m0.3304[0m     +  61.1132
     16      [36m0.9820[0m        [32m0.1535[0m       0.6741      0.7896        [94m0.3232[0m     +  61.1677
     17      [36m0.9868[0m        [32m0.1532[0m       [35m0.6807[0m      0.7997        [94m0.3203[0m     +  61.2043
     18      0.9868        0.1542       [35m0.6816[0m      0.8006        [94m0.3195[0m     +  61.1291
     19      0.9868        [32m0.1526[0m       0.6769      0.7939        0.3207        61.1414
     20      0.9844        [32m0.1485[0m       0.6698      0.7833        0.3247        61.1460
     21      0.9844        [32m0.1454[0m       0.6571      0.7660        0.3305        61.1011
     22      0.9844        [32m0.1447[0m       0.6493      0.7553        0.3345        61.1196
     23      0.9787        0.1452       0.6503      0.7561        0.3338        69.2640
     24      0.9787        [32m0.1439[0m       0.6585      0.7685        0.3288        61.0811
     25      0.9787        [32m0.1415[0m       0.6687      0.7827        0.3226        61.1648
     26      0.9844        [32m0.1393[0m       0.6769      0.7929        [94m0.3179[0m     +  61.1820
     27      0.9844        [32m0.1384[0m       0.6813      0.8006        [94m0.3156[0m     +  61.3389
     28      0.9844        [32m0.1381[0m       [35m0.6818[0m      [31m0.8013[0m        [94m0.3151[0m     +  61.3524
     29      0.9844        [32m0.1372[0m       0.6809      0.7970        0.3161        61.3877
     30      0.9844        [32m0.1351[0m       0.6760      0.7906        0.3187        61.3599
     31      0.9844        [32m0.1335[0m       0.6667      0.7809        0.3220        61.3405
     32      0.9787        [32m0.1327[0m       0.6642      0.7764        0.3242        61.3469
     33      0.9787        [32m0.1324[0m       0.6648      0.7770        0.3239        61.1305
     34      0.9787        [32m0.1316[0m       0.6696      0.7829        0.3215        61.1280
     35      0.9787        [32m0.1299[0m       0.6780      0.7922        0.3184        61.0928
     36      0.9811        [32m0.1287[0m       [35m0.6821[0m      0.7974        0.3161        61.1014
     37      0.9868        [32m0.1281[0m       [35m0.6845[0m      0.8004        0.3151        61.1088
     38      [36m0.9922[0m        [32m0.1274[0m       0.6832      0.7996        0.3155        61.0795
     39      0.9898        [32m0.1266[0m       0.6816      0.7963        0.3172        61.0796
     40      0.9898        [32m0.1253[0m       0.6792      0.7918        0.3196        61.0969
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8214897828155889
Test Accuracy: 0.7295138888888889
Number of samples added in this iteration: 100

Iteration: 6
Number of samples in training set: 220
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9345[0m        [32m0.1655[0m       [35m0.6944[0m      [31m0.8122[0m        [94m0.2978[0m     +  63.3556
      2      [36m0.9447[0m        [32m0.1546[0m       0.6738      0.7775        0.3152        63.2549
      3      [36m0.9575[0m        [32m0.1489[0m       [35m0.7201[0m      [31m0.8328[0m        [94m0.2906[0m     +  63.2346
      4      [36m0.9623[0m        [32m0.1459[0m       0.6988      0.8112        0.3053        63.2706
      5      0.9589        [32m0.1397[0m       0.7021      0.8124        0.3020        63.2247
      6      [36m0.9757[0m        [32m0.1346[0m       0.7123      0.8273        [94m0.2889[0m     +  63.2088
      7      0.9730        [32m0.1334[0m       0.7109      0.8241        0.2889        63.2548
      8      [36m0.9784[0m        [32m0.1297[0m       0.7087      0.8163        0.2939        63.1842
      9      [36m0.9814[0m        [32m0.1273[0m       0.7158      0.8277        [94m0.2855[0m     +  63.1791
     10      0.9802        [32m0.1247[0m       [35m0.7219[0m      [31m0.8345[0m        [94m0.2809[0m     +  63.2389
     11      [36m0.9829[0m        [32m0.1233[0m       0.7170      0.8260        0.2877        63.2338
     12      [36m0.9843[0m        [32m0.1212[0m       0.7172      0.8254        0.2878        63.1813
     13      0.9843        [32m0.1192[0m       0.7207      0.8312        0.2813        63.1586
     14      0.9843        [32m0.1176[0m       0.7208      0.8301        0.2830        63.1852
     15      [36m0.9871[0m        [32m0.1159[0m       0.7160      0.8241        0.2882        63.1688
     16      [36m0.9885[0m        [32m0.1145[0m       0.7191      0.8290        0.2855        63.1847
     17      0.9885        [32m0.1130[0m       [35m0.7234[0m      0.8333        0.2824        63.1966
     18      0.9885        [32m0.1119[0m       0.7196      0.8296        0.2852        63.1787
     19      [36m0.9913[0m        [32m0.1106[0m       0.7208      0.8297        0.2853        63.2043
     20      0.9913        [32m0.1094[0m       0.7224      0.8323        0.2819        63.1652
     21      0.9913        [32m0.1084[0m       0.7201      0.8305        0.2840        63.1575
     22      0.9913        [32m0.1071[0m       0.7172      0.8273        0.2873        63.1730
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8517519172294407
Test Accuracy: 0.7597222222222222
Number of samples added in this iteration: 176

Iteration: 7
Number of samples in training set: 396
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9482[0m        [32m0.1640[0m       [35m0.7167[0m      [31m0.8314[0m        [94m0.2859[0m     +  67.1238
      2      [36m0.9528[0m        [32m0.1589[0m       [35m0.7231[0m      [31m0.8433[0m        [94m0.2850[0m     +  66.9489
      3      [36m0.9585[0m        [32m0.1508[0m       [35m0.7314[0m      0.8414        [94m0.2660[0m     +  66.8725
      4      0.9580        [32m0.1423[0m       [35m0.7326[0m      0.8354        [94m0.2588[0m     +  66.8953
      5      [36m0.9661[0m        [32m0.1382[0m       [35m0.7401[0m      [31m0.8503[0m        [94m0.2559[0m     +  66.8945
      6      0.9598        [32m0.1352[0m       0.7328      0.8408        0.2655        66.8774
      7      [36m0.9663[0m        [32m0.1316[0m       [35m0.7427[0m      [31m0.8513[0m        0.2579        66.8357
      8      [36m0.9736[0m        [32m0.1283[0m       0.7349      0.8390        0.2570        66.8299
      9      0.9727        [32m0.1251[0m       [35m0.7453[0m      0.8493        [94m0.2516[0m     +  66.8148
     10      [36m0.9784[0m        [32m0.1225[0m       0.7389      0.8437        0.2562        66.8433
     11      [36m0.9788[0m        [32m0.1201[0m       0.7415      0.8488        0.2566        66.8247
     12      [36m0.9834[0m        [32m0.1178[0m       0.7333      0.8400        0.2592        66.8002
     13      [36m0.9844[0m        [32m0.1154[0m       0.7391      0.8453        0.2549        66.8106
     14      [36m0.9851[0m        [32m0.1135[0m       0.7330      0.8387        0.2578        66.8090
     15      [36m0.9865[0m        [32m0.1115[0m       0.7365      0.8441        0.2569        66.8261
     16      [36m0.9893[0m        [32m0.1098[0m       0.7280      0.8356        0.2620        66.8297
     17      0.9893        [32m0.1079[0m       0.7333      0.8414        0.2584        66.8669
     18      0.9893        [32m0.1064[0m       0.7281      0.8350        0.2618        66.8171
     19      0.9893        [32m0.1050[0m       0.7314      0.8399        0.2605        66.8722
     20      0.9893        [32m0.1036[0m       0.7271      0.8347        0.2636        66.8335
     21      [36m0.9914[0m        [32m0.1023[0m       0.7280      0.8362        0.2631        66.8503
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8622825517760875
Test Accuracy: 0.7689236111111111
Number of samples added in this iteration: 316

Iteration: 8
Number of samples in training set: 712
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9444[0m        [32m0.1592[0m       [35m0.7193[0m      [31m0.8214[0m        [94m0.2644[0m     +  73.4559
      2      [36m0.9601[0m        [32m0.1415[0m       [35m0.7483[0m      [31m0.8488[0m        [94m0.2492[0m     +  73.4387
      3      [36m0.9627[0m        [32m0.1369[0m       0.7330      0.8292        0.2576        73.3578
      4      [36m0.9684[0m        [32m0.1317[0m       0.7439      0.8433        [94m0.2485[0m     +  73.3642
      5      [36m0.9714[0m        [32m0.1289[0m       0.7307      0.8292        0.2561        73.3827
      6      [36m0.9718[0m        [32m0.1253[0m       0.7472      0.8454        [94m0.2472[0m     +  73.3086
      7      [36m0.9733[0m        [32m0.1229[0m       0.7375      0.8316        0.2498        73.3508
      8      [36m0.9745[0m        [32m0.1203[0m       [35m0.7488[0m      0.8447        [94m0.2466[0m     +  73.3350
      9      [36m0.9774[0m        [32m0.1180[0m       0.7398      0.8339        0.2518        73.3826
     10      0.9760        [32m0.1158[0m       [35m0.7510[0m      0.8467        [94m0.2443[0m     +  73.3492
     11      [36m0.9793[0m        [32m0.1141[0m       0.7405      0.8319        0.2519        73.3466
     12      0.9783        [32m0.1121[0m       [35m0.7533[0m      [31m0.8488[0m        0.2444        73.3373
     13      [36m0.9819[0m        [32m0.1106[0m       0.7382      0.8289        0.2540        73.3332
     14      0.9810        [32m0.1084[0m       [35m0.7545[0m      [31m0.8501[0m        [94m0.2440[0m     +  73.3579
     15      [36m0.9836[0m        [32m0.1074[0m       0.7363      0.8265        0.2557        73.4755
     16      [36m0.9838[0m        [32m0.1051[0m       0.7543      0.8500        0.2445        73.3141
     17      [36m0.9860[0m        [32m0.1042[0m       0.7385      0.8286        0.2556        73.2959
     18      0.9856        [32m0.1019[0m       0.7531      0.8477        0.2444        73.3217
     19      0.9860        [32m0.1010[0m       0.7394      0.8294        0.2548        73.3251
     20      [36m0.9872[0m        [32m0.0990[0m       0.7521      0.8459        0.2455        73.3077
     21      0.9872        [32m0.0979[0m       0.7406      0.8316        0.2536        73.3122
     22      0.9872        [32m0.0963[0m       0.7495      0.8428        0.2466        73.3227
     23      0.9872        [32m0.0951[0m       0.7422      0.8343        0.2526        73.3017
     24      0.9872        [32m0.0938[0m       0.7472      0.8405        0.2481        73.3084
     25      0.9872        [32m0.0926[0m       0.7455      0.8368        0.2513        73.3204
     26      [36m0.9878[0m        [32m0.0916[0m       0.7469      0.8397        0.2499        73.3212
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8671656973143076
Test Accuracy: 0.7880208333333333
Number of samples added in this iteration: 564

Iteration: 9
Number of samples in training set: 1276
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9594[0m        [32m0.1259[0m       [35m0.7587[0m      [31m0.8505[0m        [94m0.2375[0m     +  85.0326
      2      [36m0.9640[0m        [32m0.1159[0m       0.7493      0.8354        [94m0.2348[0m     +  84.9810
      3      [36m0.9702[0m        [32m0.1107[0m       0.7450      0.8277        0.2376        84.9656
      4      [36m0.9737[0m        [32m0.1068[0m       0.7443      0.8223        0.2383        84.9170
      5      [36m0.9770[0m        [32m0.1033[0m       0.7464      0.8266        0.2369        84.9924
      6      [36m0.9795[0m        [32m0.1000[0m       0.7453      0.8204        0.2403        84.9963
      7      [36m0.9820[0m        [32m0.0974[0m       0.7481      0.8243        0.2374        84.9819
      8      [36m0.9826[0m        [32m0.0951[0m       0.7467      0.8251        0.2386        84.9632
      9      [36m0.9837[0m        [32m0.0929[0m       0.7502      0.8285        0.2359        84.9843
     10      [36m0.9844[0m        [32m0.0915[0m       0.7500      0.8283        0.2357        84.9757
     11      [36m0.9855[0m        [32m0.0901[0m       0.7528      0.8330        [94m0.2323[0m     +  84.9545
     12      [36m0.9856[0m        [32m0.0892[0m       0.7547      0.8371        [94m0.2293[0m     +  84.9495
     13      [36m0.9878[0m        [32m0.0886[0m       [35m0.7597[0m      0.8446        [94m0.2239[0m     +  84.9793
     14      0.9877        [32m0.0885[0m       [35m0.7696[0m      [31m0.8555[0m        [94m0.2178[0m     +  85.0599
     15      0.9859        0.0889       [35m0.7726[0m      [31m0.8648[0m        [94m0.2162[0m     +  84.9475
     16      0.9831        0.0899       [35m0.7755[0m      [31m0.8739[0m        0.2296        84.9662
     17      0.9761        0.0940       0.7700      0.8738        0.2449        84.9736
     18      0.9641        0.1027       [35m0.7839[0m      0.8699        [94m0.2005[0m     +  84.8807
     19      0.9724        0.1030       0.7616      0.8225        0.2213        84.9493
     20      0.9845        0.0899       0.7729      0.8592        0.2170        84.9119
     21      [36m0.9898[0m        [32m0.0795[0m       0.7837      0.8629        [94m0.1978[0m     +  84.9539
     22      0.9897        [32m0.0778[0m       [35m0.7877[0m      0.8607        0.2030        84.9623
     23      [36m0.9917[0m        [32m0.0752[0m       0.7783      0.8614        0.2032        84.8967
     24      0.9913        [32m0.0743[0m       0.7816      0.8593        0.2045        84.8817
     25      [36m0.9922[0m        [32m0.0727[0m       0.7788      0.8598        0.2040        84.9004
     26      [36m0.9927[0m        [32m0.0719[0m       0.7788      0.8584        0.2051        84.8737
     27      0.9927        [32m0.0707[0m       0.7767      0.8568        0.2047        84.8705
     28      0.9927        [32m0.0697[0m       0.7771      0.8577        0.2055        84.8905
     29      [36m0.9940[0m        [32m0.0686[0m       0.7780      0.8571        0.2056        84.8851
     30      [36m0.9948[0m        [32m0.0677[0m       0.7767      0.8563        0.2067        87.9439
     31      0.9948        [32m0.0667[0m       0.7767      0.8566        0.2069        84.9007
     32      0.9948        [32m0.0658[0m       0.7764      0.8558        0.2079        84.9008
     33      0.9948        [32m0.0650[0m       0.7755      0.8546        0.2084        84.8728
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8902685526596
Test Accuracy: 0.8140625
Number of samples added in this iteration: 1000

Iteration: 10
Number of samples in training set: 2276
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9682[0m        [32m0.0943[0m       [35m0.7759[0m      [31m0.8606[0m        [94m0.2079[0m     +  105.6829
      2      [36m0.9714[0m        [32m0.0917[0m       0.7747      0.8174        0.2199        105.5838
      3      [36m0.9759[0m        [32m0.0850[0m       [35m0.7849[0m      0.8434        [94m0.2005[0m     +  105.5239
      4      [36m0.9787[0m        [32m0.0826[0m       0.7825      0.8342        0.2036        105.5840
      5      [36m0.9807[0m        [32m0.0795[0m       0.7819      0.8350        0.2019        105.5466
      6      [36m0.9823[0m        [32m0.0769[0m       0.7814      0.8361        0.2031        105.5418
      7      [36m0.9848[0m        [32m0.0742[0m       [35m0.7896[0m      0.8461        [94m0.1990[0m     +  105.5627
      8      [36m0.9851[0m        [32m0.0721[0m       [35m0.7958[0m      [31m0.8613[0m        [94m0.1916[0m     +  105.6183
      9      [36m0.9874[0m        [32m0.0699[0m       [35m0.8000[0m      [31m0.8766[0m        [94m0.1879[0m     +  105.5852
     10      [36m0.9882[0m        [32m0.0679[0m       0.7964      [31m0.8813[0m        0.1917        105.5964
     11      [36m0.9900[0m        [32m0.0656[0m       [35m0.8003[0m      0.8813        0.1913        105.5606
     12      [36m0.9920[0m        [32m0.0629[0m       [35m0.8101[0m      [31m0.8822[0m        [94m0.1872[0m     +  105.5461
     13      [36m0.9947[0m        [32m0.0605[0m       0.8036      0.8730        0.1947        105.6325
     14      0.9937        [32m0.0595[0m       0.7974      0.8637        0.2010        105.5545
     15      0.9909        0.0608       0.8045      0.8758        0.1938        105.5416
     16      0.9895        0.0608       0.8026      [31m0.8856[0m        0.1945        105.5312
     17      0.9936        [32m0.0577[0m       0.8057      [31m0.8857[0m        0.1932        105.5851
     18      0.9937        [32m0.0558[0m       [35m0.8260[0m      [31m0.8899[0m        [94m0.1810[0m     +  105.5607
     19      [36m0.9960[0m        [32m0.0535[0m       0.8128      0.8845        0.1857        105.6297
     20      [36m0.9967[0m        [32m0.0519[0m       0.8056      0.8835        0.1896        105.5508
     21      [36m0.9972[0m        [32m0.0508[0m       0.8063      0.8820        0.1882        105.5718
     22      [36m0.9973[0m        [32m0.0498[0m       0.8099      0.8854        0.1883        105.5724
     23      0.9973        [32m0.0489[0m       0.8090      0.8866        0.1896        105.5437
     24      0.9973        [32m0.0481[0m       0.8123      0.8877        0.1886        105.5787
     25      0.9973        [32m0.0474[0m       0.8120      0.8871        0.1897        105.5763
     26      [36m0.9975[0m        [32m0.0466[0m       0.8099      0.8861        0.1909        105.5760
     27      [36m0.9980[0m        [32m0.0458[0m       0.8108      0.8857        0.1913        105.5836
     28      0.9978        [32m0.0451[0m       0.8108      0.8843        0.1916        105.5805
     29      [36m0.9981[0m        [32m0.0445[0m       0.8104      0.8842        0.1922        105.6073
     30      0.9981        [32m0.0438[0m       0.8101      0.8832        0.1921        105.5586
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8924065097807183
Test Accuracy: 0.8196180555555556
Number of samples added in this iteration: 1780

Iteration: 11
Number of samples in training set: 4056
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9782[0m        [32m0.0691[0m       [35m0.8146[0m      [31m0.8606[0m        [94m0.1962[0m     +  142.3341
      2      [36m0.9815[0m        [32m0.0635[0m       0.8054      0.8551        0.1975        142.3458
      3      [36m0.9846[0m        [32m0.0600[0m       [35m0.8257[0m      [31m0.8773[0m        [94m0.1826[0m     +  142.3029
      4      0.9842        [32m0.0560[0m       0.8245      [31m0.8877[0m        [94m0.1811[0m     +  142.4038
      5      [36m0.9866[0m        [32m0.0540[0m       0.8196      [31m0.8948[0m        0.1938        142.3657
      6      0.9853        0.0549       0.8210      0.8931        0.1850        142.3497
      7      [36m0.9867[0m        [32m0.0536[0m       0.8182      0.8655        0.1908        142.3253
      8      [36m0.9879[0m        [32m0.0502[0m       0.8227      0.8734        0.1851        142.3451
      9      [36m0.9905[0m        [32m0.0477[0m       0.8156      0.8648        0.1915        142.3252
     10      [36m0.9910[0m        [32m0.0465[0m       0.8078      0.8583        0.1969        142.3453
     11      [36m0.9920[0m        [32m0.0451[0m       0.8153      0.8682        0.1917        142.4918
     12      [36m0.9928[0m        [32m0.0438[0m       0.7997      0.8512        0.2085        142.2834
     13      [36m0.9930[0m        [32m0.0428[0m       0.7849      0.8267        0.2326        142.3634
     14      0.9917        [32m0.0422[0m       0.7686      0.8072        0.2515        142.3530
     15      0.9889        0.0454       0.8026      0.8468        0.2101        142.3438
     16      0.9825        0.0517       0.7939      0.8211        0.2149        142.3274
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8832379981855292
Test Accuracy: 0.8175347222222222
Number of samples added in this iteration: 3164

Iteration: 12
Number of samples in training set: 7220
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9780[0m        [32m0.0603[0m       [35m0.8092[0m      [31m0.8660[0m        [94m0.1876[0m     +  207.7197
      2      [36m0.9798[0m        [32m0.0559[0m       [35m0.8276[0m      [31m0.8815[0m        [94m0.1734[0m     +  207.6773
      3      0.9795        [32m0.0552[0m       [35m0.8297[0m      [31m0.8867[0m        [94m0.1695[0m     +  207.6803
      4      0.9791        [32m0.0542[0m       [35m0.8347[0m      [31m0.8997[0m        0.1702        207.6947
      5      [36m0.9840[0m        [32m0.0486[0m       [35m0.8398[0m      [31m0.9004[0m        0.1699        207.6447
      6      [36m0.9863[0m        [32m0.0451[0m       0.8349      0.8881        [94m0.1689[0m     +  207.6435
      7      [36m0.9877[0m        [32m0.0426[0m       0.8316      0.8818        [94m0.1687[0m     +  207.6722
      8      [36m0.9885[0m        [32m0.0408[0m       0.8255      0.8751        0.1758        207.6647
      9      [36m0.9900[0m        [32m0.0392[0m       0.8243      0.8723        0.1783        207.6370
     10      [36m0.9908[0m        [32m0.0381[0m       0.8215      0.8694        0.1806        207.6523
     11      [36m0.9910[0m        [32m0.0371[0m       0.8075      0.8556        0.2019        207.6404
     12      0.9887        0.0378       0.7988      0.8435        0.2163        207.6632
     13      0.9850        0.0407       0.7889      0.8179        0.2217        207.6349
     14      0.9833        0.0433       0.8210      0.8742        0.1828        207.6903
     15      0.9858        0.0388       0.8314      [31m0.9011[0m        0.1819        207.6223
     16      [36m0.9914[0m        [32m0.0314[0m       0.8269      0.8983        0.1826        207.6604
     17      [36m0.9947[0m        [32m0.0277[0m       0.8300      0.8985        0.1869        207.6752
     18      [36m0.9962[0m        [32m0.0254[0m       0.8278      0.8959        0.1859        207.6486
     19      [36m0.9969[0m        [32m0.0238[0m       0.8295      0.8964        0.1905        207.6771
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.9147874038670848
Test Accuracy: 0.8611111111111112
Number of samples added in this iteration: 5624

Iteration: 13
Number of samples in training set: 12844
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9832[0m        [32m0.0420[0m       [35m0.8299[0m      [31m0.8982[0m        [94m0.1771[0m     +  323.8745
      2      [36m0.9855[0m        [32m0.0384[0m       [35m0.8431[0m      [31m0.9053[0m        [94m0.1657[0m     +  323.8735
      3      [36m0.9880[0m        [32m0.0340[0m       0.8366      0.8952        [94m0.1642[0m     +  323.8345
      4      [36m0.9897[0m        [32m0.0309[0m       0.8335      0.8948        0.1755        323.8240
      5      [36m0.9911[0m        [32m0.0287[0m       0.8411      0.9002        0.1712        323.8020
      6      [36m0.9914[0m        [32m0.0272[0m       0.8358      0.9014        0.1912        323.7472
      7      [36m0.9924[0m        [32m0.0253[0m       0.8333      0.9031        0.2081        323.7788
      8      [36m0.9928[0m        [32m0.0245[0m       0.8302      0.9008        0.2190        323.7836
      9      [36m0.9930[0m        [32m0.0229[0m       0.8307      0.8994        0.2138        323.7675
     10      [36m0.9935[0m        [32m0.0216[0m       0.8302      0.8991        0.2013        323.7515
     11      [36m0.9947[0m        [32m0.0200[0m       0.8260      0.8899        0.2036        323.7850
     12      [36m0.9950[0m        [32m0.0188[0m       [35m0.8443[0m      0.9028        0.1895        326.2486
     13      [36m0.9953[0m        [32m0.0184[0m       0.8443      [31m0.9075[0m        0.1996        323.8088
     14      [36m0.9961[0m        [32m0.0167[0m       0.8340      0.9052        0.2123        323.7834
     15      0.9953        0.0171       0.8399      0.9067        0.2084        323.7938
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.9267232051893562
Test Accuracy: 0.8769097222222222
Number of samples added in this iteration: 10000

Iteration: 14
Number of samples in training set: 22844
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9852[0m        [32m0.0327[0m       [35m0.8333[0m      [31m0.8933[0m        [94m0.1725[0m     +  530.2225
      2      [36m0.9867[0m        [32m0.0303[0m       0.8283      0.8875        0.1831        530.2274
      3      [36m0.9893[0m        [32m0.0256[0m       [35m0.8432[0m      [31m0.8950[0m        [94m0.1713[0m     +  530.1932
      4      [36m0.9905[0m        [32m0.0233[0m       0.8394      0.8943        0.1778        530.2676
      5      [36m0.9915[0m        [32m0.0213[0m       0.8356      0.8885        0.1815        530.1599
      6      [36m0.9920[0m        [32m0.0201[0m       0.8358      0.8894        0.1798        530.1176
      7      [36m0.9929[0m        [32m0.0188[0m       0.8182      0.8764        0.2056        530.1609
      8      0.9926        [32m0.0179[0m       0.8219      0.8799        0.2011        532.3887
      9      0.9924        0.0180       0.8389      0.8910        0.1918        530.1943
     10      [36m0.9939[0m        [32m0.0159[0m       0.8278      0.8905        0.2007        530.2456
     11      [36m0.9950[0m        [32m0.0141[0m       0.8394      [31m0.9000[0m        0.1946        530.2755
     12      [36m0.9959[0m        [32m0.0125[0m       0.8405      0.8996        0.1949        530.1805
     13      0.9955        0.0127       0.8290      0.8931        0.2111        530.1670
     14      [36m0.9963[0m        [32m0.0110[0m       0.8314      0.8916        0.2110        530.1650
     15      [36m0.9971[0m        [32m0.0098[0m       0.8236      0.8837        0.2254        530.1410
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8921923220182477
Test Accuracy: 0.8416666666666667
Welcome to the final iteration my friend!
Final iteration: Added the remaining 4034 samples to the training set.

Random sampling loop ended successfully!
F1 Test Data: [0.43270698259611784, 0.4701134727541653, 0.6851277013792432, 0.7481159988636651, 0.8214897828155889, 0.8517519172294407, 0.8622825517760875, 0.8671656973143076, 0.8902685526596, 0.8924065097807183, 0.8832379981855292, 0.9147874038670848, 0.9267232051893562, 0.8921923220182477]
Accuracy Test Data: [0.24774305555555556, 0.30451388888888886, 0.5230902777777777, 0.6524305555555555, 0.7295138888888889, 0.7597222222222222, 0.7689236111111111, 0.7880208333333333, 0.8140625, 0.8196180555555556, 0.8175347222222222, 0.8611111111111112, 0.8769097222222222, 0.8416666666666667]
Performance Test Data: [[0.24774305555555556, 0.30451388888888886, 0.5230902777777777, 0.6524305555555555, 0.7295138888888889, 0.7597222222222222, 0.7689236111111111, 0.7880208333333333, 0.8140625, 0.8196180555555556, 0.8175347222222222, 0.8611111111111112, 0.8769097222222222, 0.8416666666666667], [0.43270698259611784, 0.4701134727541653, 0.6851277013792432, 0.7481159988636651, 0.8214897828155889, 0.8517519172294407, 0.8622825517760875, 0.8671656973143076, 0.8902685526596, 0.8924065097807183, 0.8832379981855292, 0.9147874038670848, 0.9267232051893562, 0.8921923220182477]]
=== JOB_STATISTICS ===
=== current date     : Wed 11 Sep 2024 08:16:46 AM CEST
= Job-ID             : 889597 on tinygpu
= Job-Name           : MinicondaName
= Job-Command        : /home/woody/iwfa/iwfa044h/runner1.sh
= Initial workdir    : /home/woody/iwfa/iwfa044h
= Queue/Partition    : a100
= Slurm account      : iwfa with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 13:46:41
= Total RAM usage    : 51.7 GiB of requested  GiB (%)   
= Node list          : tg091
= Subm/Elig/Start/End: 2024-09-10T17:17:52 / 2024-09-10T17:17:52 / 2024-09-10T18:30:05 / 2024-09-11T08:16:46
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           56.6G   104.9G   209.7G        N/A     150K     500K   1,000K        N/A    
    /home/woody        819.1G  1000.0G  1500.0G        N/A   1,810K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:01:00.0, 3513810, 97 %, 16 %, 39936 MiB, 49494283 ms
