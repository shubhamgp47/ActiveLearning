{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import modAL\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image \n",
    "from plotly import graph_objects, subplots\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import one_hot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Scorer function and training setup imports\n",
    "from skorch.callbacks import EpochScoring\n",
    "from sklearn.metrics import f1_score, make_scorer, accuracy_score\n",
    "from skorch.helper import predefined_split\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.classifier import NeuralNetClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = os.path.abspath(r\"/home/woody/iwfa/iwfa044h/CleanLab_Test/1_all_winding_images/\")\n",
    "len(os.listdir(image_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dir = os.path.abspath(r\"/home/woody/iwfa/iwfa045h/labelling/1_all_winding_images/\")\n",
    "\n",
    "df_dir = os.path.abspath(r\"/home/woody/iwfa/iwfa044h/CleanLab_Test/2_labels/Updated_Labels/\")\n",
    "train_df = pd.read_csv(df_dir + \"/train_v2024-03-18.csv\")\n",
    "val_df = pd.read_csv(df_dir + \"/validation_v2024-03-18.csv\")\n",
    "test_df = pd.read_csv(df_dir + \"/test_v2024-03-18.csv\")\n",
    "\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_frame = [train_df, test_df, val_df]                       # List of data frames\n",
    "multiclass_labels = []\n",
    "\n",
    "for x in range(len(list_data_frame)):                               # Iterating to the list of data frames\n",
    "    labels = []\n",
    "    for y in tqdm(range(list_data_frame[x].shape[0])):              # Iterating to all the images of selected data frame and assigning labels\n",
    "        if list_data_frame[x]['multi-label_double_winding'][y] == 0:\n",
    "        \n",
    "            if list_data_frame[x]['multi-label_gap'][y] == 0:\n",
    "                \n",
    "                if list_data_frame[x]['multi-label_crossing'][y] == 0:\n",
    "                    labels.append('0')\n",
    "                else:\n",
    "                    labels.append('1')\n",
    "\n",
    "            else:\n",
    "                if list_data_frame[x]['multi-label_crossing'][y] == 0:\n",
    "                    labels.append('2')\n",
    "                else:\n",
    "                    labels.append('3')\n",
    "        \n",
    "        else:\n",
    "            if list_data_frame[x]['multi-label_gap'][y] == 0:\n",
    "\n",
    "                if list_data_frame[x]['multi-label_crossing'][y] == 0:\n",
    "                    labels.append('4')\n",
    "                else:\n",
    "                    labels.append('5')\n",
    "\n",
    "            else:\n",
    "                if list_data_frame[x]['multi-label_crossing'][y] == 0:\n",
    "                    labels.append('6')\n",
    "                else:\n",
    "                    labels.append('7')\n",
    "    multiclass_labels.append(labels)                                # Collecting list of train, val and test labels in another list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(multiclass_labels))\n",
    "# MultiClass training data frame \n",
    "multiclass_train_df = train_df.assign(multiclass = multiclass_labels[0])\n",
    "multiclass_train_df = multiclass_train_df[['image', 'multiclass']].dropna()\n",
    "\n",
    "# MultiClass test data frame \n",
    "multiclass_test_df = test_df.assign(multiclass = multiclass_labels[1])\n",
    "multiclass_test_df = multiclass_test_df[['image', 'multiclass']].dropna()\n",
    "\n",
    "# MultiClass validation data frame \n",
    "multiclass_val_df = val_df.assign(multiclass = multiclass_labels[2])\n",
    "multiclass_val_df = multiclass_val_df[['image', 'multiclass']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_train_df.shape, multiclass_test_df.shape, multiclass_val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# epochs = 100\n",
    "batch_size =8\n",
    "# layer_freeze= 69\n",
    "\n",
    "# learning_rate = 0.1\n",
    "# momentum_term = 0.2280969903050278\n",
    "\n",
    "# dropout_rate = 0.49809628309801696\n",
    "optimizer = 'SGD'\n",
    "\n",
    "\n",
    "# Initialising Active learner and query strategy using modAL\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import margin_sampling\n",
    "\n",
    "query_strategy = margin_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the nearest multiple of base for a given number\n",
    "def batch(number, base):                                   \n",
    "    return base * round(number / base)\n",
    "\n",
    "# Directly set the training size based on the size of the dataset and batch size\n",
    "# Calculate initial training size as 10% of the dataset\n",
    "train_size = batch(int(0.1 * len(multiclass_train_df)), batch_size)\n",
    "\n",
    "print(f\"Calculated train_size: {train_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shuffling the data frame \n",
    "multiclass_train_df = multiclass_train_df.sample(frac = 1, random_state = 1234)\n",
    "\n",
    "# Intial set must contain sample from each class\n",
    "initial_df = multiclass_train_df.groupby('multiclass').head(1).head(train_size)\n",
    "\n",
    "# Assigning remaining samples as the pool of data\n",
    "pool_df = multiclass_train_df.drop(initial_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define the target size for resizing images\n",
    "target_size = (224, 224)\n",
    "\n",
    "# Initialize empty lists for storing images and labels\n",
    "initial_train_image = []\n",
    "\n",
    "# Define a transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "# Load and preprocess images for initial training set\n",
    "for i in tqdm(range(initial_df.shape[0])):\n",
    "    image_path = os.path.join(image_dir, initial_df[\"image\"].iloc[i])\n",
    "    img = Image.open(image_path).convert(\"RGB\")  # Ensure the image is in RGB format\n",
    "    img = transform(img)\n",
    "    initial_train_image.append(img)\n",
    "\n",
    "# Convert the list of images to a PyTorch tensor and then permute\n",
    "X_train_initial = torch.stack(initial_train_image).permute(0, 2, 3, 1)  # Move the channel to the last dimension\n",
    "\n",
    "# Ensure the shape is [batch_size, 224, 224, 3]\n",
    "print(f\"Initial training data shape: {X_train_initial.shape}\")\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_multiclass = initial_df['multiclass'].astype(np.int64).to_numpy()  # Convert to NumPy array\n",
    "num_classes = len(np.unique(y_multiclass))\n",
    "y_train_initial = F.one_hot(torch.tensor(y_multiclass), num_classes=num_classes).float()\n",
    "\n",
    "# Print the shape of the resulting tensors\n",
    "print(f\"X_train_initial shape: {X_train_initial.shape}, y_train_initial shape: {y_train_initial.shape}\")\n",
    "print(y_train_initial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize empty lists for storing images and labels\n",
    "pool_train_image = []\n",
    "\n",
    "# Load and preprocess images in batches\n",
    "for batch_start in range(0, pool_df.shape[0], batch_size):\n",
    "    batch_end = min(batch_start + batch_size, pool_df.shape[0])\n",
    "    batch_images = []\n",
    "\n",
    "    for i in range(batch_start, batch_end):\n",
    "        image_path = os.path.join(image_dir, pool_df[\"image\"].iloc[i])\n",
    "        img = Image.open(image_path).convert(\"RGB\")  # Ensures the image is in RGB format\n",
    "        img = transform(img)\n",
    "        img_rgb = img.permute(1, 2, 0)  # Change shape from [C, H, W] to [H, W, C]\n",
    "        batch_images.append(img_rgb)\n",
    "\n",
    "    # Convert the list of images to a PyTorch tensor\n",
    "    X_batch = torch.stack(batch_images)\n",
    "    pool_train_image.append(X_batch)\n",
    "\n",
    "# Concatenate all batches to create the final X_pool tensor\n",
    "X_pool = torch.cat(pool_train_image)\n",
    "\n",
    "# Convert multiclass labels to one-hot encoding and ensure proper shape\n",
    "y_multiclass = pool_df['multiclass'].astype(np.int64).to_numpy()\n",
    "num_classes = len(np.unique(y_multiclass))\n",
    "y_pool_initial = F.one_hot(torch.tensor(y_multiclass), num_classes=num_classes).float()\n",
    "\n",
    "# Ensure the tensor is in the shape [n_samples, n_classes]\n",
    "y_pool_initial = y_pool_initial.squeeze()  # This removes any singleton dimensions if present\n",
    "\n",
    "\n",
    "# Print the shape of the resulting tensors\n",
    "print(f\"X_pool shape: {X_pool.shape}, y_pool_initial shape: {y_pool_initial.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize empty lists for storing images and labels\n",
    "val_images = []\n",
    "\n",
    "# Load and preprocess images\n",
    "for i in tqdm(range(multiclass_val_df.shape[0])):\n",
    "    image_path = os.path.join(image_dir, multiclass_val_df[\"image\"].iloc[i])\n",
    "    img = Image.open(image_path).convert(\"RGB\")  # Ensure the image is in RGB format\n",
    "    img = transform(img)\n",
    "    img_rgb = img.permute(1, 2, 0)  # Change shape from [C, H, W] to [H, W, C]\n",
    "    val_images.append(img_rgb)\n",
    "\n",
    "# Convert the list of images to a PyTorch tensor\n",
    "X_val = torch.stack(val_images)\n",
    "\n",
    "# Convert multiclass labels to a numpy array\n",
    "y_multiclass = multiclass_val_df['multiclass'].astype(np.int64).to_numpy()\n",
    "num_classes = len(np.unique(y_multiclass))\n",
    "y_val = F.one_hot(torch.tensor(y_multiclass), num_classes=num_classes).float()\n",
    "\n",
    "# Print the shape of the resulting tensors\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize empty lists for storing images and labels\n",
    "test_images = []\n",
    "\n",
    "# Load and preprocess images\n",
    "for i in tqdm(range(multiclass_test_df.shape[0])):\n",
    "    image_path = os.path.join(image_dir, multiclass_test_df[\"image\"].iloc[i])\n",
    "    img = Image.open(image_path).convert(\"RGB\")  # Ensure the image is in RGB format\n",
    "    img = transform(img)\n",
    "    img_rgb = img.permute(1, 2, 0)  # Change shape from [C, H, W] to [H, W, C]\n",
    "    test_images.append(img_rgb)\n",
    "\n",
    "# Convert the list of images to a PyTorch tensor\n",
    "X_test = torch.stack(test_images)\n",
    "\n",
    "# Convert multiclass labels to a numpy array\n",
    "y_multiclass = multiclass_test_df['multiclass'].astype(np.int64).to_numpy()\n",
    "num_classes = len(np.unique(y_multiclass))\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "y_test = F.one_hot(torch.tensor(y_multiclass), num_classes=num_classes).float()\n",
    "\n",
    "# Print the shape of the resulting tensors\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import time\n",
    "from skorch.callbacks import Callback\n",
    "\n",
    "class CSVLogger(Callback):\n",
    "    \"\"\"Log epoch data to a CSV file.\"\"\"\n",
    "    def __init__(self, filename, fieldnames):\n",
    "        self.filename = filename\n",
    "        self.fieldnames = fieldnames\n",
    "        self.file_exist = os.path.exists(filename)  # Check if file already exists\n",
    "\n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        logs = {key: net.history[-1, key] for key in self.fieldnames}\n",
    "\n",
    "        if not self.file_exist:\n",
    "            # Write headers to CSV\n",
    "            with open(self.filename, mode='w', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=self.fieldnames)\n",
    "                writer.writeheader()\n",
    "            self.file_exist = True\n",
    "        \n",
    "        # Write data to CSV\n",
    "        with open(self.filename, mode='a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.fieldnames)\n",
    "            writer.writerow(logs)\n",
    "\n",
    "# Initialize unique identifier for the current run\n",
    "run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Create directories for saving the results\n",
    "save_dir = f\"/home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/margin_sampling/{run_id}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Field names for the logger\n",
    "fieldnames = ['epoch', 'train_f1', 'train_loss', 'valid_acc', 'valid_f1', 'valid_loss', 'dur']\n",
    "\n",
    "# Initialize CSVLogger with the path and fieldnames\n",
    "csv_logger = CSVLogger(os.path.join(save_dir, \"training_history.csv\"), fieldnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set proxy if necessary\n",
    "os.environ['http_proxy'] = 'http://proxy:80'\n",
    "os.environ['https_proxy'] = 'http://proxy:80'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.callbacks import EpochScoring\n",
    "from sklearn.metrics import f1_score, make_scorer, accuracy_score\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import margin_sampling\n",
    "\n",
    "\n",
    "# Custom model class\n",
    "class CustomDINONormModel(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate, layer_freeze):\n",
    "        super(CustomDINONormModel, self).__init__()\n",
    "\n",
    "        # Add the local DINO repo to the Python path\n",
    "        dino_repo_path = Path('/home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches')\n",
    "        sys.path.insert(0, str(dino_repo_path))\n",
    "\n",
    "        # Import the VisionTransformer class directly from vision_transformer.py\n",
    "        from vision_transformer import vit_small\n",
    "\n",
    "        # Initialize the DINO model and load weights\n",
    "        self.dino_model = vit_small()\n",
    "        model_state = torch.load(dino_repo_path / \"/home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/dino_deitsmall16_pretrain(3).pth\", map_location=\"cpu\")\n",
    "        self.dino_model.load_state_dict(model_state, strict=False)\n",
    "\n",
    "        # Create the classifier layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(384, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "        self._freeze_layers(layer_freeze)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[-1] == 3:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.dino_model(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _freeze_layers(self, num_freeze_layers):\n",
    "        for param in list(self.dino_model.parameters())[:num_freeze_layers]:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.002\n",
    "momentum_term = 0.24729309193472406\n",
    "dropout_rate = 0.49709490164030934\n",
    "num_classes = 8\n",
    "layer_freeze = 65\n",
    "\n",
    "# Setup the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize the model\n",
    "model = CustomDINONormModel(num_classes=num_classes, dropout_rate=dropout_rate, layer_freeze=layer_freeze).to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EarlyStopping, Checkpoint, Callback\n",
    "\n",
    "# Function to convert one-hot encoded labels to integer labels\n",
    "def convert_one_hot_to_labels(y):\n",
    "    return np.argmax(y, axis=1) if len(y.shape) > 1 else y\n",
    "\n",
    "# Ensure that y_train_initial, y_pool_initial, y_val, and y_test are in the correct format\n",
    "y_train_initial_np = convert_one_hot_to_labels(y_train_initial.clone().detach().cpu().numpy())\n",
    "y_pool_initial_np = convert_one_hot_to_labels(y_pool_initial.clone().detach().cpu().numpy())\n",
    "y_val_np = convert_one_hot_to_labels(y_val.clone().detach().cpu().numpy())\n",
    "y_test_np = convert_one_hot_to_labels(y_test.clone().detach().cpu().numpy())\n",
    "\n",
    "# Convert initial datasets to NumPy\n",
    "X_train_initial_np = X_train_initial.clone().detach().cpu().numpy()\n",
    "X_pool_np = X_pool.clone().detach().cpu().numpy()\n",
    "X_val_np = X_val.clone().detach().cpu().numpy()\n",
    "X_test_np = X_test.clone().detach().cpu().numpy()\n",
    "\n",
    "# Initialize cumulative datasets\n",
    "X_cumulative = X_train_initial_np.copy()\n",
    "y_cumulative = y_train_initial_np.copy()\n",
    "\n",
    "# Define scoring functions\n",
    "f1_scorer = make_scorer(f1_score, average='micro', zero_division=1)\n",
    "train_f1 = EpochScoring(f1_scorer, on_train=True, name='train_f1', lower_is_better=False)\n",
    "valid_f1 = EpochScoring(f1_scorer, on_train=False, name='valid_f1', lower_is_better=False)\n",
    "\n",
    "# Define a validation split\n",
    "valid_ds = Dataset(X_val_np, y_val_np)\n",
    "train_split = predefined_split(valid_ds)\n",
    "\n",
    "# Early stopping\n",
    "es = EarlyStopping(monitor='valid_loss', patience=8, lower_is_better=True)\n",
    "cp = Checkpoint(dirname='model_checkpoints', monitor='valid_loss_best')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NeuralNetClassifier(\n",
    "    module=model,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=optim.SGD,\n",
    "    optimizer__momentum=momentum_term,\n",
    "    lr=learning_rate,\n",
    "    max_epochs=100,\n",
    "    train_split=train_split,\n",
    "    device=device,\n",
    "    callbacks=[train_f1, valid_f1, es,csv_logger,cp],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the ActiveLearner\n",
    "learner = ActiveLearner(\n",
    "    estimator=classifier,\n",
    "    query_strategy=margin_sampling,\n",
    "    X_training=X_cumulative,\n",
    "    y_training=y_cumulative\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize EarlyStopping and other callbacks\n",
    "total_samples = X_train_initial.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to keep record of number of samples\n",
    "no_of_samples = [X_train_initial.shape[0]]\n",
    "performance_test_data = []\n",
    "performance_val_data = []\n",
    "acc_test_data = []\n",
    "f1_test_data = []\n",
    "\n",
    "# Function to get class indices from predictions\n",
    "def get_class_indices(y_pred):\n",
    "    if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    return y_pred\n",
    "\n",
    "# Initial performance calculation\n",
    "print(\"Computing initial performance on test set...\")\n",
    "initial_y_pred = learner.predict(X_test.numpy())\n",
    "initial_y_pred = get_class_indices(initial_y_pred)\n",
    "\n",
    "# Convert y_test to class indices\n",
    "y_test_class_indices = get_class_indices(y_test.numpy())\n",
    "\n",
    "# Calculate initial F1 score and accuracy\n",
    "initial_f1 = f1_score(y_test_class_indices, initial_y_pred, average='micro')\n",
    "initial_acc = accuracy_score(y_test_class_indices, initial_y_pred)\n",
    "print(f\"Initial F1 score: {initial_f1}\")\n",
    "print(f\"Initial accuracy: {initial_acc}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Initialize tracking for Early Stopping\n",
    "best_f1_score = 0.0\n",
    "patience = 8  # Number of rounds to continue without improvement\n",
    "wait = 0  # Current wait time\n",
    "\n",
    "# Active learning loop parameters\n",
    "n_queries = 12\n",
    "initial_fraction = 0.1  # 10% of the dataset initially selected\n",
    "start_point = 2000  # Initial number of instances selected\n",
    "acc_test_data = []\n",
    "f1_test_data = []\n",
    "power=1\n",
    "\n",
    "for i in range(n_queries):\n",
    "    # Determine the number of samples to query\n",
    "    if i == 0:\n",
    "        n_instances = 8\n",
    "    else:\n",
    "        power += 0.25\n",
    "        n_instances = batch(int(np.ceil(np.power(10, power))), batch_size)\n",
    "    print(f\"\\nQuery {i + 1}: Requesting {n_instances} samples.\")\n",
    "    print(f\"Number of samples in pool before query: {X_pool_np.shape[0]}\")\n",
    "\n",
    "    # Perform the query\n",
    "    query_idx, query_instance = learner.query(X_pool_np, n_instances=n_instances)\n",
    "    X_query, y_query = X_pool_np[query_idx], y_pool_initial_np[query_idx]\n",
    "    y_query = convert_one_hot_to_labels(y_query)\n",
    "\n",
    "    # Update the cumulative datasets\n",
    "    X_cumulative = np.vstack((X_cumulative, X_query)) if i > 0 else X_query\n",
    "    y_cumulative = np.concatenate((y_cumulative, y_query)) if i > 0 else y_query\n",
    "\n",
    "    # Retrain the learner with the cumulative data\n",
    "    learner.teach(X=X_cumulative, y=y_cumulative, only_new=False)\n",
    "\n",
    "    # Evaluate the learner's performance\n",
    "    y_pred = learner.predict(X_test_np)\n",
    "    accuracy = accuracy_score(y_test_np, y_pred)\n",
    "    f1 = f1_score(y_test_np, y_pred, average='micro')\n",
    "    acc_test_data.append(accuracy)\n",
    "    f1_test_data.append(f1)\n",
    "\n",
    "    # Output the performance metrics\n",
    "    print(f\"Accuracy after query {i + 1}: {accuracy}\")\n",
    "    print(f\"F1 Score after query {i + 1}: {f1}\")\n",
    "    print(f\"Number of samples used for retraining: {len(X_cumulative)}\")\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        wait = 0  # reset the wait counter\n",
    "    else:\n",
    "        wait += 1  # increment the wait counter\n",
    "        if wait >= patience:\n",
    "            print(\"Stopping early due to no improvement in F1 score.\")\n",
    "            break\n",
    "\n",
    "    # Remove queried instances from the pool\n",
    "    X_pool_np = np.delete(X_pool_np, query_idx, axis=0)\n",
    "    y_pool_initial_np = np.delete(y_pool_initial_np, query_idx, axis=0)\n",
    "    print(f\"Number of samples in pool after query: {X_pool_np.shape[0]}\")\n",
    "\n",
    "# Log the final performance across all queries\n",
    "print(f\"Final F1 scores across iterations: {f1_test_data}\")\n",
    "print(f\"Final accuracies across iterations: {acc_test_data}\")\n",
    "\n",
    "# Save the performance results\n",
    "performance_filename = \"/home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/performance_resultsMS.npy\"\n",
    "np.save(performance_filename, {\"f1_scores\": f1_test_data, \"accuracies\": acc_test_data})\n",
    "print(f\"Performance results saved to {performance_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ActiveLearning01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
