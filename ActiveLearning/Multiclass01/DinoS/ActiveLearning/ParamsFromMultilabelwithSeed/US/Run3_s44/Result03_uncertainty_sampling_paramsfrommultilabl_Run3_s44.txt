     32      0.8333        0.9935       0.4170      0.4170        1.7781        7.3171
     33      0.9167        0.9616       0.4167      0.4167        1.7773        7.2915
     34      0.7917        0.9817       0.4148      0.4148        1.7789        7.2946
     35      0.8750        0.9509       0.4101      0.4101        1.7838        7.3123
     36      0.8333        0.9615       0.4071      0.4071        1.7835        7.2934
     37      0.8333        0.9526       0.4089      0.4089        1.7798        7.3135
     38      0.8750        0.9508       0.4083      0.4083        1.7771        7.3037
     39      0.8750        0.9540       0.4062      0.4062        1.7762        7.3242
     40      0.8750        0.9204       0.4049      0.4049        1.7775        7.3123
     41      0.8333        0.9308       0.4064      0.4064        1.7759        7.3005
     42      0.8750        0.9194       0.4062      0.4062        1.7770        7.3277
     43      0.9167        0.9034       0.4045      0.4045        1.7764        7.3126
     44      0.9167        0.9149       0.4005      0.4005        1.7795        7.2971
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 1: 0.44635416666666666
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_0.pt

Iteration:  2
Selecting 32 informative samples:

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      0.5357        1.5047       0.4226      0.4226        1.7613     +  8.6192
      2      0.7143        1.4370       0.4153      0.4153        1.7460     +  8.6683
      3      0.7321        1.3738       0.4181      0.4181        1.7368     +  8.6327
      4      0.7500        1.3134       0.4167      0.4167        1.7312     +  8.4471
      5      0.7500        1.3083       0.4158      0.4158        1.7305     +  8.4470
      6      0.7500        1.2667       0.4158      0.4158        1.7310        8.1353
      7      0.7321        1.2469       0.4123      0.4123        1.7281     +  7.9930
      8      0.7500        1.2453       0.4128      0.4128        1.7248     +  8.1037
      9      0.7500        1.2186       0.4118      0.4118        1.7241     +  7.9383
     10      0.7500        1.2105       0.4113      0.4113        1.7237     +  8.1885
     11      0.7679        1.2052       0.4082      0.4082        1.7221     +  7.9296
     12      0.7500        1.1855       0.4101      0.4101        1.7173     +  7.9118
     13      0.7321        1.1835       0.4078      0.4078        1.7172     +  7.9021
     14      0.7500        1.1560       0.4069      0.4069        1.7174        7.8956
     15      0.7500        1.1694       0.4045      0.4045        1.7203        7.6474
     16      0.7500        1.1385       0.4052      0.4052        1.7176        7.6869
     17      0.7500        1.1434       0.4054      0.4054        1.7188        7.6373
     18      0.7500        1.1319       0.4043      0.4043        1.7148     +  7.4154
     19      0.7321        1.1380       0.4056      0.4056        1.7157        7.8396
     20      0.7500        1.1442       0.4036      0.4036        1.7158        7.4013
     21      0.7500        1.1302       0.4023      0.4023        1.7150        7.4081
     22      0.7500        1.1060       0.4021      0.4021        1.7160        7.3892
     23      0.7679        1.0946       0.4036      0.4036        1.7154        7.4421
     24      0.7679        1.0932       0.4021      0.4021        1.7147     +  7.4218
     25      0.7500        1.0873       0.3983      0.3983        1.7171        7.5941
     26      0.7500        1.1021       0.3991      0.3991        1.7168        7.4204
     27      0.7679        1.0770       0.3993      0.3993        1.7161        7.3850
     28      0.7500        1.0617       0.4016      0.4016        1.7182        7.3989
     29      0.7500        1.0902       0.4040      0.4040        1.7172        7.3941
     30      0.7500        1.0826       0.4002      0.4002        1.7162        7.3760
     31      0.7500        1.0438       0.3995      0.3995        1.7184        7.4093
     32      0.7679        1.0538       0.3990      0.3990        1.7179        7.4246
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 2: 0.44809027777777777
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_1.pt

Iteration:  3
Selecting 56 informative samples:

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      0.5179        1.4736       0.4104      0.4104        1.6807     +  7.5573
      2      0.6875        1.3651       0.4139      0.4139        1.6665     +  7.5018
      3      0.7054        1.2898       0.4155      0.4155        1.6626     +  7.4952
      4      0.6964        1.2488       0.4158      0.4158        1.6625     +  7.4902
      5      0.6875        1.2138       0.4158      0.4158        1.6628        7.5060
      6      0.6875        1.1996       0.4160      0.4160        1.6599     +  7.4988
      7      0.6875        1.1713       0.4148      0.4148        1.6578     +  7.5222
      8      0.6875        1.1598       0.4142      0.4142        1.6569     +  7.5373
      9      0.6875        1.1315       0.4139      0.4139        1.6554     +  7.5267
     10      0.6875        1.1305       0.4139      0.4139        1.6524     +  7.4999
     11      0.6875        1.1112       0.4139      0.4139        1.6488     +  7.5150
     12      0.6964        1.1132       0.4141      0.4141        1.6471     +  7.5313
     13      0.6875        1.0899       0.4137      0.4137        1.6441     +  7.5060
     14      0.6875        1.1104       0.4137      0.4137        1.6443        7.5287
     15      0.6964        1.0799       0.4128      0.4128        1.6417     +  7.5336
     16      0.6875        1.0802       0.4130      0.4130        1.6424        7.5227
     17      0.6875        1.0585       0.4123      0.4123        1.6397     +  7.5119
     18      0.6875        1.0710       0.4128      0.4128        1.6411        7.5339
     19      0.6964        1.0440       0.4127      0.4127        1.6399        7.5107
     20      0.7143        1.0536       0.4125      0.4125        1.6406        7.4809
     21      0.6875        1.0322       0.4120      0.4120        1.6338     +  7.4891
     22      0.6964        1.0257       0.4128      0.4128        1.6383        7.4984
     23      0.7321        1.0225       0.4120      0.4120        1.6348        7.5174
     24      0.7143        1.0174       0.4127      0.4127        1.6402        7.5119
     25      0.7321        1.0075       0.4115      0.4115        1.6322     +  7.5261
     26      0.7500        0.9966       0.4130      0.4130        1.6392        7.5470
     27      0.7321        0.9870       0.4116      0.4116        1.6312     +  7.5270
     28      0.7857        0.9960       0.4128      0.4128        1.6401        7.5205
     29      0.7232        0.9843       0.4115      0.4115        1.6288     +  7.5377
     30      0.7946        0.9842       0.4125      0.4125        1.6401        7.5041
     31      0.8125        0.9585       0.4108      0.4108        1.6282     +  7.5230
     32      0.8036        0.9521       0.4128      0.4128        1.6464        7.5299
     33      0.7946        0.9574       0.4113      0.4113        1.6242     +  7.5388
     34      0.8304        0.9451       0.4130      0.4130        1.6466        7.5314
     35      0.8036        0.9495       0.4122      0.4122        1.6229     +  7.5161
     36      0.8036        0.9419       0.4130      0.4130        1.6501        7.5126
     37      0.7857        0.9401       0.4130      0.4130        1.6170     +  7.5001
     38      0.8036        0.9502       0.4135      0.4135        1.6485        7.5081
     39      0.7857        0.9384       0.4149      0.4149        1.6128     +  7.5373
     40      0.8036        0.9308       0.4141      0.4141        1.6518        7.5382
     41      0.7857        0.9443       0.4139      0.4139        1.6200        7.5313
     42      0.8304        0.9047       0.4125      0.4125        1.6454        7.5363
     43      0.8214        0.8985       0.4142      0.4142        1.6236        7.5357
     44      0.8393        0.8948       0.4132      0.4132        1.6457        7.5247
     45      0.8214        0.9003       0.4146      0.4146        1.6231        7.5316
     46      0.8125        0.8936       0.4130      0.4130        1.6487        7.5312
     47      0.8304        0.9007       0.4188      0.4188        1.6181        7.5267
     48      0.8125        0.8939       0.4139      0.4139        1.6453        7.5162
     49      0.8214        0.8883       0.4149      0.4149        1.6290        7.5064
     50      0.8482        0.8813       0.4158      0.4158        1.6308        7.5163
     51      0.8482        0.8561       0.4160      0.4160        1.6322        7.4846
     52      0.8214        0.8665       0.4165      0.4165        1.6372        7.4859
     53      0.8393        0.8738       0.4172      0.4172        1.6290        7.4884
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 3: 0.45434027777777775
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_2.pt

Iteration:  4
Selecting 96 informative samples:

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      0.6827        1.2272       0.4679      0.4679        1.6457     +  7.8954
      2      0.6923        1.1159       0.4323      0.4323        1.5880     +  8.8283
      3      0.8077        1.0339       0.4493      0.4493        1.6286        8.6470
      4      0.7981        1.0243       0.4543      0.4543        1.6001        11.2062
      5      0.8173        0.9894       0.4490      0.4490        1.5989        12.9120
      6      0.8317        0.9720       0.4476      0.4476        1.5933        12.8639
      7      0.8317        0.9570       0.4462      0.4462        1.5921        12.9085
      8      0.8269        0.9462       0.4410      0.4410        1.5821     +  12.9295
      9      0.8558        0.9269       0.4458      0.4458        1.5943        12.8941
     10      0.8317        0.9205       0.4488      0.4488        1.5651     +  12.7874
     11      0.8365        0.9090       0.4309      0.4309        1.5542     +  12.8961
     12      0.8510        0.8959       0.4443      0.4443        1.5795        12.8910
     13      0.8654        0.8872       0.4401      0.4401        1.5445     +  12.8654
     14      0.8606        0.8659       0.4392      0.4392        1.5410     +  12.8547
     15      0.8510        0.8608       0.4484      0.4484        1.5560        12.8514
     16      0.8654        0.8557       0.4524      0.4524        1.5207     +  12.8475
     17      0.8846        0.8421       0.4422      0.4422        1.5347        12.8611
     18      0.8894        0.8255       0.4594      0.4594        1.5576        12.8586
     19      0.8846        0.8289       0.4608      0.4608        1.5100     +  12.8243
     20      0.8798        0.8240       0.4668      0.4668        1.4830     +  12.8615
     21      0.8702        0.8146       0.4429      0.4429        1.5428        12.8698
     22      0.8894        0.8091       0.4705      0.4705        1.5734        12.9052
     23      0.8894        0.8027       0.4760      0.4760        1.5011        12.8686
     24      0.8798        0.7809       0.4781      0.4781        1.4655     +  12.8154
     25      0.8894        0.7880       0.4589      0.4589        1.5095        12.8626
     26      0.8894        0.7759       0.4715      0.4715        1.5461        12.8614
     27      0.8894        0.7803       0.4804      0.4804        1.5174        12.8571
     28      0.8942        0.7652       0.4979      0.4979        1.4432     +  12.8413
     29      0.8750        0.7761       0.4668      0.4668        1.4969        12.8253
     30      0.8942        0.7604       0.4720      0.4720        1.5495        12.8741
     31      0.8894        0.7526       0.4852      0.4852        1.4983        12.8434
     32      0.8894        0.7442       0.4995      0.4995        1.4476        12.8325
     33      0.8894        0.7347       0.4842      0.4842        1.4625        12.7820
     34      0.8894        0.7310       0.4828      0.4828        1.4932        12.7695
     35      0.8990        0.7268       0.4880      0.4880        1.4836        12.7745
     36      0.8894        0.7210       0.4962      0.4962        1.4452        12.8218
     37      0.8894        0.7069       0.4814      0.4814        1.4708        12.8654
     38      0.8942        0.7086       0.4832      0.4832        1.4792        12.8285
     39      0.8894        0.7011       0.4920      0.4920        1.4736        12.8200
     40      0.8942        0.6960       0.5021      0.5021        1.4448        12.8052
     41      0.9135        0.6964       0.5021      0.5021        1.4377     +  12.8076
     42      0.8990        0.6974       0.4951      0.4951        1.4517        12.8124
     43      0.8990        0.6962       0.4906      0.4906        1.4759        12.8320
     44      0.8894        0.6875       0.4986      0.4986        1.4744        12.8264
     45      0.8990        0.6790       0.5210      0.5210        1.4110     +  12.8215
     46      0.9087        0.6705       0.5130      0.5130        1.4255        12.8265
     47      0.8894        0.6788       0.4913      0.4913        1.4588        12.8211
     48      0.9038        0.6700       0.4976      0.4976        1.4890        12.8038
     49      0.9038        0.6696       0.5047      0.5047        1.4495        12.8182
     50      0.9087        0.6566       0.5325      0.5325        1.4018     +  12.8361
     51      0.8846        0.6749       0.4979      0.4979        1.4578        12.8506
     52      0.8894        0.6736       0.4929      0.4929        1.5083        12.8540
     53      0.8894        0.6623       0.5017      0.5017        1.4983        12.8378
     54      0.8942        0.6621       0.5337      0.5337        1.3908     +  12.8570
     55      0.8942        0.6556       0.5283      0.5283        1.4139        12.9253
     56      0.8990        0.6516       0.4915      0.4915        1.4781        12.9051
     57      0.9038        0.6424       0.5021      0.5021        1.4764        12.9176
     58      0.9038        0.6401       0.5252      0.5252        1.4045        12.8561
     59      0.9183        0.6289       0.5302      0.5302        1.4029        12.8904
     60      0.9135        0.6326       0.4983      0.4983        1.4716        12.8612
     61      0.9038        0.6144       0.5076      0.5076        1.4536        12.8702
     62      0.9087        0.6273       0.5389      0.5389        1.3840     +  12.8145
     63      0.9135        0.6233       0.5243      0.5243        1.4119        12.8423
     64      0.9135        0.6212       0.5026      0.5026        1.4545        12.8625
     65      0.9135        0.6120       0.5073      0.5073        1.4552        12.8733
     66      0.9135        0.6154       0.5273      0.5273        1.4018        12.8234
     67      0.9135        0.6023       0.5285      0.5285        1.4038        12.8265
     68      0.9183        0.6078       0.5198      0.5198        1.4205        12.8513
     69      0.9231        0.5988       0.5160      0.5160        1.4267        12.8904
     70      0.9231        0.5932       0.5109      0.5109        1.4359        12.7584
     71      0.9231        0.5967       0.5297      0.5297        1.4006        12.7953
     72      0.9135        0.5911       0.5219      0.5219        1.4145        12.8186
     73      0.9231        0.5969       0.5109      0.5109        1.4386        12.8555
     74      0.9135        0.5917       0.5337      0.5337        1.3954        12.8674
     75      0.9135        0.5921       0.5389      0.5389        1.3818     +  12.8341
     76      0.9279        0.5856       0.5165      0.5165        1.4276        12.9131
     77      0.9183        0.5779       0.5182      0.5182        1.4207        12.9150
     78      0.9327        0.5856       0.5288      0.5288        1.4033        12.8945
     79      0.9279        0.5689       0.5358      0.5358        1.3907        12.8500
     80      0.9327        0.5800       0.5325      0.5325        1.3950        12.7446
     81      0.9279        0.5810       0.5092      0.5092        1.4507        12.7810
     82      0.9135        0.5798       0.5274      0.5274        1.4062        12.7810
     83      0.9375        0.5654       0.5359      0.5359        1.3911        12.8074
     84      0.9423        0.5604       0.5269      0.5269        1.4064        12.7981
     85      0.9279        0.5600       0.5264      0.5264        1.4106        12.7860
     86      0.9423        0.5594       0.5351      0.5351        1.3911        12.7786
     87      0.9375        0.5603       0.5217      0.5217        1.4144        12.7745
     88      0.9279        0.5578       0.5257      0.5257        1.4154        12.8096
     89      0.9279        0.5509       0.5434      0.5434        1.3788     +  12.7568
     90      0.9327        0.5548       0.5458      0.5458        1.3792        12.8376
     91      0.9279        0.5594       0.5285      0.5285        1.4070        12.8213
     92      0.9279        0.5483       0.5196      0.5196        1.4236        12.8461
     93      0.9279        0.5449       0.5380      0.5380        1.3903        12.8744
     94      0.9375        0.5506       0.5417      0.5417        1.3826        12.8650
     95      0.9375        0.5454       0.5375      0.5375        1.3887        12.7695
     96      0.9375        0.5485       0.5274      0.5274        1.4075        12.8516
     97      0.9327        0.5412       0.5389      0.5389        1.3884        12.8607
     98      0.9375        0.5436       0.5332      0.5332        1.3976        12.8003
     99      0.9375        0.5340       0.5245      0.5245        1.4195        12.7812
    100      0.9375        0.5334       0.5378      0.5378        1.3876        12.8008
F1 Score after query 4: 0.5878472222222222
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_3.pt

Iteration:  5
Selecting 176 informative samples:

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7370        0.8832       0.4387      0.4387        1.5060     +  13.3825
      2      0.5260        1.3793       0.4875      0.4875        1.5607        13.3849
      3      0.6849        1.1637       0.5602      0.5602        1.3535     +  13.3991
      4      0.7161        1.0612       0.5290      0.5290        1.3943        13.4553
      5      0.7891        0.9228       0.5495      0.5495        1.3872        13.4464
      6      0.8177        0.8397       0.5710      0.5710        1.3195     +  13.5210
      7      0.8177        0.8020       0.5856      0.5856        1.2980     +  13.4646
      8      0.8359        0.7735       0.5703      0.5703        1.3290        13.3542
      9      0.8411        0.7518       0.5738      0.5738        1.3195        13.4423
     10      0.8542        0.7399       0.5863      0.5863        1.2991        13.5215
     11      0.8568        0.7218       0.6036      0.6036        1.2708     +  13.3522
     12      0.8646        0.7118       0.5891      0.5891        1.2963        13.4689
     13      0.8646        0.6959       0.6141      0.6141        1.2524     +  13.4534
     14      0.8698        0.6953       0.6064      0.6064        1.2671        13.4951
     15      0.8724        0.6846       0.6233      0.6233        1.2398     +  13.5158
     16      0.8776        0.6748       0.6128      0.6128        1.2521        13.4670
     17      0.8802        0.6685       0.6330      0.6330        1.2179     +  13.4314
     18      0.8776        0.6643       0.6137      0.6137        1.2512        13.4229
     19      0.8880        0.6549       0.6087      0.6087        1.2591        13.3964
     20      0.8906        0.6540       0.6189      0.6189        1.2375        13.3820
     21      0.8802        0.6448       0.6042      0.6042        1.2774        13.5130
     22      0.8958        0.6402       0.6149      0.6149        1.2444        13.6059
     23      0.8932        0.6259       0.6200      0.6200        1.2368        13.5560
     24      0.8776        0.6385       0.5653      0.5653        1.3791        13.5174
     25      0.8958        0.6358       0.6212      0.6212        1.2296        13.5494
     26      0.8984        0.6172       0.5958      0.5958        1.3009        13.4773
     27      0.9141        0.6179       0.6212      0.6212        1.2327        13.4712
     28      0.8958        0.6236       0.5644      0.5644        1.3951        13.4110
     29      0.9193        0.6285       0.6519      0.6519        1.1779     +  13.3729
     30      0.9089        0.6138       0.6009      0.6009        1.2919        13.4945
     31      0.9193        0.6017       0.6366      0.6366        1.1932        13.4647
     32      0.9141        0.5932       0.6260      0.6260        1.2138        13.5246
     33      0.9297        0.5914       0.6453      0.6453        1.1819        13.4650
     34      0.9062        0.5988       0.6172      0.6172        1.2449        13.4490
     35      0.9297        0.5815       0.6458      0.6458        1.1816        13.5240
     36      0.9297        0.5750       0.6455      0.6455        1.1798        13.4071
     37      0.9323        0.5708       0.6490      0.6490        1.1805        13.3631
     38      0.9375        0.5674       0.6587      0.6587        1.1715     +  13.4159
     39      0.9323        0.5669       0.6568      0.6568        1.1780        13.5173
     40      0.9323        0.5675       0.6550      0.6550        1.1784        13.4051
     41      0.9115        0.5740       0.6366      0.6366        1.1958        13.4874
     42      0.9010        0.5926       0.6226      0.6226        1.2139        13.5275
     43      0.9036        0.5919       0.6550      0.6550        1.1685     +  13.5463
     44      0.9219        0.5683       0.6549      0.6549        1.1621     +  13.5659
     45      0.9089        0.5637       0.6569      0.6569        1.1606     +  13.5804
     46      0.9115        0.5584       0.6608      0.6608        1.1598     +  13.4437
     47      0.9219        0.5634       0.6597      0.6597        1.1615        13.4693
     48      0.9167        0.5545       0.6585      0.6585        1.1546     +  13.5255
     49      0.9219        0.5505       0.6622      0.6622        1.1549        13.4990
     50      0.9245        0.5396       0.6622      0.6622        1.1575        13.4508
     51      0.9271        0.5365       0.6597      0.6597        1.1617        13.4864
     52      0.9297        0.5326       0.6641      0.6641        1.1546     +  13.3746
     53      0.9219        0.5329       0.6618      0.6618        1.1609        13.4253
     54      0.9271        0.5285       0.6620      0.6620        1.1516     +  13.3785
     55      0.9245        0.5252       0.6627      0.6627        1.1526        13.4865
     56      0.9297        0.5217       0.6620      0.6620        1.1584        13.4472
     57      0.9323        0.5156       0.6609      0.6609        1.1576        13.3928
     58      0.9375        0.5097       0.6568      0.6568        1.1685        13.4730
     59      0.9323        0.5147       0.6625      0.6625        1.1513     +  13.4194
     60      0.9323        0.5167       0.6627      0.6627        1.1526        13.4618
     61      0.9297        0.5075       0.6625      0.6625        1.1514        13.4105
     62      0.9297        0.5113       0.6623      0.6623        1.1478     +  13.3829
     63      0.9297        0.5075       0.6637      0.6637        1.1508        13.4421
     64      0.9245        0.5082       0.6639      0.6639        1.1483        13.4456
     65      0.9323        0.4955       0.6630      0.6630        1.1517        13.3975
     66      0.9349        0.5026       0.6639      0.6639        1.1429     +  13.4132
     67      0.9245        0.5003       0.6618      0.6618        1.1494        13.4513
     68      0.9349        0.4963       0.6641      0.6641        1.1439        13.4830
     69      0.9297        0.4938       0.6611      0.6611        1.1511        13.4610
     70      0.9349        0.4838       0.6604      0.6604        1.1555        13.4166
     71      0.9375        0.4860       0.6604      0.6604        1.1588        13.4464
     72      0.9349        0.4819       0.6641      0.6641        1.1442        13.4418
     73      0.9375        0.4838       0.6595      0.6595        1.1588        13.3722
     74      0.9349        0.4775       0.6634      0.6634        1.1480        13.4219
     75      0.9375        0.4768       0.6635      0.6635        1.1489        13.4382
     76      0.9297        0.4727       0.6597      0.6597        1.1601        13.5280
     77      0.9349        0.4750       0.6613      0.6613        1.1516        13.3953
     78      0.9323        0.4707       0.6642      0.6642        1.1495        13.5571
     79      0.9349        0.4692       0.6625      0.6625        1.1514        13.3928
     80      0.9401        0.4688       0.6606      0.6606        1.1545        13.3895
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 5: 0.6859375
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_4.pt

Iteration:  6
Selecting 320 informative samples:

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.6634        1.0008       0.6028      0.6028        1.1409     +  15.4153
      2      0.6420        1.0859       0.5627      0.5627        1.1825        14.7102
      3      0.8196        0.8869       0.5757      0.5757        1.2289        14.5737
      4      0.8693        0.8035       0.3043      0.3043        1.7926        14.4734
      5      0.7415        0.9954       0.5182      0.5182        1.3598        14.4837
      6      0.7372        0.9170       0.5780      0.5780        1.1485        14.5147
      7      0.6179        1.2529       0.7052      0.7052        1.0918     +  14.4450
      8      0.7770        1.0292       0.7260      0.7260        0.9296     +  14.4639
      9      0.8381        0.8150       0.6406      0.6406        1.0236        14.4748
     10      0.9105        0.7091       0.6804      0.6804        0.9679        14.6416
     11      0.9134        0.6648       0.6533      0.6533        1.0049        14.5345
     12      0.9148        0.6459       0.6476      0.6476        1.0239        14.5898
     13      0.9176        0.6292       0.7113      0.7113        0.9376        14.5956
     14      0.9233        0.6087       0.7203      0.7203        0.9224     +  14.5695
     15      0.9290        0.5955       0.7052      0.7052        0.9425        14.6854
     16      0.9332        0.5822       0.7050      0.7050        0.9262        14.5752
     17      0.9318        0.5786       0.6675      0.6675        1.0011        14.6546
     18      0.9389        0.5606       0.7063      0.7063        0.9233        14.6751
     19      0.9361        0.5571       0.6837      0.6837        0.9854        14.6363
     20      0.9432        0.5435       0.7095      0.7095        0.9339        14.5241
     21      0.9418        0.5363       0.7106      0.7106        0.9204     +  14.7718
     22      0.9432        0.5334       0.7003      0.7003        0.9447        14.6784
     23      0.9432        0.5250       0.6747      0.6747        0.9775        14.8350
     24      0.9361        0.5336       0.6510      0.6510        1.0567        14.5169
     25      0.9418        0.5151       0.7063      0.7063        0.9119     +  14.6798
     26      0.9446        0.5135       0.7073      0.7073        0.9075     +  14.5820
     27      0.9446        0.5104       0.6799      0.6799        0.9996        14.4938
     28      0.9460        0.4974       0.7094      0.7094        0.9070     +  14.5165
     29      0.9489        0.4970       0.6667      0.6667        1.0349        14.5329
     30      0.9446        0.4905       0.6330      0.6330        1.0724        14.6230
     31      0.9389        0.5098       0.6785      0.6785        1.0185        14.6058
     32      0.9460        0.4748       0.6868      0.6868        1.0039        14.7363
     33      0.9460        0.4711       0.6937      0.6937        0.9596        14.7201
     34      0.9489        0.4671       0.6788      0.6788        1.0440        14.7809
     35      0.9460        0.4624       0.6911      0.6911        0.9639        14.6393
     36      0.9474        0.4668       0.6627      0.6627        1.0171        14.5789
     37      0.9375        0.4862       0.6545      0.6545        1.0842        14.5819
     38      0.9517        0.4538       0.7030      0.7030        0.9189        14.8581
     39      0.9474        0.4591       0.6894      0.6894        0.9960        14.5948
     40      0.9503        0.4420       0.6967      0.6967        0.9329        14.5726
     41      0.9474        0.4448       0.6922      0.6922        0.9819        14.5082
     42      0.9503        0.4380       0.6936      0.6936        0.9447        14.7973
     43      0.9489        0.4395       0.7161      0.7161        0.8977     +  14.5780
     44      0.9503        0.4451       0.6991      0.6991        0.9359        14.7857
     45      0.9474        0.4360       0.6924      0.6924        0.9818        14.5246
     46      0.9503        0.4293       0.7045      0.7045        0.9110        14.5473
     47      0.9489        0.4301       0.6927      0.6927        0.9722        14.6706
     48      0.9503        0.4190       0.6950      0.6950        0.9543        14.4176
     49      0.9489        0.4198       0.7111      0.7111        0.9037        14.5271
     50      0.9503        0.4171       0.7016      0.7016        0.9187        14.4996
     51      0.9503        0.4308       0.5719      0.5719        1.3663        14.6671
     52      0.8239        0.7046       0.6851      0.6851        0.9715        14.4195
     53      0.9432        0.4533       0.7191      0.7191        0.8997        14.4913
     54      0.9503        0.4172       0.7038      0.7038        0.9351        14.6300
     55      0.9545        0.4090       0.7045      0.7045        0.9266        14.6507
     56      0.9531        0.4010       0.7007      0.7007        0.9477        14.7825
     57      0.9517        0.3942       0.6977      0.6977        0.9512        14.6689
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 6: 0.7828125
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_5.pt

Iteration:  7
Selecting 560 informative samples:

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7215        0.9504       0.6422      0.6422        0.9818     +  16.2724
      2      0.7532        0.9093       0.6733      0.6733        0.9246     +  16.0384
      3      0.8679        0.6983       0.7271      0.7271        0.8371     +  16.0142
      4      0.8892        0.6576       0.6748      0.6748        0.9651        16.0060
      5      0.9051        0.6134       0.6854      0.6854        0.9424        15.9294
      6      0.9146        0.5808       0.6882      0.6882        0.9355        16.2453
      7      0.8647        0.6449       0.7128      0.7128        0.8642        16.1547
      8      0.9138        0.5825       0.7278      0.7278        0.8406        16.2103
      9      0.9280        0.5396       0.7267      0.7267        0.8433        16.2760
     10      0.9296        0.5181       0.7363      0.7363        0.8223     +  16.2211
     11      0.9359        0.5071       0.7373      0.7373        0.8263        16.1556
     12      0.9359        0.4970       0.7365      0.7365        0.8354        16.0576
     13      0.9343        0.4896       0.7533      0.7533        0.7955     +  16.1116
     14      0.9383        0.4724       0.7278      0.7278        0.8844        16.2107
     15      0.9407        0.4653       0.7375      0.7375        0.8423        16.4038
     16      0.9415        0.4565       0.7545      0.7545        0.7938     +  16.3477
     17      0.9486        0.4487       0.7054      0.7054        0.9386        16.1904
     18      0.9422        0.4554       0.7290      0.7290        0.8749        16.3833
     19      0.9494        0.4423       0.7438      0.7438        0.8282        16.2044
     20      0.9470        0.4342       0.7210      0.7210        0.8791        16.0297
     21      0.9502        0.4230       0.7528      0.7528        0.8022        16.2109
     22      0.9565        0.4163       0.7316      0.7316        0.8625        16.0763
     23      0.9533        0.4120       0.7276      0.7276        0.8843        16.2371
     24      0.9549        0.4021       0.7583      0.7583        0.7947        16.2134
     25      0.9573        0.3985       0.7493      0.7493        0.8145        16.2798
     26      0.9581        0.3902       0.7295      0.7295        0.8769        16.1997
     27      0.9573        0.3882       0.7474      0.7474        0.8148        16.0628
     28      0.9597        0.3831       0.7224      0.7224        0.8772        16.0090
     29      0.9604        0.3784       0.7318      0.7318        0.8762        16.0583
     30      0.9636        0.3706       0.7340      0.7340        0.8615        16.3452
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 7: 0.7881944444444445
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_6.pt

Iteration:  8
Selecting 1000 informative samples:

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7487        0.8209       0.6280      0.6280        0.9736     +  22.5939
      2      0.7036        0.9834       0.7533      0.7533        0.7041     +  20.4016
      3      0.8273        0.6749       0.7769      0.7769        0.6547     +  19.7736
      4      0.8503        0.6249       0.7507      0.7507        0.7321        19.9068
      5      0.8644        0.5892       0.7536      0.7536        0.7406        19.5375
      6      0.8830        0.5551       0.7368      0.7368        0.8078        21.4204
      7      0.8803        0.5612       0.7554      0.7554        0.7043        19.6327
      8      0.9019        0.5209       0.7557      0.7557        0.7286        19.7596
      9      0.9037        0.5098       0.5771      0.5771        1.2039        19.2833
     10      0.9028        0.5126       0.7707      0.7707        0.7215        19.6493
     11      0.9090        0.4764       0.8109      0.8109        0.6260     +  19.0905
     12      0.9125        0.4667       0.7144      0.7144        0.8563        19.3409
     13      0.9143        0.4562       0.7592      0.7592        0.7158        19.2893
     14      0.9152        0.4515       0.7408      0.7408        0.7936        19.2125
     15      0.9201        0.4383       0.7590      0.7590        0.7206        19.2662
     16      0.9231        0.4350       0.7939      0.7939        0.6797        19.2822
     17      0.9178        0.4330       0.6851      0.6851        0.9841        19.0105
     18      0.9196        0.4278       0.7740      0.7740        0.6817        19.4341
     19      0.9258        0.4099       0.7924      0.7924        0.6423        19.0818
     20      0.9280        0.3955       0.7679      0.7679        0.7024        19.3553
     21      0.9329        0.3932       0.8035      0.8035        0.6194     +  19.3527
     22      0.9170        0.4280       0.7601      0.7601        0.6975        20.1649
     23      0.9293        0.3920       0.7826      0.7826        0.6623        19.2175
     24      0.9346        0.3772       0.7661      0.7661        0.6990        19.4490
     25      0.9399        0.3682       0.8116      0.8116        0.6089     +  19.5227
     26      0.9395        0.3680       0.7868      0.7868        0.6576        19.4325
     27      0.9461        0.3578       0.7825      0.7825        0.6757        19.1536
     28      0.9439        0.3526       0.7906      0.7906        0.6626        19.2479
     29      0.9435        0.3495       0.7484      0.7484        0.7458        19.0258
     30      0.9461        0.3457       0.7755      0.7755        0.6856        18.8773
     31      0.9439        0.3424       0.7882      0.7882        0.6675        19.3301
     32      0.9523        0.3346       0.8175      0.8175        0.6011     +  19.3214
     33      0.9519        0.3282       0.8219      0.8219        0.5953     +  19.5986
     34      0.9492        0.3279       0.8280      0.8280        0.5745     +  19.5433
     35      0.9545        0.3192       0.7998      0.7998        0.6493        18.9105
     36      0.9293        0.3788       0.6335      0.6335        1.1989        19.1581
     37      0.9028        0.4558       0.8227      0.8227        0.5910        19.0649
     38      0.9510        0.3219       0.8259      0.8259        0.5788        19.2462
     39      0.9563        0.3089       0.8269      0.8269        0.5808        19.5716
     40      0.9563        0.3035       0.8194      0.8194        0.6059        19.3075
     41      0.9598        0.2951       0.8252      0.8252        0.5948        19.1822
     42      0.9580        0.2950       0.8248      0.8248        0.5895        19.0451
     43      0.9227        0.3637       0.5710      0.5710        1.3662        19.1614
     44      0.9099        0.4137       0.8128      0.8128        0.6070        19.0175
     45      0.9567        0.2936       0.8127      0.8127        0.6140        19.6479
     46      0.9647        0.2825       0.8238      0.8238        0.5977        19.1879
     47      0.9660        0.2760       0.8260      0.8260        0.5990        19.3686
     48      0.9647        0.2721       0.8250      0.8250        0.5981        18.9969
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 8: 0.8197916666666667
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_7.pt

Iteration:  9
Selecting 1776 informative samples:

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.8126        0.6334       0.7595      0.7595        0.6998     +  23.7700
      2      0.8332        0.5866       0.7712      0.7712        0.6487     +  24.1619
      3      0.8673        0.5090       0.7745      0.7745        0.6347     +  24.5200
      4      0.8772        0.4835       0.7689      0.7689        0.6452        24.5072
 24.9542
      9      0.9119        0.3859       0.7786      0.7786        0.6629        24.4860
     10      0.9114        0.3855       0.7835      0.7835        0.6728        24.4587
     11      0.9146        0.3795       0.7877      0.7877        0.6554        24.1166
     12      0.9045        0.4044       0.7910      0.7910        0.6507        20.5624
     13      0.9233        0.3564       0.7870      0.7870        0.6692        17.8930
     14      0.9255        0.3466       0.7988      0.7988        0.6423        17.8593
     15      0.9290        0.3382       0.8026      0.8026        0.6320        17.8916
     16      0.9342        0.3272       0.7998      0.7998        0.6332        17.8826
     17      0.9374        0.3149       0.7962      0.7962        0.6516        17.8853
     18      0.9361        0.3108       0.7972      0.7972        0.6452        17.9447
     19      0.9428        0.3050       0.8016      0.8016        0.6442        17.9675
     20      0.9443        0.2977       0.7974      0.7974        0.6553        17.9787
     21      0.9428        0.2912       0.7936      0.7936        0.6485        17.9792
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 9: 0.8126736111111111
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_8.pt

Iteration:  10
Selecting 3160 informative samples:

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.8942        0.4276       0.7783      0.7783        0.7440     +  26.4897
      2      0.8907        0.4190       0.7979      0.7979        0.6678     +  26.4094
      3      0.9158        0.3646       0.7970      0.7970        0.6741        26.4128
      4      0.9258        0.3379       0.8016      0.8016        0.6686        26.2969
      5      0.9287        0.3243       0.8050      0.8050        0.6457     +  26.3857
      6      0.9360        0.3049       0.8094      0.8094        0.6434     +  26.4061
      7      0.9353        0.2979       0.8078      0.8078        0.6426     +  26.4116
      8      0.9383        0.2910       0.8219      0.8219        0.6152     +  26.3675
      9      0.9451        0.2715       0.8210      0.8210        0.6116     +  26.3787
     10      0.9517        0.2566       0.8194      0.8194        0.6185        26.3691
     11      0.9508        0.2495       0.8168      0.8168        0.6258        26.3856
     12      0.9504        0.2530       0.8069      0.8069        0.6759        26.3468
     13      0.9521        0.2496       0.8080      0.8080        0.6875        26.2759
     14      0.9482        0.2499       0.8276      0.8276        0.6059     +  26.3583
     15      0.9594        0.2272       0.8250      0.8250        0.6235        26.4063
     16      0.9424        0.2650       0.8236      0.8236        0.6042     +  26.3615
     17      0.9649        0.2115       0.8236      0.8236        0.6145        26.3321
     18      0.9668        0.2045       0.8257      0.8257        0.6149        26.3876
     19      0.9703        0.1914       0.8313      0.8313        0.6007     +  26.4032
     20      0.9535        0.2249       0.8325      0.8325        0.6074        26.3656
     21      0.9685        0.1898       0.8311      0.8311        0.6147        26.3907
     22      0.9724        0.1793       0.8262      0.8262        0.6249        26.2940
     23      0.9775        0.1678       0.8358      0.8358        0.6090        26.3531
     24      0.9775        0.1652       0.8392      0.8392        0.6038        26.3790
     25      0.9801        0.1574       0.8363      0.8363        0.6047        26.3961
     26      0.9814        0.1526       0.8262      0.8262        0.6408        26.3357
     27      0.9825        0.1502       0.8292      0.8292        0.6401        26.4128
     28      0.9839        0.1446       0.8280      0.8280        0.6417        26.4179
     29      0.9821        0.1439       0.8307      0.8307        0.6455        26.3935
     30      0.9451        0.2398       0.8069      0.8069        0.7518        26.3855
     31      0.9500        0.2299       0.8385      0.8385        0.5942     +  26.2773
     32      0.9772        0.1542       0.8306      0.8306        0.6297        26.3773
     33      0.9838        0.1363       0.8337      0.8337        0.6294        26.3837
     34      0.9839        0.1363       0.8347      0.8347        0.6193        26.3490
     35      0.9843        0.1288       0.8365      0.8365        0.6362        26.3390
     36      0.9879        0.1215       0.8318      0.8318        0.6394        26.3152
     37      0.9892        0.1168       0.8252      0.8252        0.6720        26.3557
     38      0.9883        0.1174       0.8286      0.8286        0.6622        26.3923
     39      0.9894        0.1127       0.8325      0.8325        0.6601        26.3685
     40      0.9914        0.1075       0.8295      0.8295        0.6691        26.3196
     41      0.9918        0.1054       0.8332      0.8332        0.6626        26.4233
     42      0.9924        0.1019       0.8314      0.8314        0.6665        26.3892
     43      0.9919        0.1017       0.8186      0.8186        0.7158        26.3922
     44      0.9928        0.0980       0.8309      0.8309        0.6733        26.3284
     45      0.9925        0.0971       0.8290      0.8290        0.6757        26.3226
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 10: 0.8272569444444444
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_9.pt

Iteration:  11
Selecting 5624 informative samples:

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.9741        0.1543       0.8104      0.8104        0.7474     +  42.6890
      2      0.9468        0.2341       0.7819      0.7819        0.6946     +  41.4687
      3      0.9500        0.2209       0.8035      0.8035        0.7486        41.3930
      4      0.9765        0.1290       0.8116      0.8116        0.7475        41.3435
      5      0.9828        0.1121       0.8278      0.8278        0.6842     +  41.4620
      6      0.9855        0.1043       0.8184      0.8184        0.7355        41.4109
      7      0.9891        0.0929       0.8288      0.8288        0.6944        41.2148
      8      0.9896        0.0888       0.8339      0.8339        0.6791     +  41.3244
      9      0.9897        0.0859       0.8231      0.8231        0.7293        41.2972
     10      0.9899        0.0854       0.8222      0.8222        0.7359        41.2065
     11      0.9893        0.0841       0.8372      0.8372        0.6745     +  41.3001
     12      0.9919        0.0781       0.8422      0.8422        0.6722     +  41.3290
     13      0.9936        0.0709       0.8443      0.8443        0.6655     +  41.3689
     14      0.9940        0.0694       0.8451      0.8451        0.6777        41.3431
     15      0.9935        0.0675       0.8380      0.8380        0.6854        41.2685
     16      0.9942        0.0654       0.8394      0.8394        0.7040        41.2846
     17      0.9940        0.0637       0.8392      0.8392        0.6953        41.3095
     18      0.9944        0.0619       0.8467      0.8467        0.6782        41.2351
     19      0.9949        0.0592       0.8448      0.8448        0.6916        41.2570
     20      0.9944        0.0602       0.8434      0.8434        0.7017        41.4013
     21      0.9949        0.0589       0.8431      0.8431        0.6950        41.2672
     22      0.9952        0.0565       0.8448      0.8448        0.6995        41.2729
     23      0.9957        0.0538       0.8398      0.8398        0.7246        41.2971
     24      0.9966        0.0515       0.8389      0.8389        0.7178        41.6041
     25      0.9965        0.0507       0.8405      0.8405        0.7228        43.5127
     26      0.9971        0.0490       0.8424      0.8424        0.7148        43.5159
     27      0.9967        0.0484       0.8410      0.8410        0.7202        43.5286
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 11: 0.8336805555555555
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_10.pt

Iteration:  12
Selecting 10000 informative samples:

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.9913        0.0671       0.8186      0.8186        0.7408     +  72.8022
      2      0.9698        0.1330       0.8102      0.8102        0.8942        72.0154
      3      0.9880        0.0727       0.8446      0.8446        0.6441     +  71.5001
      4      0.9869        0.0721       0.8424      0.8424        0.6871        71.3280
      5      0.9909        0.0613       0.8490      0.8490        0.6385     +  71.5158
      6      0.9933        0.0540       0.8484      0.8484        0.6629        71.4825
      7      0.9937        0.0498       0.8405      0.8405        0.6987        71.5313
      8      0.9945        0.0482       0.8578      0.8578        0.6188     +  71.2811
      9      0.9943        0.0467       0.8295      0.8295        0.7348        71.1293
     10      0.9947        0.0452       0.8510      0.8510        0.6715        71.1672
     11      0.9960        0.0407       0.8503      0.8503        0.6628        71.1248
     12      0.9964        0.0401       0.8521      0.8521        0.6809        71.1648
     13      0.9964        0.0382       0.8398      0.8398        0.7031        71.1470
     14      0.9970        0.0358       0.8528      0.8528        0.6849        71.1299
     15      0.9972        0.0349       0.8590      0.8590        0.6722        71.1480
     16      0.9972        0.0342       0.8497      0.8497        0.6944        69.2349
     17      0.9976        0.0326       0.8564      0.8564        0.6704        67.9610
     18      0.9973        0.0330       0.8465      0.8465        0.7040        70.6021
     19      0.9980        0.0315       0.8491      0.8491        0.7073        71.1036
     20      0.9982        0.0301       0.8484      0.8484        0.7113        71.0786
     21      0.9981        0.0297       0.8469      0.8469        0.7315        71.1311
     22      0.9982        0.0288       0.8523      0.8523        0.7115        71.1606
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 12: 0.8352430555555554
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_11.pt

Iteration:  13
Selecting 4056 informative samples:

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.9982        0.0263       0.8589      0.8589        0.6898     +  78.9506
      2      0.9984        0.0256       0.8583      0.8583        0.6927        79.0440
      3      0.9986        0.0250       0.8559      0.8559        0.7032        79.1325
      4      0.9986        0.0243       0.8566      0.8566        0.7180        78.9761
      5      0.9987        0.0240       0.8538      0.8538        0.7284        78.7532
      6      0.9988        0.0232       0.8576      0.8576        0.7151        78.4379
      7      0.9981        0.0244       0.8493      0.8493        0.7473        78.3862
      8      0.9984        0.0236       0.8556      0.8556        0.7266        78.4470
      9      0.9986        0.0230       0.8554      0.8554        0.7283        78.4552
     10      0.9987        0.0227       0.8568      0.8568        0.7254        78.3766
     11      0.9987        0.0228       0.8576      0.8576        0.7212        78.4779
     12      0.9988        0.0221       0.8578      0.8578        0.7194        78.5506
     13      0.9990        0.0214       0.8562      0.8562        0.7236        78.4373
     14      0.9990        0.0209       0.8571      0.8571        0.7253        78.4201
     15      0.9988        0.0212       0.8618      0.8618        0.7017        78.3542
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 13: 0.8376736111111112
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\model_checkpoint_iteration_12.pt
Performance results saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\uncertainty_sampling\Run3_s44\performance_results.npy