Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      0.4600        1.1462       0.5988      0.3243        1.3208     +  7.5881
      2      0.5945        1.1209       0.6266      0.3693        1.3106     +  7.5093
      3      0.4644        1.1232       0.5766      0.4481        1.3253        7.5147
      4      0.5772        1.1176       0.6484      0.3719        1.2682     +  7.5386
      5      0.5989        1.0945       0.5832      0.3285        1.3042        7.5034
      6      0.6182        1.0835       0.6474      0.2495        1.2659     +  7.5060
      7      0.5976        1.1020       0.5813      0.2067        1.2983        7.5081
      8      0.6247        1.0732       0.6472      0.2501        1.2508     +  7.4814
      9      0.4767        1.0732       0.6002      0.2132        1.2726        7.5126
     10      0.6105        1.0592       0.6488      0.2499        1.2313     +  7.5053
     11      0.4992        1.0640       0.5932      0.2111        1.2742        7.5044
     12      0.6305        1.0541       0.6453      0.2519        1.2300     +  7.4950
     13      0.4848        1.0482       0.6116      0.2220        1.2495        7.4644
     14      0.5105        1.0294       0.6444      0.2527        1.2181     +  7.4637
     15      0.4156        1.0250       0.6309      0.2327        1.2211        7.4656
     16      0.6298        1.0057       0.6438      0.2485        1.2104     +  7.4497
     17      0.5032        1.0095       0.6314      0.2381        1.2174        7.4644
     18      0.6251        1.0136       0.6450      0.2502        1.2030     +  7.4990
     19      0.6186        0.9970       0.6347      0.2377        1.2084        7.4807
     20      0.5055        0.9980       0.6464      0.2506        1.2006     +  7.4716
     21      0.6251        0.9858       0.6396      0.2473        1.2006        7.4869
     22      0.7321        0.9894       0.6519      0.2491        1.1936     +  7.4853
     23      0.6232        0.9903       0.6326      0.2497        1.2105        7.4890
     24      0.4314        0.9964       0.6465      0.2430        1.1930     +  7.4859
     25      0.6215        0.9810       0.6392      0.2580        1.2010        7.4744
     26      0.6574        0.9758       0.6382      0.2289        1.1997        7.4972
     27      0.5026        0.9756       0.6420      0.2585        1.1979        7.4825
     28      0.6683        0.9753       0.6495      0.2352        1.1928     +  7.5092
     29      0.4980        0.9741       0.6382      0.2565        1.1963        7.4543
     30      0.6598        0.9774       0.6469      0.2308        1.1983        7.4917
     31      0.4897        0.9805       0.6405      0.2580        1.1906     +  7.4616
     32      0.6646        0.9675       0.6424      0.2178        1.2063        7.4669
     33      0.6042        0.9787       0.6500      0.2644        1.1906        7.5044
     34      0.6568        0.9718       0.6042      0.2161        1.2320        7.5081
     35      0.4224        0.9749       0.6521      0.2668        1.2013        7.5055
     36      0.5171        0.9879       0.5783      0.3254        1.2481        7.5075
     37      0.6321        0.9925       0.6557      0.2634        1.1936        7.4915
     38      0.5176        0.9791       0.5766      0.2069        1.2442        7.4833
     39      0.7328        0.9599       0.6545      0.2560        1.1684     +  7.5112
     40      0.5211        0.9440       0.6177      0.2409        1.2024        7.5203
     41      0.7068        0.9297       0.6594      0.2553        1.1524     +  7.5139
     42      0.6311        0.9158       0.6321      0.2524        1.1786        7.4920
     43      0.6172        0.9277       0.6606      0.2536        1.1526        7.5033
     44      0.6478        0.9059       0.6450      0.2585        1.1582        7.5002
     45      0.6738        0.9213       0.6627      0.2467        1.1587        7.4817
     46      0.7273        0.9182       0.6392      0.2605        1.1701        7.4911
     47      0.5881        0.9144       0.6587      0.2390        1.1606        7.4979
     48      0.4924        0.9288       0.6438      0.2637        1.1649        7.4906
     49      0.5566        0.9188       0.6595      0.2454        1.1560        7.4905
     50      0.5107        0.9163       0.6429      0.2586        1.1541        7.5040
     51      0.5453        0.9097       0.6635      0.2557        1.1445     +  7.5018
     52      0.6520        0.9052       0.6455      0.2592        1.1466        7.5166
     53      0.5499        0.8794       0.6519      0.2549        1.1495        7.4970
     54      0.5288        0.8819       0.6488      0.2622        1.1428     +  7.5094
     55      0.5499        0.8785       0.6531      0.2578        1.1368     +  7.5012
     56      0.6520        0.8781       0.6479      0.2601        1.1356     +  7.5058
     57      0.5436        0.8729       0.6575      0.2561        1.1229     +  7.5082
     58      0.6633        0.8633       0.6347      0.2545        1.1480        7.5256
     59      0.6687        0.8701       0.6634      0.2544        1.1241        7.4995
     60      0.6473        0.8711       0.6378      0.2574        1.1455        7.4784
     61      0.4585        0.8631       0.6620      0.2552        1.1237        7.4652
     62      0.6578        0.8649       0.6411      0.2624        1.1439        7.4604
     63      0.6788        0.8719       0.6582      0.2470        1.1368        7.4699
     64      0.6538        0.8575       0.6497      0.2647        1.1358        7.4695
     65      0.5499        0.8513       0.6399      0.2446        1.1492        7.4796
     66      0.5231        0.8547       0.6547      0.2680        1.1303        7.4910
     67      0.5449        0.8522       0.6500      0.2539        1.1368        7.4905
     68      0.6538        0.8378       0.6502      0.2613        1.1220     +  7.4941
     69      0.6788        0.8337       0.6620      0.2590        1.1217     +  7.4884
     70      0.6460        0.8350       0.6491      0.2623        1.1243        7.4606
     71      0.6693        0.8612       0.6585      0.2529        1.1324        7.4903
     72      0.6414        0.8546       0.6401      0.2662        1.1398        7.4889
     73      0.6788        0.8471       0.6691      0.2511        1.1143     +  7.4966
     74      0.5188        0.8411       0.6344      0.2628        1.1372        7.4940
     75      0.5357        0.8493       0.6682      0.2493        1.1162        7.5070
     76      0.5231        0.8325       0.6484      0.2677        1.1211        7.4938
     77      0.4702        0.8347       0.6606      0.2533        1.1147        7.4643
     78      0.5170        0.8212       0.6566      0.2634        1.1023     +  7.4635
     79      0.6420        0.8210       0.6516      0.2604        1.1216        7.4710
     80      0.5288        0.8090       0.6531      0.2648        1.1082        7.4653
     81      0.6420        0.8105       0.6589      0.2577        1.1191        7.4825
     82      0.5344        0.8106       0.6479      0.2641        1.1111        7.5038
     83      0.4694        0.8250       0.6663      0.2585        1.1050        7.4853
     84      0.6578        0.8264       0.6406      0.2642        1.1196        7.4994
     85      0.6041        0.8069       0.6724      0.2575        1.0961     +  7.5107
     86      0.5218        0.8145       0.6398      0.2627        1.1168        7.4926
     87      0.6814        0.8038       0.6705      0.2596        1.0987        7.4946
     88      0.6633        0.8018       0.6443      0.2623        1.1090        7.5169
     89      0.6824        0.7898       0.6648      0.2593        1.0924     +  7.5119
     90      0.7551        0.7851       0.6476      0.2665        1.1061        7.5025
     91      0.5547        0.7800       0.6675      0.2600        1.0903     +  7.5129
     92      0.6687        0.7851       0.6505      0.2650        1.0976        7.5008
     93      0.6824        0.7716       0.6641      0.2625        1.0867     +  7.4715
     94      0.5449        0.7709       0.6524      0.2653        1.0904        7.4852
     95      0.6824        0.7508       0.6580      0.2614        1.0931        7.4978
     96      0.6788        0.7703       0.6582      0.2657        1.0846     +  7.4995
     97      0.7713        0.7620       0.6543      0.2610        1.0965        7.5031
     98      0.6788        0.7665       0.6620      0.2678        1.0783     +  7.5070
     99      0.5535        0.7564       0.6557      0.2623        1.0949        7.5208
    100      0.5864        0.7628       0.6597      0.2690        1.0823        7.5208
Iteration 4: Test F1 Score: 0.7111111111111111
Model checkpoint saved to D:/Shubham/results/Multiclass01/RandomSampling/model_checkpoint_iteration_3.pt

Iteration 5: Requesting 96 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      0.4720        0.8761       0.6429      0.2218        1.1446     +  7.8742
      2      0.3638        0.9864       0.5608      0.1768        1.2405        7.7698
      3      0.3731        0.9988       0.5417      0.1689        1.2660        7.7404
      4      0.3724        0.9495       0.6806      0.2597        1.0524     +  7.7507
      5      0.3999        0.8603       0.6847      0.2610        1.0436     +  7.7223
      6      0.4008        0.8724       0.6778      0.2607        1.0458        7.7403
      7      0.4036        0.8411       0.6653      0.2548        1.0652        7.7412
      8      0.4028        0.8506       0.6741      0.2631        1.0467        7.7348
      9      0.4089        0.8196       0.6806      0.2636        1.0375     +  7.7378
     10      0.4205        0.8093       0.6750      0.2558        1.0556        7.7521
     11      0.4150        0.8200       0.6722      0.2614        1.0395        7.7479
     12      0.4099        0.8133       0.6785      0.2642        1.0358     +  7.7542
     13      0.4265        0.8122       0.6717      0.2639        1.0420        7.7741
     14      0.4144        0.7976       0.6668      0.2617        1.0452        7.7668
     15      0.4286        0.8073       0.6655      0.2583        1.0473        7.7525
     16      0.5225        0.7759       0.6753      0.2639        1.0359        7.7545
     17      0.4298        0.7765       0.6602      0.2614        1.0513        7.7658
     18      0.4192        0.7838       0.6792      0.2719        1.0218     +  7.7464
     19      0.4437        0.7695       0.6778      0.2673        1.0255        7.7636
     20      0.4450        0.7482       0.6764      0.2649        1.0274        7.7758
     21      0.4522        0.7527       0.6882      0.2662        1.0285        7.7659
     22      0.4481        0.7532       0.6946      0.2673        1.0189     +  7.7816
     23      0.4471        0.7484       0.6899      0.2711        1.0173     +  7.7141
     24      0.4571        0.7351       0.6870      0.2720        1.0114     +  7.7392
     25      0.4565        0.7286       0.6722      0.2690        1.0225        7.7420
     26      0.4435        0.7273       0.6630      0.2647        1.0265        7.7497
     27      0.4614        0.7133       0.6877      0.2733        1.0079     +  7.7876
     28      0.4633        0.7110       0.6972      0.2772        0.9960     +  7.7478
     29      0.4616        0.7001       0.7045      0.2751        1.0067        7.7597
     30      0.4628        0.7120       0.6979      0.2606        1.0265        7.7843
     31      0.4473        0.7275       0.6983      0.2546        1.0209        7.7571
     32      0.4472        0.7236       0.7094      0.2694        0.9875     +  7.7678
     33      0.4596        0.7064       0.7038      0.2762        0.9962        7.7539
     34      0.4659        0.6872       0.6689      0.2670        1.0306        7.7729
     35      0.4444        0.7074       0.6625      0.2641        1.0326        7.7735
     36      0.4699        0.6927       0.6752      0.2724        1.0031        7.7781
     37      0.4733        0.6743       0.6727      0.2757        1.0019        7.7698
     38      0.4668        0.6877       0.6856      0.2779        0.9992        7.7654
     39      0.4715        0.6724       0.6997      0.2797        0.9891        7.7646
     40      0.4721        0.6636       0.7113      0.2815        0.9801     +  7.7404
     41      0.4688        0.6511       0.7024      0.2656        1.0060        7.7414
     42      0.4666        0.6737       0.6981      0.2589        1.0181        7.7696
     43      0.4536        0.6818       0.7035      0.2676        1.0093        7.7709
     44      0.4666        0.6585       0.7038      0.2788        0.9814        7.7665
     45      0.4672        0.6431       0.6903      0.2740        0.9958        7.8030
     46      0.4738        0.6369       0.6840      0.2741        0.9885        7.7995
     47      0.4706        0.6374       0.6962      0.2789        0.9807        7.7699
     48      0.4779        0.6258       0.7026      0.2841        0.9760     +  7.7803
     49      0.4731        0.6251       0.6990      0.2827        0.9758     +  7.7751
     50      0.6021        0.6178       0.6943      0.2823        0.9808        7.7556
     51      0.4809        0.6100       0.6981      0.2820        0.9749     +  7.7640
     52      0.4791        0.6095       0.7127      0.2856        0.9635     +  7.7732
     53      0.4774        0.6104       0.7069      0.2742        0.9866        7.7328
     54      0.4731        0.6266       0.7106      0.2705        0.9883        7.7386
     55      0.4749        0.6298       0.7083      0.2657        0.9895        7.7307
     56      0.4643        0.6468       0.7172      0.2777        0.9721        7.7413
     57      0.4745        0.6167       0.7028      0.2819        0.9679        7.7382
     58      0.4817        0.6015       0.6726      0.2733        0.9923        7.7523
     59      0.6016        0.6143       0.6585      0.2678        1.0178        7.7569
     60      0.5926        0.6259       0.6613      0.2712        1.0072        7.7320
     61      0.4773        0.6150       0.6819      0.2816        0.9692        7.7379
     62      0.5477        0.5824       0.7233      0.2942        0.9360     +  7.7362
     63      0.4802        0.5746       0.7234      0.2888        0.9511        7.7638
     64      0.4816        0.5852       0.7203      0.2844        0.9672        7.7570
     65      0.4774        0.5828       0.7194      0.2903        0.9470        7.7423
     66      0.4798        0.5645       0.7142      0.2865        0.9596        7.7541
     67      0.4798        0.5679       0.7174      0.2903        0.9511        7.7540
     68      0.4792        0.5609       0.7073      0.2869        0.9507        7.7483
     69      0.5449        0.5603       0.7196      0.2915        0.9399        7.7433
     70      0.4787        0.5614       0.7160      0.2886        0.9476        7.7142
     71      0.4853        0.5491       0.7148      0.2861        0.9541        7.7381
     72      0.4785        0.5533       0.7201      0.2867        0.9586        7.7262
     73      0.4802        0.5575       0.7193      0.2881        0.9553        7.7489
     74      0.4792        0.5489       0.7177      0.2911        0.9426        7.7493
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 5: Test F1 Score: 0.7322916666666666
Model checkpoint saved to D:/Shubham/results/Multiclass01/RandomSampling/model_checkpoint_iteration_4.pt

Iteration 6: Requesting 176 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      0.5649        0.6962       0.7161      0.2720        0.9688     +  8.3168
      2      0.4411        0.7638       0.6851      0.2458        1.0366        8.2262
      3      0.4180        0.7197       0.7280      0.3010        0.9210     +  8.2311
      4      0.4765        0.6808       0.6599      0.2283        1.0985        8.2203
      5      0.4072        0.7498       0.7273      0.2902        0.9374        8.2414
      6      0.4957        0.6640       0.7149      0.2909        0.9213        8.2479
      7      0.4431        0.6654       0.6641      0.2109        1.1112        8.2355
      8      0.4407        0.7168       0.7181      0.2808        0.9444        8.2190
      9      0.4530        0.6340       0.6845      0.2475        1.0380        8.2131
     10      0.4352        0.6656       0.7344      0.2943        0.8971     +  8.2091
     11      0.5181        0.6200       0.6785      0.2613        0.9935        8.2137
     12      0.4716        0.6347       0.7248      0.2836        0.9321        8.2519
     13      0.5225        0.6014       0.6840      0.2680        0.9748        8.2137
     14      0.4807        0.6229       0.7036      0.2578        0.9899        8.2454
     15      0.5027        0.6249       0.6786      0.2680        0.9828        8.2286
     16      0.5245        0.5920       0.7450      0.3064        0.8869     +  8.2567
     17      0.6013        0.5753       0.6851      0.2555        1.0186        8.2393
     18      0.5043        0.6200       0.7500      0.3126        0.8675     +  8.2306
     19      0.5518        0.5673       0.7189      0.2945        0.9105        8.1989
     20      0.5286        0.5661       0.6943      0.2559        1.0222        8.2280
     21      0.5674        0.5792       0.7385      0.3124        0.8657     +  8.2271
     22      0.5325        0.5496       0.7333      0.2976        0.9233        8.2261
     23      0.5329        0.5454       0.7030      0.2783        0.9533        8.2239
     24      0.5868        0.5491       0.7373      0.2970        0.9206        8.2204
     25      0.5656        0.5347       0.6974      0.2857        0.9379        8.2287
     26      0.5549        0.5473       0.7203      0.2780        0.9584        8.2230
     27      0.5818        0.5486       0.7021      0.2996        0.9427        8.2295
     28      0.5449        0.5975       0.7149      0.2635        0.9631        8.2263
     29      0.5186        0.6102       0.7049      0.2920        0.9469        8.2236
     30      0.6074        0.5338       0.7288      0.2939        0.9017        8.2299
     31      0.6379        0.5091       0.7161      0.3026        0.8929        8.2452
     32      0.6514        0.5118       0.7455      0.3127        0.8625     +  8.2351
     33      0.6293        0.4915       0.7304      0.3105        0.8645        8.2100
     34      0.6049        0.4932       0.7304      0.3092        0.8719        8.2451
     35      0.6445        0.4889       0.7332      0.3106        0.8646        8.2282
     36      0.6561        0.4820       0.7304      0.3146        0.8647        8.2268
     37      0.6794        0.4823       0.7340      0.3154        0.8619     +  8.2090
     38      0.6162        0.4739       0.7257      0.3107        0.8748        8.2074
     39      0.6835        0.4700       0.7398      0.3164        0.8515     +  8.1914
     40      0.7510        0.4678       0.7118      0.3002        0.9040        8.2072
     41      0.6573        0.4670       0.7420      0.3171        0.8508     +  8.2041
     42      0.6671        0.4633       0.7264      0.3093        0.8707        8.2386
     43      0.6155        0.4625       0.7259      0.3089        0.8807        8.2487
     44      0.6502        0.4623       0.7299      0.3121        0.8675        8.2240
     45      0.7275        0.4510       0.7311      0.3119        0.8627        8.2352
     46      0.6910        0.4546       0.7271      0.3120        0.8731        8.2298
     47      0.7018        0.4496       0.7299      0.3079        0.8733        8.2258
     48      0.6601        0.4439       0.7233      0.3108        0.8836        8.2168
     49      0.6090        0.4539       0.7378      0.3048        0.8870        8.2287
     50      0.6647        0.4406       0.7260      0.3175        0.8937        8.2495
     51      0.6927        0.4419       0.7382      0.3125        0.8621        8.2274
     52      0.6999        0.4361       0.7292      0.3116        0.8684        8.2133
     53      0.7018        0.4312       0.7302      0.3102        0.8678        8.2260
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 6: Test F1 Score: 0.7675347222222222
Model checkpoint saved to D:/Shubham/results/Multiclass01/RandomSampling/model_checkpoint_iteration_5.pt

Iteration 7: Requesting 320 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      0.5921        0.5864       0.7090      0.3079        0.9192     +  9.1995
      2      0.4458        0.6921       0.5894      0.2211        1.2885        9.0932
      3      0.5521        0.6449       0.7280      0.3229        0.8512     +  9.0933
      4      0.6040        0.5313       0.7113      0.3120        0.8771        9.0930
      5      0.6449        0.5248       0.6920      0.3006        0.9415        9.1020
      6      0.5673        0.5656       0.7372      0.3207        0.8264     +  9.0959
      7      0.5677        0.4983       0.7132      0.3138        0.8541        9.0891
      8      0.6152        0.5018       0.7293      0.3201        0.8299        9.0725
      9      0.5901        0.4914       0.7101      0.3129        0.8622        9.0864
     10      0.6017        0.4825       0.7179      0.3145        0.8563        9.0867
     11      0.5936        0.4758       0.7127      0.3110        0.8600        9.0518
     12      0.6203        0.4608       0.7271      0.3149        0.8351        9.0552
     13      0.5955        0.4521       0.7222      0.3162        0.8444        9.0840
     14      0.6444        0.4491       0.7179      0.3127        0.8464        9.1024
     15      0.6223        0.4432       0.7224      0.3135        0.8448        9.0626
     16      0.6234        0.4351       0.6845      0.2941        0.9590        9.0972
     17      0.6195        0.4990       0.6193      0.2326        1.1600        9.0848
     18      0.6068        0.5348       0.7351      0.3156        0.8337        9.0907
     19      0.6497        0.4320       0.7389      0.3235        0.8257     +  9.0566
     20      0.6862        0.4136       0.7193      0.3082        0.8792        9.0664
     21      0.6353        0.4149       0.7365      0.3222        0.8398        9.0850
     22      0.6611        0.3993       0.7200      0.3103        0.8780        9.0774
     23      0.6389        0.4005       0.7285      0.3169        0.8501        9.0599
     24      0.6502        0.3892       0.7250      0.3138        0.8560        9.0346
     25      0.6720        0.3928       0.6969      0.2852        0.9336        9.0510
     26      0.6596        0.3988       0.7292      0.3260        0.8530        9.0448
     27      0.6722        0.3787       0.7342      0.3212        0.8312        9.1061
     28      0.6549        0.3759       0.7304      0.3163        0.8396        9.0688
     29      0.6674        0.3749       0.7438      0.3314        0.8193     +  9.0734
     30      0.6938        0.3658       0.7401      0.3250        0.8183     +  9.0700
     31      0.7567        0.3609       0.7399      0.3259        0.8156     +  9.0689
     32      0.7360        0.3563       0.7439      0.3273        0.8127     +  9.0853
     33      0.7485        0.3568       0.7399      0.3322        0.8329        9.0690
     34      0.7066        0.3545       0.7300      0.3159        0.8478        9.0942
     35      0.6985        0.3456       0.7450      0.3278        0.8110     +  9.0850
     36      0.6943        0.3419       0.7436      0.3279        0.8090     +  9.0728
     37      0.7114        0.3410       0.7429      0.3259        0.8123        9.0530
     38      0.7715        0.3382       0.7444      0.3266        0.8151        9.0531
     39      0.7292        0.3365       0.7460      0.3268        0.8173        9.0631
     40      0.6690        0.3329       0.7417      0.3244        0.8209        9.0729
     41      0.7289        0.3293       0.7415      0.3328        0.8325        9.0892
     42      0.7360        0.3279       0.7446      0.3271        0.8185        9.0897
     43      0.7384        0.3208       0.7444      0.3308        0.8138        9.0830
     44      0.7524        0.3203       0.7443      0.3286        0.8109        9.1000
     45      0.7519        0.3174       0.7465      0.3278        0.8110        9.0853
     46      0.7450        0.3158       0.7462      0.3269        0.8189        9.1005
     47      0.7381        0.3136       0.7427      0.3273        0.8125        9.0681
     48      0.7528        0.3129       0.7469      0.3285        0.8106        9.0743
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 7: Test F1 Score: 0.7657986111111111
Model checkpoint saved to D:/Shubham/results/Multiclass01/RandomSampling/model_checkpoint_iteration_6.pt

Iteration 8: Requesting 560 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.6405        0.5157       0.6882      0.2374        1.0983     +  10.5960
      2      0.3776        0.6877       0.7087      0.3156        0.9076     +  10.5024
      3      0.4461        0.5324       0.7146      0.3143        0.8304     +  10.5522
      4      0.6037        0.4518       0.6894      0.2888        0.9526        10.5366
      5      0.6858        0.4111       0.7099      0.3154        0.9040        10.5601
      6      0.6931        0.4086       0.7158      0.3233        0.8858        10.5295
      7      0.6790        0.3969       0.7193      0.3238        0.8430        10.5540
      8      0.6761        0.3764       0.7274      0.3012        0.8714        10.5515
      9      0.6772        0.3653       0.7385      0.3273        0.8218     +  10.5687
     10      0.6659        0.3462       0.7193      0.3190        0.8899        10.5723
     11      0.7456        0.3420       0.7141      0.3172        0.8792        10.5386
     12      0.6643        0.3370       0.7075      0.3121        0.9375        10.5328
     13      0.7557        0.3323       0.7247      0.3251        0.8590        10.5015
     14      0.7344        0.3253       0.6762      0.2889        0.9838        10.5410
     15      0.6475        0.3756       0.7562      0.3436        0.7946     +  10.5566
     16      0.7173        0.3109       0.7578      0.3460        0.7739     +  10.5601
     17      0.6972        0.3070       0.7425      0.3358        0.8381        10.5407
     18      0.7579        0.2965       0.7523      0.4600        0.8151        10.5528
     19      0.7632        0.2921       0.7524      0.3409        0.8093        10.5553
     20      0.7052        0.2882       0.7403      0.4469        0.8605        10.5454
     21      0.7475        0.2825       0.7557      0.3380        0.8025        10.5512
     22      0.7132        0.2774       0.7505      0.4568        0.8182        10.5347
     23      0.7444        0.2748       0.7615      0.4652        0.7757        10.5191
     24      0.7533        0.2728       0.7627      0.3426        0.7689     +  10.5189
     25      0.7493        0.2659       0.7585      0.4619        0.7843        10.5256
     26      0.7175        0.2650       0.7615      0.3377        0.7617     +  10.5629
     27      0.7212        0.2630       0.7585      0.4636        0.7886        10.5426
     28      0.7528        0.2600       0.7557      0.3291        0.7958        10.5456
     29      0.7498        0.2551       0.7543      0.4527        0.8130        10.5511
     30      0.7537        0.2513       0.7635      0.3412        0.7522     +  10.5605
     31      0.7227        0.2496       0.7681      0.3464        0.7441     +  10.5446
     32      0.7543        0.2495       0.7625      0.3395        0.7678        10.5546
     33      0.7827        0.2443       0.7644      0.4663        0.7616        10.5684
     34      0.7792        0.2415       0.7635      0.3420        0.7583        10.5539
     35      0.7580        0.2394       0.7670      0.4695        0.7459        10.5449
     36      0.7826        0.2374       0.7646      0.3383        0.7704        10.5204
     37      0.7863        0.2368       0.7641      0.4659        0.7627        10.5512
     38      0.7266        0.2336       0.7641      0.4676        0.7608        10.5470
     39      0.7827        0.2292       0.7632      0.3401        0.7631        10.5564
     40      0.7542        0.2301       0.7646      0.3413        0.7630        10.5354
     41      0.7866        0.2265       0.7635      0.4637        0.7799        10.5189
     42      0.7585        0.2248       0.7642      0.4664        0.7622        10.5389
     43      0.7875        0.2216       0.7618      0.4651        0.7760        10.5214
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 8: Test F1 Score: 0.784375
Model checkpoint saved to D:/Shubham/results/Multiclass01/RandomSampling/model_checkpoint_iteration_7.pt

Iteration 9: Requesting 1000 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.6597        0.3914       0.6047      0.3484        1.5106     +  13.2432
      2      0.6583        0.3912       0.7911      0.3626        0.6713     +  13.1491
      3      0.7045        0.2873       0.7905      0.3639        0.6667     +  13.1956
      4      0.7591        0.2691       0.7773      0.3532        0.7275        13.2044
      5      0.7481        0.2631       0.7833      0.3591        0.7201        13.1888
      6      0.7315        0.2535       0.7750      0.3524        0.7419        13.1947
      7      0.7295        0.2496       0.7872      0.3594        0.7004        13.2005
      8      0.7372        0.2401       0.7818      0.3530        0.7085        13.1788
      9      0.7574        0.2406       0.7628      0.3499        0.7880        13.2000
     10      0.7630        0.2612       0.7852      0.3540        0.7105        13.1779
     11      0.7378        0.2328       0.7856      0.3565        0.7071        13.1901
     12      0.7594        0.2263       0.7804      0.4787        0.7159        13.2276
     13      0.7596        0.2274       0.7868      0.3550        0.7013        13.2250
     14      0.7384        0.2184       0.7842      0.3548        0.7212        13.2186
     15      0.7408        0.2161       0.7821      0.3536        0.7320        13.2270
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 9: Test F1 Score: 0.7871527777777777
Model checkpoint saved to D:/Shubham/results/Multiclass01/RandomSampling/model_checkpoint_iteration_8.pt

Iteration 10: Requesting 1776 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7033        0.2900       0.7882      0.3637        0.7253     +  17.9795
      2      0.7266        0.2621       0.6939      0.2973        1.0331        17.8606
      3      0.7189        0.2718       0.7519      0.3431        0.8025        17.9015
      4      0.7217        0.2430       0.7526      0.3431        0.7967        17.9194
      5      0.7315        0.2367       0.7934      0.3673        0.6749     +  17.9010
      6      0.7391        0.2294       0.7920      0.4919        0.6802        17.8549
      7      0.7433        0.2236       0.7769      0.3593        0.7317        17.9304
      8      0.7379        0.2183       0.7877      0.3654        0.7022        17.8607
      9      0.7625        0.2104       0.7809      0.3608        0.7260        17.8511
     10      0.7483        0.2087       0.7844      0.3633        0.7199        17.8873
     11      0.7569        0.2033       0.7833      0.3629        0.7377        17.9022
     12      0.7688        0.2016       0.7811      0.4868        0.7497        17.9222
     13      0.7572        0.1972       0.7627      0.4755        0.7926        17.9010
     14      0.7596        0.1922       0.7885      0.4901        0.7267        17.8977
     15      0.7647        0.1890       0.7903      0.3659        0.7295        17.9089
     16      0.7725        0.1854       0.7444      0.3379        0.8512        17.8884
     17      0.7616        0.1813       0.7714      0.3559        0.7891        17.9161
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 10: Test F1 Score: 0.7729166666666667
Model checkpoint saved to D:/Shubham/results/Multiclass01/RandomSampling/model_checkpoint_iteration_9.pt

Iteration 11: Requesting 3160 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7152        0.2483       0.7326      0.4145        1.1076     +  26.3526
      2      0.7365        0.2188       0.7667      0.4490        0.9173     +  26.2453
      3      0.7412        0.1998       0.7976      0.4708        0.7449     +  26.2268
      4      0.7438        0.1901       0.8142      0.4984        0.6300     +  26.3070
      5      0.7406        0.1836       0.8059      0.4903        0.6743        26.2275
      6      0.7489        0.1820       0.8049      0.4883        0.6694        26.3056
      7      0.7478        0.1757       0.4936      0.2533        1.6145        26.2261
      8      0.7359        0.1966       0.7991      0.4815        0.6904        26.2394
      9      0.7378        0.1655       0.7970      0.4768        0.7281        26.3013
     10      0.7486        0.1602       0.8123      0.3698        0.6441        26.2679
     11      0.7483        0.1562       0.8078      0.3669        0.6712        26.3078
     12      0.7638        0.1523       0.8063      0.3614        0.6979        26.2187
     13      0.7642        0.1487       0.8085      0.3687        0.6785        26.3263
     14      0.7636        0.1434       0.8102      0.3671        0.6785        26.3251
     15      0.7601        0.1413       0.8092      0.3664        0.6774        26.3120
     16      0.7652        0.1370       0.8149      0.3684        0.6796        26.2606
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 11: Test F1 Score: 0.8128472222222223
Model checkpoint saved to D:/Shubham/results/Multiclass01/RandomSampling/model_checkpoint_iteration_10.pt

Iteration 12: Requesting 5624 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7262        0.2035       0.7894      0.3583        0.7729     +  41.3445
      2      0.7287        0.1879       0.7974      0.3547        0.7804        41.1118
      3      0.7399        0.1688       0.8049      0.3721        0.7053     +  41.1508
      4      0.7425        0.1611       0.7929      0.3685        0.7542        41.1664
      5      0.7473        0.1574       0.8083      0.3710        0.7097        41.2129
      6      0.7493        0.1511       0.8017      0.3663        0.7481        41.2442
      7      0.7512        0.1442       0.7674      0.3527        0.8323        41.2191
      8      0.7502        0.1411       0.7964      0.3605        0.8208        41.1248
      9      0.7508        0.1351       0.7957      0.3657        0.7804        41.1389
     10      0.7529        0.1313       0.8061      0.3617        0.7318        41.1952
     11      0.7476        0.1359       0.7821      0.3631        0.8444        41.0822
     12      0.7539        0.1248       0.7981      0.3711        0.7680        41.1449
     13      0.7563        0.1213       0.8035      0.3643        0.7830        41.1871
     14      0.7568        0.1182       0.8043      0.3736        0.7458        41.1437
     15      0.7586        0.1135       0.7927      0.3679        0.8128        41.2107
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 12: Test F1 Score: 0.8177083333333333
Model checkpoint saved to D:/Shubham/results/Multiclass01/RandomSampling/model_checkpoint_iteration_11.pt

Iteration 13: Requesting 10000 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7316        0.1590       0.7856      0.5964        0.8862     +  67.6598
      2      0.7355        0.1409       0.7620      0.4489        1.0327        67.5996
      3      0.7514        0.1351       0.7929      0.4740        0.8621     +  67.6276
      4      0.7511        0.1276       0.7873      0.3434        0.8870        67.6273
      5      0.7576        0.1218       0.7792      0.4617        0.9536        67.6542
      6      0.7626        0.1153       0.7861      0.3423        0.9121        67.5172
      7      0.7673        0.1128       0.7773      0.4593        0.9871        67.6412
      8      0.7739        0.1086       0.7812      0.3395        0.9749        67.5497
      9      0.7847        0.1056       0.8066      0.4877        0.8160     +  67.6354
     10      0.7803        0.1048       0.7990      0.3542        0.8599        67.6398
     11      0.7815        0.0991       0.7965      0.3549        0.8564        67.5786
     12      0.7832        0.0954       0.8042      0.3615        0.8557        67.6594
     13      0.7876        0.0903       0.7778      0.4624        1.0191        67.5990
     14      0.7944        0.0892       0.8123      0.4928        0.7758     +  67.6276
     15      0.7963        0.0852       0.8149      0.3710        0.7824        67.6218
     16      0.7985        0.0838       0.8135      0.6217        0.7816        67.6654
     17      0.8020        0.0826       0.8068      0.4867        0.8436        67.5581
     18      0.7986        0.0811       0.8163      0.4973        0.7965        67.5472
     19      0.8060        0.0769       0.8248      0.5062        0.7293     +  67.6148
     20      0.8102        0.0723       0.8148      0.4947        0.8041        67.6507
     21      0.8144        0.0718       0.8194      0.4992        0.8026        67.6766
     22      0.8132        0.0691       0.8149      0.5023        0.8189        67.5944
     23      0.8177        0.0660       0.8109      0.4915        0.8527        67.6230
     24      0.8154        0.0697       0.8224      0.5073        0.7717        67.6081
     25      0.8196        0.0652       0.8153      0.3833        0.8518        67.6458
     26      0.8129        0.0904       0.8319      0.5210        0.7140     +  67.6641
     27      0.8194        0.0663       0.8217      0.3872        0.7922        67.5663
     28      0.8264        0.0604       0.8241      0.3929        0.7936        67.6515
     29      0.8277        0.0605       0.8203      0.5169        0.8119        67.6169
     30      0.8301        0.0594       0.8137      0.5142        0.8682        67.7411
     31      0.8333        0.0545       0.8259      0.5190        0.7824        67.6925
     32      0.8380        0.0553       0.8238      0.5183        0.7741        67.6077
     33      0.8416        0.0536       0.8186      0.5177        0.8221        67.5593
     34      0.8427        0.0523       0.8236      0.5172        0.7979        67.5704
     35      0.8443        0.0497       0.8267      0.5270        0.7999        67.6442
     36      0.8465        0.0477       0.8292      0.5267        0.7937        67.6477
     37      0.8467        0.0498       0.8229      0.5281        0.8364        67.6754
     38      0.8452        0.0517       0.8231      0.5239        0.8278        67.5671
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 13: Test F1 Score: 0.8293402777777777
Model checkpoint saved to D:/Shubham/results/Multiclass01/RandomSampling/model_checkpoint_iteration_12.pt

Iteration 14: Requesting all remaining samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.8248        0.0918       0.8106      0.4024        0.7642     +  78.4440
      2      0.8232        0.0733       0.8260      0.4180        0.6980     +  78.4436
      3      0.8347        0.0658       0.8187      0.4050        0.7641        78.5211
      4      0.8398        0.0585       0.8194      0.5180        0.7531        78.3960
      5      0.8414        0.0574       0.8184      0.4119        0.7917        78.3574
      6      0.8432        0.0541       0.8066      0.4085        0.8733        78.4828
      7      0.8464        0.0498       0.8198      0.5378        0.8142        78.4927
      8      0.8486        0.0489       0.8205      0.5195        0.8240        78.3588
      9      0.8486        0.0469       0.8165      0.4115        0.8158        78.3931
     14      0.8617        0.0432       0.8361      0.5237        0.7147        78.3836
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 14: Test F1 Score: 0.8435763888888889
Model checkpoint saved to D:/Shubham/results/Multiclass01/RandomSampling/model_checkpoint_iteration_13.pt
F1 Test Data: [0.2878472222222222, 0.5786458333333333, 0.6487847222222223, 0.7111111111111111, 0.7322916666666666, 0.7675347222222222, 0.7657986111111111, 0.784375, 0.7871527777777777, 0.7729166666666667, 0.8128472222222223, 0.8177083333333333, 0.8293402777777777, 0.8435763888888889]
Performance Test Data: [[0.2878472222222222, 0.5786458333333333, 0.6487847222222223, 0.7111111111111111, 0.7322916666666666, 0.7675347222222222, 0.7657986111111111, 0.784375, 0.7871527777777777, 0.7729166666666667, 0.8128472222222223, 0.8177083333333333, 0.8293402777777777, 0.8435763888888889]]
PS C:\Users\localuserSG\ActiveLearning\Multiclass01\DinoSmall\RandomSampling>



