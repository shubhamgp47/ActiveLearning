 3      0.6250        1.2502       0.4828      0.4828        1.5667     +  7.4953
      4      0.6161        1.2098       0.4826      0.4826        1.5569     +  7.5176
      5      0.6250        1.2066       0.4743      0.4743        1.5478     +  7.5056
      6      0.6339        1.1826       0.4839      0.4839        1.5511        7.5115
      7      0.6518        1.1487       0.4878      0.4878        1.5565        7.5451
      8      0.6339        1.1552       0.4849      0.4849        1.5506        7.5131
      9      0.6071        1.1268       0.4854      0.4854        1.5537        7.5229
     10      0.6250        1.1127       0.4837      0.4837        1.5471     +  7.5310
     11      0.6607        1.0993       0.4839      0.4839        1.5591        7.5335
     12      0.6071        1.1182       0.4811      0.4811        1.5466     +  7.5585
     13      0.6071        1.0955       0.4861      0.4861        1.5622        7.5311
     14      0.6161        1.0761       0.4696      0.4696        1.5293     +  7.5413
     15      0.7321        1.0695       0.4901      0.4901        1.5866        7.5525
     16      0.5982        1.0810       0.4668      0.4668        1.5006     +  7.5352
     17      0.7768        1.0886       0.4898      0.4898        1.5915        7.5311
     18      0.5982        1.0876       0.4641      0.4641        1.5336        7.5449
     19      0.7679        1.0461       0.4873      0.4873        1.5617        7.5180
     20      0.6696        1.0243       0.4701      0.4701        1.5122        7.5138
     21      0.7946        1.0239       0.4896      0.4896        1.5902        7.5007
     22      0.6250        1.0350       0.4651      0.4651        1.5039        7.5190
     23      0.8750        1.0301       0.4896      0.4896        1.5817        7.5314
     24      0.6786        1.0075       0.4717      0.4717        1.4994     +  7.5408
     25      0.8571        0.9984       0.4889      0.4889        1.5876        7.5441
     26      0.6696        0.9959       0.4714      0.4714        1.5009        7.5448
     27      0.8571        0.9922       0.4894      0.4894        1.5787        7.5596
     28      0.7232        0.9641       0.4722      0.4722        1.5040        7.5335
     29      0.8661        0.9526       0.4872      0.4872        1.5870        7.5422
     30      0.7321        0.9717       0.4734      0.4734        1.5090        7.5470
     31      0.8482        0.9487       0.4891      0.4891        1.5801        7.5241
     32      0.8214        0.9316       0.4764      0.4764        1.5041        7.5320
     33      0.9107        0.9218       0.4847      0.4847        1.5837        7.5383
     34      0.8304        0.9202       0.4745      0.4745        1.5166        7.5084
     35      0.8929        0.9133       0.4839      0.4839        1.5730        7.4861
     36      0.9018        0.8813       0.4786      0.4786        1.5059        7.4978
     37      0.9107        0.8739       0.4851      0.4851        1.5852        7.5276
     38      0.8482        0.9015       0.4816      0.4816        1.5084        7.5099
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 3: 0.5366319444444444
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run3_s44\model_checkpoint_iteration_2.pt

Iteration:  4
Selecting 96 informative samples:

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      0.5721        1.3488       0.4932      0.4932        1.4726     +  7.8023
      2      0.6731        1.2479       0.4748      0.4748        1.4861        7.8279
      3      0.6731        1.2130       0.4479      0.4479        1.6454        7.7652
      4      0.5481        1.2797       0.5361      0.5361        1.4040     +  7.7719
      5      0.6779        1.1668       0.4682      0.4682        1.4993        7.7320
      6      0.7500        1.2099       0.4602      0.4602        1.5609        7.7238
      7      0.7019        1.1502       0.5193      0.5193        1.3892     +  7.7331
      8      0.8317        1.0627       0.4708      0.4708        1.4569        7.7565
      9      0.8606        1.0346       0.4580      0.4580        1.5085        7.7583
     10      0.8077        1.0457       0.5411      0.5411        1.3869     +  7.7588
     11      0.8077        1.0639       0.4868      0.4868        1.4717        7.7452
     12      0.8221        1.1322       0.4479      0.4479        1.6502        7.7890
     13      0.5385        1.2133       0.4663      0.4663        1.4924        7.7599
     14      0.7308        1.2261       0.4651      0.4651        1.5676        7.7599
     15      0.7067        1.0583       0.4913      0.4913        1.3922        7.7748
     16      0.8702        0.9648       0.4793      0.4793        1.4007        7.7705
     17      0.8990        0.9233       0.4800      0.4800        1.4046        7.7533
     18      0.9038        0.9115       0.4821      0.4821        1.4054        7.7624
     19      0.8990        0.9056       0.4908      0.4908        1.3777     +  7.7621
     20      0.8942        0.9048       0.4800      0.4800        1.4282        7.7368
     21      0.8942        0.8946       0.4832      0.4832        1.4172        7.7438
     22      0.8798        0.8815       0.4941      0.4941        1.3756     +  7.7584
     23      0.9038        0.8750       0.4873      0.4873        1.3795        7.7324
     24      0.8990        0.8593       0.4800      0.4800        1.4471        7.7861
     25      0.8942        0.8588       0.4882      0.4882        1.3901        7.7782
     26      0.8990        0.8476       0.5087      0.5087        1.3582     +  7.7630
     27      0.8990        0.8470       0.4866      0.4866        1.3868        7.7710
     28      0.9038        0.8320       0.4818      0.4818        1.4369        7.7750
     29      0.8990        0.8351       0.4887      0.4887        1.3911        7.7615
     30      0.9135        0.8154       0.5174      0.5174        1.3537     +  7.7693
     31      0.9087        0.8176       0.4837      0.4837        1.3957        7.7565
     32      0.8942        0.8128       0.4814      0.4814        1.4344        7.7676
     33      0.8942        0.8162       0.4892      0.4892        1.3984        7.7654
     34      0.9183        0.7969       0.5253      0.5253        1.3393     +  7.7396
     35      0.9087        0.7984       0.4946      0.4946        1.3791        7.7350
     36      0.9087        0.7873       0.4901      0.4901        1.3938        7.7383
     37      0.8990        0.7761       0.4986      0.4986        1.3792        7.7223
     38      0.9087        0.7725       0.5080      0.5080        1.3460        7.7437
     39      0.9183        0.7682       0.4882      0.4882        1.4159        7.7564
     40      0.9038        0.7727       0.4899      0.4899        1.3930        7.7649
     41      0.9135        0.7633       0.5229      0.5229        1.3371     +  7.7642
     42      0.9183        0.7522       0.4979      0.4979        1.3575        7.7544
     43      0.9135        0.7456       0.4911      0.4911        1.4058        7.7648
     44      0.8990        0.7441       0.4991      0.4991        1.3735        7.7666
     45      0.9183        0.7416       0.5068      0.5068        1.3493        7.7472
     46      0.9183        0.7319       0.5043      0.5043        1.3645        7.7734
     47      0.9135        0.7262       0.4984      0.4984        1.3780        7.7581
     48      0.9135        0.7265       0.5054      0.5054        1.3691        7.7684
     49      0.9183        0.7280       0.5075      0.5075        1.3490        7.7597
     50      0.9231        0.7195       0.5061      0.5061        1.3525        7.7274
     51      0.9183        0.7123       0.4892      0.4892        1.4372        7.7191
     52      0.9135        0.7180       0.4974      0.4974        1.3870        7.7181
     53      0.9231        0.7057       0.5238      0.5238        1.3387        7.7392
     54      0.9231        0.7081       0.5201      0.5201        1.3416        7.7454
     55      0.9183        0.7059       0.4967      0.4967        1.4018        7.7733
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 4: 0.5623263888888889
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run3_s44\model_checkpoint_iteration_3.pt

Iteration:  5
Selecting 176 informative samples:

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      0.8047        1.0202       0.3889      0.3889        1.5633     +  8.1949
      2      0.5859        1.2588       0.2748      0.2748        1.8398        8.1880
      3      0.7135        1.1876       0.5757      0.5757        1.3172     +  8.1936
      4      0.8333        0.8874       0.6238      0.6238        1.2413     +  8.1896
      5      0.8594        0.8222       0.6290      0.6290        1.2246     +  8.2034
      6      0.8776        0.7984       0.6299      0.6299        1.2232     +  8.2080
      7      0.8750        0.7809       0.6293      0.6293        1.2196     +  8.2546
      8      0.8776        0.7613       0.6283      0.6283        1.2170     +  8.2788
      9      0.8776        0.7538       0.6281      0.6281        1.2133     +  8.2545
     10      0.8828        0.7505       0.6253      0.6253        1.2139        8.2512
     11      0.8776        0.7259       0.6234      0.6234        1.2150        8.2565
     12      0.8828        0.7290       0.6220      0.6220        1.2137        8.2653
     13      0.8828        0.7206       0.6248      0.6248        1.2077     +  8.2731
     14      0.8828        0.7143       0.6234      0.6234        1.2057     +  8.2507
     15      0.8854        0.7026       0.6245      0.6245        1.2050     +  8.2666
     16      0.8880        0.6906       0.6247      0.6247        1.2024     +  8.2546
     17      0.8828        0.6819       0.6238      0.6238        1.2004     +  8.2156
     18      0.8880        0.6749       0.6238      0.6238        1.1974     +  8.2166
     19      0.8880        0.6793       0.6226      0.6226        1.1956     +  8.2168
     20      0.8828        0.6650       0.6219      0.6219        1.1985        8.2212
     21      0.8880        0.6675       0.6247      0.6247        1.1911     +  8.2409
     22      0.8854        0.6623       0.6219      0.6219        1.1994        8.2468
     23      0.8906        0.6551       0.6260      0.6260        1.1899     +  8.2652
     24      0.8906        0.6508       0.6259      0.6259        1.1886     +  8.2391
     25      0.8932        0.6451       0.6236      0.6236        1.1955        8.2264
     26      0.8984        0.6314       0.6295      0.6295        1.1850     +  8.2431
     27      0.8958        0.6329       0.6273      0.6273        1.1839     +  8.2346
     28      0.8932        0.6339       0.6286      0.6286        1.1800     +  8.2361
     29      0.9036        0.6239       0.6274      0.6274        1.1843        8.2567
     30      0.9089        0.6171       0.6281      0.6281        1.1829        8.2520
     31      0.9062        0.6151       0.6300      0.6300        1.1759     +  8.2250
     32      0.9036        0.6107       0.6257      0.6257        1.1870        8.2215
     33      0.9010        0.6000       0.6257      0.6257        1.1836        8.2105
     34      0.9036        0.5954       0.6295      0.6295        1.1782        8.2080
     35      0.9036        0.6007       0.6264      0.6264        1.1814        8.2455
     36      0.9089        0.5929       0.6309      0.6309        1.1750     +  8.2461
     37      0.9062        0.5938       0.6278      0.6278        1.1812        8.2328
     38      0.9115        0.5836       0.6293      0.6293        1.1791        8.2572
     39      0.9062        0.5836       0.6307      0.6307        1.1701     +  8.2573
     40      0.9089        0.5816       0.6307      0.6307        1.1717        8.2519
     41      0.9141        0.5766       0.6273      0.6273        1.1753        8.2545
     42      0.9141        0.5728       0.6314      0.6314        1.1731        8.2682
     43      0.9115        0.5666       0.6293      0.6293        1.1748        8.2475
     44      0.9167        0.5679       0.6314      0.6314        1.1688     +  8.2472
     45      0.9167        0.5646       0.6351      0.6351        1.1650     +  8.2524
     46      0.9167        0.5588       0.6274      0.6274        1.1784        8.2292
     47      0.9245        0.5534       0.6323      0.6323        1.1658        8.2355
     48      0.9193        0.5504       0.6340      0.6340        1.1663        8.2307
     49      0.9193        0.5517       0.6306      0.6306        1.1685        8.2565
     50      0.9219        0.5453       0.6361      0.6361        1.1630     +  8.2529
     51      0.9271        0.5427       0.6326      0.6326        1.1668        8.2506
     52      0.9271        0.5418       0.6378      0.6378        1.1599     +  8.2785
     53      0.9193        0.5380       0.6342      0.6342        1.1636        8.2541
     54      0.9219        0.5351       0.6398      0.6398        1.1570     +  8.2512
     55      0.9323        0.5323       0.6299      0.6299        1.1762        8.2611
     56      0.9271        0.5302       0.6464      0.6464        1.1480     +  8.2490
     57      0.9297        0.5348       0.6266      0.6266        1.1784        8.2458
     58      0.9219        0.5289       0.6406      0.6406        1.1580        8.2475
     59      0.9271        0.5247       0.6295      0.6295        1.1729        8.2585
     60      0.9323        0.5203       0.6378      0.6378        1.1643        8.2483
     61      0.9349        0.5097       0.6365      0.6365        1.1614        8.2296
     62      0.9375        0.5101       0.6349      0.6349        1.1599        8.2129
     63      0.9245        0.5068       0.6340      0.6340        1.1600        8.2133
     64      0.9375        0.5101       0.6405      0.6405        1.1577        8.2236
     65      0.9271        0.5064       0.6318      0.6318        1.1699        8.2483
     66      0.9375        0.5030       0.6396      0.6396        1.1551        8.2440
     67      0.9479        0.4934       0.6314      0.6314        1.1712        8.2651
     68      0.9297        0.4976       0.6424      0.6424        1.1490        8.2415
     69      0.9375        0.5013       0.6309      0.6309        1.1731        8.2450
     70      0.9375        0.4942       0.6420      0.6420        1.1487        8.2594
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 5: 0.6829861111111111
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run3_s44\model_checkpoint_iteration_4.pt

Iteration:  6
Selecting 320 informative samples:

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      0.7188        1.0244       0.4179      0.4179        1.4376     +  9.0484
      2      0.7429        1.0719       0.5326      0.5326        1.2113     +  9.1031
      3      0.8040        0.9430       0.5234      0.5234        1.2166        9.0716
      4      0.8210        0.9037       0.6266      0.6266        1.1571     +  9.0729
      5      0.8295        0.8734       0.5920      0.5920        1.2121        9.0687
      6      0.8224        0.8742       0.5615      0.5615        1.1675        9.0944
      7      0.8295        0.8462       0.6670      0.6670        1.0951     +  9.0700
      8      0.8267        0.8264       0.5120      0.5120        1.4313        9.0837
      9      0.7727        0.8960       0.6878      0.6878        1.0346     +  9.0891
     10      0.8267        0.8426       0.4592      0.4592        1.3381        9.0731
     11      0.8324        0.8235       0.7059      0.7059        1.0087     +  9.0703
     12      0.8494        0.7578       0.6625      0.6625        1.0710        9.0670
     13      0.8494        0.7382       0.5760      0.5760        1.2322        9.0597
     14      0.8466        0.7547       0.4863      0.4863        1.4266        9.0709
     15      0.8466        0.7506       0.6080      0.6080        1.1579        9.0902
     16      0.8537        0.7163       0.5540      0.5540        1.2776        9.0807
     17      0.8636        0.7140       0.6694      0.6694        1.0473        9.1056
     18      0.8622        0.6948       0.7076      0.7076        0.9861     +  9.1143
     19      0.8636        0.6746       0.6898      0.6898        1.0265        9.0897
     20      0.8750        0.6652       0.7135      0.7135        0.9654     +  9.0919
     21      0.8849        0.6429       0.7071      0.7071        0.9607     +  9.0666
     22      0.8878        0.6328       0.7019      0.7019        0.9775        9.0600
     23      0.8892        0.6270       0.6821      0.6821        1.0038        9.0645
     24      0.8963        0.6194       0.6727      0.6727        1.0236        9.0633
     25      0.8935        0.6175       0.5799      0.5799        1.2260        9.0315
     26      0.9048        0.6266       0.5464      0.5464        1.2961        9.0501
     27      0.8835        0.6264       0.7101      0.7101        0.9572     +  9.0522
     28      0.8977        0.5966       0.7023      0.7023        0.9781        9.0638
     29      0.9091        0.5834       0.6835      0.6835        0.9798        9.0807
     30      0.9091        0.5816       0.5849      0.5849        1.2211        9.0627
     31      0.9077        0.5953       0.6637      0.6637        1.0556        9.0726
     32      0.9105        0.5643       0.7181      0.7181        0.9383     +  9.0634
     33      0.9247        0.5447       0.7085      0.7085        0.9323     +  9.0717
     34      0.9261        0.5363       0.7082      0.7082        0.9386        9.0744
     35      0.9318        0.5396       0.6837      0.6837        0.9995        9.0853
     36      0.9247        0.5347       0.7007      0.7007        0.9498        9.0544
     37      0.9347        0.5211       0.7144      0.7144        0.9198     +  9.0512
     38      0.9332        0.5129       0.7097      0.7097        0.9174     +  9.0382
     39      0.9389        0.5118       0.7009      0.7009        0.9355        9.0104
     40      0.9432        0.5025       0.6887      0.6887        0.9825        9.0459
     41      0.9361        0.5088       0.7080      0.7080        0.9254        9.0778
     42      0.9446        0.4911       0.7049      0.7049        0.9335        9.0740
     43      0.9403        0.4915       0.7151      0.7151        0.9078     +  9.0858
     44      0.9503        0.4825       0.7002      0.7002        0.9328        9.0622
     45      0.9545        0.4784       0.7073      0.7073        0.9284        9.0828
     46      0.9503        0.4754       0.7040      0.7040        0.9369        9.0973
     47      0.9531        0.4704       0.7092      0.7092        0.9139        9.0765
     48      0.9602        0.4617       0.6974      0.6974        0.9241        9.0856
     49      0.9574        0.4556       0.7063      0.7063        0.9147        9.0733
     50      0.9616        0.4557       0.7142      0.7142        0.9388        9.0674
     51      0.9545        0.4518       0.6908      0.6908        0.9502        9.0641
     52      0.9616        0.4522       0.7108      0.7108        0.9165        9.0625
     53      0.9602        0.4434       0.6929      0.6929        0.9681        9.0634
     54      0.9645        0.4419       0.7045      0.7045        0.9171        9.0631
     55      0.9602        0.4331       0.7120      0.7120        0.9208        9.0923
     56      0.9602        0.4373       0.7059      0.7059        0.9347        9.0921
     57      0.9645        0.4308       0.6991      0.6991        0.9144        9.0900
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 6: 0.7777777777777778
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run3_s44\model_checkpoint_iteration_5.pt

Iteration:  7
Selecting 560 informative samples:

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.8449        0.7524       0.6590      0.6590        1.0403     +  10.5128
      2      0.7951        0.8512       0.3052      0.3052        1.7101        10.5063
      3      0.7476        0.9711       0.6415      0.6415        0.9721     +  10.5566
      4      0.9043        0.6593       0.7181      0.7181        0.8799     +  10.5349
      5      0.9233        0.5962       0.7325      0.7325        0.8568     +  10.5590
      6      0.9304        0.5777       0.6953      0.6953        0.9463        10.5610
      7      0.9375        0.5559       0.7207      0.7207        0.8750        10.5522
      8      0.9454        0.5466       0.6958      0.6958        0.9529        10.5447
      9      0.9446        0.5295       0.7323      0.7323        0.8591        10.5772
     10      0.9438        0.5199       0.7142      0.7142        0.8812        10.5706
     11      0.9256        0.5467       0.6000      0.6000        1.1320        10.5603
     12      0.9430        0.5301       0.7337      0.7337        0.8524     +  10.5295
     13      0.9454        0.5010       0.7299      0.7299        0.8516     +  10.5180
     14      0.9494        0.4966       0.7066      0.7066        0.9134        10.5408
     15      0.9509        0.4806       0.7351      0.7351        0.8413     +  10.5533
     16      0.9517        0.4772       0.7158      0.7158        0.8858        10.5545
     17      0.9597        0.4608       0.7292      0.7292        0.8518        10.5432
     18      0.9597        0.4531       0.7302      0.7302        0.8533        10.5620
     19      0.9573        0.4473       0.7314      0.7314        0.8586        10.5740
     20      0.9573        0.4426       0.7316      0.7316        0.8472        10.5979
     21      0.9620        0.4369       0.7181      0.7181        0.8834        10.5939
     22      0.9612        0.4323       0.7295      0.7295        0.8699        10.5870
     23      0.9644        0.4243       0.7340      0.7340        0.8469        10.5730
     24      0.9644        0.4175       0.7392      0.7392        0.8475        10.5724
     25      0.9636        0.4235       0.7299      0.7299        0.8684        10.5422
     26      0.9644        0.4126       0.7274      0.7274        0.8623        10.5967
     27      0.9628        0.4118       0.7401      0.7401        0.8452        10.6115
     28      0.9644        0.4100       0.7332      0.7332        0.8519        10.6213
     29      0.9636        0.4043       0.7363      0.7363        0.8398     +  10.6035
     30      0.9644        0.4015       0.7358      0.7358        0.8490        10.5829
     31      0.9668        0.3929       0.7375      0.7375        0.8381     +  10.5841
     32      0.9415        0.4548       0.7222      0.7222        0.8623        10.5679
     33      0.9636        0.3948       0.7224      0.7224        0.8898        10.5997
     34      0.9684        0.3812       0.7448      0.7448        0.8268     +  10.5827
     35      0.9438        0.4587       0.7514      0.7514        0.8200     +  10.5510
     36      0.9691        0.3745       0.7321      0.7321        0.8509        10.5957
     37      0.9652        0.3780       0.7326      0.7326        0.8541        10.5768
     38      0.9715        0.3661       0.7448      0.7448        0.8177     +  10.5860
     39      0.9684        0.3666       0.7458      0.7458        0.8161     +  10.5798
     40      0.9691        0.3632       0.7523      0.7523        0.8211        10.5838
     41      0.9707        0.3591       0.7354      0.7354        0.8344        10.5862
     42      0.9715        0.3534       0.7410      0.7410        0.8256        10.5939
     43      0.9715        0.3529       0.7526      0.7526        0.8184        10.5669
     44      0.9668        0.3490       0.7224      0.7224        0.8716        10.5613
     45      0.9723        0.3471       0.7418      0.7418        0.8242        10.5609
     46      0.9731        0.3444       0.7510      0.7510        0.8202        10.5357
     47      0.9731        0.3394       0.7413      0.7413        0.8394        10.5282
     48      0.9707        0.3347       0.7345      0.7345        0.8502        10.5746
     49      0.9739        0.3303       0.7491      0.7491        0.8332        10.5818
     50      0.9739        0.3249       0.7502      0.7502        0.8287        10.5636
     51      0.9747        0.3287       0.7495      0.7495        0.8171        10.5545
     52      0.9731        0.3259       0.7425      0.7425        0.8340        10.5747
     53      0.9763        0.3207       0.7410      0.7410        0.8387        10.5650
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 7: 0.8210069444444444
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run3_s44\model_checkpoint_iteration_6.pt

Iteration:  8
Selecting 1000 informative samples:

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7354        0.9031       0.6939      0.6939        0.9133     +  13.2268
      2      0.7610        0.8579       0.7851      0.7851        0.7179     +  13.2123
      3      0.8211        0.6845       0.7714      0.7714        0.7679        13.1937
      4      0.9055        0.5243       0.7844      0.7844        0.6945     +  13.2188
      5      0.8891        0.5324       0.7309      0.7309        0.8705        13.2143
      6      0.9059        0.4956       0.7094      0.7094        0.8641        13.2044
      7      0.9165        0.4690       0.7774      0.7774        0.7235        13.2126
      8      0.9333        0.4293       0.7839      0.7839        0.7035        13.1635
      9      0.9404        0.4078       0.7720      0.7720        0.7460        13.1796
     10      0.9421        0.4088       0.7849      0.7849        0.7142        13.2141
     11      0.9519        0.3763       0.7908      0.7908        0.7023        13.2153
     12      0.9572        0.3662       0.7910      0.7910        0.6860     +  13.2183
     13      0.8896        0.5022       0.7481      0.7481        0.8098        13.1998
     14      0.8569        0.6288       0.7925      0.7925        0.6836     +  13.2081
     15      0.9461        0.3824       0.7908      0.7908        0.6893        13.2236
     16      0.9563        0.3507       0.8010      0.8010        0.6630     +  13.1884
     17      0.9607        0.3415       0.7981      0.7981        0.6717        13.1496
     18      0.9611        0.3290       0.7964      0.7964        0.6658        13.1690
     19      0.9647        0.3248       0.8010      0.8010        0.6671        13.1794
     20      0.9686        0.3112       0.8035      0.8035        0.6614     +  13.2137
     21      0.9673        0.3077       0.8010      0.8010        0.6632        13.2110
     22      0.9691        0.2987       0.8007      0.8007        0.6697        13.2203
     23      0.9691        0.2986       0.8017      0.8017        0.6752        13.2171
     24      0.9722        0.2909       0.7955      0.7955        0.6852        13.2112
     25      0.9722        0.2819       0.8050      0.8050        0.6689        13.2119
     26      0.9726        0.2818       0.7967      0.7967        0.6876        13.1882
     27      0.9766        0.2731       0.8002      0.8002        0.6812        13.1752
     28      0.9708        0.2780       0.8012      0.8012        0.6857        13.2236
     29      0.9731        0.2777       0.8010      0.8010        0.6848        13.2272
     30      0.9735        0.2724       0.8069      0.8069        0.6638        13.2132
     31      0.9775        0.2592       0.8007      0.8007        0.6888        13.2215
     32      0.9806        0.2525       0.8028      0.8028        0.6915        13.2151
     33      0.9788        0.2519       0.8063      0.8062        0.6792        13.1781
     34      0.9788        0.2510       0.8064      0.8064        0.6878        13.2000
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 8: 0.8059027777777777
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run3_s44\model_checkpoint_iteration_7.pt

Iteration:  9
Selecting 1776 informative samples:

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7844        0.7688       0.7260      0.7260        0.9321     +  17.9036
      2      0.6295        1.1434       0.6762      0.6762        0.8712     +  17.9333
      3      0.8225        0.6492       0.8078      0.8078        0.6017     +  17.9441
      4      0.8802        0.5028       0.8391      0.8391        0.5227     +  17.9375
      5      0.8948        0.4490       0.8234      0.8234        0.5788        17.9282
      6      0.9045        0.4341       0.8332      0.8332        0.5307        17.9967
      7      0.9223        0.3896       0.8382      0.8382        0.5264        18.0164
      8      0.9245        0.3733       0.8503      0.8503        0.5044     +  17.9937
      9      0.9364        0.3537       0.8514      0.8514        0.4867     +  18.0920
     10      0.9426        0.3322       0.8533      0.8533        0.4906        17.9173
     11      0.9453        0.3155       0.8566      0.8566        0.4864     +  17.9243
     12      0.9416        0.3216       0.8510      0.8510        0.4981        17.9174
     13      0.9562        0.2895       0.8523      0.8523        0.4944        17.9714
     14      0.9470        0.3022       0.8556      0.8556        0.4934        17.9390
     15      0.9408        0.3154       0.8557      0.8557        0.4929        17.9339
     16      0.9589        0.2700       0.8542      0.8542        0.4932        17.9449
     17      0.9604        0.2658       0.8500      0.8500        0.5047        17.9016
     18      0.9584        0.2716       0.8460      0.8460        0.5103        17.9038
     19      0.9594        0.2642       0.8396      0.8396        0.5234        17.9186
     20      0.9609        0.2546       0.8469      0.8469        0.5079        17.9492
     21      0.9631        0.2496       0.8417      0.8417        0.5286        17.9394
     22      0.9653        0.2409       0.8554      0.8554        0.4979        17.9422
     23      0.9671        0.2361       0.8446      0.8446        0.5300        17.9445
     24      0.9686        0.2269       0.8446      0.8446        0.5367        17.8956
     25      0.9629        0.2391       0.8328      0.8328        0.5694        17.9016
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 9: 0.8213541666666667
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run3_s44\model_checkpoint_iteration_8.pt

Iteration:  10
Selecting 3160 informative samples:

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.8957        0.4082       0.8438      0.8438        0.4769     +  26.3509
      2      0.8960        0.4131       0.8436      0.8436        0.4822        26.2477
      3      0.9007        0.3967       0.8439      0.8439        0.4904        26.3633
      4      0.9268        0.3215       0.8477      0.8477        0.5015        26.3696
      5      0.9289        0.3167       0.7918      0.7918        0.5464        26.3494
      6      0.9375        0.2952       0.8519      0.8519        0.4806        26.3024
      7      0.9357        0.2937       0.8523      0.8523        0.4968        26.2928
      8      0.9493        0.2621       0.8451      0.8451        0.5199        26.3402
      9      0.9417        0.2727       0.8495      0.8495        0.5082        26.3168
     10      0.9506        0.2495       0.8455      0.8455        0.5249        26.3312
     11      0.9514        0.2437       0.8512      0.8512        0.5103        26.2396
     12      0.9518        0.2446       0.8441      0.8441        0.5227        26.3022
     13      0.9575        0.2286       0.8422      0.8422        0.5415        26.3523
     14      0.9607        0.2201       0.8481      0.8481        0.5323        26.3522
     15      0.9614        0.2154       0.8431      0.8431        0.5471        26.3238
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 10: 0.8253472222222222
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run3_s44\model_checkpoint_iteration_9.pt

Iteration:  11
Selecting 5624 informative samples:

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.9502        0.2346       0.8363      0.8363        0.5567     +  41.2296
      2      0.9541        0.2150       0.8497      0.8497        0.5157     +  41.1472
      3      0.9626        0.1886       0.8486      0.8486        0.5335        41.2886
      4      0.8714        0.4620       0.8540      0.8540        0.5168        41.2627
      5      0.9609        0.1904       0.8467      0.8467        0.5157     +  41.1983
      6      0.9678        0.1684       0.8453      0.8453        0.5151     +  41.3268
      7      0.9705        0.1577       0.8432      0.8432        0.5248        41.2424
      8      0.9683        0.1555       0.8448      0.8448        0.5246        41.2657
      9      0.9729        0.1460       0.8450      0.8450        0.5254        41.2507
     10      0.9700        0.1525       0.8502      0.8502        0.5399        41.2086
     11      0.9767        0.1339       0.8417      0.8417        0.5456        41.2311
     12      0.9780        0.1296       0.8441      0.8441        0.5460        41.2554
     13      0.9799        0.1226       0.8411      0.8411        0.5579        41.2005
     14      0.9800        0.1197       0.8356      0.8356        0.5676        41.2874
     15      0.9808        0.1171       0.8444      0.8444        0.5568        41.2938
     16      0.9799        0.1175       0.8372      0.8372        0.5794        41.2370
     17      0.9758        0.1263       0.8446      0.8446        0.5778        41.2789
     18      0.9806        0.1142       0.8408      0.8408        0.5699        41.3096
     19      0.9828        0.1051       0.8429      0.8429        0.5772        41.1894
     20      0.9829        0.1051       0.8354      0.8354        0.6015        41.2705
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 11: 0.8309027777777777
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run3_s44\model_checkpoint_iteration_10.pt

Iteration:  12
Selecting 10000 informative samples:

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.9856        0.0983       0.8415      0.8415        0.5769     +  67.9705
      2      0.9848        0.0960       0.8462      0.8462        0.5783        67.7618
      3      0.9866        0.0885       0.8443      0.8443        0.5842        67.8330
      4      0.9862        0.0874       0.8413      0.8413        0.6038        67.6832
      5      0.9875        0.0841       0.8354      0.8354        0.6138        67.6887
      6      0.9883        0.0791       0.8375      0.8375        0.6110        67.6836
      7      0.9859        0.0852       0.8271      0.8271        0.6440        67.7446
      8      0.9847        0.0835       0.8293      0.8293        0.6198        67.7779
      9      0.9886        0.0734       0.8384      0.8384        0.6095        67.9754
     10      0.9893        0.0711       0.8321      0.8321        0.6438        68.1164
     11      0.9896        0.0687       0.8285      0.8285        0.6500        67.6636
     12      0.9904        0.0650       0.8332      0.8332        0.6465        67.7421
     13      0.9786        0.1028       0.8330      0.8330        0.6390        67.7444
     14      0.9928        0.0583       0.8264      0.8264        0.6685        67.7547
     15      0.9936        0.0550       0.8200      0.8200        0.7109        67.7792
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 12: 0.8208333333333333
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run3_s44\model_checkpoint_iteration_11.pt

Iteration:  13
Selecting 4056 informative samples:

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.9922        0.0549       0.8205      0.8205        0.7179     +  78.5084
      2      0.9947        0.0478       0.8085      0.8085        0.7795        78.5601
      3      0.9936        0.0485       0.8354      0.8354        0.6675     +  78.4785
      4      0.9949        0.0459       0.8177      0.8177        0.7419        78.4322
      5      0.9758        0.1052       0.8345      0.8345        0.6546     +  78.4365
      6      0.9898        0.0565       0.8299      0.8299        0.6870        78.4952
      7      0.9801        0.0869       0.8200      0.8200        0.7223        78.4828
      8      0.9938        0.0473       0.8203      0.8203        0.7359        78.4018
      9      0.9949        0.0411       0.8153      0.8153        0.7819        78.3540
     10      0.9961        0.0378       0.8149      0.8149        0.7739        78.4991
     11      0.9966        0.0355       0.8116      0.8116        0.8117        78.4734
     12      0.9972        0.0342       0.8165      0.8165        0.7885        78.4182
     13      0.9974        0.0325       0.8205      0.8205        0.7713        78.4960
     14      0.9974        0.0328       0.8175      0.8175        0.8056        78.4282
     15      0.9970        0.0329       0.8170      0.8170        0.7946        78.4505
     16      0.9976        0.0300       0.8156      0.8156        0.8296        78.5331
     17      0.9975        0.0296       0.8156      0.8156        0.8379        78.3854
     18      0.9978        0.0281       0.8094      0.8094        0.8692        83.5575
     19      0.9982        0.0269       0.8155      0.8155        0.8519        88.9646
Stopping since valid_loss has not improved in the last 15 epochs.
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run3_s44\model_checkpoint_iteration_12.pt
Performance results saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run3_s44\performance_results.npy
PS C:\Users\localuserSG\ActiveLearning\Multiclass01\DinoSmall\ES>
