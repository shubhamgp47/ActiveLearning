     74      0.2609        0.6012       0.3953      0.1799        0.6082  7.3110
     75      0.1961        0.5980       0.3951      0.1801        0.6078  7.3011
     76      0.2222        0.5977       0.3955      0.1803        0.6076  7.3285
     77      0.2456        0.6121       0.3955      0.1799        0.6072  7.3198
     78      0.3556        0.5998       0.3955      0.1801        0.6068  7.3248
     79      0.2963        0.5993       0.3950      0.1807        0.6064  7.3050
     80      0.1250        0.6028       0.3932      0.1817        0.6058  7.3285
     81      0.3294        0.6020       0.3939      0.1816        0.6055  7.3159
     82      0.2963        0.5888       0.3932      0.1826        0.6050  7.3123
     83      0.2456        0.6023       0.3944      0.1812        0.6049  7.2839
     84      0.2800        0.6149       0.3958      0.1808        0.6047  7.2948
     85      0.2222        0.5902       0.3958      0.1808        0.6042  7.2864
     86      0.5439        0.5943       0.3955      0.1810        0.6038  7.2882
     87      0.3074        0.5845       0.3953      0.1808        0.6034  7.2586
     88      0.2741        0.6030       0.3950      0.1808        0.6030  7.3096
     89      0.1754        0.5936       0.3948      0.1812        0.6028  7.2852
     90      0.2889        0.5817       0.3946      0.1817        0.6023  7.2799
     91      0.2333        0.5893       0.3950      0.1814        0.6020  7.3122
     92      0.3636        0.5935       0.3951      0.1812        0.6017  7.2722
     93      0.2772        0.5821       0.3950      0.1819        0.6013  7.3080
     94      0.2646        0.5945       0.3964      0.1816        0.6009  7.3185
     95      0.2333        0.5824       0.3970      0.1812        0.6006  7.3133
     96      0.2424        0.5850       0.3972      0.1811        0.6003  7.2905
     97      0.2540        0.5808       0.3969      0.1823        0.5997  7.2896
     98      0.2333        0.5941       0.3953      0.1831        0.5989  7.3060
     99      0.1754        0.5871       0.3946      0.1835        0.5984  7.2977
    100      0.2807        0.5753       0.3948      0.1837        0.5982  7.3035
Accuracy after query 1: 0.39565972222222223
F1 Score after query 1: 0.18109154489821408
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_0.pt

Query 2: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 2 is 32
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.2133        0.6129       0.3939      0.1857        0.5972  7.3695
      2      0.3056        0.6166       0.3931      0.1879        0.5961  7.3497
      3      0.3056        0.6230       0.3924      0.1895        0.5951  7.3672
      4      0.2446        0.6237       0.3913      0.1926        0.5940  7.3879
      5      0.2933        0.6165       0.3913      0.1935        0.5933  7.3868
      6      0.3012        0.6147       0.3911      0.1950        0.5924  7.3967
      7      0.2764        0.6173       0.3910      0.1967        0.5916  7.3704
      8      0.3015        0.6203       0.3905      0.1991        0.5908  7.3756
      9      0.3500        0.6052       0.3905      0.2015        0.5901  7.3825
     10      0.3168        0.6055       0.3901      0.2051        0.5892  7.3792
     11      0.3307        0.6078       0.3901      0.2068        0.5885  7.3840
     12      0.3121        0.6015       0.3908      0.2095        0.5877  7.3835
     13      0.3064        0.6075       0.3908      0.2123        0.5870  7.3848
     14      0.3639        0.6052       0.3908      0.2141        0.5863  7.3839
     15      0.3580        0.6092       0.3908      0.2144        0.5858  7.3748
     16      0.3210        0.6138       0.3908      0.2151        0.5852  7.3507
     17      0.2972        0.6098       0.3908      0.2166        0.5844  7.3658
     18      0.3524        0.6078       0.3908      0.2177        0.5837  7.3605
     19      0.2835        0.6144       0.3906      0.2194        0.5830  7.3606
     20      0.3526        0.6031       0.3906      0.2204        0.5824  7.3923
     21      0.2894        0.5970       0.3908      0.2222        0.5817  7.3886
     22      0.3369        0.6014       0.3910      0.2258        0.5810  7.3902
     23      0.3735        0.6032       0.3911      0.2272        0.5803  7.4031
     24      0.3704        0.6046       0.3911      0.2299        0.5795  7.4019
     25      0.3173        0.5962       0.3911      0.2333        0.5787  7.3925
     26      0.3895        0.5986       0.3917      0.2357        0.5780  7.3988
     27      0.3746        0.6058       0.3915      0.2376        0.5772  7.4079
     28      0.3732        0.6021       0.3915      0.2393        0.5765  7.3547
     29      0.3618        0.5933       0.3913      0.2442        0.5758  7.3741
     30      0.4040        0.6081       0.3924      0.2433        0.5752  7.3856
     31      0.4071        0.6010       0.3927      0.2477        0.5745  7.3959
     32      0.3099        0.5966       0.3920      0.2575        0.5737  7.3598
     33      0.4017        0.5952       0.3927      0.2617        0.5730  7.3552
     34      0.4276        0.5913       0.3934      0.2692        0.5722  7.3649
     35      0.4055        0.5906       0.3925      0.2754        0.5715  7.3398
     36      0.3951        0.5998       0.3929      0.2800        0.5707  7.3658
     37      0.3522        0.5822       0.3924      0.2823        0.5700  7.3909
     38      0.3356        0.5995       0.3903      0.2878        0.5692  7.3784
     39      0.4950        0.5880       0.3901      0.2905        0.5685  7.3796
     40      0.4286        0.5876       0.3918      0.2949        0.5677  7.3998
     41      0.3617        0.5917       0.3925      0.2977        0.5670  7.3873
     42      0.3370        0.5959       0.3913      0.3027        0.5661  7.3646
     43      0.3631        0.5868       0.3924      0.3050        0.5653  7.3734
     44      0.4041        0.5937       0.3911      0.3078        0.5644  7.3775
     45      0.3606        0.5894       0.3911      0.3108        0.5634  7.3750
     46      0.5084        0.5775       0.3908      0.3155        0.5624  7.3539
     47      0.4105        0.5826       0.3917      0.3170        0.5616  7.3673
     48      0.3538        0.5891       0.3922      0.3212        0.5607  7.3490
     49      0.4650        0.5771       0.3931      0.3233        0.5599  7.3530
     50      0.4990        0.5769       0.3929      0.3259        0.5591  7.3401
     51      0.4797        0.5843       0.3953      0.3270        0.5583  7.3417
     52      0.5010        0.5785       0.3958      0.3289        0.5576  7.3613
     53      0.4048        0.5786       0.3953      0.3322        0.5566  7.3817
     54      0.4369        0.5789       0.3948      0.3382        0.5554  7.3857
     55      0.4856        0.5733       0.3932      0.3419        0.5541  7.3872
     56      0.4010        0.5830       0.3960      0.3423        0.5532  7.3692
     57      0.3523        0.5766       0.3972      0.3439        0.5524  7.3814
     58      0.5186        0.5667       0.3983      0.3476        0.5515  7.3838
     59      0.5424        0.5640       0.3984      0.3510        0.5504  7.3685
     60      0.4444        0.5705       0.3984      0.3556        0.5495  7.3791
     61      0.4299        0.5715       0.3974      0.3625        0.5486  7.3900
     62      0.4243        0.5736       0.3960      0.3700        0.5475  7.3824
     63      0.4119        0.5697       0.3990      0.3717        0.5466  7.3855
     64      0.4829        0.5639       0.4050      0.3673        0.5460  7.3830
     65      0.4474        0.5725       0.4062      0.3695        0.5451  7.3543
     66      0.5689        0.5715       0.4064      0.3761        0.5440  7.3425
     67      0.4815        0.5621       0.4068      0.3801        0.5430  7.3596
     68      0.5147        0.5660       0.4066      0.3841        0.5420  7.3881
     69      0.5328        0.5725       0.4071      0.3895        0.5411  7.3918
     70      0.5084        0.5676       0.4073      0.3944        0.5402  7.3869
     71      0.5146        0.5500       0.4082      0.3965        0.5391  7.4056
     72      0.5970        0.5567       0.4095      0.4015        0.5382  7.3865
     73      0.5509        0.5634       0.4083      0.4090        0.5369  7.3825
     74      0.4095        0.5650       0.4102      0.4116        0.5360  7.4045
     75      0.6024        0.5510       0.4118      0.4109        0.5350  7.3995
     76      0.4886        0.5588       0.4127      0.4120        0.5340  7.3661
     77      0.4713        0.5604       0.4149      0.4126        0.5332  7.3843
     78      0.5517        0.5561       0.4144      0.4210        0.5321  7.3908
     79      0.6428        0.5470       0.4151      0.4249        0.5311  7.3918
     80      0.6361        0.5469       0.4165      0.4290        0.5301  7.3732
     81      0.5269        0.5471       0.4184      0.4342        0.5292  7.3458
     82      0.5862        0.5547       0.4189      0.4407        0.5281  7.3444
     83      0.5912        0.5526       0.4186      0.4466        0.5270  7.3898
     84      0.5306        0.5462       0.4207      0.4554        0.5260  7.4587
     85      0.5566        0.5423       0.4243      0.4550        0.5252  7.3853
     86      0.6478        0.5496       0.4247      0.4558        0.5242  7.4362
     87      0.5563        0.5384       0.4260      0.4624        0.5230  7.3675
     88      0.5044        0.5572       0.4283      0.4684        0.5220  7.3840
     89      0.6667        0.5425       0.4300      0.4740        0.5210  7.3470
     90      0.5466        0.5438       0.4313      0.4762        0.5201  7.3233
     91      0.5591        0.5483       0.4316      0.4779        0.5192  7.3619
     92      0.5010        0.5481       0.4319      0.4781        0.5184  7.3489
     93      0.6020        0.5447       0.4333      0.4760        0.5175  7.3248
     94      0.5972        0.5379       0.4354      0.4844        0.5164  7.3839
     95      0.5912        0.5376       0.4375      0.4860        0.5156  7.3411
     96      0.6555        0.5293       0.4392      0.4877        0.5145  7.3518
     97      0.6052        0.5393       0.4399      0.4891        0.5136  7.3243
     98      0.7075        0.5295       0.4413      0.4934        0.5126  7.3196
     99      0.5799        0.5390       0.4437      0.5029        0.5115  7.3281
    100      0.5945        0.5339       0.4457      0.5072        0.5104  7.3068
Accuracy after query 2: 0.48333333333333334
F1 Score after query 2: 0.5387484097777866
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_1.pt

Query 3: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 3 is 56
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.6359        0.5401       0.4472      0.5056        0.5092  7.5197
      2      0.5785        0.5386       0.4505      0.5039        0.5080  7.5143
      3      0.5070        0.5415       0.4569      0.5040        0.5068  7.4990
      4      0.5499        0.5417       0.4623      0.5046        0.5054  7.4959
      5      0.5632        0.5362       0.4646      0.5092        0.5040  7.4937
      6      0.5968        0.5401       0.4660      0.5157        0.5023  7.4927
      7      0.6138        0.5381       0.4701      0.5226        0.5008  7.4983
      8      0.5808        0.5338       0.4731      0.5266        0.4993  7.5075
      9      0.5238        0.5394       0.4793      0.5298        0.4980  7.4876
     10      0.5394        0.5344       0.4863      0.5309        0.4970  7.4944
     11      0.5873        0.5353       0.4932      0.5365        0.4956  7.5049
     12      0.6246        0.5286       0.4950      0.5439        0.4939  7.4630
     13      0.5836        0.5313       0.4986      0.5479        0.4925  7.4905
     14      0.5779        0.5306       0.5017      0.5569        0.4908  7.4726
     15      0.6580        0.5254       0.5056      0.5618        0.4895  7.4823
     16      0.7002        0.5238       0.5097      0.5669        0.4883  7.4938
     17      0.5840        0.5305       0.5125      0.5719        0.4869  7.5070
     18      0.5541        0.5292       0.5182      0.5820        0.4855  7.5202
     19      0.5841        0.5226       0.5222      0.5911        0.4842  7.5070
     20      0.5947        0.5239       0.5260      0.5960        0.4828  7.4918
     21      0.5931        0.5249       0.5314      0.6041        0.4815  7.5104
     22      0.6372        0.5283       0.5345      0.6113        0.4802  7.5846
     23      0.6805        0.5167       0.5375      0.6171        0.4789  7.6243
     24      0.6872        0.5185       0.5418      0.6243        0.4776  7.6199
     25      0.6392        0.5142       0.5457      0.6313        0.4763  7.6202
     26      0.6986        0.5155       0.5488      0.6317        0.4753  7.6177
     27      0.7004        0.5075       0.5536      0.6382        0.4741  7.6047
     28      0.6375        0.5152       0.5576      0.6471        0.4726  7.6190
     29      0.7106        0.5111       0.5620      0.6531        0.4716  7.6303
     30      0.6432        0.5133       0.5655      0.6576        0.4706  7.6059
     31      0.6449        0.5132       0.5691      0.6623        0.4695  7.5213
     32      0.7409        0.5045       0.5738      0.6730        0.4679  7.5108
     33      0.6630        0.5120       0.5753      0.6752        0.4667  7.4968
     34      0.6481        0.5123       0.5806      0.6837        0.4655  7.5237
     35      0.7023        0.5031       0.5837      0.6876        0.4644  7.5359
     36      0.6913        0.5051       0.5868      0.6924        0.4631  7.5018
     37      0.6385        0.5054       0.5908      0.6969        0.4619  7.5044
     38      0.6952        0.5042       0.5938      0.7008        0.4606  7.4870
     39      0.7086        0.5040       0.5955      0.7026        0.4597  7.5042
     40      0.6663        0.5051       0.5990      0.7067        0.4584  7.4985
     41      0.7048        0.5015       0.6028      0.7122        0.4573  7.4832
     42      0.6730        0.4999       0.6078      0.7183        0.4562  7.4806
     43      0.6884        0.4994       0.6064      0.7175        0.4553  7.5079
     44      0.6864        0.4937       0.6090      0.7211        0.4542  7.4654
     45      0.7410        0.4940       0.6111      0.7228        0.4532  7.4726
     46      0.7190        0.5019       0.6168      0.7286        0.4521  7.4663
     47      0.6539        0.4964       0.6193      0.7310        0.4511  7.4694
     48      0.7212        0.4941       0.6224      0.7347        0.4502  7.5226
     49      0.7357        0.4923       0.6273      0.7407        0.4490  7.4961
     50      0.7173        0.4968       0.6351      0.7506        0.4477  7.5047
     51      0.7588        0.4883       0.6389      0.7543        0.4466  7.5200
     52      0.6883        0.4876       0.6429      0.7579        0.4456  7.4743
     53      0.7437        0.4934       0.6469      0.7621        0.4446  7.4990
     54      0.6644        0.4906       0.6498      0.7658        0.4436  7.5158
     55      0.6975        0.4862       0.6523      0.7675        0.4427  7.4961
     56      0.7154        0.4922       0.6556      0.7715        0.4417  7.4990
     57      0.7597        0.4853       0.6573      0.7727        0.4407  7.5001
     58      0.7597        0.4825       0.6613      0.7759        0.4397  7.5113
     59      0.7748        0.4827       0.6630      0.7775        0.4389  7.4931
     60      0.7389        0.4797       0.6644      0.7787        0.4379  7.4800
     61      0.7631        0.4831       0.6642      0.7783        0.4371  7.5130
     62      0.7465        0.4842       0.6681      0.7816        0.4362  7.4827
     63      0.7567        0.4810       0.6710      0.7842        0.4353  7.4790
     64      0.7291        0.4847       0.6722      0.7856        0.4343  7.5010
     65      0.8046        0.4754       0.6748      0.7877        0.4332  7.5275
     66      0.7267        0.4768       0.6753      0.7894        0.4324  7.5052
     67      0.7642        0.4856       0.6766      0.7915        0.4314  7.5164
     68      0.7662        0.4787       0.6757      0.7897        0.4308  7.5002
     69      0.7622        0.4804       0.6769      0.7908        0.4300  7.5228
     70      0.7539        0.4713       0.6783      0.7928        0.4289  7.5087
     71      0.7740        0.4785       0.6793      0.7931        0.4281  7.5052
     72      0.7841        0.4758       0.6778      0.7922        0.4275  7.5103
     73      0.7689        0.4795       0.6814      0.7955        0.4265  7.4932
     74      0.7782        0.4782       0.6844      0.7989        0.4255  7.5113
     75      0.7597        0.4734       0.6854      0.7999        0.4246  7.4975
     76      0.8152        0.4725       0.6854      0.8001        0.4239  7.4741
     77      0.7915        0.4684       0.6854      0.8003        0.4231  7.4707
     78      0.6981        0.4732       0.6873      0.8017        0.4223  7.4795
     79      0.8070        0.4666       0.6870      0.8022        0.4214  7.4823
     80      0.7739        0.4664       0.6878      0.8029        0.4207  7.4820
     81      0.7708        0.4723       0.6884      0.8026        0.4203  7.4739
     82      0.7903        0.4669       0.6887      0.8029        0.4196  7.5005
     83      0.7433        0.4680       0.6901      0.8043        0.4188  7.5060
     84      0.7616        0.4638       0.6901      0.8042        0.4182  7.4765
     85      0.7488        0.4661       0.6908      0.8046        0.4175  7.4887
     86      0.8140        0.4637       0.6920      0.8052        0.4167  7.5024
     87      0.7553        0.4620       0.6927      0.8060        0.4161  7.4980
     88      0.7959        0.4599       0.6946      0.8079        0.4153  7.4996
     89      0.8329        0.4612       0.6950      0.8081        0.4146  7.5016
     90      0.7895        0.4678       0.6951      0.8089        0.4139  7.4803
     91      0.7725        0.4576       0.6953      0.8095        0.4131  7.4788
     92      0.8179        0.4591       0.6957      0.8100        0.4125  7.4605
     93      0.7361        0.4608       0.6967      0.8109        0.4118  7.4640
     94      0.8072        0.4529       0.6970      0.8111        0.4112  7.4673
     95      0.8246        0.4562       0.6970      0.8113        0.4106  7.4857
     96      0.8315        0.4581       0.6979      0.8122        0.4099  7.5134
     97      0.8184        0.4548       0.6981      0.8124        0.4093  7.5355
     98      0.8126        0.4536       0.6972      0.8116        0.4086  7.5137
     99      0.8115        0.4546       0.6981      0.8131        0.4079  7.4941
    100      0.8321        0.4575       0.6988      0.8139        0.4073  7.5070
Accuracy after query 3: 0.7144097222222222
F1 Score after query 3: 0.8343892392395021
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_2.pt

Query 4: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 4 is 96
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.7917        0.4532       0.6986      0.8141        0.4057  7.7547
      2      0.7990        0.4535       0.6974      0.8134        0.4045  7.7736
      3      0.8143        0.4511       0.6977      0.8142        0.4030  7.7722
      4      0.8282        0.4479       0.6974      0.8145        0.4016  7.7557
      5      0.8471        0.4464       0.6977      0.8166        0.4000  7.7530
      6      0.8276        0.4455       0.6979      0.8170        0.3986  7.7607
      7      0.8247        0.4417       0.6995      0.8187        0.3974  7.7455
      8      0.8361        0.4412       0.6986      0.8181        0.3964  7.7577
      9      0.8271        0.4434       0.6991      0.8193        0.3951  7.7327
     10      0.8311        0.4391       0.6981      0.8198        0.3939  7.7616
     11      0.8068        0.4406       0.6967      0.8193        0.3929  7.7800
     12      0.8289        0.4371       0.6944      0.8187        0.3919  7.7768
     13      0.7894        0.4381       0.6941      0.8180        0.3911  7.7666
     14      0.8612        0.4344       0.6920      0.8177        0.3902  7.7629
     15      0.8660        0.4321       0.6925      0.8185        0.3889  7.7820
     16      0.8544        0.4332       0.6920      0.8185        0.3879  7.7743
     17      0.8387        0.4313       0.6924      0.8193        0.3869  7.7905
     18      0.8634        0.4330       0.6910      0.8194        0.3859  7.7622
     19      0.8632        0.4304       0.6908      0.8192        0.3851  7.7482
     20      0.8585        0.4231       0.6925      0.8203        0.3839  7.7703
     21      0.8588        0.4281       0.6913      0.8198        0.3831  7.7590
     22      0.8632        0.4264       0.6915      0.8201        0.3823  7.7382
     23      0.8395        0.4284       0.6899      0.8194        0.3817  7.7423
     24      0.8582        0.4259       0.6910      0.8202        0.3809  7.7489
     25      0.8728        0.4231       0.6908      0.8203        0.3800  7.7499
     26      0.8527        0.4221       0.6906      0.8202        0.3794  7.7794
     27      0.8814        0.4205       0.6920      0.8210        0.3784  7.7689
     28      0.8657        0.4204       0.6943      0.8227        0.3774  7.7650
     29      0.8496        0.4195       0.6934      0.8220        0.3769  7.7531
     30      0.8651        0.4186       0.6925      0.8217        0.3763  7.7669
     31      0.8579        0.4187       0.6917      0.8214        0.3758  7.7495
     32      0.8634        0.4157       0.6946      0.8231        0.3748  7.7625
     33      0.8605        0.4196       0.6946      0.8230        0.3742  7.7400
     34      0.8821        0.4140       0.6946      0.8234        0.3736  7.7686
     35      0.8835        0.4123       0.6948      0.8237        0.3730  7.7564
     36      0.8636        0.4132       0.6939      0.8233        0.3726  7.7661
     37      0.8453        0.4137       0.6936      0.8234        0.3721  7.7428
     38      0.8636        0.4118       0.6953      0.8245        0.3713  7.7266
     39      0.8603        0.4120       0.6951      0.8243        0.3708  7.7252
     40      0.8669        0.4091       0.6950      0.8245        0.3702  7.7335
     41      0.8761        0.4116       0.6950      0.8246        0.3696  7.7461
     42      0.8719        0.4143       0.6957      0.8246        0.3693  7.7739
     43      0.8712        0.4117       0.6943      0.8243        0.3687  7.7817
     44      0.8875        0.4060       0.6941      0.8239        0.3683  7.7532
     45      0.8657        0.4056       0.6939      0.8238        0.3680  7.7705
     46      0.8799        0.4071       0.6944      0.8242        0.3674  7.7811
     47      0.8816        0.4039       0.6943      0.8243        0.3669  7.7497
     48      0.8811        0.4026       0.6944      0.8244        0.3664  7.7500
     49      0.8829        0.4012       0.6948      0.8243        0.3660  7.7849
     50      0.8659        0.4017       0.6934      0.8238        0.3657  7.7668
     51      0.8760        0.3995       0.6943      0.8243        0.3652  7.7533
     52      0.8815        0.4007       0.6948      0.8244        0.3647  7.7570
     53      0.8827        0.4027       0.6943      0.8252        0.3641  7.7353
     54      0.8874        0.3957       0.6943      0.8246        0.3637  7.7401
     55      0.8894        0.3970       0.6941      0.8247        0.3634  7.7604
     56      0.8644        0.3990       0.6943      0.8247        0.3629  7.7559
     57      0.8857        0.3979       0.6944      0.8249        0.3624  7.7743
     58      0.8825        0.3977       0.6950      0.8253        0.3618  7.7838
     59      0.8866        0.3970       0.6951      0.8261        0.3614  7.7529
     60      0.8806        0.3977       0.6953      0.8263        0.3610  7.7651
     61      0.8650        0.3954       0.6953      0.8265        0.3606  7.7829
     62      0.8916        0.3946       0.6936      0.8248        0.3604  7.7931
     63      0.8664        0.3949       0.6946      0.8253        0.3600  7.7740
     64      0.8796        0.3952       0.6946      0.8255        0.3595  7.7726
     65      0.8861        0.3910       0.6943      0.8252        0.3592  7.7649
     66      0.8774        0.3911       0.6934      0.8250        0.3589  7.7492
     67      0.8841        0.3899       0.6957      0.8264        0.3582  7.7560
     68      0.8936        0.3862       0.6950      0.8262        0.3579  7.7440
     69      0.8938        0.3889       0.6946      0.8259        0.3576  7.7361
     70      0.8706        0.3905       0.6951      0.8265        0.3572  7.7356
     71      0.8704        0.3908       0.6941      0.8256        0.3570  7.7542
     72      0.8962        0.3864       0.6950      0.8263        0.3565  7.7329
     73      0.8873        0.3874       0.6932      0.8251        0.3563  7.7585
     74      0.8848        0.3883       0.6941      0.8256        0.3560  7.7741
     75      0.8835        0.3844       0.6937      0.8254        0.3557  7.7494
     76      0.8789        0.3841       0.6951      0.8268        0.3552  7.7638
     77      0.8981        0.3822       0.6922      0.8251        0.3552  7.7585
     78      0.9049        0.3856       0.6920      0.8251        0.3548  7.7672
     79      0.9037        0.3811       0.6910      0.8246        0.3547  7.7492
     80      0.8993        0.3827       0.6939      0.8262        0.3540  7.7273
     81      0.9022        0.3823       0.6936      0.8260        0.3537  7.7732
     82      0.8904        0.3817       0.6924      0.8256        0.3535  7.7568
     83      0.9065        0.3767       0.6937      0.8263        0.3532  7.7185
     84      0.8827        0.3802       0.6913      0.8248        0.3531  7.7183
     85      0.8783        0.3803       0.6917      0.8250        0.3527  7.7404
     86      0.8879        0.3819       0.6915      0.8249        0.3525  7.7147
     87      0.8830        0.3782       0.6894      0.8235        0.3524  7.7576
     88      0.8940        0.3789       0.6913      0.8247        0.3520  7.7598
     89      0.8907        0.3739       0.6911      0.8251        0.3516  7.7785
     90      0.8975        0.3753       0.6937      0.8263        0.3512  7.7769
     91      0.8962        0.3769       0.6934      0.8261        0.3510  7.7590
     92      0.8922        0.3766       0.6920      0.8253        0.3508  7.7668
     93      0.8949        0.3738       0.6913      0.8251        0.3505  7.7530
     94      0.9005        0.3755       0.6905      0.8249        0.3504  7.7582
     95      0.8864        0.3766       0.6891      0.8235        0.3503  7.7535
     96      0.8848        0.3754       0.6891      0.8236        0.3501  7.7692
     97      0.8964        0.3735       0.6859      0.8217        0.3500  7.7724
     98      0.8908        0.3708       0.6894      0.8243        0.3493  7.7477
     99      0.8902        0.3742       0.6918      0.8257        0.3487  7.7521
    100      0.8950        0.3713       0.6913      0.8253        0.3484  7.7432
Accuracy after query 4: 0.7270833333333333
F1 Score after query 4: 0.8425704231114309
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_3.pt

Query 5: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 5 is 176
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.8680        0.3804       0.6813      0.8189        0.3493  8.1942
      2      0.8724        0.3784       0.6845      0.8213        0.3483  8.2415
      3      0.8726        0.3767       0.6844      0.8215        0.3477  8.2342
      4      0.8796        0.3724       0.6840      0.8211        0.3473  8.2571
      5      0.8810        0.3745       0.6837      0.8209        0.3470  8.2551
      6      0.8780        0.3737       0.6854      0.8226        0.3460  8.2341
      7      0.8753        0.3735       0.6859      0.8229        0.3454  8.2334
      8      0.8733        0.3742       0.6821      0.8198        0.3458  8.2510
      9      0.8775        0.3709       0.6828      0.8206        0.3452  8.2349
     10      0.8740        0.3719       0.6861      0.8234        0.3441  8.2358
     11      0.8754        0.3687       0.6868      0.8237        0.3436  8.2355
     12      0.8778        0.3709       0.6885      0.8248        0.3429  8.2194
     13      0.8715        0.3703       0.6880      0.8248        0.3426  8.1895
     14      0.8759        0.3694       0.6870      0.8239        0.3424  8.2274
     15      0.8745        0.3693       0.6884      0.8250        0.3418  8.2161
     16      0.8792        0.3678       0.6859      0.8235        0.3418  8.2280
     17      0.8796        0.3677       0.6859      0.8236        0.3414  8.2232
     18      0.8819        0.3659       0.6875      0.8247        0.3407  8.2216
     19      0.8814        0.3660       0.6872      0.8246        0.3405  8.2185
     20      0.8731        0.3662       0.6884      0.8253        0.3398  8.2343
     21      0.8681        0.3630       0.6894      0.8267        0.3389  8.2191
     22      0.8777        0.3640       0.6887      0.8255        0.3390  8.2213
     23      0.8771        0.3634       0.6882      0.8254        0.3387  8.2152
     24      0.8740        0.3624       0.6873      0.8250        0.3385  8.2352
     25      0.8746        0.3631       0.6884      0.8255        0.3382  8.2265
     26      0.8739        0.3615       0.6877      0.8252        0.3378  8.2305
     27      0.8793        0.3598       0.6870      0.8248        0.3377  8.2105
     28      0.8816        0.3577       0.6896      0.8264        0.3372  8.2074
     29      0.8780        0.3605       0.6896      0.8263        0.3368  8.1979
     30      0.8745        0.3579       0.6870      0.8249        0.3369  8.1860
     31      0.8783        0.3592       0.6891      0.8260        0.3364  8.2312
     32      0.8806        0.3568       0.6898      0.8269        0.3359  8.2361
     33      0.8700        0.3597       0.6882      0.8262        0.3357  8.2597
     34      0.8788        0.3567       0.6908      0.8280        0.3349  8.2768
     35      0.8730        0.3571       0.6911      0.8280        0.3346  8.2287
     36      0.8811        0.3581       0.6901      0.8275        0.3347  10.5219
     37      0.8754        0.3577       0.6872      0.8255        0.3349  13.5124
     46      0.8852        0.3499       0.6908      0.8285        0.3318  13.5693
     47      0.8810        0.3499       0.6915      0.8291        0.3312  13.5054
     48      0.8807        0.3508       0.6901      0.8282        0.3313  13.4789
     49      0.8732        0.3515       0.6887      0.8275        0.3311  13.4900
     50      0.8843        0.3496       0.6917      0.8291        0.3304  13.4145
     51      0.8751        0.3477       0.6920      0.8293        0.3301  13.4298
     52      0.8729        0.3479       0.6891      0.8278        0.3304  13.5392
     53      0.8846        0.3470       0.6894      0.8281        0.3301  13.5355
     54      0.8857        0.3457       0.6894      0.8281        0.3298  9.0083
     55      0.8833        0.3474       0.6910      0.8294        0.3292  8.1034
     56      0.8768        0.3477       0.6911      0.8294        0.3289  8.3636
     57      0.8808        0.3461       0.6920      0.8304        0.3282  8.1780
     58      0.8814        0.3461       0.6932      0.8314        0.3276  8.2495
     59      0.8850        0.3463       0.6906      0.8295        0.3279  8.1854
     60      0.8808        0.3434       0.6925      0.8312        0.3271  8.1613
     61      0.8793        0.3434       0.6922      0.8310        0.3270  8.1806
     62      0.8799        0.3434       0.6929      0.8313        0.3267  8.1755
     63      0.8808        0.3464       0.6922      0.8309        0.3266  8.2412
     64      0.8889        0.3436       0.6929      0.8313        0.3263  8.2413
     65      0.8805        0.3433       0.6917      0.8306        0.3263  8.2197
     66      0.8837        0.3423       0.6934      0.8316        0.3256  8.2476
     67      0.8785        0.3412       0.6920      0.8311        0.3256  8.2469
     68      0.8814        0.3413       0.6927      0.8315        0.3251  8.2539
     69      0.8813        0.3420       0.6924      0.8315        0.3250  8.2465
     70      0.8838        0.3402       0.6917      0.8311        0.3248  8.2319
     71      0.8856        0.3410       0.6950      0.8325        0.3241  8.2377
     72      0.8782        0.3375       0.6927      0.8319        0.3240  8.2546
     73      0.8779        0.3419       0.6911      0.8305        0.3242  8.2357
     74      0.8761        0.3399       0.6924      0.8315        0.3236  8.2357
     75      0.8817        0.3390       0.6944      0.8325        0.3232  8.2436
     76      0.8859        0.3392       0.6932      0.8320        0.3230  8.2077
     77      0.8829        0.3382       0.6931      0.8319        0.3228  8.2361
     78      0.8813        0.3395       0.6920      0.8317        0.3227  8.2445
     79      0.8856        0.3363       0.6925      0.8317        0.3225  8.2435
     80      0.8835        0.3380       0.6943      0.8332        0.3220  8.2099
     81      0.8815        0.3371       0.6957      0.8341        0.3216  8.2400
     82      0.8783        0.3368       0.6937      0.8329        0.3215  8.2465
     83      0.8797        0.3363       0.6946      0.8337        0.3211  8.2365
     84      0.8920        0.3347       0.6953      0.8340        0.3208  8.2482
     85      0.8871        0.3327       0.6955      0.8341        0.3206  8.2406
     86      0.8822        0.3343       0.6934      0.8324        0.3205  8.2431
     87      0.8838        0.3335       0.6960      0.8349        0.3199  8.2383
     88      0.8813        0.3351       0.6957      0.8346        0.3197  8.2215
     89      0.8846        0.3339       0.6937      0.8330        0.3196  8.2035
     90      0.8810        0.3360       0.6958      0.8350        0.3192  8.2210
     91      0.8840        0.3335       0.6974      0.8360        0.3187  8.2006
     92      0.8798        0.3324       0.6948      0.8338        0.3188  8.2260
     93      0.8805        0.3332       0.6955      0.8347        0.3185  8.2259
     94      0.8876        0.3317       0.6955      0.8349        0.3183  8.2237
     95      0.8878        0.3302       0.6970      0.8361        0.3178  8.2341
     96      0.8782        0.3312       0.6967      0.8357        0.3177  8.2199
     97      0.8898        0.3299       0.6970      0.8362        0.3174  8.2324
     98      0.8847        0.3299       0.6934      0.8334        0.3174  8.2095
     99      0.8828        0.3295       0.6977      0.8364        0.3168  8.2390
    100      0.8757        0.3309       0.6984      0.8373        0.3167  8.2237
Accuracy after query 5: 0.7286458333333333
F1 Score after query 5: 0.8470379036395711
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_4.pt

Query 6: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 6 is 320
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.8704        0.3381       0.6913      0.8315        0.3166  9.3004
      2      0.8681        0.3387       0.6924      0.8319        0.3152  9.0748
      3      0.8677        0.3374       0.6937      0.8327        0.3141  9.0559
      4      0.8699        0.3367       0.6967      0.8351        0.3129  9.0328
      5      0.8732        0.3350       0.7005      0.8371        0.3117  9.0764
      6      0.8743        0.3348       0.6965      0.8344        0.3116  9.0753
      7      0.8712        0.3331       0.6979      0.8353        0.3107  9.0876
      8      0.8733        0.3337       0.6977      0.8345        0.3103  9.0996
      9      0.8694        0.3319       0.7007      0.8369        0.3093  9.0644
     10      0.8677        0.3308       0.6983      0.8347        0.3092  9.0832
     11      0.8713        0.3308       0.7019      0.8377        0.3083  9.0799
     12      0.8709        0.3317       0.7014      0.8369        0.3079  9.0857
     13      0.8742        0.3299       0.7031      0.8384        0.3069  9.1048
     14      0.8802        0.3278       0.7047      0.8404        0.3062  9.0746
     15      0.8751        0.3301       0.7033      0.8388        0.3061  9.0847
     16      0.8828        0.3284       0.7036      0.8392        0.3055  9.0652
     17      0.8749        0.3269       0.7021      0.8384        0.3055  9.0773
     18      0.8748        0.3271       0.6972      0.8360        0.3063  9.0696
     19      0.8780        0.3247       0.7031      0.8389        0.3045  9.1079
     20      0.8764        0.3242       0.7092      0.8431        0.3031  9.0966
     21      0.8746        0.3253       0.7071      0.8419        0.3029  9.0886
     22      0.8761        0.3234       0.7083      0.8428        0.3022  9.1155
     23      0.8745        0.3247       0.7095      0.8438        0.3015  9.1046
     24      0.8813        0.3215       0.7082      0.8430        0.3010  9.0917
     25      0.8781        0.3227       0.7106      0.8446        0.3004  9.0829
     26      0.8797        0.3211       0.7184      0.8480        0.2992  9.0829
     27      0.8854        0.3209       0.7149      0.8473        0.2992  9.0915
     28      0.8802        0.3215       0.7082      0.8428        0.2998  9.1058
     29      0.8841        0.3204       0.7137      0.8469        0.2983  9.0487
     30      0.8788        0.3204       0.7156      0.8480        0.2977  9.0757
     31      0.8815        0.3190       0.7174      0.8488        0.2971  9.0723
     32      0.8819        0.3193       0.7191      0.8502        0.2963  9.0723
     33      0.8865        0.3180       0.7194      0.8499        0.2960  9.0744
     34      0.8825        0.3164       0.7189      0.8500        0.2956  9.0675
     35      0.8823        0.3172       0.7200      0.8502        0.2951  9.0614
     36      0.8812        0.3166       0.7165      0.8487        0.2949  9.0732
     37      0.8814        0.3163       0.7172      0.8490        0.2944  9.0953
     38      0.8856        0.3142       0.7168      0.8486        0.2937  9.0853
     39      0.8863        0.3138       0.7220      0.8523        0.2926  9.0776
     40      0.8864        0.3135       0.7207      0.8512        0.2924  9.0673
     41      0.8858        0.3148       0.7186      0.8497        0.2925  9.0625
     42      0.8873        0.3128       0.7238      0.8532        0.2911  9.0654
     43      0.8864        0.3107       0.7170      0.8485        0.2915  9.0373
     44      0.8850        0.3109       0.7248      0.8532        0.2902  9.0601
     45      0.8858        0.3118       0.7238      0.8529        0.2898  9.1162
     46      0.8856        0.3104       0.7243      0.8532        0.2895  9.0849
     47      0.8869        0.3104       0.7210      0.8516        0.2895  9.1014
     48      0.8875        0.3085       0.7214      0.8519        0.2889  9.1240
     49      0.8876        0.3079       0.7219      0.8521        0.2881  9.0823
     50      0.8798        0.3094       0.7182      0.8498        0.2886  9.0892
     51      0.8924        0.3070       0.7196      0.8506        0.2879  9.0638
     52      0.8891        0.3072       0.7207      0.8509        0.2874  9.1030
     53      0.8806        0.3062       0.7193      0.8500        0.2871  9.0847
     54      0.8918        0.3061       0.7312      0.8573        0.2850  9.1001
     55      0.8854        0.3058       0.7276      0.8548        0.2850  9.0599
     56      0.8855        0.3063       0.7245      0.8530        0.2850  9.0692
     57      0.8851        0.3056       0.7207      0.8506        0.2857  9.0546
     58      0.8929        0.3035       0.7155      0.8465        0.2859  9.0786
     59      0.8907        0.3061       0.7266      0.8544        0.2834  9.0949
     60      0.8914        0.3047       0.7290      0.8559        0.2827  9.1070
     61      0.8896        0.3034       0.7229      0.8517        0.2835  9.0829
     62      0.8921        0.3034       0.7189      0.8486        0.2837  9.0959
     63      0.8877        0.3035       0.7266      0.8547        0.2818  9.0941
     64      0.8884        0.3005       0.7299      0.8563        0.2807  9.0887
     65      0.8886        0.3028       0.7220      0.8502        0.2820  9.0692
     66      0.8913        0.3007       0.7278      0.8551        0.2800  9.0951
     67      0.8890        0.3008       0.7264      0.8538        0.2802  9.0896
     68      0.8908        0.2996       0.7248      0.8521        0.2804  9.0631
     69      0.8886        0.3011       0.7300      0.8560        0.2787  9.0460
     70      0.8876        0.3001       0.7269      0.8531        0.2789  9.0679
     71      0.8937        0.2976       0.7299      0.8555        0.2774  9.0639
     72      0.8958        0.2965       0.7214      0.8482        0.2789  9.0846
     73      0.8900        0.2970       0.7245      0.8510        0.2782  9.0854
     74      0.8959        0.2966       0.7269      0.8527        0.2772  9.0830
     75      0.8955        0.2952       0.7297      0.8555        0.2757  9.0757
     76      0.8914        0.2953       0.7217      0.8481        0.2775  9.0816
     77      0.8960        0.2939       0.7340      0.8586        0.2742  9.0825
     78      0.8944        0.2949       0.7278      0.8531        0.2750  9.0884
     79      0.8902        0.2947       0.7281      0.8540        0.2748  9.0678
     80      0.8948        0.2955       0.7280      0.8535        0.2744  9.0930
     81      0.8989        0.2932       0.7280      0.8525        0.2744  9.0484
     82      0.8974        0.2920       0.7193      0.8447        0.2759  9.0412
     83      0.8942        0.2935       0.7300      0.8535        0.2733  9.0489
     84      0.8975        0.2912       0.7231      0.8471        0.2745  9.0707
     85      0.8992        0.2918       0.7297      0.8535        0.2725  9.0961
     86      0.8964        0.2897       0.7323      0.8555        0.2715  9.0632
     87      0.8990        0.2899       0.7309      0.8538        0.2714  9.0867
     88      0.8998        0.2900       0.7352      0.8581        0.2700  9.0879
     89      0.9021        0.2904       0.7257      0.8495        0.2720  9.0631
     90      0.9025        0.2886       0.7323      0.8555        0.2701  9.1026
     91      0.9028        0.2880       0.7269      0.8511        0.2714  9.0897
     92      0.8995        0.2892       0.7321      0.8544        0.2693  9.0904
     93      0.9019        0.2877       0.7332      0.8556        0.2682  9.1099
     94      0.9065        0.2866       0.7394      0.8616        0.2666  9.0541
     95      0.9004        0.2852       0.7241      0.8467        0.2699  9.0849
     96      0.9063        0.2856       0.7316      0.8526        0.2677  9.0641
     97      0.8988        0.2872       0.7337      0.8557        0.2662  9.0716
     98      0.9054        0.2850       0.7309      0.8521        0.2673  9.0868
     99      0.9023        0.2847       0.7309      0.8522        0.2671  9.1105
    100      0.9023        0.2853       0.7392      0.8602        0.2646  9.0867
Accuracy after query 6: 0.7550347222222222
F1 Score after query 6: 0.8648344287069193
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_5.pt

Query 7: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 7 is 560
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.8948        0.2886       0.7495      0.8687        0.2624  10.7692
      2      0.8971        0.2889       0.7472      0.8664        0.2617  10.5601
      3      0.9002        0.2874       0.7519      0.8689        0.2604  10.5506
      4      0.9005        0.2863       0.7474      0.8659        0.2600  10.5482
      5      0.9053        0.2847       0.7512      0.8685        0.2587  10.5465
      6      0.9022        0.2850       0.7542      0.8701        0.2576  10.5255
      7      0.9017        0.2837       0.7557      0.8713        0.2566  10.5411
      8      0.9014        0.2830       0.7535      0.8697        0.2565  10.5449
      9      0.8999        0.2837       0.7533      0.8683        0.2557  10.5706
     10      0.9014        0.2819       0.7542      0.8694        0.2548  10.5467
     11      0.8999        0.2812       0.7557      0.8705        0.2540  10.5815
     12      0.9071        0.2792       0.7556      0.8706        0.2535  10.5841
     13      0.9050        0.2793       0.7559      0.8698        0.2528  10.5595
     14      0.9044        0.2793       0.7580      0.8718        0.2519  10.5509
     15      0.9069        0.2779       0.7587      0.8719        0.2512  10.5779
     16      0.9028        0.2771       0.7620      0.8735        0.2500  10.5403
     17      0.9081        0.2752       0.7615      0.8733        0.2494  10.5455
     18      0.9076        0.2751       0.7595      0.8721        0.2491  10.5101
     19      0.9062        0.2742       0.7582      0.8706        0.2489  10.5352
     20      0.9083        0.2736       0.7594      0.8716        0.2482  10.5820
     21      0.9089        0.2734       0.7622      0.8731        0.2472  10.5706
     22      0.9146        0.2725       0.7597      0.8715        0.2469  10.5731
     23      0.9093        0.2706       0.7623      0.8724        0.2461  10.5589
     24      0.9127        0.2695       0.7613      0.8720        0.2457  10.5714
     25      0.9154        0.2694       0.7630      0.8725        0.2450  10.5584
     26      0.9129        0.2686       0.7632      0.8726        0.2445  10.5654
     27      0.9110        0.2679       0.7632      0.8724        0.2442  10.5740
     28      0.9142        0.2661       0.7635      0.8727        0.2438  10.5706
     29      0.9130        0.2663       0.7648      0.8740        0.2430  10.5382
     30      0.9156        0.2659       0.7597      0.8695        0.2433  10.5364
     31      0.9162        0.2643       0.7656      0.8733        0.2420  10.5533
     32      0.9155        0.2635       0.7649      0.8720        0.2415  10.5708
     33      0.9138        0.2633       0.7648      0.8730        0.2412  10.5579
     34      0.9121        0.2612       0.7623      0.8712        0.2413  10.5606
     35      0.9138        0.2624       0.7620      0.8698        0.2414  10.5797
     36      0.9135        0.2606       0.7668      0.8732        0.2399  10.5725
     37      0.9162        0.2588       0.7689      0.8745        0.2390  10.5719
     38      0.9157        0.2586       0.7674      0.8730        0.2389  10.5714
     39      0.9155        0.2587       0.7658      0.8721        0.2390  10.5570
     40      0.9166        0.2569       0.7691      0.8735        0.2381  10.5446
     41      0.9196        0.2581       0.7694      0.8739        0.2379  10.5245
     42      0.9196        0.2562       0.7651      0.8713        0.2385  10.5458
     43      0.9212        0.2554       0.7715      0.8755        0.2367  10.5588
     44      0.9201        0.2541       0.7726      0.8756        0.2361  10.5608
     45      0.9183        0.2546       0.7719      0.8743        0.2366  10.5836
     46      0.9211        0.2543       0.7774      0.8793        0.2348  10.5565
     47      0.9196        0.2523       0.7708      0.8724        0.2360  10.6289
     48      0.9189        0.2535       0.7740      0.8757        0.2352  10.5692
     49      0.9225        0.2517       0.7684      0.8704        0.2365  10.5899
     50      0.9220        0.2517       0.7722      0.8742        0.2349  10.5506
     51      0.9195        0.2496       0.7672      0.8706        0.2359  10.5159
     52      0.9213        0.2497       0.7774      0.8782        0.2330  10.5233
     53      0.9238        0.2487       0.7793      0.8781        0.2325  10.5264
     54      0.9252        0.2477       0.7747      0.8760        0.2329  10.5753
     55      0.9208        0.2479       0.7734      0.8741        0.2332  10.5606
     56      0.9252        0.2464       0.7835      0.8819        0.2308  10.5796
     57      0.9228        0.2462       0.7781      0.8775        0.2315  10.5583
     58      0.9260        0.2450       0.7814      0.8787        0.2306  10.5715
     59      0.9250        0.2447       0.7753      0.8720        0.2318  10.5625
     60      0.9302        0.2440       0.7774      0.8759        0.2309  10.5390
     61      0.9275        0.2431       0.7748      0.8735        0.2312  10.5792
     62      0.9264        0.2426       0.7804      0.8778        0.2296  10.5577
     63      0.9286        0.2422       0.7781      0.8767        0.2299  10.5383
     64      0.9284        0.2411       0.7826      0.8790        0.2288  10.5364
     65      0.9270        0.2408       0.7776      0.8726        0.2302  10.5709
     66      0.9326        0.2410       0.7835      0.8807        0.2279  10.5625
     67      0.9304        0.2391       0.7757      0.8725        0.2299  10.5865
     68      0.9304        0.2385       0.7797      0.8764        0.2292  10.5733
     69      0.9279        0.2380       0.7806      0.8755        0.2286  10.5868
     70      0.9326        0.2373       0.7740      0.8704        0.2301  10.5722
     71      0.9323        0.2368       0.7809      0.8775        0.2276  10.5614
     72      0.9293        0.2373       0.7823      0.8784        0.2269  10.5716
     73      0.9334        0.2352       0.7830      0.8778        0.2261  10.5723
     74      0.9336        0.2350       0.7821      0.8778        0.2262  10.5176
     75      0.9369        0.2345       0.7842      0.8799        0.2254  10.5448
     76      0.9321        0.2333       0.7764      0.8704        0.2280  10.5524
     77      0.9351        0.2325       0.7854      0.8805        0.2248  10.5635
     78      0.9344        0.2318       0.7814      0.8758        0.2258  10.5691
     79      0.9335        0.2306       0.7830      0.8775        0.2253  10.5432
     80      0.9364        0.2320       0.7828      0.8769        0.2252  10.5687
     81      0.9366        0.2308       0.7851      0.8782        0.2242  10.5323
     82      0.9372        0.2290       0.7884      0.8812        0.2232  10.5553
     83      0.9361        0.2287       0.7840      0.8774        0.2237  10.5435
     84      0.9385        0.2282       0.7882      0.8809        0.2229  10.5533
     85      0.9407        0.2284       0.7748      0.8668        0.2279  10.5157
     86      0.9389        0.2275       0.7844      0.8763        0.2231  10.5247
     87      0.9413        0.2267       0.7948      0.8875        0.2214  10.5153
     88      0.9369        0.2255       0.7951      0.8879        0.2214  10.5514
     89      0.9401        0.2257       0.7845      0.8750        0.2227  10.5782
     90      0.9394        0.2258       0.7771      0.8692        0.2251  10.5798
     91      0.9431        0.2235       0.7839      0.8755        0.2221  10.5679
     92      0.9437        0.2236       0.7849      0.8757        0.2218  10.5784
     93      0.9419        0.2231       0.7844      0.8749        0.2216  10.5553
     94      0.9452        0.2209       0.7903      0.8797        0.2200  10.6092
     95      0.9422        0.2222       0.7898      0.8774        0.2201  10.7027
     96      0.9445        0.2223       0.7910      0.8788        0.2198  10.6506
     97      0.9440        0.2200       0.7991      0.8887        0.2184  10.6652
     98      0.9431        0.2205       0.7880      0.8767        0.2200  10.5212
     99      0.9440        0.2193       0.7929      0.8795        0.2188  10.5712
    100      0.9446        0.2194       0.7837      0.8686        0.2222  10.5770
Accuracy after query 7: 0.7786458333333334
F1 Score after query 7: 0.8774662569972529
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_6.pt

Query 8: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 8 is 1000
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9308        0.2290       0.7785      0.8691        0.2205  13.4043
      2      0.9318        0.2263       0.7884      0.8749        0.2169  13.2132
      3      0.9336        0.2251       0.7880      0.8740        0.2173  13.2059
      4      0.9328        0.2245       0.7927      0.8778        0.2151  13.2287
      5      0.9324        0.2223       0.7946      0.8814        0.2128  13.2271
      6      0.9359        0.2226       0.7901      0.8738        0.2154  13.2113
      7      0.9373        0.2205       0.7859      0.8730        0.2160  13.2262
      8      0.9375        0.2198       0.7950      0.8803        0.2123  13.2570
      9      0.9380        0.2188       0.7910      0.8758        0.2139  13.2383
     10      0.9369        0.2172       0.7962      0.8807        0.2110  13.2590
     11      0.9381        0.2170       0.7861      0.8731        0.2153  13.2380
     12      0.9387        0.2152       0.7957      0.8803        0.2104  13.2611
     13      0.9409        0.2137       0.7974      0.8821        0.2093  13.2248
     14      0.9376        0.2147       0.7979      0.8811        0.2085  13.2224
     15      0.9399        0.2129       0.7984      0.8813        0.2080  13.1846
     16      0.9399        0.2114       0.7991      0.8802        0.2085  13.2015
     17      0.9411        0.2105       0.7941      0.8778        0.2095  13.2707
     18      0.9368        0.2122       0.7960      0.8796        0.2079  13.2431
     19      0.9419        0.2091       0.8010      0.8807        0.2069  13.2324
     20      0.9420        0.2076       0.7960      0.8774        0.2102  13.2421
     21      0.9412        0.2082       0.7998      0.8804        0.2062  13.2550
     22      0.9446        0.2066       0.7988      0.8789        0.2079  13.2500
     23      0.9433        0.2060       0.8019      0.8798        0.2066  13.2276
     24      0.9444        0.2046       0.8003      0.8786        0.2073  13.2098
     25      0.9454        0.2032       0.8059      0.8822        0.2041  13.2162
     26      0.9449        0.2031       0.8021      0.8808        0.2053  13.2451
     27      0.9453        0.2023       0.8031      0.8817        0.2041  13.2292
     28      0.9451        0.2002       0.8069      0.8814        0.2039  13.2655
     29      0.9480        0.2001       0.8092      0.8801        0.2045  13.2874
     30      0.9461        0.1998       0.8049      0.8809        0.2032  13.1866
     31      0.9486        0.1979       0.8064      0.8794        0.2041  13.2293
     32      0.9446        0.1981       0.8028      0.8786        0.2039  13.2140
     33      0.9490        0.1966       0.8066      0.8825        0.2024  13.1668
     34      0.9500        0.1950       0.8104      0.8832        0.2018  13.2021
     35      0.9489        0.1952       0.8094      0.8813        0.2021  13.2028
     36      0.9462        0.1942       0.8095      0.8779        0.2045  13.2176
     37      0.9504        0.1930       0.8113      0.8839        0.2003  13.2107
     38      0.9520        0.1912       0.8130      0.8789        0.2034  13.2123
     39      0.9507        0.1911       0.8106      0.8794        0.2019  13.2150
     40      0.9489        0.1915       0.8116      0.8798        0.2018  13.1918
     41      0.9522        0.1896       0.8141      0.8789        0.2030  13.1860
     42      0.9522        0.1893       0.8127      0.8842        0.1997  13.1826
     43      0.9509        0.1893       0.8116      0.8786        0.2013  13.1896
     44      0.9544        0.1871       0.8087      0.8760        0.2065  13.2226
     45      0.9535        0.1861       0.8132      0.8845        0.1995  13.2281
     46      0.9548        0.1847       0.8141      0.8823        0.1993  13.2357
     47      0.9556        0.1839       0.8146      0.8777        0.2026  13.2188
     48      0.9550        0.1849       0.8125      0.8808        0.1991  13.2311
     49      0.9549        0.1842       0.8113      0.8837        0.2008  13.2266
     50      0.9568        0.1818       0.8174      0.8792        0.2005  13.2159
     51      0.9561        0.1824       0.8130      0.8847        0.2008  13.1699
     52      0.9573        0.1814       0.8165      0.8819        0.1997  13.3220
     53      0.9544        0.1807       0.8134      0.8837        0.1976  13.3283
     54      0.9570        0.1794       0.8158      0.8837        0.1991  13.3497
     55      0.9560        0.1788       0.8128      0.8783        0.2028  13.2661
     56      0.9583        0.1770       0.8141      0.8806        0.2004  13.2324
     57      0.9565        0.1773       0.8151      0.8783        0.2007  13.2029
     58      0.9605        0.1769       0.8156      0.8847        0.1996  13.2308
     59      0.9573        0.1771       0.8120      0.8817        0.1972  13.2059
     60      0.9606        0.1755       0.8139      0.8821        0.1991  13.1868
     61      0.9605        0.1745       0.8116      0.8798        0.2018  13.1791
     62      0.9616        0.1733       0.8144      0.8840        0.2004  13.2326
     63      0.9633        0.1729       0.8177      0.8813        0.1996  13.1956
     64      0.9614        0.1719       0.8167      0.8851        0.1976  13.2118
     65      0.9628        0.1710       0.8149      0.8825        0.2005  13.1947
     66      0.9604        0.1717       0.8130      0.8855        0.1974  13.2263
     67      0.9637        0.1712       0.8167      0.8871        0.1967  13.2180
     68      0.9615        0.1701       0.8141      0.8862        0.1942  13.1901
     69      0.9591        0.1731       0.8139      0.8869        0.1953  13.1802
     70      0.9644        0.1680       0.8182      0.8883        0.1935  13.1778
     71      0.9657        0.1668       0.8160      0.8850        0.1952  13.2101
     72      0.9674        0.1658       0.8174      0.8859        0.1949  13.2165
     73      0.9677        0.1647       0.8182      0.8853        0.1953  13.2091
     74      0.9652        0.1658       0.8165      0.8859        0.1945  13.2178
     75      0.9684        0.1643       0.8182      0.8858        0.1950  13.2030
     76      0.9659        0.1652       0.8163      0.8842        0.1944  13.2355
     77      0.9674        0.1632       0.8161      0.8835        0.1967  13.2061
     78      0.9639        0.1637       0.8210      0.8895        0.1925  13.1982
     79      0.9624        0.1652       0.8168      0.8841        0.1937  13.1722
     80      0.9701        0.1610       0.8149      0.8845        0.1941  13.2411
     81      0.9657        0.1620       0.8104      0.8825        0.1955  13.2226
     82      0.9696        0.1600       0.8155      0.8824        0.1957  13.2255
     83      0.9675        0.1595       0.8132      0.8814        0.1977  13.2027
     84      0.9676        0.1598       0.8181      0.8851        0.1929  13.2283
     85      0.9711        0.1580       0.8139      0.8839        0.1939  13.2360
     86      0.9622        0.1625       0.8095      0.8786        0.1959  13.2253
     87      0.9694        0.1579       0.8132      0.8832        0.1939  13.1537
     88      0.9707        0.1567       0.8087      0.8783        0.1998  13.1851
     89      0.9696        0.1557       0.8118      0.8831        0.1949  13.2169
     90      0.9671        0.1563       0.8134      0.8841        0.1934  13.1951
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 8: 0.8164930555555555
F1 Score after query 8: 0.9034858349729312
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_7.pt

Query 9: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 9 is 1776
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9582        0.1646       0.8073      0.8911        0.1975  18.0354
      2      0.9599        0.1617       0.8059      0.8909        0.1977  17.7762
      3      0.9598        0.1625       0.8069      0.8907        0.1986  17.8021
      4      0.9626        0.1588       0.8017      0.8887        0.2054  17.9427
      5      0.9588        0.1608       0.8078      0.8907        0.1990  17.9432
      6      0.9635        0.1582       0.8075      0.8905        0.2000  17.9436
      7      0.9608        0.1587       0.8059      0.8897        0.1977  17.9294
      8      0.9652        0.1549       0.8052      0.8893        0.2031  17.9486
      9      0.9623        0.1560       0.8033      0.8860        0.1948  17.8830
     10      0.9658        0.1527       0.8043      0.8883        0.2048  17.8784
     11      0.9606        0.1553       0.8092      0.8893        0.1955  17.9385
     12      0.9677        0.1513       0.8080      0.8895        0.1988  17.9219
     13      0.9664        0.1510       0.8089      0.8895        0.1992  17.9204
     14      0.9660        0.1500       0.8102      0.8900        0.1965  17.9187
     15      0.9674        0.1484       0.8095      0.8896        0.1959  17.9090
     16      0.9660        0.1492       0.8090      0.8884        0.1968  17.8942
     17      0.9685        0.1461       0.8095      0.8897        0.1966  17.8526
     18      0.9685        0.1457       0.8102      0.8896        0.1937  17.9277
     19      0.9681        0.1452       0.8116      0.8898        0.1945  17.8948
     20      0.9701        0.1440       0.8092      0.8888        0.1966  17.9045
     21      0.9706        0.1427       0.8120      0.8890        0.1940  17.8962
     22      0.9697        0.1425       0.8099      0.8888        0.1981  17.9013
     23      0.9695        0.1420       0.8094      0.8891        0.1984  17.8496
     24      0.9705        0.1415       0.8104      0.8866        0.1933  17.8798
     25      0.9713        0.1399       0.8132      0.8894        0.1937  17.9353
     26      0.9727        0.1393       0.8116      0.8895        0.1952  17.9160
     27      0.9727        0.1380       0.8094      0.8888        0.1984  17.9124
     28      0.9734        0.1372       0.8109      0.8888        0.1964  17.9334
     29      0.9730        0.1370       0.8122      0.8897        0.1960  17.9184
     30      0.9721        0.1368       0.8120      0.8888        0.1943  17.8845
     31      0.9740        0.1353       0.8109      0.8886        0.1963  17.8854
     32      0.9740        0.1347       0.8135      0.8883        0.1930  17.9469
     33      0.9708        0.1353       0.8102      0.8886        0.1979  17.9381
     34      0.9722        0.1338       0.8078      0.8880        0.1998  17.9295
     35      0.9743        0.1325       0.8123      0.8886        0.1939  17.9118
     36      0.9746        0.1315       0.8113      0.8895        0.1964  17.8678
     37      0.9754        0.1311       0.8115      0.8889        0.1951  17.8691
     38      0.9759        0.1304       0.8168      0.8901        0.1914  17.9272
     39      0.9735        0.1312       0.8128      0.8835        0.1929  17.9139
     40      0.9761        0.1290       0.8189      0.8914        0.1919  17.9212
     41      0.9759        0.1288       0.8057      0.8873        0.2023  17.8958
     42      0.9760        0.1280       0.8130      0.8893        0.1945  17.9113
     43      0.9741        0.1280       0.8073      0.8767        0.1967  17.8729
     44      0.9759        0.1275       0.8099      0.8882        0.2015  17.8801
     45      0.9768        0.1261       0.8071      0.8870        0.2027  17.9040
     46      0.9778        0.1254       0.8118      0.8879        0.1968  17.9407
     47      0.9772        0.1246       0.8095      0.8882        0.1969  17.9146
     48      0.9785        0.1243       0.8191      0.8899        0.1920  17.9291
     49      0.9781        0.1236       0.8148      0.8896        0.1944  17.8822
     50      0.9766        0.1236       0.8099      0.8787        0.1938  17.8962
     51      0.9788        0.1221       0.8207      0.8907        0.1894  17.9151
     52      0.9779        0.1216       0.8080      0.8872        0.2007  18.0890
     53      0.9796        0.1210       0.8186      0.8899        0.1919  17.9576
     54      0.9729        0.1265       0.8220      0.8893        0.1882  17.9471
     55      0.9788        0.1197       0.8193      0.8898        0.1923  17.9425
     56      0.9794        0.1196       0.8167      0.8845        0.1896  17.9367
     57      0.9798        0.1189       0.8161      0.8895        0.1927  17.9194
     58      0.9803        0.1184       0.8118      0.8886        0.1987  17.9386
     59      0.9806        0.1177       0.8181      0.8852        0.1909  17.9339
     60      0.9796        0.1170       0.8064      0.8734        0.1997  17.9750
     61      0.9792        0.1171       0.8193      0.8887        0.1919  17.9492
     62      0.9811        0.1162       0.8219      0.8880        0.1902  17.9478
     63      0.9805        0.1155       0.8115      0.8874        0.1974  17.8791
     64      0.9811        0.1146       0.8175      0.8847        0.1917  17.9125
     65      0.9818        0.1142       0.8198      0.8864        0.1922  17.9591
     66      0.9820        0.1144       0.8120      0.8807        0.1949  17.9428
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 9: 0.8072916666666666
F1 Score after query 9: 0.9004379301084171
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_8.pt

Query 10: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 10 is 3160
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9689        0.1294       0.8012      0.8865        0.2013  26.0927
      2      0.9731        0.1224       0.8042      0.8874        0.1980  26.1472
      3      0.9729        0.1212       0.8014      0.8867        0.2041  26.3109
      4      0.9729        0.1205       0.8043      0.8883        0.1991  26.3171
      5      0.9731        0.1189       0.8087      0.8896        0.1895  26.3478
      6      0.9738        0.1194       0.8078      0.8901        0.1894  26.2546
      7      0.9746        0.1167       0.8123      0.8912        0.1874  26.3266
      8      0.9761        0.1153       0.8052      0.8890        0.1946  26.3033
      9      0.9755        0.1150       0.8125      0.8917        0.1878  26.3155
     10      0.9766        0.1138       0.8130      0.8916        0.1861  26.2283
     11      0.9766        0.1129       0.8080      0.8903        0.1911  26.3052
     12      0.9766        0.1120       0.8092      0.8911        0.1903  26.3146
     13      0.9757        0.1116       0.8057      0.8900        0.1946  26.3140
     14      0.9721        0.1153       0.8260      0.8971        0.1727  26.2847
     15      0.9761        0.1102       0.8151      0.8931        0.1849  26.2562
     16      0.9774        0.1091       0.8115      0.8921        0.1890  26.3056
     17      0.9780        0.1085       0.8160      0.8931        0.1864  26.3161
     18      0.9783        0.1077       0.8144      0.8928        0.1874  26.3182
     19      0.9696        0.1156       0.8120      0.8904        0.1777  26.2438
     20      0.9775        0.1075       0.8109      0.8921        0.1883  26.2711
     21      0.9789        0.1056       0.8149      0.8929        0.1869  26.3004
     22      0.9787        0.1047       0.8134      0.8924        0.1882  26.2955
     23      0.9791        0.1045       0.8068      0.8893        0.1933  26.3309
     24      0.9778        0.1050       0.8078      0.8907        0.1960  26.3374
     25      0.9802        0.1029       0.8161      0.8923        0.1877  26.2788
     26      0.9793        0.1025       0.8127      0.8917        0.1889  26.2847
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 10: 0.8210069444444444
F1 Score after query 10: 0.9100548799259466
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_9.pt

Query 11: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 11 is 5624
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9683        0.1181       0.8189      0.8928        0.1787  40.8349
      2      0.9717        0.1127       0.8182      0.8913        0.1771  40.9580
      3      0.9733        0.1104       0.8214      0.8935        0.1780  41.0746
      4      0.9741        0.1087       0.8227      0.8936        0.1778  41.1868
      5      0.9758        0.1070       0.8280      0.8960        0.1770  41.2142
      6      0.9752        0.1057       0.8273      0.8938        0.1769  41.1348
      7      0.9763        0.1043       0.8260      0.8944        0.1775  41.2400
      8      0.9762        0.1037       0.8257      0.8933        0.1782  41.1946
      9      0.9767        0.1025       0.8288      0.8954        0.1798  41.1114
     10      0.9773        0.1011       0.8276      0.8932        0.1765  41.2050
     11      0.9777        0.0999       0.8267      0.8936        0.1790  41.1526
     12      0.9782        0.0988       0.8276      0.8943        0.1809  41.0764
     13      0.9786        0.0978       0.8313      0.8947        0.1780  41.1971
     14      0.9786        0.0965       0.8280      0.8938        0.1794  41.1494
     15      0.9790        0.0950       0.8281      0.8936        0.1799  41.1404
     16      0.9789        0.0938       0.8278      0.8920        0.1790  41.1597
     17      0.9787        0.0937       0.8276      0.8917        0.1805  41.1490
     18      0.9799        0.0926       0.8280      0.8921        0.1816  41.1519
     19      0.9793        0.0914       0.8271      0.8943        0.1858  41.1617
     20      0.9801        0.0905       0.8273      0.8912        0.1817  41.1332
     21      0.9802        0.0900       0.8274      0.8920        0.1841  41.1580
     22      0.9723        0.1032       0.8286      0.8891        0.1806  41.1784
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 11: 0.8444444444444444
F1 Score after query 11: 0.9174342567264926
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_10.pt

Query 12: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 12 is 10000
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9732        0.0962       0.8165      0.8894        0.1972  66.8734
      2      0.9791        0.0889       0.8212      0.8911        0.1997  67.5785
      3      0.9794        0.0868       0.8201      0.8911        0.1982  67.5375
      4      0.9797        0.0850       0.8219      0.8909        0.1971  67.5723
      5      0.9809        0.0831       0.8240      0.8920        0.1968  67.4343
      6      0.9814        0.0820       0.8198      0.8913        0.2031  67.5248
      7      0.9823        0.0806       0.8212      0.8914        0.2029  67.6287
      8      0.9828        0.0792       0.8200      0.8909        0.2065  67.4562
      9      0.9839        0.0778       0.8205      0.8907        0.2068  67.5110
     10      0.9825        0.0794       0.8292      0.8922        0.1958  67.4495
     11      0.9832        0.0769       0.8201      0.8890        0.2054  67.6393
     12      0.9847        0.0747       0.8233      0.8893        0.2036  67.5972
     13      0.9854        0.0738       0.8247      0.8890        0.2043  67.5392
     14      0.9862        0.0724       0.8243      0.8875        0.2070  67.4841
     15      0.9862        0.0715       0.8220      0.8884        0.2097  67.4742
     16      0.9863        0.0706       0.8203      0.8886        0.2128  67.9077
     17      0.9874        0.0695       0.8203      0.8885        0.2168  67.5163
     18      0.9862        0.0692       0.8205      0.8874        0.2132  67.6348
     19      0.9869        0.0681       0.8184      0.8864        0.2161  67.4862
     20      0.9789        0.0813       0.8181      0.8814        0.2121  67.5875
     21      0.9870        0.0672       0.8179      0.8851        0.2189  67.6325
     22      0.9877        0.0657       0.8196      0.8833        0.2158  67.6190
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 12: 0.8487847222222222
F1 Score after query 12: 0.9197811373898581
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_11.pt

Query 13: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 13 is 4056
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9769        0.0810       0.7057      0.8359        0.3851  77.9521
      2      0.9813        0.0738       0.8266      0.8742        0.1951  78.2921
      3      0.9853        0.0670       0.8144      0.8819        0.2038  78.3932
      4      0.9872        0.0641       0.8198      0.8843        0.2109  78.2371
      5      0.9868        0.0643       0.8106      0.8775        0.2064  78.2749
      6      0.9841        0.0699       0.6785      0.8119        0.3663  78.2219
      7      0.9828        0.0714       0.8165      0.8755        0.2031  78.1945
      8      0.9871        0.0609       0.8210      0.8769        0.2016  78.1802
      9      0.9876        0.0604       0.8148      0.8770        0.2021  78.2231
     10      0.9885        0.0588       0.8201      0.8694        0.2108  78.2363
     11      0.9878        0.0587       0.8108      0.8771        0.2130  78.1644
     12      0.9889        0.0570       0.8132      0.8738        0.2134  78.2094
     13      0.9890        0.0565       0.8149      0.8732        0.2104  78.1972
     14      0.9880        0.0582       0.8134      0.8795        0.2145  78.3283
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 13: 0.85
F1 Score after query 13: 0.9146146626591257
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\model_checkpoint_iteration_12.pt
Best F1 score across iterations: 0.9197811373898581
Performance results saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run3_s44\performance_results.npy