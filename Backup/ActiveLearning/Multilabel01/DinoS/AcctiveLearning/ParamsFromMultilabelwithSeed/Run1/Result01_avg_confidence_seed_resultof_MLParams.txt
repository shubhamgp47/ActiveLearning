   0.8403        0.3177  8.1870
     93      0.9044        0.3413       0.7101      0.8387        0.3172  8.2167
     94      0.9020        0.3405       0.7127      0.8408        0.3172  8.1990
     95      0.9072        0.3428       0.7120      0.8404        0.3169  8.2206
     96      0.9063        0.3409       0.7118      0.8401        0.3166  8.2459
     97      0.9061        0.3402       0.7123      0.8411        0.3166  8.2321
     98      0.9054        0.3390       0.7101      0.8387        0.3158  8.2423
     99      0.8989        0.3374       0.7122      0.8403        0.3158  8.2198
    100      0.9073        0.3392       0.7120      0.8402        0.3155  8.2110
Accuracy after query 5: 0.7357638888888889
F1 Score after query 5: 0.8389931212352238
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\model_checkpoint_iteration_4.pt 

Query 6: Using the exact images and labels from the CSV for this iteration.     
Number of new samples used for training in Query 6 is 320
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.9009        0.3401       0.7198      0.8458        0.3175  9.2579
      2      0.8950        0.3407       0.7184      0.8461        0.3194  9.0746
      3      0.8881        0.3392       0.7175      0.8460        0.3181  9.0609
      4      0.8915        0.3371       0.7189      0.8467        0.3176  9.0744
      5      0.8920        0.3378       0.7184      0.8463        0.3156  9.0205
      6      0.8896        0.3359       0.7172      0.8450        0.3144  9.0513
      7      0.9003        0.3344       0.7189      0.8482        0.3179  9.0512
      8      0.8953        0.3350       0.7207      0.8497        0.3189  9.0808
      9      0.8997        0.3331       0.7198      0.8483        0.3153  9.0556
     10      0.8875        0.3341       0.7196      0.8488        0.3150  9.0648
     11      0.9001        0.3319       0.7203      0.8499        0.3160  9.0727
     12      0.8966        0.3325       0.7203      0.8492        0.3141  9.0896
     13      0.9007        0.3288       0.7194      0.8485        0.3130  9.0737
     14      0.9053        0.3286       0.7205      0.8494        0.3123  9.0873
     15      0.8996        0.3282       0.7215      0.8509        0.3136  9.0636
     16      0.9051        0.3266       0.7207      0.8493        0.3107  9.0881
     17      0.8979        0.3275       0.7217      0.8507        0.3112  9.0458
     18      0.8944        0.3261       0.7203      0.8489        0.3096  9.0416
     19      0.9003        0.3249       0.7222      0.8499        0.3089  9.0398
     20      0.9050        0.3245       0.7215      0.8498        0.3081  9.0597
     21      0.9070        0.3235       0.7215      0.8494        0.3072  9.0750
     22      0.9046        0.3236       0.7234      0.8522        0.3088  9.0714
     23      0.9019        0.3238       0.7226      0.8505        0.3067  9.0780
     24      0.9019        0.3219       0.7234      0.8515        0.3070  9.0634
     25      0.9068        0.3209       0.7227      0.8505        0.3058  9.0639
     26      0.8996        0.3201       0.7240      0.8507        0.3043  9.0622
     27      0.9019        0.3198       0.7262      0.8533        0.3055  9.0611
     28      0.9046        0.3188       0.7245      0.8532        0.3059  9.0908
     29      0.9037        0.3190       0.7238      0.8504        0.3033  9.0766
     30      0.9030        0.3190       0.7260      0.8517        0.3026  9.0792
     31      0.9086        0.3163       0.7267      0.8547        0.3048  9.0389
     32      0.9132        0.3171       0.7274      0.8566        0.3054  9.0360
     33      0.9098        0.3155       0.7295      0.8560        0.3035  9.0434
     34      0.9050        0.3142       0.7286      0.8555        0.3032  9.0698
     35      0.9023        0.3163       0.7295      0.8561        0.3025  9.0733
     36      0.9060        0.3136       0.7288      0.8538        0.3006  9.0906
     37      0.9093        0.3136       0.7297      0.8563        0.3017  9.0812
     38      0.9092        0.3110       0.7286      0.8539        0.3003  9.0574
     39      0.9119        0.3113       0.7300      0.8568        0.3009  9.0771
     40      0.9062        0.3099       0.7318      0.8575        0.3003  9.0692
     41      0.9048        0.3105       0.7304      0.8574        0.3004  9.0773
     42      0.9113        0.3109       0.7314      0.8580        0.2998  9.0865
     43      0.9083        0.3112       0.7332      0.8564        0.2973  9.0999
     44      0.9098        0.3087       0.7316      0.8562        0.2970  9.0555
     45      0.9061        0.3067       0.7328      0.8571        0.2963  9.0436
     46      0.9099        0.3064       0.7330      0.8593        0.2986  9.0524
     47      0.9171        0.3067       0.7349      0.8582        0.2964  9.0836
     48      0.9140        0.3064       0.7340      0.8579        0.2960  9.0944
     49      0.9108        0.3058       0.7354      0.8601        0.2971  9.0638
     50      0.9135        0.3050       0.7349      0.8605        0.2968  9.0915
     51      0.9131        0.3042       0.7377      0.8597        0.2931  9.0867
     52      0.9182        0.3024       0.7358      0.8593        0.2950  9.0885
     53      0.9140        0.3028       0.7363      0.8592        0.2938  9.0710
     54      0.9107        0.3026       0.7372      0.8600        0.2941  9.0838
     55      0.9172        0.3004       0.7366      0.8596        0.2934  9.0777
     56      0.9108        0.3011       0.7377      0.8600        0.2920  9.0960
     57      0.9138        0.3007       0.7378      0.8606        0.2919  9.0501
     58      0.9163        0.3004       0.7391      0.8614        0.2927  9.0424
     59      0.9129        0.2999       0.7375      0.8606        0.2920  9.0533
     60      0.9190        0.2994       0.7401      0.8608        0.2901  9.0760
     61      0.9124        0.2988       0.7401      0.8623        0.2918  9.0793
     62      0.9132        0.2977       0.7408      0.8613        0.2889  9.0616
     63      0.9101        0.2965       0.7391      0.8608        0.2894  9.0575
     64      0.9148        0.2969       0.7382      0.8606        0.2893  9.0671
     65      0.9192        0.2956       0.7391      0.8611        0.2889  9.0530
     66      0.9210        0.2948       0.7378      0.8627        0.2915  9.0832
     67      0.9155        0.2949       0.7398      0.8613        0.2878  9.0477
     68      0.9205        0.2943       0.7417      0.8640        0.2883  9.0838
     69      0.9218        0.2944       0.7417      0.8634        0.2874  9.0685
     70      0.9193        0.2924       0.7420      0.8626        0.2868  9.0579
     71      0.9208        0.2913       0.7406      0.8640        0.2892  9.0285
     72      0.9203        0.2923       0.7420      0.8636        0.2869  9.0406
     73      0.9210        0.2908       0.7434      0.8651        0.2877  9.0740
     74      0.9168        0.2900       0.7434      0.8641        0.2856  9.0708
     75      0.9225        0.2899       0.7417      0.8640        0.2865  9.0740
     76      0.9229        0.2898       0.7439      0.8644        0.2848  9.0773
     77      0.9248        0.2878       0.7439      0.8655        0.2857  9.0777
     78      0.9180        0.2888       0.7438      0.8639        0.2836  9.0671
     79      0.9212        0.2875       0.7448      0.8652        0.2840  9.0632
     80      0.9186        0.2878       0.7453      0.8640        0.2820  9.0718
     81      0.9256        0.2849       0.7405      0.8645        0.2865  9.0758
     82      0.9236        0.2876       0.7408      0.8648        0.2863  9.0762
     83      0.9238        0.2864       0.7443      0.8658        0.2836  9.0778
     84      0.9230        0.2860       0.7474      0.8659        0.2815  9.0415
     85      0.9265        0.2844       0.7444      0.8635        0.2795  9.0489
     86      0.9223        0.2845       0.7474      0.8664        0.2817  9.0602
     87      0.9244        0.2823       0.7469      0.8665        0.2817  9.0975
     88      0.9259        0.2836       0.7474      0.8657        0.2799  9.0716
     89      0.9275        0.2827       0.7467      0.8660        0.2805  9.0753
     90      0.9252        0.2816       0.7467      0.8666        0.2815  9.0760
     91      0.9273        0.2825       0.7481      0.8660        0.2780  9.0872
     92      0.9189        0.2827       0.7484      0.8662        0.2780  9.1034
     93      0.9255        0.2793       0.7481      0.8662        0.2785  9.0729
     94      0.9267        0.2803       0.7486      0.8673        0.2794  9.0724
     95      0.9245        0.2786       0.7472      0.8673        0.2810  9.0792
     96      0.9234        0.2791       0.7472      0.8654        0.2755  9.0697
     97      0.9258        0.2779       0.7488      0.8670        0.2783  9.0498
     98      0.9263        0.2763       0.7483      0.8662        0.2754  9.0392
     99      0.9286        0.2761       0.7474      0.8664        0.2765  9.0662
    100      0.9226        0.2783       0.7484      0.8668        0.2758  9.0716
Accuracy after query 6: 0.7546875
F1 Score after query 6: 0.8597821450133134
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\model_checkpoint_iteration_5.pt 

Query 7: Using the exact images and labels from the CSV for this iteration.     
Number of new samples used for training in Query 7 is 560
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9150        0.2806       0.7477      0.8675        0.2774  10.6964
      2      0.9147        0.2792       0.7453      0.8671        0.2783  10.5291
      3      0.9124        0.2787       0.7479   1
      4      0.9129        0.2795       0.7476      0.8679        0.2756  10.5528
      5      0.9175        0.2748       0.7484      0.8679        0.2721  10.5299
      6      0.9153        0.2751       0.7483      0.8683        0.2744  10.5380
      7      0.9202        0.2742       0.7509      0.8689        0.2690  10.5409
      8      0.9184        0.2735       0.7477      0.8685        0.2742  10.5041
      9      0.9196        0.2727       0.7486      0.8679        0.2701  10.5957
     10      0.9199        0.2717       0.7505      0.8687        0.2679  10.5530
     11      0.9203        0.2715       0.7512   
   0.8694        0.2681  10.5713
     12      0.9184        0.2693       0.7502   
   0.8695        0.2702  10.5665
     13      0.9232        0.2695       0.7528   
   0.8702        0.2670  10.5544
     14      0.9244        0.2688       0.7524      0.8694        0.2646  10.5486
     15      0.9203        0.2676       0.7516      0.8699        0.2679  10.5551
     16      0.9182        0.2675       0.7516      0.8694        0.2654  10.5414
     17      0.9216        0.2652       0.7516      0.8691        0.2640  10.5582
     18      0.9287        0.2638       0.7523      0.8696        0.2629  10.5508
     19      0.9245        0.2632       0.7535   
   0.8709        0.2638  10.5288
     20      0.9250        0.2636       0.7512      0.8701        0.2656  10.5203
     21      0.9278        0.2626       0.7547      0.8708        0.2604  10.5199
     22      0.9267        0.2614       0.7538      0.8707        0.2618  10.5506
     23      0.9292        0.2602       0.7535   
   0.8711        0.2621  10.5524
     24      0.9253        0.2600       0.7543      0.8709        0.2607  10.5649
     25      0.9266        0.2600       0.7533      0.8709        0.2614  10.5683
     26      0.9306        0.2585       0.7545   
   0.8716        0.2601  10.5348
     27      0.9278        0.2585       0.7519      0.8706        0.2637  10.5505
     28      0.9297        0.2567       0.7547      0.8708        0.2581  10.5484
     29      0.9305        0.2561       0.7552      0.8712        0.2575  10.5593
     30      0.9335        0.2548       0.7561      0.8715        0.2566  10.5444
     31      0.9304        0.2547       0.7580   
   0.8726        0.2545  10.5129
     32      0.9322        0.2540       0.7540      0.8712        0.2604  10.5377
     33      0.9316        0.2537       0.7578   
   0.8732        0.2556  10.5231
     34      0.9320        0.2529       0.7582      0.8730        0.2547  10.5575
     35      0.9338        0.2514       0.7590   
   0.8735        0.2549  10.5264
     36      0.9317        0.2532       0.7505      0.8705        0.2616  10.5492
     37      0.9328        0.2507       0.7587      0.8734        0.2546  10.5484
     38      0.9316        0.2500       0.7531      0.8711        0.2587  10.5260
     39      0.9357        0.2494       0.7628   
   0.8749        0.2498  10.5604
     40      0.9323        0.2491       0.7599      0.8736        0.2518  10.5360
     41      0.9388        0.2477       0.7608      0.8741        0.2525  10.5240
     42      0.9381        0.2463       0.7602      0.8736        0.2516  10.5142
     43      0.9355        0.2467       0.7550      0.8723        0.2559  10.5136
     44      0.9356        0.2465       0.7618      0.8748        0.2519  10.5364
     45      0.9399        0.2450       0.7632   
   0.8752        0.2491  10.5694
     46      0.9353        0.2428       0.7632   
   0.8752        0.2504  10.5536
     47      0.9377        0.2432       0.7604      0.8746        0.2527  10.5355
     48      0.9417        0.2439       0.7547      0.8722        0.2557  10.5497
     49      0.9361        0.2436       0.7623      0.8751        0.2499  10.5308
     50      0.9430        0.2409       0.7656   
   0.8767        0.2477  10.5327
     51      0.9395        0.2400       0.7672   
   0.8770        0.2444  10.5824
     52      0.9395        0.2406       0.7606      0.8737        0.2507  10.5527
     53      0.9422        0.2394       0.7655      0.8760        0.2464  10.5514
     54      0.9441        0.2382       0.7663      0.8765        0.2468  10.5208
     55      0.9393        0.2386       0.7708   
   0.8783        0.2393  10.5416
     56      0.9377        0.2393       0.7606      0.8747        0.2507  10.5225
     57      0.9420        0.2370       0.7616      0.8749        0.2495  10.5469
     58      0.9375        0.2383       0.7595      0.8745        0.2520  10.5543
     59      0.9367        0.2375       0.7648      0.8764        0.2468  10.5701
     60      0.9389        0.2364       0.7620      0.8756        0.2489  10.5397
     61      0.9436        0.2350       0.7675      0.8773        0.2437  10.5581
     62      0.9440        0.2337       0.7747   
   0.8794        0.2351  10.5441
     63      0.9463        0.2326       0.7752   
   0.8802        0.2355  10.5576
     64      0.9447        0.2318       0.7748      0.8799        0.2372  10.5371
     65      0.9410        0.2316       0.7651      0.8761        0.2467  10.5583
     66      0.9385        0.2354       0.7641      0.8764        0.2472  10.5279
     67      0.9460        0.2316       0.7773   
   0.8812        0.2357  10.5368
     68      0.9492        0.2287       0.7755      0.8804        0.2349  10.5660
     69      0.9485        0.2288       0.7641      0.8765        0.2472  10.5518
     70      0.9453        0.2299       0.7766      0.8808        0.2342  10.5480
     71      0.9498        0.2271       0.7786   
   0.8813        0.2326  10.5665
     72      0.9487        0.2259       0.7753      0.8803        0.2355  10.5401
     73      0.9475        0.2263       0.7799   
   0.8818        0.2300  10.5513
     74      0.9483        0.2263       0.7738      0.8796        0.2375  10.5534
     75      0.9502        0.2242       0.7790   
   0.8820        0.2325  10.5431
     76      0.9466        0.2253       0.7674      0.8778        0.2444  10.5027
     77      0.9390        0.2302       0.7710      0.8798        0.2393  10.5189
     78      0.9453        0.2270       0.7719      0.8801        0.2393  10.5118
     79      0.9515        0.2230       0.7806   
   0.8827        0.2310  10.5478
     80      0.9499        0.2220       0.7795      0.8822        0.2311  10.5339
     81      0.9507        0.2206       0.7826   
   0.8828        0.2278  10.5338
     82      0.9476        0.2214       0.7795      0.8826        0.2306  10.5505
     83      0.9515        0.2213       0.7821   
   0.8834        0.2290  10.5504
     84      0.9521        0.2193       0.7823      0.8827        0.2279  10.5571
     85      0.9544        0.2187       0.7802      0.8824        0.2314  10.5559
     86      0.9525        0.2195       0.7755      0.8807        0.2368  10.5556
     87      0.9389        0.2242       0.7878   
   0.8856        0.2245  10.5337
     88      0.9512        0.2179       0.7880      0.8853        0.2232  10.5342
     89      0.9539        0.2164       0.7898   
   0.8856        0.2225  10.5262
     90      0.9510        0.2171       0.7878      0.8818        0.2213  10.5699
     91      0.9527        0.2154       0.7884      0.8819        0.2206  10.5676
     92      0.9521        0.2154       0.7851      0.8842        0.2254  10.5569
     93      0.9556        0.2151       0.7832      0.8835        0.2289  10.5476
     94      0.9546        0.2152       0.7859      0.8847        0.2248  10.5530
     95      0.9511        0.2160       0.7616      0.8751        0.2461  10.5931
     96      0.9275        0.2305       0.7917   
   0.8859        0.2196  10.5585
     97      0.9595        0.2120       0.7910      0.8841        0.2182  10.5345
     98      0.9565        0.2112       0.7896      0.8836        0.2181  10.5517
     99      0.9571        0.2107       0.7875      0.8848        0.2237  10.5256
    100      0.9585        0.2096       0.7899      0.8847        0.2183  10.5269
Accuracy after query 7: 0.7836805555555556
F1 Score after query 7: 0.8845839823504756       
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\model_checkpoint_iteration_6.pt

Query 8: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 8 is 1000
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9412        0.2205       0.7931   
   0.8854        0.2172  13.3994
      2      0.9416        0.2188       0.7899   
   0.8859        0.2207  13.1422
      3      0.9414        0.2190       0.7849      0.8835        0.2292  13.1314
      4      0.9383        0.2194       0.7932   
   0.8873        0.2209  13.1991
      5      0.9445        0.2165       0.7892      0.8854        0.2251  13.1665
      6      0.9428        0.2163       0.7925   
   0.8875        0.2206  13.1817
      7      0.9426        0.2153       0.7957   
   0.8886        0.2157  13.1392
      8      0.9461        0.2132       0.7896      0.8857        0.2228  13.1949
      9      0.9449        0.2133       0.7988      0.8870        0.2112  13.1873
     10      0.9478        0.2099       0.7720      0.8772        0.2361  13.2140
     11      0.9425        0.2119       0.7852      0.8842        0.2259  13.1776
     12      0.9462        0.2095       0.7847      0.8833        0.2250  13.2048
     13      0.9459        0.2092       0.7905      0.8856        0.2204  13.1761
     14      0.9469        0.2070       0.7929      0.8876        0.2131  13.2265
     15      0.9514        0.2049       0.7977   
   0.8896        0.2113  13.1977
     16      0.9501        0.2046       0.7958      0.8885        0.2140  13.1954
     17      0.9523        0.2024       0.7969      0.8887        0.2135  13.1495
     18      0.9527        0.2039       0.7986   
   0.8898        0.2097  13.1898
     19      0.9549        0.2015       0.7760      0.8774        0.2307  13.1958
     20      0.9451        0.2052       0.7962      0.8888        0.2133  13.1809
     21      0.9525        0.2017       0.8024   
   0.8906        0.2088  13.1922
     22      0.9507        0.2007       0.8026      0.8901        0.2060  13.2038
     23      0.9550        0.1984       0.7988      0.8896        0.2081  13.1834
     24      0.9511        0.1992       0.7609      0.8707        0.2457  13.2066
     25      0.9393        0.2059       0.8030      0.8906        0.2046  13.1634
     26      0.9566        0.1953       0.8002      0.8899        0.2066  13.1622
     27      0.9567        0.1950       0.7951      0.8883        0.2117  13.1644
     28      0.9537        0.1947       0.7983      0.8895        0.2090  13.1899
     29      0.9523        0.1961       0.8010      0.8905        0.2078  13.1927
     30      0.9569        0.1933       0.8019      0.8905        0.2041  13.1859
     31      0.9575        0.1920       0.8071      0.8904        0.2006  13.1682
     32      0.9584        0.1902       0.7981      0.8887        0.2090  13.1853
     33      0.9525        0.1939       0.7521      0.8670        0.2516  13.1778
     34      0.9440        0.1998       0.7943      0.8861        0.2114  13.1578
     35      0.9581        0.1889       0.8031   
   0.8907        0.2015  13.1581
     36      0.9587        0.1875       0.8097   
   0.8922        0.1997  13.1742
     37      0.9576        0.1882       0.8109      0.8895        0.1984  13.1847
     38      0.9574        0.1882       0.8063      0.8919        0.1998  13.2191
     39      0.9571        0.1860       0.7653      0.8724        0.2404  13.1610
     40      0.9456        0.1954       0.8115   
   0.8923        0.1974  13.2011
     41      0.9605        0.1845       0.8030      0.8909        0.2035  13.1953
     42      0.9618        0.1829       0.8033      0.8913        0.2012  13.1845
     43      0.9623        0.1831       0.8132      0.8913        0.1959  13.1768
     44      0.9601        0.1833       0.7696      0.8743        0.2354  13.1892
     45      0.9444        0.1924       0.8090   
   0.8928        0.1983  13.1768
     46      0.9629        0.1798       0.8069      0.8923        0.1988  13.1967
     47      0.9624        0.1801       0.8080      0.8925        0.1987  13.2065
     48      0.9639        0.1790       0.8130   
   0.8934        0.1958  13.2074
     49      0.9650        0.1774       0.8153   
   0.8942        0.1946  13.1900
     50      0.9598        0.1803       0.7837      0.8801        0.2170  13.2075
     51      0.9580        0.1815       0.8160      0.8941        0.1944  13.1961
     52      0.9636        0.1764       0.8175   
   0.8947        0.1936  13.1885
     53      0.9652        0.1758       0.8177      0.8945        0.1928  13.1446
     54      0.9668        0.1745       0.8168      0.8938        0.1931  13.1608
     55      0.9639        0.1753       0.8153      0.8890        0.1933  13.1758
     56      0.9642        0.1752       0.8139      0.8944        0.1961  13.1680
     57      0.9671        0.1729       0.8158      0.8900        0.1923  13.1633
     58      0.9638        0.1754       0.8184      0.8890        0.1944  13.1784
     59      0.9666        0.1720       0.8194   
   0.8957        0.1919  13.1860
     60      0.9659        0.1708       0.8205   
   0.8965        0.1917  13.1974
     61      0.9668        0.1703       0.8163      0.8952        0.1945  13.1679
     62      0.9668        0.1706       0.8214   
   0.8967        0.1913  13.1498
     63      0.9664        0.1699       0.8214      0.8949        0.1891  13.1641
     64      0.9618        0.1723       0.7606      0.8710        0.2412  13.1951
     65      0.9397        0.1884       0.8196      0.8959        0.1919  13.1895
     66      0.9687        0.1673       0.8220      0.8967        0.1903  13.1749
     67      0.9677        0.1664       0.8196      0.8961        0.1909  13.1966
     68      0.9692        0.1655       0.8224   
   0.8976        0.1910  13.1904
     69      0.9695        0.1656       0.8227      0.8965        0.1885  13.2056
     70      0.9676        0.1652       0.8201      0.8947        0.1884  13.1803
     71      0.9693        0.1643       0.8210      0.8973        0.1902  13.1436
     72      0.9687        0.1640       0.8227      0.8961        0.1877  13.1664
     73      0.9699        0.1629       0.8252   
   0.8979        0.1874  13.1971
     74      0.9715        0.1618       0.8241      0.8976        0.1875  13.1807
     75      0.9675        0.1623       0.8234      0.8946        0.1867  13.1981
     76      0.9686        0.1619       0.8191      0.8960        0.1908  13.1799
     77      0.9670        0.1622       0.8253   
   0.8991        0.1884  13.1834
     78      0.9709        0.1599       0.8248      0.8949        0.1860  13.1797
     79      0.9696        0.1602       0.8264      0.8957        0.1851  13.1925
     80      0.9691        0.1598       0.8262      0.8986        0.1869  13.1906
     81      0.9704        0.1580       0.8278      0.8959        0.1846  13.1454
     82      0.9719        0.1569       0.8245      0.8983        0.1883  13.1938
     83      0.9678        0.1590       0.8285      0.8986        0.1851  13.1963
     84      0.9707        0.1570       0.8292      0.8966        0.1838  13.1703
     85      0.9709        0.1561       0.8309      0.8970        0.1827  13.2041
     86      0.9690        0.1571       0.8299      0.8953        0.1829  13.1615
     87      0.9716        0.1548       0.8274   
   0.8993        0.1857  13.1605
     88      0.9704        0.1549       0.8306   
   0.8998        0.1839  13.1327
     89      0.9708        0.1548       0.8300      0.8964        0.1830  13.1528
     90      0.9730        0.1524       0.8306      0.8985        0.1823  13.1351
     91      0.9708        0.1538       0.8325   
   0.9012        0.1826  13.1951
     92      0.9716        0.1529       0.8340      0.9004        0.1810  13.1838
     93      0.9727        0.1516       0.8352   
   0.9019        0.1814  13.1695
     94      0.9712        0.1527       0.8356      0.8970        0.1803  13.1849
     95      0.9728        0.1505       0.8344      0.9014        0.1814  13.1814
     96      0.9741        0.1505       0.8339      0.9015        0.1819  13.1958
     97      0.9729        0.1489       0.8344      0.9010        0.1809  13.1756
     98      0.9731        0.1492       0.8356      0.9009        0.1799  13.1733
     99      0.9729        0.1492       0.8354      0.9000        0.1791  13.1753
    100      0.9736        0.1479       0.8347      0.8997        0.1794  13.1967
Accuracy after query 8: 0.7982638888888889
F1 Score after query 8: 0.8897224888932649       
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\model_checkpoint_iteration_7.pt

Query 9: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 9 is 1776
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9359        0.1831       0.8309   
   0.9005        0.1811  17.8426
      2      0.9556        0.1634       0.8363   
   0.9019        0.1754  17.8139
      3      0.9559        0.1641       0.8396   
   0.9019        0.1738  17.8216
      4      0.9598        0.1582       0.8401   
   0.9041        0.1734  17.8798
      5      0.9615        0.1569       0.8411   
   0.9054        0.1729  17.8700
      6      0.9627        0.1549       0.8448      0.9048        0.1697  17.8913
      7      0.9530        0.1632       0.8358      0.9015        0.1754  17.8827
      8      0.9629        0.1546       0.8434      0.9043        0.1707  17.8835
      9      0.9656        0.1526       0.8349      0.9046        0.1771  17.8730
     10      0.9622        0.1527       0.8401   
   0.9059        0.1727  17.8827
     11      0.9654        0.1501       0.8330      0.9033        0.1778  17.8294
     12      0.9659        0.1494       0.8307      0.9031        0.1802  17.8610
     13      0.9671        0.1483       0.8155      0.8988        0.1892  17.8937
     14      0.9659        0.1478       0.8245      0.9013        0.1833  17.8979
     15      0.9677        0.1467       0.8068      0.8963        0.1940  17.8864
     16      0.9660        0.1474       0.8130      0.8977        0.1902  17.8962
     17      0.9664        0.1457       0.8290      0.9028        0.1800  17.8545
     18      0.9673        0.1437       0.8422   
   0.9059        0.1709  17.8289
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 9: 0.8163194444444445
F1 Score after query 9: 0.9029031972886187       
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\model_checkpoint_iteration_8.pt

Query 10: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 10 is 3160
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9540        0.1584       0.7872   
   0.8164        0.2115  26.2172
      2      0.9589        0.1523       0.7911   
   0.8193        0.2079  26.0481
      3      0.9606        0.1495       0.8179   
   0.8554        0.1842  26.1667
      4      0.9614        0.1476       0.8229   
   0.8618        0.1813  26.2547
      5      0.9630        0.1451       0.8469   
   0.8926        0.1684  26.2515
      6      0.9647        0.1433       0.8193      0.8550        0.1837  26.2454
      7      0.9640        0.1422       0.8194      0.8555        0.1830  26.1910
      8      0.9658        0.1410       0.8120      0.8454        0.1931  26.2064
      9      0.9662        0.1398       0.8505   
   0.8988        0.1630  26.2440
     10      0.9679        0.1372       0.8321      0.8705        0.1754  26.2557
     11      0.9668        0.1359       0.8521   
   0.9007        0.1596  26.2716
     12      0.9683        0.1346       0.8399      0.8827        0.1711  26.1933
     13      0.9690        0.1337       0.8531   
   0.9029        0.1588  26.2228
     14      0.9685        0.1325       0.8507      0.9015        0.1585  26.2231
     15      0.9699        0.1309       0.8526   
   0.9045        0.1574  26.2224
     16      0.9697        0.1300       0.8516      0.8980        0.1621  26.1773
     17      0.9702        0.1289       0.8547   
   0.9101        0.1598  26.1699
     18      0.9711        0.1280       0.8559   
   0.9111        0.1596  26.1928
     19      0.9710        0.1269       0.8549      0.9058        0.1563  26.2164
     20      0.9711        0.1258       0.8540      0.9053        0.1561  26.2055
     21      0.9718        0.1245       0.8552      0.9073        0.1567  26.1623
     22      0.9721        0.1237       0.8526      0.9027        0.1558  26.1773
     23      0.9721        0.1224       0.8549      0.9064        0.1557  26.2247
     24      0.9726        0.1216       0.8535      0.9024        0.1569  26.2156
     25      0.9734        0.1209       0.8521      0.9034        0.1555  26.2130
     26      0.9721        0.1200       0.8533      0.9061        0.1559  26.1707
     27      0.9733        0.1183       0.8557      0.9078        0.1566  26.2261
     28      0.9734        0.1184       0.8549      0.9077        0.1567  26.2300
     29      0.9743        0.1175       0.8542      0.9069        0.1562  26.2209
     30      0.9744        0.1162       0.8530      0.9046        0.1562  26.2217
     31      0.9751        0.1156       0.8530      0.9070        0.1575  26.2166
     32      0.9743        0.1142       0.8538   
   0.9120        0.1628  26.1972
     33      0.9755        0.1140       0.8550      0.9100        0.1590  26.1962
     34      0.9752        0.1127       0.8535      0.9096        0.1595  26.2096
     35      0.9753        0.1122       0.8543      0.9097        0.1588  26.1729
     36      0.9752        0.1116       0.8557   
   0.9124        0.1615  26.2357
     37      0.9762        0.1102       0.8543      0.9118        0.1608  26.1802
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 10: 0.8362847222222223
F1 Score after query 10: 0.9160525101168414      
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\model_checkpoint_iteration_9.pt

Query 11: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 11 is 5624
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9598        0.1306       0.8528   
   0.9021        0.1552  40.7423
      2      0.9645        0.1255       0.8566   
   0.9041        0.1522  40.9271
      3      0.9674        0.1204       0.8595      0.9035        0.1517  41.0520
      4      0.9697        0.1183       0.8630   
   0.9077        0.1500  41.1447
      5      0.9702        0.1156       0.8575   
   0.9106        0.1501  41.1540
      6      0.9725        0.1130       0.8564   
   0.9111        0.1518  41.0364
      7      0.9735        0.1108       0.8599      0.9105        0.1490  41.1530
      8      0.9694        0.1145       0.8582   
   0.9116        0.1514  41.1166
      9      0.9745        0.1081       0.8543   
   0.9117        0.1570  41.0453
     10      0.9721        0.1094       0.8552      0.9106        0.1542  41.1092
     11      0.9747        0.1061       0.8536      0.9109        0.1566  41.1082
     12      0.9762        0.1041       0.8620      0.9096        0.1464  41.0232
     13      0.9764        0.1027       0.8479      0.9083        0.1625  41.0370
     14      0.9763        0.1019       0.8582   
   0.9128        0.1515  41.0790
     15      0.9761        0.1015       0.8602      0.9124        0.1496  41.0814
     16      0.9777        0.0988       0.8554      0.9114        0.1541  41.1306
     17      0.9780        0.0978       0.8594      0.9121        0.1494  41.0876
     18      0.9792        0.0967       0.8556      0.9113        0.1542  41.0617
     19      0.9789        0.0964       0.8564   
   0.9129        0.1519  41.0924
     20      0.9805        0.0943       0.8580      0.9115        0.1503  41.0512
     21      0.9774        0.0971       0.8613      0.9114        0.1472  41.0497
     22      0.9808        0.0924       0.8564      0.9112        0.1525  41.0575
     23      0.9820        0.0912       0.8620      0.9097        0.1481  41.0811
     24      0.9825        0.0907       0.8585   
   0.9132        0.1507  41.0947
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 11: 0.8350694444444444
F1 Score after query 11: 0.9062599943443206      
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\model_checkpoint_iteration_10.pt

Query 12: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 12 is 10000
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9718        0.1054       0.8488   
   0.9035        0.1572  66.8077
      2      0.9796        0.0917       0.8557   
   0.9070        0.1497  67.2881
      3      0.9809        0.0891       0.8528      0.9060        0.1541  67.4886
      4      0.9820        0.0873       0.8595   
   0.9091        0.1482  67.4583
      5      0.9829        0.0853       0.8587      0.9089        0.1492  67.4303
      6      0.9832        0.0843       0.8568      0.9084        0.1511  67.4603
      7      0.9828        0.0839       0.8595      0.9085        0.1441  67.4356
      8      0.9843        0.0816       0.8569      0.9079        0.1480  67.4725
      9      0.9846        0.0802       0.8594      0.9085        0.1461  67.4398
     10      0.9853        0.0787       0.8611   
   0.9101        0.1458  67.4560
     11      0.9853        0.0778       0.8601      0.9074        0.1477  67.5347
     12      0.9856        0.0768       0.8613      0.9099        0.1472  67.4829
     13      0.9859        0.0758       0.8599      0.9074        0.1476  67.4582
     14      0.9861        0.0744       0.8589      0.9064        0.1467  67.4355
     15      0.9870        0.0732       0.8575      0.9060        0.1495  67.5251
     16      0.9835        0.0778       0.8543      0.9046        0.1514  67.5307
     17      0.9858        0.0727       0.8526      0.9038        0.1510  67.5677
     18      0.9878        0.0700       0.8533      0.9035        0.1525  67.4511
     19      0.9873        0.0693       0.8538      0.9038        0.1530  67.4470
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 12: 0.8270833333333333
F1 Score after query 12: 0.902978188006006       
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\model_checkpoint_iteration_11.pt

Query 13: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 13 is 4056
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9683        0.0948       0.7000   
   0.8347        0.3170  77.8599
      2      0.9844        0.0735       0.8354   
   0.9014        0.1844  78.2449
      3      0.9874        0.0682       0.8502      0.8963        0.1546  78.2241
      4      0.9776        0.0807       0.7679      0.8768        0.2425  78.1646
      5      0.9874        0.0680       0.8186      0.8943        0.2128  78.2002
      6      0.9880        0.0649       0.8503      0.8977        0.1548  78.2216
      7      0.9887        0.0635       0.8422      0.8880        0.1628  78.1501
      8      0.9893        0.0623       0.8488   
   0.9062        0.1683  78.1213
      9      0.9879        0.0630       0.8082      0.8882        0.2508  78.2053
     10      0.9884        0.0620       0.8514   
   0.9070        0.1651  78.1299
     11      0.9896        0.0597       0.8410      0.8836        0.1708  78.1597
     12      0.9889        0.0598       0.8500      0.9040        0.1631  78.1606
     13      0.9869        0.0632       0.8203      0.8959        0.2206  78.1308
     14      0.9898        0.0580       0.8484      0.9046        0.1669  78.1966
     15      0.9835        0.0640       0.7389      0.8520        0.3442  78.205ilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\model_checkpoint_iteration_12.pt
Best F1 score across iterations: 0.9160525101168414
Performance results saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\performance_results.npy 