4
      7      0.8853        0.3964       0.7050      0.8336        0.3549  8.2385
      8      0.8873        0.3955       0.7059      0.8341        0.3543  8.2650
      9      0.8793        0.3948       0.7052      0.8340        0.3538  8.2400
     10      0.8820        0.3926       0.7059      0.8345        0.3534  8.2466
     11      0.8905        0.3916       0.7016      0.8337        0.3529  8.2422
     12      0.8823        0.3925       0.7050      0.8348        0.3524  8.2537
     13      0.8841        0.3894       0.7024      0.8343        0.3519  8.2348
     14      0.8889        0.3910       0.7063      0.8353        0.3513  8.2233
     15      0.8855        0.3905       0.7012      0.8338        0.3508  8.2079
     16      0.8829        0.3897       0.7056      0.8356        0.3501  8.2126
     17      0.8872        0.3903       0.7061      0.8363        0.3496  8.2582
     18      0.8898        0.3873       0.7054      0.8360        0.3491  8.2670
     19      0.8874        0.3863       0.7030      0.8354        0.3487  8.2694
     20      0.8861        0.3850       0.7038      0.8357        0.3482  8.2572
     21      0.8846        0.3878       0.7068      0.8370        0.3477  8.2405
     22      0.8842        0.3844       0.7005      0.8353        0.3475  8.2633
     23      0.8816        0.3833       0.7082      0.8375        0.3469  8.2449
     24      0.8978        0.3826       0.7052      0.8365        0.3465  8.2616
     25      0.8881        0.3827       0.7049      0.8372        0.3459  8.2584
     26      0.8887        0.3836       0.7050      0.8370        0.3455  8.2758
     27      0.8887        0.3810       0.7075      0.8376        0.3451  8.2782
     28      0.8889        0.3815       0.7066      0.8385        0.3448  8.2534
     29      0.8889        0.3805       0.7047      0.8373        0.3443  8.2384
     30      0.8906        0.3796       0.7028      0.8361        0.3439  8.2158
     31      0.8874        0.3818       0.7052      0.8375        0.3435  8.2170
     32      0.8904        0.3811       0.7059      0.8384        0.3430  8.2718
     33      0.8887        0.3782       0.7054      0.8379        0.3426  8.2691
     34      0.8920        0.3784       0.7056      0.8378        0.3422  8.2536
     35      0.8927        0.3763       0.7030      0.8362        0.3418  8.2545
     36      0.8945        0.3757       0.7057      0.8385        0.3412  8.2617
     37      0.8842        0.3746       0.7054      0.8377        0.3408  8.2671
     38      0.8992        0.3744       0.7042      0.8368        0.3404  8.2328
     39      0.8967        0.3755       0.7050      0.8376        0.3399  8.2632
     40      0.8977        0.3730       0.7045      0.8374        0.3395  8.2635
     41      0.8988        0.3727       0.7040      0.8372        0.3391  8.2632
     42      0.8940        0.3716       0.7042      0.8373        0.3387  8.2615
     43      0.8984        0.3671       0.7042      0.8376        0.3383  8.2778
     44      0.9005        0.3723       0.6986      0.8346        0.3380  8.2195
     45      0.8925        0.3703       0.7066      0.8392        0.3373  8.1890
     46      0.8935        0.3696       0.7066      0.8390        0.3369  8.2161
     47      0.8996        0.3684       0.7047      0.8385        0.3366  8.2195
     48      0.8951        0.3692       0.7054      0.8388        0.3361  8.2126
     49      0.8949        0.3668       0.7066      0.8396        0.3358  8.1977
     50      0.8889        0.3662       0.7043      0.8386        0.3353  8.1976
     51      0.8893        0.3682       0.7057      0.8392        0.3350  8.2123
     52      0.8989        0.3658       0.7028      0.8378        0.3346  8.2317
     53      0.8938        0.3680       0.7024      0.8383        0.3344  8.2148
     54      0.8979        0.3639       0.7057      0.8398        0.3339  8.2216
     55      0.8932        0.3661       0.7024      0.8381        0.3337  8.2200
     56      0.8997        0.3652       0.7026      0.8382        0.3334  8.2167
     57      0.8925        0.3650       0.7031      0.8383        0.3332  8.1895
     58      0.8974        0.3619       0.7045      0.8394        0.3327  8.2060
     59      0.9030        0.3618       0.7050      0.8398        0.3323  8.1808
     60      0.8983        0.3635       0.7042      0.8394        0.3320  8.2069
     61      0.8983        0.3608       0.7021      0.8389        0.3316  8.2191
     62      0.8982        0.3591       0.7023      0.8388        0.3314  8.2224
     63      0.8967        0.3603       0.7035      0.8389        0.3310  8.2023
     64      0.8957        0.3616       0.7031      0.8390        0.3306  8.2206
     65      0.8956        0.3605       0.7049      0.8397        0.3303  8.2315
     66      0.8975        0.3590       0.7042      0.8399        0.3299  8.2280
     67      0.8881        0.3597       0.7054      0.8403        0.3296  8.2223
     68      0.9002        0.3588       0.7024      0.8394        0.3293  8.2178
     69      0.8922        0.3591       0.7024      0.8395        0.3290  8.2159
     70      0.8989        0.3588       0.7031      0.8397        0.3287  8.2482
     71      0.9068        0.3587       0.7005      0.8377        0.3285  8.2304
     72      0.9022        0.3598       0.7014      0.8389        0.3282  8.1889
     73      0.9009        0.3547       0.7014      0.8389        0.3279  8.1937
     74      0.8961        0.3550       0.7012      0.8389        0.3276  8.2021
     75      0.9056        0.3558       0.7016      0.8389        0.3273  8.2403
     76      0.9002        0.3534       0.7050      0.8412        0.3268  8.2435
     77      0.9024        0.3528       0.7033      0.8405        0.3265  8.2140
     78      0.9004        0.3532       0.7033      0.8400        0.3263  8.2343
     79      0.9018        0.3530       0.7028      0.8394        0.3261  8.2159
     80      0.8909        0.3531       0.7038      0.8405        0.3256  8.2480
     81      0.9005        0.3525       0.7040      0.8410        0.3251  8.2197
     82      0.8980        0.3532       0.7043      0.8414        0.3249  8.2263
     83      0.8972        0.3507       0.7047      0.8414        0.3245  8.2256
     84      0.9001        0.3499       0.7042      0.8411        0.3243  8.2171
     85      0.9042        0.3505       0.7054      0.8418        0.3240  8.2080
     86      0.8992        0.3500       0.7002      0.8382        0.3237  8.1992
     87      0.9061        0.3486       0.7057      0.8421        0.3232  8.1853
     88      0.8975        0.3487       0.7038      0.8411        0.3229  8.2003
     89      0.9020        0.3485       0.7009      0.8384        0.3227  8.2020
     90      0.9089        0.3454       0.6997      0.8369        0.3225  8.2012
     91      0.9085        0.3477       0.7042      0.8409        0.3222  8.2214
     92      0.9005        0.3466       0.7056      0.8422        0.3218  8.2030
     93      0.9045        0.3463       0.7050      0.8420        0.3213  8.2167
     94      0.9114        0.3440       0.7005      0.8393        0.3211  8.2192
     95      0.9023        0.3460       0.7009      0.8395        0.3209  8.2299
     96      0.9018        0.3461       0.7040      0.8411        0.3206  8.2233
     97      0.8981        0.3433       0.6988      0.8380        0.3205  8.2199
     98      0.8975        0.3429       0.7050      0.8419        0.3202  8.2138
     99      0.9035        0.3426       0.7052      0.8419        0.3199  8.2293
    100      0.9074        0.3439       0.6986      0.8373        0.3198  8.1995
Accuracy after query 5: 0.7291666666666666
F1 Score after query 5: 0.8454911118918985
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run2\model_checkpoint_iteration_4.pt

Query 6: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 6 is 320
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.8953        0.3460       0.6983      0.8375        0.3194  9.1263
      2      0.8987        0.3450       0.6979      0.8379        0.3186  9.0199
      3      0.9011        0.3422       0.7003      0.8402        0.3178  9.0619
      4      0.8994        0.3424       0.6976      0.8382        0.3171  9.0917
      5      0.9046        0.3408       0.7000      0.8403        0.3163  9.0897
      6      0.8978        0.3405       0.7057      0.8435        0.3154  9.0840
      7      0.9016        0.3406       0.6990      0.8395        0.3148  9.0592
      8      0.9010        0.3370       0.6941      0.8368        0.3146  9.0677
      9      0.9019        0.3374       0.7030      0.8430        0.3135  9.0767
     10      0.9056        0.3389       0.7068      0.8457        0.3127  9.0841
     11      0.9027        0.3363       0.7003      0.8418        0.3121  9.0789
     12      0.9075        0.3344       0.7007      0.8416        0.3118  9.0852
     13      0.9038        0.3338       0.7057      0.8455        0.3114  9.0396
     14      0.8997        0.3331       0.7028      0.8434        0.3107  9.0988
     15      0.9020        0.3328       0.7092      0.8473        0.3102  9.0714
     16      0.9022        0.3309       0.6964      0.8395        0.3099  9.0812
     17      0.9085        0.3310       0.7017      0.8434        0.3090  9.0788
     18      0.9068        0.3307       0.7045      0.8457        0.3081  9.0910
     19      0.9068        0.3295       0.7069      0.8467        0.3077  9.0707
     20      0.9096        0.3298       0.7068      0.8467        0.3072  9.1038
     21      0.9065        0.3276       0.7064      0.8459        0.3070  9.0807
     22      0.9060        0.3276       0.7120      0.8493        0.3067  9.0940
     23      0.9069        0.3260       0.7120      0.8491        0.3062  9.0774
     24      0.9036        0.3264       0.7003      0.8428        0.3055  9.0916
     25      0.9064        0.3261       0.7045      0.8454        0.3047  9.0644
     26      0.9054        0.3260       0.7047      0.8453        0.3040  9.0641
     27      0.9058        0.3244       0.6984      0.8414        0.3039  9.0397
     28      0.9084        0.3228       0.7038      0.8453        0.3034  9.0650
     29      0.9096        0.3221       0.7075      0.8482        0.3026  9.0595
     30      0.9113        0.3236       0.7021      0.8435        0.3024  9.0628
     31      0.9067        0.3219       0.7045      0.8451        0.3019  9.0728
     32      0.9100        0.3212       0.7023      0.8437        0.3011  9.0651
     33      0.9123        0.3204       0.7035      0.8453        0.3003  9.0751
     34      0.9122        0.3192       0.7089      0.8493        0.2998  9.0849
     35      0.9080        0.3187       0.7057      0.8463        0.2996  9.0852
     36      0.9084        0.3200       0.7061      0.8470        0.2990  9.0915
     37      0.9087        0.3179       0.7156      0.8525        0.2990  9.0834
     38      0.9103        0.3165       0.7087      0.8492        0.2985  9.0742
     39      0.9089        0.3165       0.7059      0.8470        0.2978  9.0601
     40      0.9109        0.3153       0.7043      0.8462        0.2971  9.0585
     41      0.9139        0.3143       0.7099      0.8491        0.2965  9.0531
     42      0.9102        0.3138       0.7113      0.8507        0.2963  9.0566
     43      0.9153        0.3127       0.7106      0.8498        0.2962  9.1089
     44      0.9131        0.3127       0.6970      0.8386        0.2966  9.0804
     45      0.9149        0.3126       0.7158      0.8527        0.2958  9.0842
     46      0.9158        0.3108       0.7076      0.8477        0.2948  9.0623
     47      0.9151        0.3121       0.7002      0.8412        0.2947  9.0783
     48      0.9160        0.3102       0.7076      0.8484        0.2935  9.0726
     49      0.9143        0.3104       0.7069      0.8474        0.2932  9.0729
     50      0.9145        0.3082       0.7151      0.8523        0.2934  9.0769
     51      0.9152        0.3077       0.7026      0.8445        0.2928  9.0858
     52      0.9158        0.3084       0.7075      0.8478        0.2922  9.0614
     53      0.9140        0.3068       0.7050      0.8457        0.2914  9.0539
     54      0.9140        0.3073       0.7139      0.8516        0.2916  9.0506
     55      0.9197        0.3054       0.7087      0.8492        0.2906  9.0507
     56      0.9172        0.3056       0.7043      0.8455        0.2905  9.0886
     57      0.9185        0.3049       0.7149      0.8520        0.2903  9.0899
     58      0.9143        0.3061       0.7064      0.8468        0.2898  9.0874
     59      0.9215        0.3040       0.7045      0.8450        0.2893  9.0816
     60      0.9163        0.3026       0.7007      0.8415        0.2900  9.0787
     61      0.9194        0.3019       0.7104      0.8492        0.2884  9.0942
     62      0.9188        0.3025       0.7047      0.8450        0.2884  9.0758
     63      0.9234        0.3018       0.7050      0.8450        0.2876  9.0986
     64      0.9186        0.2998       0.7099      0.8489        0.2871  9.0710
     65      0.9221        0.3006       0.7137      0.8518        0.2871  9.0873
     66      0.9156        0.3002       0.7082      0.8473        0.2863  9.0520
     67      0.9189        0.2994       0.7255      0.8577        0.2877  9.0523
     68      0.9235        0.2987       0.7075      0.8462        0.2856  9.0718
     69      0.9199        0.2984       0.7083      0.8467        0.2853  9.0918
     70      0.9187        0.2985       0.7132      0.8508        0.2853  9.0873
     71      0.9217        0.2960       0.7092      0.8480        0.2849  9.0774
     72      0.9249        0.2954       0.7139      0.8524        0.2847  9.0956
     73      0.9181        0.2952       0.7089      0.8479        0.2843  9.0946
     74      0.9257        0.2936       0.7122      0.8516        0.2840  9.0987
     75      0.9240        0.2936       0.7139      0.8525        0.2834  9.0776
     76      0.9265        0.2935       0.7095      0.8480        0.2829  9.0865
     77      0.9264        0.2923       0.7203      0.8555        0.2828  9.0658
     78      0.9224        0.2924       0.7236      0.8571        0.2838  9.0715
     79      0.9272        0.2933       0.7125      0.8511        0.2824  9.0384
     80      0.9256        0.2914       0.7099      0.8479        0.2821  9.0467
     81      0.9248        0.2916       0.7050      0.8437        0.2820  9.0370
     82      0.9259        0.2902       0.7042      0.8430        0.2820  9.0694
     83      0.9270        0.2885       0.7234      0.8575        0.2818  9.1025
     84      0.9242        0.2898       0.7188      0.8546        0.2806  9.0770
     85      0.9277        0.2879       0.7125      0.8506        0.2804  9.0717
     86      0.9248        0.2882       0.7144      0.8525        0.2806  9.0720
     87      0.9267        0.2890       0.7125      0.8505        0.2799  9.0759
     88      0.9233        0.2877       0.7168      0.8524        0.2789  9.0820
     89      0.9294        0.2880       0.7141      0.8515        0.2792  9.0822
     90      0.9258        0.2867       0.7141      0.8513        0.2790  9.0949
     91      0.9339        0.2836       0.7238      0.8572        0.2792  9.0759
     92      0.9321        0.2854       0.7174      0.8537        0.2785  9.0575
     93      0.9299        0.2847       0.7160      0.8521        0.2778  9.0565
     94      0.9302        0.2840       0.7158      0.8522        0.2776  9.0509
     95      0.9324        0.2829       0.7142      0.8499        0.2771  9.0712
     96      0.9342        0.2819       0.7111      0.8479        0.2770  9.1452
     97      0.9296        0.2826       0.7122      0.8482        0.2765  9.0916
     98      0.9327        0.2808       0.7273      0.8586        0.2778  9.0899
     99      0.9289        0.2820       0.7304      0.8588        0.2787  9.1077
    100      0.9285        0.2812       0.7144      0.8502        0.2756  9.1050
Accuracy after query 6: 0.7477430555555555
F1 Score after query 6: 0.8555721445206358
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run2\model_checkpoint_iteration_5.pt

Query 7: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 7 is 560
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9226        0.2822       0.7312      0.8589        0.2799  10.6402
      2      0.9253        0.2799       0.7309      0.8589        0.2789  10.5274
      3      0.9279        0.2788       0.7332      0.8599        0.2775  10.4813
      4      0.9263        0.2781       0.7365      0.8616        0.2758  10.5015
      5      0.9295        0.2769       0.7356      0.8607        0.2757  10.4824
      6      0.9244        0.2780       0.7333      0.8598        0.2765  10.5252
      7      0.9261        0.2762       0.7352      0.8606        0.2766  10.5192
      8      0.9309        0.2740       0.7347      0.8603        0.2756  10.5593
      9      0.9292        0.2744       0.7354      0.8604        0.2755  10.5311
     10      0.9288        0.2734       0.7368      0.8619        0.2731  10.5578
     11      0.9287        0.2712       0.7375      0.8622        0.2724  10.5322
     12      0.9327        0.2716       0.7384      0.8613        0.2747  10.5335
     13      0.9286        0.2707       0.7373      0.8619        0.2729  10.5214
     14      0.9305        0.2693       0.7375      0.8621        0.2722  10.5353
     15      0.9334        0.2695       0.7389      0.8612        0.2742  10.5135
     16      0.9352        0.2670       0.7389      0.8629        0.2711  10.5094
     17      0.9299        0.2678       0.7396      0.8628        0.2696  10.5116
     18      0.9347        0.2653       0.7384      0.8621        0.2709  10.5561
     19      0.9312        0.2658       0.7401      0.8628        0.2704  10.5486
     20      0.9340        0.2652       0.7408      0.8623        0.2712  10.5714
     21      0.9362        0.2647       0.7408      0.8617        0.2717  10.5432
     22      0.9402        0.2626       0.7415      0.8634        0.2690  10.5545
     23      0.9385        0.2620       0.7420      0.8634        0.2688  10.5482
     24      0.9387        0.2606       0.7438      0.8639        0.2684  10.5395
     25      0.9368        0.2608       0.7429      0.8631        0.2694  10.5802
     26      0.9395        0.2589       0.7439      0.8646        0.2672  10.5170
     27      0.9388        0.2606       0.7431      0.8646        0.2665  10.5273
     28      0.9367        0.2593       0.7438      0.8631        0.2693  10.5315
     29      0.9372        0.2581       0.7450      0.8650        0.2658  10.5643
     30      0.9332        0.2597       0.7434      0.8641        0.2666  10.5519
     31      0.9376        0.2559       0.7443      0.8648        0.2658  10.5570
     32      0.9356        0.2561       0.7462      0.8661        0.2640  10.5669
     33      0.9338        0.2581       0.7455      0.8659        0.2638  10.5582
     34      0.9398        0.2533       0.7465      0.8654        0.2650  10.5582
     35      0.9401        0.2541       0.7479      0.8666        0.2615  10.5392
     36      0.9325        0.2562       0.7458      0.8657        0.2648  10.5624
     37      0.9402        0.2519       0.7479      0.8668        0.2613  10.5452
     38      0.9420        0.2514       0.7486      0.8666        0.2630  10.5312
     39      0.9376        0.2524       0.7491      0.8669        0.2627  10.5094
     40      0.9435        0.2497       0.7493      0.8676        0.2602  10.5305
     41      0.9348        0.2522       0.7474      0.8665        0.2580  10.5587
     42      0.9419        0.2501       0.7498      0.8677        0.2605  10.5491
     43      0.9391        0.2499       0.7469      0.8667        0.2598  10.5411
     44      0.9418        0.2482       0.7491      0.8667        0.2584  10.5440
     45      0.9416        0.2478       0.7495      0.8669        0.2576  10.5501
     46      0.9421        0.2466       0.7462      0.8648        0.2655  10.5584
     47      0.9386        0.2494       0.7523      0.8685        0.2574  10.5482
     48      0.9433        0.2454       0.7497      0.8675        0.2550  10.5484
     49      0.9417        0.2451       0.7505      0.8675        0.2550  10.5063
     50      0.9470        0.2438       0.7542      0.8690        0.2546  10.5060
     51      0.9299        0.2485       0.7500      0.8679        0.2558  10.4968
     52      0.9441        0.2432       0.7493      0.8675        0.2597  10.5686
     53      0.9431        0.2423       0.7524      0.8685        0.2533  10.5533
     54      0.9444        0.2407       0.7519      0.8684        0.2571  10.5330
     55      0.9475        0.2405       0.7524      0.8683        0.2530  10.5430
     56      0.9297        0.2462       0.7502      0.8689        0.2550  10.5536
     57      0.9455        0.2398       0.7510      0.8682        0.2575  10.5682
     58      0.9474        0.2383       0.7517      0.8682        0.2562  10.5586
     59      0.9478        0.2372       0.7533      0.8690        0.2539  10.5550
     60      0.9479        0.2370       0.7502      0.8660        0.2496  10.5381
     61      0.9353        0.2405       0.7517      0.8685        0.2526  10.5238
     62      0.9435        0.2377       0.7488      0.8662        0.2482  10.5203
     63      0.9464        0.2370       0.7524      0.8689        0.2543  10.5384
     64      0.9455        0.2372       0.7536      0.8696        0.2537  10.5707
     65      0.9486        0.2337       0.7507      0.8677        0.2576  10.5540
     66      0.9486        0.2343       0.7545      0.8696        0.2529  10.5463
     67      0.9505        0.2316       0.7561      0.8699        0.2490  10.5501
     68      0.9462        0.2335       0.7484      0.8652        0.2466  10.5633
     69      0.9435        0.2342       0.7528      0.8684        0.2465  10.5832
     70      0.9494        0.2326       0.7557      0.8702        0.2485  10.5580
     71      0.9420        0.2330       0.7448      0.8618        0.2676  10.5593
     72      0.9382        0.2349       0.7547      0.8698        0.2477  10.5133
     73      0.9506        0.2295       0.7559      0.8699        0.2461  10.5320
     74      0.9488        0.2295       0.7521      0.8674        0.2445  10.5124
     75      0.9438        0.2310       0.7549      0.8696        0.2451  10.5309
     76      0.9500        0.2275       0.7552      0.8706        0.2508  10.5473
     77      0.9530        0.2263       0.7552      0.8694        0.2448  10.5735
     78      0.9458        0.2287       0.7566      0.8714        0.2488  10.5428
     79      0.9486        0.2267       0.7564      0.8694        0.2436  10.5354
     80      0.9377        0.2310       0.7483      0.8650        0.2436  10.5262
     81      0.9501        0.2259       0.7552      0.8709        0.2498  10.5557
     82      0.9505        0.2252       0.7552      0.8703        0.2506  10.5172
     83      0.9539        0.2242       0.7601      0.8720        0.2446  10.5184
     84      0.9539        0.2228       0.7514      0.8651        0.2412  10.5015
     85      0.9425        0.2282       0.7507      0.8667        0.2417  10.4979
     86      0.9530        0.2227       0.7594      0.8716        0.2422  10.5566
     87      0.9555        0.2204       0.7559      0.8712        0.2483  10.5516
     88      0.9484        0.2238       0.7595      0.8712        0.2420  10.5427
     89      0.9390        0.2268       0.7495      0.8665        0.2416  10.5425
     90      0.9535        0.2201       0.7606      0.8730        0.2438  10.5307
     91      0.9516        0.2191       0.7604      0.8732        0.2425  10.5324
     92      0.9558        0.2180       0.7609      0.8722        0.2405  10.5460
     93      0.9439        0.2205       0.7528      0.8670        0.2396  10.5319
     94      0.9549        0.2159       0.7613      0.8732        0.2414  10.5315
     95      0.9491        0.2229       0.7521      0.8649        0.2371  10.5210
     96      0.9574        0.2156       0.7616      0.8735        0.2410  10.4954
     97      0.9547        0.2169       0.7495      0.8630        0.2370  10.5391
     98      0.9548        0.2150       0.7530      0.8654        0.2368  10.5547
     99      0.9594        0.2140       0.7554      0.8676        0.2368  10.5686
    100      0.9566        0.2145       0.7530      0.8640        0.2356  10.5347
Accuracy after query 7: 0.7822916666666667
F1 Score after query 7: 0.8786673025961725
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run2\model_checkpoint_iteration_6.pt

Query 8: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 8 is 1000
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9299        0.2278       0.7634      0.8735        0.2353  13.3853
      2      0.9409        0.2225       0.7686      0.8748        0.2346  13.1344
      3      0.9410        0.2221       0.7665      0.8759        0.2398  13.1617
      4      0.9442        0.2214       0.7625      0.8688        0.2277  13.1594
      5      0.9453        0.2197       0.7714      0.8775        0.2312  13.1689
      6      0.9441        0.2183       0.7731      0.8782        0.2334  13.2078
      7      0.9423        0.2184       0.7701      0.8750        0.2282  13.2018
      8      0.9447        0.2190       0.7634      0.8671        0.2255  13.2321
      9      0.9464        0.2153       0.7705      0.8775        0.2349  13.2192
     10      0.9463        0.2149       0.7740      0.8787        0.2312  13.2440
     11      0.9470        0.2140       0.7750      0.8790        0.2301  13.2393
     12      0.9510        0.2114       0.7741      0.8731        0.2215  13.2133
     13      0.9435        0.2136       0.7752      0.8789        0.2310  13.1726
     14      0.9418        0.2136       0.7760      0.8792        0.2293  13.1832
     15      0.9478        0.2116       0.7780      0.8795        0.2243  13.2026
     16      0.9527        0.2086       0.7776      0.8774        0.2205  13.2238
     17      0.9437        0.2099       0.7811      0.8810        0.2261  13.1947
     18      0.9544        0.2070       0.7781      0.8725        0.2185  13.1977
     19      0.9450        0.2095       0.7825      0.8819        0.2248  13.2060
     20      0.9475        0.2073       0.7814      0.8814        0.2247  13.2091
     21      0.9526        0.2047       0.7750      0.8689        0.2198  13.1949
     22      0.9516        0.2037       0.7840      0.8786        0.2160  13.2029
     23      0.9494        0.2046       0.7764      0.8726        0.2178  13.1719
     24      0.9491        0.2024       0.7743      0.8781        0.2342  13.2037
     25      0.9508        0.2025       0.7766      0.8785        0.2351  13.2158
     26      0.9537        0.2005       0.7891      0.8819        0.2140  13.2541
     27      0.9499        0.2013       0.7873      0.8781        0.2124  13.2684
     28      0.9576        0.1975       0.7858      0.8830        0.2234  13.2658
     29      0.9577        0.1968       0.7785      0.8785        0.2335  13.2385
     30      0.9524        0.1985       0.7910      0.8825        0.2133  13.2304
     31      0.9547        0.1968       0.7943      0.8816        0.2099  13.1989
     32      0.9542        0.1959       0.7946      0.8873        0.2169  13.1889
     33      0.9563        0.1950       0.7960      0.8862        0.2138  13.2165
     34      0.9575        0.1928       0.7736      0.8750        0.2403  13.2386
     35      0.9446        0.2011       0.7882      0.8860        0.2160  13.2433
     36      0.9579        0.1916       0.7922      0.8859        0.2182  13.2299
     37      0.9586        0.1908       0.7918      0.8854        0.2197  13.2074
     38      0.9587        0.1906       0.7988      0.8880        0.2145  13.2386
     39      0.9549        0.1910       0.8003      0.8879        0.2115  13.2407
     40      0.9602        0.1887       0.7920      0.8853        0.2197  13.1866
     41      0.9516        0.1923       0.7821      0.8802        0.2308  13.2100
     42      0.9623        0.1872       0.7854      0.8815        0.2276  13.2297
     43      0.9569        0.1881       0.7903      0.8839        0.2208  13.2485
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 8: 0.8069444444444445
F1 Score after query 8: 0.8925214186255754
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run2\model_checkpoint_iteration_7.pt

Query 9: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 9 is 1776
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9458        0.1987       0.7833      0.8819        0.2262  18.0364
      2      0.9462        0.1992       0.7990      0.8914        0.2114  17.8372
      3      0.9486        0.1949       0.8056      0.8943        0.2056  17.8055
      4      0.9504        0.1927       0.8010      0.8917        0.2117  17.9152
      5      0.9526        0.1904       0.8054      0.8939        0.2085  17.9364
      6      0.9510        0.1895       0.8115      0.8965        0.2037  17.9598
      7      0.9530        0.1881       0.8057      0.8943        0.2070  17.9727
      8      0.9521        0.1865       0.8115      0.8964        0.2045  17.9472
      9      0.9532        0.1863       0.8172      0.8963        0.1960  17.9654
     10      0.9545        0.1839       0.8141      0.8974        0.2021  17.9107
     11      0.9550        0.1829       0.8170      0.8988        0.2009  17.9396
     12      0.9564        0.1807       0.8064      0.8927        0.2085  17.9513
     13      0.9565        0.1801       0.8182      0.8996        0.1994  17.9032
     14      0.9574        0.1787       0.8217      0.8987        0.1936  17.8834
     15      0.9554        0.1792       0.8116      0.8964        0.2041  17.9052
     16      0.9597        0.1759       0.8205      0.9002        0.1978  17.8494
     17      0.9592        0.1745       0.8139      0.8964        0.2029  17.8269
     18      0.9581        0.1738       0.8175      0.8985        0.2005  17.8877
     19      0.9599        0.1721       0.8217      0.8970        0.1907  17.8931
     20      0.9541        0.1765       0.8205      0.9005        0.1965  17.8800
     21      0.9606        0.1701       0.8233      0.9008        0.1952  17.8963
     22      0.9616        0.1686       0.8227      0.9007        0.1943  17.9254
     23      0.9612        0.1674       0.8151      0.8968        0.2032  17.8519
     24      0.9588        0.1683       0.8245      0.9014        0.1909  17.8655
     25      0.9553        0.1712       0.8250      0.9000        0.1876  17.9037
     26      0.9610        0.1657       0.8247      0.8994        0.1871  17.9170
     27      0.9638        0.1642       0.8253      0.9016        0.1909  17.8977
     28      0.9620        0.1636       0.8233      0.9026        0.1930  17.9278
     29      0.9644        0.1627       0.8260      0.9019        0.1880  17.8918
     30      0.9619        0.1619       0.8264      0.9012        0.1855  17.8650
     31      0.9620        0.1616       0.8306      0.8979        0.1812  17.8776
     32      0.9622        0.1609       0.8264      0.9014        0.1846  17.8988
     33      0.9610        0.1616       0.8280      0.9004        0.1828  17.8961
     34      0.9645        0.1576       0.8260      0.9015        0.1849  17.8672
     35      0.9649        0.1568       0.8264      0.9022        0.1861  17.9154
     36      0.9645        0.1563       0.8269      0.9010        0.1827  17.8932
     37      0.9674        0.1554       0.8292      0.9002        0.1821  17.8263
     38      0.9622        0.1567       0.8290      0.8950        0.1805  17.9145
     39      0.9617        0.1561       0.8281      0.9025        0.1820  17.9212
     40      0.9637        0.1532       0.8274      0.9023        0.1835  17.9148
     41      0.9674        0.1518       0.8316      0.9020        0.1807  17.8920
     42      0.9665        0.1513       0.8321      0.9037        0.1806  17.9238
     43      0.9688        0.1498       0.8274      0.9027        0.1848  17.8571
     44      0.9665        0.1502       0.8325      0.9033        0.1803  17.8634
     45      0.9662        0.1499       0.8325      0.9041        0.1792  17.9098
     46      0.9669        0.1481       0.8309      0.8916        0.1791  17.9232
     47      0.9672        0.1479       0.8319      0.9040        0.1799  17.8951
     48      0.9662        0.1466       0.8311      0.8923        0.1785  17.9057
     49      0.9695        0.1452       0.8276      0.8854        0.1807  17.8934
     50      0.9666        0.1465       0.8233      0.9015        0.1888  17.8582
     51      0.9679        0.1438       0.8309      0.9043        0.1799  17.8549
     52      0.9690        0.1437       0.8351      0.8960        0.1762  17.8971
     53      0.9676        0.1443       0.8094      0.8674        0.1881  17.9112
     54      0.9696        0.1423       0.8365      0.9014        0.1763  17.8937
     55      0.9696        0.1410       0.8332      0.9040        0.1785  17.9107
     56      0.9704        0.1403       0.8293      0.8882        0.1776  17.8689
     57      0.9681        0.1407       0.8337      0.8952        0.1760  17.8227
     58      0.9662        0.1420       0.8347      0.8961        0.1750  17.8986
     59      0.9690        0.1386       0.8352      0.9025        0.1768  17.9206
     60      0.9673        0.1399       0.8356      0.9010        0.1756  17.9126
     61      0.9695        0.1374       0.8210      0.8768        0.1814  17.8876
     62      0.9718        0.1366       0.8328      0.8973        0.1760  17.9072
     63      0.9706        0.1365       0.8290      0.9038        0.1797  17.8872
     64      0.9698        0.1361       0.8250      0.8865        0.1771  17.8596
     65      0.9718        0.1341       0.8311      0.8924        0.1756  17.8939
     66      0.9710        0.1340       0.8283      0.9008        0.1789  17.9086
     67      0.9738        0.1327       0.8253      0.8850        0.1777  17.9317
     68      0.9688        0.1354       0.8328      0.8941        0.1746  17.8956
     69      0.9676        0.1356       0.8323      0.9032        0.1759  17.9056
     70      0.9727        0.1313       0.8321      0.8935        0.1753  17.8313
     71      0.9716        0.1309       0.8288      0.8885        0.1750  17.8847
     72      0.9732        0.1297       0.8226      0.8797        0.1791  17.8952
     73      0.9718        0.1298       0.8307      0.8936        0.1744  17.8876
     74      0.9736        0.1290       0.8257      0.8865        0.1753  17.9208
     75      0.9735        0.1291       0.8187      0.8769        0.1795  17.8835
     76      0.9747        0.1272       0.8220      0.8825        0.1771  17.8552
     77      0.9738        0.1270       0.8342      0.8979        0.1741  17.8628
     78      0.9752        0.1262       0.8290      0.8918        0.1749  17.8966
     79      0.9726        0.1261       0.8035      0.8561        0.1881  17.8962
     80      0.9727        0.1264       0.8286      0.8918        0.1737  17.9097
     81      0.9718        0.1261       0.8118      0.8670        0.1832  17.8853
     82      0.9749        0.1237       0.8137      0.8692        0.1832  17.9090
     83      0.9736        0.1238       0.8229      0.8788        0.1776  17.8942
     84      0.9737        0.1235       0.8245      0.8839        0.1743  17.8513
     85      0.9760        0.1219       0.8115      0.8670        0.1843  17.9127
     86      0.9756        0.1216       0.8130      0.8690        0.1824  17.9038
     87      0.9715        0.1240       0.8241      0.8818        0.1756  17.9300
     88      0.9736        0.1216       0.8319      0.8964        0.1726  17.9052
     89      0.9767        0.1201       0.8236      0.8808        0.1769  17.8861
     90      0.9762        0.1200       0.8295      0.9004        0.1808  17.8619
     91      0.9738        0.1208       0.8311      0.8976        0.1748  17.8558
     92      0.9761        0.1191       0.8017      0.8519        0.1939  17.9228
     93      0.9754        0.1179       0.8094      0.8663        0.1847  17.9104
     94      0.9734        0.1194       0.8281      0.8936        0.1735  17.8933
     95      0.9751        0.1179       0.8108      0.8635        0.1850  17.9119
     96      0.9769        0.1166       0.8314      0.8998        0.1763  17.8816
     97      0.9777        0.1158       0.8262      0.8854        0.1760  17.8271
     98      0.9777        0.1162       0.8050      0.8565        0.1899  17.9012
     99      0.9771        0.1151       0.8299      0.9002        0.1793  17.9107
    100      0.9755        0.1150       0.8276      0.8992        0.1829  17.9019
Accuracy after query 9: 0.8164930555555555
F1 Score after query 9: 0.9006010664873921
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run2\model_checkpoint_iteration_8.pt

Query 10: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 10 is 3160
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9622        0.1337       0.8392      0.8852        0.1701  26.1783
      2      0.9669        0.1268       0.8352      0.8813        0.1712  26.0806
      3      0.9684        0.1255       0.8458      0.8941        0.1650  26.2271
      4      0.9691        0.1235       0.8472      0.8936        0.1648  26.3082
      5      0.9708        0.1220       0.8564      0.9047        0.1618  26.3101
      6      0.9699        0.1209       0.8557      0.9057        0.1605  26.2551
      7      0.9722        0.1195       0.8595      0.9109        0.1610  26.2416
      8      0.9715        0.1189       0.8597      0.9114        0.1594  26.2865
      9      0.9723        0.1174       0.8623      0.9128        0.1592  30.3983
     10      0.9721        0.1169       0.8543      0.9115        0.1638  32.3420
     11      0.9731        0.1161       0.8554      0.9110        0.1621  31.9124
     12      0.9726        0.1149       0.8602      0.9121        0.1596  32.1407
     13      0.9732        0.1143       0.8556      0.9119        0.1640  32.2285
     14      0.9733        0.1141       0.8481      0.9099        0.1667  32.2313
     15      0.9745        0.1123       0.8464      0.9093        0.1677  31.8934
     16      0.9736        0.1120       0.8622      0.9142        0.1589  31.8712
     17      0.9750        0.1108       0.8582      0.9125        0.1589  31.7987
     18      0.9743        0.1102       0.8469      0.9097        0.1672  31.8561
     19      0.9750        0.1095       0.8582      0.9130        0.1598  32.9603
     20      0.9758        0.1089       0.8434      0.9081        0.1697  32.2978
     21      0.9755        0.1076       0.8536      0.9116        0.1632  32.0805
48
     26      0.9768        0.1036       0.8575      0.9115        0.1618  32.3092
     27      0.9783        0.1025       0.8547      0.9104        0.1639  28.1567
     28      0.9783        0.1016       0.8543      0.9106        0.1632  26.1710
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 10: 0.8317708333333333
F1 Score after query 10: 0.9075349804693476
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run2\model_checkpoint_iteration_9.pt

Query 11: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 11 is 5624
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9689        0.1124       0.6354      0.8206        0.4068  42.7782
      2      0.9691        0.1134       0.7342      0.8581        0.3027  42.8631
      3      0.9718        0.1082       0.7609      0.8627        0.2947  43.2367
      4      0.9736        0.1063       0.8151      0.8955        0.1909  43.3552
      5      0.9740        0.1043       0.8002      0.8862        0.2241  43.3581
      6      0.9742        0.1033       0.7842      0.8780        0.2492  42.0690
      7      0.9751        0.1017       0.7898      0.8795        0.2469  41.0917
      8      0.9755        0.1008       0.8134      0.8933        0.1989  41.0724
      9      0.9762        0.0992       0.8042      0.8883        0.2196  41.0744
     10      0.9764        0.0985       0.7946      0.8836        0.2340  41.0379
     11      0.9761        0.0983       0.8071      0.8917        0.2058  41.0612
     12      0.9765        0.0968       0.8144      0.8950        0.1938  41.0704
     13      0.9779        0.0947       0.7957      0.8848        0.2290  41.0634
     14      0.9776        0.0947       0.8076      0.8911        0.2113  41.1532
     15      0.9777        0.0936       0.7906      0.8823        0.2369  41.1752
     16      0.9795        0.0924       0.8000      0.8871        0.2223  41.1102
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 11: 0.8135416666666667
F1 Score after query 11: 0.8963355496146853
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run2\model_checkpoint_iteration_10.pt

Query 12: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 12 is 10000
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9754        0.0946       0.8495      0.8938        0.1626  66.8665
      2      0.9768        0.0924       0.8483      0.8930        0.1604  67.3631
      3      0.9789        0.0893       0.8389      0.8829        0.1655  67.5888
      4      0.9790        0.0880       0.8425      0.8851        0.1635  67.4356
      5      0.9801        0.0865       0.8361      0.8787        0.1681  67.5806
      6      0.9805        0.0850       0.8345      0.8771        0.1671  67.5185
      7      0.9808        0.0836       0.8443      0.8910        0.1649  67.5222
      8      0.9811        0.0821       0.8458      0.8897        0.1651  67.4752
      9      0.9816        0.0811       0.8536      0.8969        0.1649  67.4584
     10      0.9819        0.0797       0.8387      0.8815        0.1671  67.4522
     11      0.9820        0.0789       0.8354      0.8785        0.1680  67.5183
     12      0.9822        0.0774       0.8361      0.8733        0.1743  67.4936
     13      0.9836        0.0752       0.8384      0.8769        0.1724  67.4352
     14      0.9838        0.0740       0.8335      0.8694        0.1775  67.5376
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 12: 0.8260416666666667
F1 Score after query 12: 0.9011946715925275
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run2\model_checkpoint_iteration_11.pt

Query 13: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 13 is 4056
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9816        0.0785       0.8007      0.8890        0.2100  77.9234
      2      0.9820        0.0746       0.8394      0.8858        0.1656  78.2750
      3      0.9843        0.0721       0.8340      0.8998        0.1888  78.3193
      4      0.9822        0.0744       0.8356      0.9032        0.1842  78.2473
      5      0.9848        0.0699       0.8526      0.9064        0.1698  78.2116
      6      0.9840        0.0703       0.8450      0.8964        0.1707  78.2708
      7      0.9863        0.0673       0.8474      0.9043        0.1797  78.2002
      8      0.9862        0.0661       0.8559      0.9034        0.1628  78.2299
      9      0.9865        0.0653       0.8559      0.9028        0.1658  78.1487
     10      0.9867        0.0650       0.8405      0.8970        0.1793  78.0514
     11      0.9877        0.0630       0.8464      0.8934        0.1658  78.2406
     12      0.9879        0.0619       0.8503      0.8957        0.1695  78.1672
     13      0.9877        0.0614       0.8503      0.8987        0.1681  78.2018
     14      0.9872        0.0615       0.8547      0.9031        0.1668  78.3501
     15      0.9886        0.0594       0.8498      0.8968        0.1696  78.2386
     16      0.9890        0.0592       0.8363      0.8851        0.1796  78.1550
     17      0.9886        0.0585       0.8467      0.9003        0.1750  78.3977
     18      0.9894        0.0572       0.8422      0.8913        0.1771  78.5385
     19      0.9897        0.0560       0.8521      0.9008        0.1775  78.1425
     20      0.9885        0.0566       0.8530      0.8974        0.1695  78.2375
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 13: 0.8362847222222223
F1 Score after query 13: 0.9092861515659624
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run2\model_checkpoint_iteration_12.pt
Best F1 score across iterations: 0.9092861515659624
Performance results saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\withseed\avg_confidence\Run2\performance_results.npy