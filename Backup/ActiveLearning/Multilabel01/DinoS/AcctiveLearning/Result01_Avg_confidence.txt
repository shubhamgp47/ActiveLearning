     66      0.3985        0.6227       0.4483      0.0394        0.6418  7.2861
     67      0.2929        0.6157       0.4483      0.0411        0.6414  7.2881
     68      0.4389        0.6036       0.4483      0.0421        0.6411  7.2945
     69      0.3094        0.6162       0.4483      0.0457        0.6408  7.2930
     70      0.3750        0.6152       0.4483      0.0485        0.6402  7.3001
     71      0.3926        0.6042       0.4483      0.0491        0.6398  7.2658
     72      0.2872        0.6065       0.4483      0.0505        0.6396  7.2629
     73      0.3796        0.6199       0.4483      0.0538        0.6392  7.3008
     74      0.3434        0.5977       0.4483      0.0577        0.6388  7.2686
     75      0.4936        0.5991       0.4483      0.0624        0.6384  7.2289
     76      0.5905        0.6136       0.4483      0.0623        0.6380  7.2599
     77      0.3247        0.5985       0.4483      0.0647        0.6374  7.2503
     78      0.4078        0.6079       0.4483      0.0620        0.6370  7.2875
     79      0.3556        0.6074       0.4483      0.0560        0.6369  7.2701
     80      0.2372        0.6195       0.4483      0.0558        0.6365  7.2921
     81      0.3111        0.5897       0.4483      0.0573        0.6363  7.2840
     82      0.2824        0.6106       0.4483      0.0647        0.6360  7.2791
     83      0.3723        0.5991       0.4483      0.0667        0.6358  7.3808
     84      0.2872        0.6097       0.4483      0.0716        0.6354  7.3221
     85      0.3238        0.6013       0.4483      0.0739        0.6350  7.3202
     86      0.4040        0.5954       0.4486      0.0853        0.6344  7.3208
     87      0.4296        0.6014       0.4486      0.0851        0.6343  7.2812
     88      0.3556        0.5913       0.4484      0.0815        0.6340  7.2925
     89      0.3556        0.6003       0.4483      0.0803        0.6339  7.2960
     90      0.3417        0.6063       0.4484      0.0823        0.6337  7.3793
     91      0.3761        0.5997       0.4484      0.0827        0.6334  7.3625
     92      0.2824        0.5890       0.4484      0.0860        0.6328  7.3543
     93      0.7374        0.5923       0.4488      0.0930        0.6322  7.3576
     94      0.3486        0.6018       0.4495      0.1018        0.6316  7.3187
     95      0.5397        0.5996       0.4491      0.0986        0.6312  7.3318
     96      0.4040        0.5895       0.4495      0.0992        0.6308  7.3695
     97      0.3111        0.5873       0.4500      0.1037        0.6306  7.3370
     98      0.4389        0.5968       0.4509      0.1075        0.6303  7.3946
     99      0.3556        0.5942       0.4516      0.1126        0.6299  7.4154
    100      0.4190        0.5901       0.4519      0.1166        0.6296  7.4046
Accuracy after query 1: 0.4930555555555556
F1 Score after query 1: 0.1311039496053991
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_0.pt

Query 2: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 2 is 32
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.3449        0.6288       0.4559      0.1305        0.6286  7.4915
      2      0.6512        0.6397       0.4587      0.1386        0.6276  7.4629
      3      0.3409        0.6364       0.4599      0.1440        0.6267  7.4912
      4      0.4530        0.6301       0.4630      0.1503        0.6257  8.0522
      5      0.3401        0.6260       0.4675      0.1617        0.6244  8.1556
      6      0.3597        0.6250       0.4698      0.1678        0.6233  8.0364
      7      0.5678        0.6299       0.4753      0.1785        0.6221  8.0392
      8      0.3045        0.6319       0.4778      0.1819        0.6211  8.0315
      9      0.3173        0.6292       0.4804      0.1848        0.6201  8.0484
     10      0.3714        0.6229       0.4821      0.1890        0.6191  8.0448
     11      0.3919        0.6303       0.4847      0.1940        0.6178  8.0578
     12      0.3758        0.6160       0.4878      0.2030        0.6163  8.0288
     13      0.2969        0.6358       0.4884      0.2012        0.6153  8.0456
     14      0.3169        0.6320       0.4903      0.2050        0.6141  8.0471
     15      0.3480        0.6219       0.4922      0.2080        0.6130  8.0412
     16      0.3273        0.6186       0.4927      0.2081        0.6120  8.0499
     17      0.3294        0.6222       0.4955      0.2141        0.6106  8.0624
     18      0.3223        0.6258       0.4965      0.2166        0.6094  8.0470
     19      0.3173        0.6146       0.4977      0.2199        0.6081  8.0528
     20      0.3559        0.6254       0.5007      0.2280        0.6066  8.0469
     21      0.3166        0.6156       0.5016      0.2366        0.6051  8.0468
     22      0.5280        0.6154       0.5026      0.2380        0.6038  8.0935
     23      0.4018        0.6192       0.5030      0.2417        0.6024  7.3860
     24      0.4437        0.6097       0.5035      0.2456        0.6008  7.4516
     25      0.3485        0.6201       0.5038      0.2490        0.5993  7.4334
     26      0.3858        0.6129       0.5040      0.2498        0.5981  7.4622
     27      0.3904        0.6129       0.5040      0.2445        0.5968  7.4650
     28      0.4352        0.6139       0.5043      0.2388        0.5956  7.3797
     29      0.4163        0.6082       0.5043      0.2459        0.5939  7.3971
     30      0.4432        0.6018       0.5043      0.2510        0.5923  7.3904
     31      0.4893        0.6047       0.5047      0.2571        0.5909  7.4058
     32      0.4262        0.6069       0.5043      0.2602        0.5896  7.4211
     33      0.4847        0.6083       0.5045      0.2663        0.5882  7.4292
     34      0.4807        0.6101       0.5047      0.2678        0.5868  7.4141
     35      0.4796        0.6009       0.5049      0.2720        0.5855  7.4470
     36      0.4458        0.6031       0.5054      0.2724        0.5843  7.3959
     37      0.4209        0.5963       0.5049      0.2809        0.5827  7.3877
     38      0.4577        0.6036       0.5050      0.2851        0.5814  7.4362
     39      0.4935        0.5936       0.5052      0.2919        0.5801  7.3808
     40      0.4904        0.5966       0.5061      0.2946        0.5788  7.4197
     41      0.5028        0.5900       0.5059      0.3016        0.5773  7.4644
     42      0.5038        0.5912       0.5057      0.3080        0.5759  7.3742
     43      0.5478        0.5981       0.5064      0.3101        0.5746  7.4356
     44      0.5554        0.5910       0.5062      0.3111        0.5735  7.4386
     45      0.4712        0.5913       0.5068      0.3218        0.5718  7.3927
     46      0.4989        0.5896       0.5071      0.3270        0.5703  7.3894
     47      0.5227        0.5864       0.5071      0.3308        0.5690  7.4065
     48      0.5373        0.5896       0.5082      0.3401        0.5675  7.3845
     49      0.5449        0.5907       0.5078      0.3380        0.5663  7.3708
     50      0.4670        0.5904       0.5089      0.3457        0.5646  7.3853
     51      0.5380        0.5856       0.5087      0.3535        0.5631  7.3802
     52      0.5333        0.5842       0.5094      0.3649        0.5613  7.3870
     53      0.7729        0.5827       0.5085      0.3781        0.5596  7.3705
     54      0.6022        0.5772       0.5083      0.3876        0.5578  7.3824
     55      0.6012        0.5728       0.5092      0.3910        0.5563  7.3689
     56      0.5118        0.5794       0.5095      0.3977        0.5547  7.3888
     57      0.5788        0.5776       0.5097      0.4007        0.5531  7.4504
     58      0.5902        0.5727       0.5102      0.4033        0.5517  7.4499
     59      0.6308        0.5740       0.5102      0.4030        0.5504  7.4933
     60      0.5754        0.5697       0.5106      0.4068        0.5488  7.9996
     61      0.5641        0.5680       0.5108      0.4126        0.5472  8.1515
     62      0.5919        0.5631       0.5111      0.4209        0.5456  8.1866
     63      0.6447        0.5623       0.5104      0.4271        0.5441  8.0560
     64      0.6154        0.5654       0.5113      0.4320        0.5425  8.0925
     65      0.6427        0.5719       0.5118      0.4431        0.5409  7.5068
     66      0.6284        0.5683       0.5122      0.4574        0.5392  7.3966
     67      0.7098        0.5624       0.5111      0.4696        0.5376  7.4001
     68      0.6474        0.5659       0.5116      0.4762        0.5363  7.3817
     69      0.6404        0.5637       0.5104      0.4827        0.5349  7.4185
     70      0.6827        0.5533       0.5135      0.4837        0.5334  7.3896
     71      0.5743        0.5611       0.5137      0.4952        0.5318  7.4143
     72      0.6927        0.5572       0.5163      0.5088        0.5301  7.4426
     73      0.6916        0.5607       0.5175      0.5149        0.5287  7.5399
     74      0.6284        0.5559       0.5214      0.5233        0.5271  7.4467
     75      0.5577        0.5641       0.5231      0.5295        0.5257  7.4122
     76      0.7198        0.5523       0.5236      0.5385        0.5245  7.4517
     77      0.6489        0.5560       0.5276      0.5465        0.5230  7.4469
     78      0.6775        0.5467       0.5300      0.5528        0.5216  7.3808
     79      0.6862        0.5407       0.5326      0.5564        0.5204  7.4310
     80      0.6847        0.5543       0.5342      0.5600        0.5191  7.4213
     81      0.7242        0.5426       0.5361      0.5663        0.5176  7.4265
     82      0.6873        0.5384       0.5382      0.5692        0.5160  7.4213
     83      0.7171        0.5434       0.5398      0.5712        0.5146  7.4523
     84      0.7131        0.5428       0.5437      0.5806        0.5133  7.4164
     85      0.7166        0.5423       0.5453      0.5860        0.5120  7.4376
     86      0.7727        0.5425       0.5486      0.5899        0.5106  7.4170
     87      0.7338        0.5464       0.5490      0.5943        0.5092  7.3929
     88      0.7296        0.5406       0.5507      0.5954        0.5080  7.4115
     89      0.8009        0.5387       0.5509      0.5975        0.5067  7.4412
     90      0.7268        0.5300       0.5517      0.5977        0.5055  7.4378
     91      0.7614        0.5323       0.5543      0.6017        0.5039  7.4994
     92      0.7428        0.5320       0.5550      0.6027        0.5028  7.3990
     93      0.7650        0.5273       0.5576      0.6106        0.5016  7.4671
     94      0.6903        0.5354       0.5590      0.6137        0.5003  7.4460
     95      0.7415        0.5287       0.5613      0.6196        0.4991  7.4621
     96      0.7490        0.5398       0.5658      0.6257        0.4978  7.4806
     97      0.8081        0.5188       0.5684      0.6331        0.4966  7.4599
     98      0.7485        0.5230       0.5738      0.6416        0.4953  7.4551
     99      0.8206        0.5240       0.5734      0.6434        0.4944  7.3861
    100      0.7422        0.5314       0.5809      0.6516        0.4932  7.4591
Accuracy after query 2: 0.6369791666666667
F1 Score after query 2: 0.7162000335468629
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_1.pt

Query 3: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 3 is 56
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.6862        0.5297       0.5755      0.6442        0.4926  7.6040
      2      0.6913        0.5320       0.5715      0.6339        0.4920  7.5571
      3      0.7076        0.5274       0.5708      0.6317        0.4912  7.6090
      4      0.6557        0.5261       0.5722      0.6331        0.4904  7.5784
      5      0.6983        0.5271       0.5731      0.6331        0.4896  7.5779
      6      0.7390        0.5256       0.5717      0.6290        0.4890  7.5574
      7      0.6954        0.5189       0.5740      0.6344        0.4882  7.5450
      8      0.7183        0.5227       0.5762      0.6362        0.4874  7.5091
      9      0.7116        0.5187       0.5797      0.6412        0.4864  7.5315
     10      0.7043        0.5213       0.5826      0.6466        0.4853  7.5452
     11      0.7618        0.5128       0.5859      0.6502        0.4843  7.5428
     12      0.6900        0.5221       0.5899      0.6543        0.4832  7.5934
     13      0.7314        0.5141       0.5910      0.6556        0.4825  7.5644
     14      0.7049        0.5152       0.5924      0.6586        0.4817  7.5687
     15      0.7716        0.5114       0.5948      0.6618        0.4808  7.5245
     16      0.7291        0.5145       0.5990      0.6683        0.4797  7.5804
     17      0.7815        0.5092       0.6030      0.6731        0.4786  7.4754
     18      0.7137        0.5158       0.6042      0.6746        0.4778  7.4686
     19      0.7753        0.5153       0.6061      0.6760        0.4769  7.6351
     20      0.7154        0.5132       0.6056      0.6762        0.4762  7.5575
     21      0.7164        0.5113       0.6075      0.6789        0.4753  7.5652
     22      0.7079        0.5118       0.6116      0.6855        0.4743  7.6317
     23      0.7458        0.5126       0.6132      0.6873        0.4734  7.4926
     24      0.7456        0.5107       0.6158      0.6911        0.4725  7.5793
     25      0.7413        0.5079       0.6161      0.6912        0.4718  7.5373
     26      0.7610        0.5101       0.6193      0.6961        0.4709  7.5138
     27      0.7522        0.5085       0.6203      0.6980        0.4701  7.5803
     28      0.6920        0.5101       0.6255      0.7058        0.4688  7.5902
     29      0.7619        0.5055       0.6286      0.7108        0.4678  7.5685
     30      0.7356        0.5073       0.6318      0.7155        0.4669  7.5421
     31      0.7212        0.5030       0.6347      0.7192        0.4662  7.6126
     32      0.7330        0.5055       0.6385      0.7244        0.4653  7.5639
     33      0.7703        0.5045       0.6436      0.7317        0.4641  7.5950
     34      0.7872        0.5010       0.6453      0.7347        0.4633  7.5332
     35      0.7796        0.4979       0.6476      0.7398        0.4622  7.5550
     36      0.8061        0.5053       0.6486      0.7413        0.4615  7.6085
     37      0.7685        0.4972       0.6503      0.7433        0.4607  7.6193
     38      0.7652        0.4967       0.6503      0.7428        0.4602  7.5657
     39      0.7962        0.4965       0.6514      0.7443        0.4594  7.5557
     40      0.7689        0.4960       0.6538      0.7471        0.4584  7.5674
     41      0.7382        0.5015       0.6547      0.7483        0.4575  7.5325
     42      0.7803        0.4979       0.6550      0.7485        0.4570  7.5346
     43      0.7938        0.4914       0.6566      0.7501        0.4563  7.5727
     44      0.7732        0.5026       0.6566      0.7503        0.4557  7.5493
     45      0.7489        0.4962       0.6604      0.7547        0.4548  7.5230
     46      0.7683        0.4992       0.6611      0.7564        0.4541  7.5239
     47      0.7549        0.4942       0.6625      0.7583        0.4534  7.5171
     48      0.7683        0.4876       0.6651      0.7619        0.4524  7.5193
     49      0.8354        0.4856       0.6674      0.7643        0.4517  7.5123
     50      0.8188        0.4867       0.6687      0.7666        0.4508  7.4995
     51      0.7785        0.4967       0.6705      0.7688        0.4500  7.5322
     52      0.7983        0.4981       0.6700      0.7684        0.4496  7.5255
     53      0.7701        0.4929       0.6693      0.7682        0.4491  7.5291
     54      0.8048        0.4878       0.6710      0.7704        0.4482  7.5413
     55      0.7606        0.4857       0.6753      0.7751        0.4472  7.5311
     56      0.8086        0.4856       0.6734      0.7734        0.4469  7.5261
     57      0.7817        0.4871       0.6741      0.7741        0.4463  7.5239
     58      0.7673        0.4886       0.6760      0.7762        0.4454  7.5244
     59      0.7771        0.4880       0.6773      0.7777        0.4448  7.5285
     60      0.7695        0.4836       0.6795      0.7805        0.4440  7.5341
     61      0.8122        0.4850       0.6785      0.7792        0.4435  7.5229
     62      0.8332        0.4809       0.6800      0.7805        0.4428  7.5412
     63      0.8008        0.4823       0.6807      0.7816        0.4422  7.4979
     64      0.8169        0.4819       0.6828      0.7838        0.4415  7.4792
     65      0.7748        0.4863       0.6839      0.7858        0.4409  7.4945
     66      0.8200        0.4782       0.6859      0.7885        0.4399  7.5064
     67      0.7845        0.4816       0.6880      0.7902        0.4392  7.5410
     68      0.8102        0.4794       0.6889      0.7914        0.4387  7.5407
     69      0.8363        0.4786       0.6880      0.7905        0.4382  7.5471
     70      0.8170        0.4819       0.6891      0.7923        0.4373  7.5587
     71      0.8109        0.4751       0.6908      0.7931        0.4363  7.5697
     72      0.7907        0.4810       0.6941      0.7961        0.4355  7.5580
     73      0.8340        0.4765       0.6957      0.7977        0.4349  7.6097
     74      0.8259        0.4762       0.6931      0.7959        0.4346  7.5820
     75      0.8297        0.4758       0.6932      0.7962        0.4341  7.5217
     76      0.8454        0.4731       0.6955      0.7980        0.4332  7.5035
     77      0.8289        0.4749       0.6970      0.8000        0.4324  7.5272
     78      0.8374        0.4715       0.7007      0.8036        0.4313  7.5650
     79      0.8199        0.4747       0.7030      0.8059        0.4305  7.5546
     80      0.8624        0.4733       0.7026      0.8061        0.4299  7.5700
     81      0.8383        0.4644       0.7002      0.8039        0.4297  7.5970
     82      0.8199        0.4695       0.6993      0.8034        0.4292  7.5603
     83      0.8146        0.4719       0.7000      0.8046        0.4286  7.5766
     84      0.8260        0.4705       0.6990      0.8041        0.4281  7.5340
     85      0.8417        0.4689       0.7016      0.8071        0.4274  7.5212
     86      0.8404        0.4693       0.7023      0.8077        0.4267  7.5398
     87      0.8198        0.4718       0.7042      0.8101        0.4259  7.5432
     88      0.8299        0.4676       0.7047      0.8107        0.4252  7.5133
     89      0.8302        0.4667       0.7036      0.8102        0.4249  7.5322
     90      0.8157        0.4689       0.7028      0.8097        0.4244  7.5542
     91      0.8232        0.4679       0.7021      0.8093        0.4239  7.5375
     92      0.8372        0.4648       0.7031      0.8104        0.4233  7.5332
     93      0.8342        0.4717       0.7010      0.8092        0.4228  7.5355
     94      0.8365        0.4687       0.7033      0.8105        0.4221  7.5241
     95      0.8312        0.4628       0.7010      0.8095        0.4216  7.4946
     96      0.8303        0.4664       0.6998      0.8088        0.4213  7.4984
     97      0.8186        0.4665       0.7033      0.8114        0.4203  7.5102
     98      0.8578        0.4646       0.7028      0.8112        0.4199  7.5252
     99      0.8360        0.4576       0.7050      0.8128        0.4192  7.5274
    100      0.8321        0.4605       0.7021      0.8114        0.4189  7.5214
Accuracy after query 3: 0.7239583333333334
F1 Score after query 3: 0.8358453576899206
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_2.pt

Query 4: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 4 is 96
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.8547        0.4597       0.7031      0.8126        0.4175  7.7717
      2      0.8498        0.4608       0.7043      0.8139        0.4164  7.7928
      3      0.8521        0.4554       0.7028      0.8135        0.4153  7.7891
      4      0.8438        0.4565       0.7038      0.8149        0.4140  7.8091
      5      0.8570        0.4573       0.7033      0.8150        0.4127  7.7788
      6      0.8760        0.4475       0.7021      0.8146        0.4119  7.7863
      7      0.8741        0.4534       0.7026      0.8152        0.4107  7.7981
      8      0.8554        0.4521       0.7052      0.8169        0.4089  7.8025
      9      0.8646        0.4499       0.7061      0.8177        0.4076  7.7651
     10      0.8699        0.4487       0.7045      0.8171        0.4068  7.7763
     11      0.8577        0.4474       0.7050      0.8179        0.4056  7.7751
     12      0.8632        0.4455       0.7036      0.8169        0.4052  7.7719
     13      0.8576        0.4472       0.7031      0.8171        0.4041  7.8101
     14      0.8555        0.4434       0.7012      0.8162        0.4033  7.8036
     15      0.8829        0.4441       0.7003      0.8160        0.4027  7.7927
     16      0.8744        0.4416       0.7019      0.8181        0.4013  7.8098
     17      0.8776        0.4398       0.7005      0.8177        0.4005  7.7893
     18      0.8730        0.4413       0.7002      0.8173        0.3997  7.7918
     19      0.8638        0.4417       0.6990      0.8170        0.3992  7.7832
     20      0.8749        0.4374       0.6993      0.8174        0.3982  7.7984
     21      0.8637        0.4424       0.7003      0.8180        0.3971  7.8081
     22      0.8685        0.4368       0.7007      0.8185        0.3963  7.8318
     23      0.8760        0.4381       0.6995      0.8183        0.3956  7.7962
     24      0.8772        0.4364       0.7007      0.8188        0.3948  7.8031
     25      0.8823        0.4316       0.6993      0.8182        0.3942  7.7788
     26      0.8746        0.4306       0.6990      0.8179        0.3935  7.7609
     27      0.8559        0.4378       0.7016      0.8200        0.3920  7.7804
     28      0.8859        0.4283       0.7016      0.8201        0.3914  7.8025
     29      0.8746        0.4307       0.7014      0.8200        0.3908  7.8037
     30      0.8762        0.4260       0.7007      0.8198        0.3902  7.8148
     31      0.8701        0.4293       0.7002      0.8205        0.3893  7.7892
     32      0.8798        0.4321       0.6998      0.8208        0.3886  7.8180
     33      0.8833        0.4233       0.6986      0.8196        0.3885  7.8126
     34      0.8796        0.4248       0.6981      0.8198        0.3876  7.7964
     35      0.8747        0.4230       0.6981      0.8200        0.3869  7.7865
     36      0.8748        0.4243       0.6977      0.8201        0.3864  7.7810
     37      0.8737        0.4233       0.6986      0.8208        0.3855  7.7912
     38      0.8711        0.4242       0.6974      0.8198        0.3850  7.8036
     39      0.8765        0.4245       0.6970      0.8200        0.3844  7.7791
     40      0.8911        0.4170       0.6974      0.8203        0.3836  7.7809
     41      0.8871        0.4186       0.6972      0.8206        0.3831  7.7545
     42      0.8733        0.4196       0.6958      0.8195        0.3827  7.7726
     43      0.8805        0.4135       0.6962      0.8195        0.3821  7.7607
     44      0.8777        0.4149       0.6951      0.8195        0.3818  7.7915
     45      0.8830        0.4178       0.6958      0.8197        0.3812  7.7960
     46      0.8752        0.4132       0.6965      0.8205        0.3801  7.7936
     47      0.8887        0.4147       0.6951      0.8196        0.3801  7.7902
     48      0.8764        0.4142       0.6969      0.8210        0.3791  7.7910
     49      0.8701        0.4113       0.6967      0.8215        0.3786  7.7784
     50      0.8703        0.4147       0.6977      0.8219        0.3776  7.7842
     51      0.8866        0.4132       0.6962      0.8215        0.3775  7.7993
     52      0.8848        0.4118       0.6955      0.8209        0.3771  7.8084
     53      0.8790        0.4115       0.6924      0.8187        0.3773  7.7670
     54      0.8729        0.4096       0.6948      0.8209        0.3762  7.8041
     55      0.8822        0.4060       0.6960      0.8218        0.3753  7.7756
     56      0.8870        0.4095       0.6955      0.8220        0.3747  7.7683
     57      0.8889        0.4057       0.6962      0.8221        0.3741  7.7506
     58      0.8752        0.4074       0.6924      0.8189        0.3747  7.7790
     59      0.8799        0.4057       0.6931      0.8194        0.3740  7.8137
     60      0.8680        0.4045       0.6944      0.8205        0.3732  7.7919
     61      0.8756        0.4039       0.6939      0.8204        0.3728  7.7905
     62      0.8678        0.4079       0.6943      0.8205        0.3724  7.8042
     63      0.8880        0.4043       0.6927      0.8193        0.3720  7.7841
     64      0.8672        0.4030       0.6944      0.8215        0.3712  7.8005
     65      0.8829        0.4011       0.6913      0.8188        0.3715  7.7695
     66      0.8556        0.4047       0.6939      0.8213        0.3703  7.7967
     67      0.8753        0.3989       0.6939      0.8214        0.3698  7.7878
     68      0.8746        0.3990       0.6943      0.8217        0.3693  7.7912
     69      0.8777        0.3984       0.6924      0.8208        0.3690  7.7894
     70      0.8755        0.4014       0.6946      0.8224        0.3680  7.8063
     71      0.8832        0.3963       0.6939      0.8217        0.3679  7.7725
     72      0.8843        0.3957       0.6932      0.8213        0.3675  7.7975
     73      0.8779        0.3964       0.6937      0.8219        0.3671  7.7632
     74      0.8828        0.3959       0.6970      0.8242        0.3660  7.7961
     75      0.8755        0.3968       0.6986      0.8250        0.3653  7.8046
     76      0.8703        0.3965       0.6964      0.8239        0.3653  7.7874
     77      0.8765        0.3948       0.6934      0.8219        0.3654  7.8025
     78      0.8764        0.3971       0.6944      0.8230        0.3649  7.7922
     79      0.8833        0.3943       0.6950      0.8232        0.3645  7.8176
     80      0.8705        0.3906       0.6932      0.8223        0.3643  7.8142
     81      0.8735        0.3952       0.6929      0.8220        0.3640  7.8054
     82      0.8767        0.3905       0.6943      0.8228        0.3635  7.7982
     83      0.8726        0.3917       0.6960      0.8241        0.3623  7.7970
     84      0.8682        0.3935       0.6969      0.8251        0.3616  7.7994
     85      0.8760        0.3904       0.6951      0.8237        0.3618  7.7886
     86      0.8744        0.3877       0.6932      0.8224        0.3619  7.7788
     87      0.8815        0.3875       0.6915      0.8206        0.3622  7.7610
     88      0.8699        0.3885       0.6925      0.8220        0.3615  7.7564
     89      0.8748        0.3867       0.6903      0.8201        0.3616  7.7776
     90      0.8688        0.3864       0.6917      0.8215        0.3608  7.7753
     91      0.8735        0.3875       0.6934      0.8229        0.3602  7.7851
     92      0.8867        0.3842       0.6939      0.8231        0.3597  7.7769
     93      0.8727        0.3853       0.6934      0.8230        0.3594  7.7970
     94      0.8797        0.3841       0.6913      0.8218        0.3593  7.8012
     95      0.8685        0.3834       0.6927      0.8225        0.3587  7.7827
     96      0.8797        0.3831       0.6941      0.8236        0.3579  7.7979
     97      0.8727        0.3824       0.6936      0.8234        0.3576  7.7819
     98      0.8735        0.3839       0.6925      0.8225        0.3575  7.7849
     99      0.8802        0.3817       0.6929      0.8229        0.3571  7.7952
    100      0.8801        0.3826       0.6931      0.8233        0.3567  7.7908
Accuracy after query 4: 0.73125
F1 Score after query 4: 0.8464155174760778
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_3.pt

Query 5: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 5 is 176
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.8591        0.3894       0.6934      0.8242        0.3554  8.2443
      2      0.8629        0.3888       0.6948      0.8246        0.3548  8.2312
      3      0.8548        0.3903       0.6946      0.8252        0.3540  8.2402
      4      0.8612        0.3859       0.6941      0.8245        0.3536  8.2669
      5      0.8629        0.3863       0.6946      0.8258        0.3527  8.2608
      6      0.8610        0.3866       0.6960      0.8264        0.3519  8.3841
      7      0.8595        0.3864       0.6957      0.8268        0.3511  8.2751
      8      0.8583        0.3860       0.6955      0.8268        0.3508  8.2559
      9      0.8607        0.3844       0.6951      0.8265        0.3498  8.2447
     10      0.8547        0.3838       0.6950      0.8266        0.3498  8.3075
     11      0.8540        0.3842       0.6962      0.8274        0.3491  8.3593
     12      0.8583        0.3847       0.6939      0.8261        0.3489  8.3353
     13      0.8648        0.3802       0.6941      0.8265        0.3477  8.2741
     14      0.8623        0.3778       0.6950      0.8269        0.3475  8.2946
     15      0.8572        0.3813       0.6937      0.8263        0.3473  8.2277
     16      0.8679        0.3787       0.6946      0.8268        0.3467  8.2471
     17      0.8651        0.3767       0.6943      0.8271        0.3461  8.2373
     18      0.8641        0.3769       0.6941      0.8265        0.3459  8.2543
     19      0.8579        0.3780       0.6937      0.8264        0.3454  8.2632
     20      0.8727        0.3750       0.6937      0.8270        0.3451  8.2676
     21      0.8635        0.3743       0.6936      0.8274        0.3441  8.2652
     22      0.8659        0.3722       0.6922      0.8270        0.3445  8.2682
     23      0.8552        0.3771       0.6929      0.8274        0.3431  8.2950
     24      0.8701        0.3721       0.6931      0.8271        0.3430  8.2788
     25      0.8697        0.3723       0.6932      0.8274        0.3424  8.2599
     26      0.8649        0.3721       0.6934      0.8279        0.3416  8.2658
     27      0.8733        0.3716       0.6931      0.8274        0.3414  8.2528
     28      0.8715        0.3707       0.6918      0.8270        0.3410  8.3196
     29      0.8651        0.3692       0.6920      0.8275        0.3404  8.3125
     30      0.8637        0.3693       0.6934      0.8272        0.3403  8.2626
     31      0.8651        0.3674       0.6937      0.8283        0.3393  8.2666
     32      0.8672        0.3666       0.6925      0.8274        0.3395  8.2792
     33      0.8590        0.3685       0.6917      0.8269        0.3392  8.2742
     34      0.8684        0.3665       0.6920      0.8271        0.3387  8.2745
     35      0.8577        0.3660       0.6913      0.8270        0.3386  8.2687
     36      0.8725        0.3645       0.6911      0.8272        0.3380  8.3076
     37      0.8736        0.3645       0.6915      0.8270        0.3379  8.2193
     38      0.8613        0.3648       0.6913      0.8271        0.3374  8.2376
     39      0.8649        0.3628       0.6913      0.8273        0.3370  8.3041
     40      0.8706        0.3619       0.6911      0.8274        0.3368  8.2812
     41      0.8689        0.3615       0.6913      0.8275        0.3368  8.2529
     42      0.8654        0.3631       0.6917      0.8275        0.3365  8.3152
     43      0.8735        0.3614       0.6903      0.8270        0.3357  8.2634
     44      0.8616        0.3601       0.6906      0.8274        0.3354  8.2764
     45      0.8630        0.3623       0.6918      0.8280        0.3347  8.2636
     46      0.8631        0.3591       0.6918      0.8278        0.3345  8.2956
     47      0.8628        0.3613       0.6925      0.8283        0.3341  8.9558
     48      0.8684        0.3587       0.6913      0.8279        0.3339  9.1091
     49      0.8695        0.3587       0.6915      0.8282        0.3336  8.9517
     50      0.8589        0.3565       0.6913      0.8281        0.3335  8.9741
     51      0.8613        0.3573       0.6918      0.8283        0.3326  8.9540
     52      0.8742        0.3548       0.6910      0.8282        0.3328  8.9794
     53      0.8673        0.3553       0.6913      0.8284        0.3323  8.6914
     54      0.8672        0.3548       0.6911      0.8286        0.3322  8.2772
     55      0.8640        0.3547       0.6908      0.8281        0.3317  8.2640
     56      0.8647        0.3545       0.6915      0.8288        0.3316  8.2595
     57      0.8720        0.3538       0.6913      0.8281        0.3313  8.2778
     58      0.8672        0.3528       0.6918      0.8287        0.3306  8.2494
     59      0.8801        0.3512       0.6913      0.8285        0.3304  8.2623
     60      0.8693        0.3542       0.6917      0.8285        0.3300  8.2577
     61      0.8680        0.3533       0.6913      0.8288        0.3297  8.2724
     62      0.8684        0.3511       0.6908      0.8287        0.3297  8.2807
     63      0.8664        0.3500       0.6918      0.8292        0.3292  8.2846
     64      0.8736        0.3494       0.6915      0.8288        0.3289  8.2710
     65      0.8730        0.3510       0.6913      0.8291        0.3293  8.2722
     66      0.8675        0.3516       0.6901      0.8285        0.3288  8.2928
     67      0.8698        0.3498       0.6913      0.8290        0.3282  8.2986
     68      0.8793        0.3478       0.6911      0.8292        0.3280  8.2692
     69      0.8725        0.3479       0.6896      0.8284        0.3281  8.2941
     70      0.8699        0.3485       0.6911      0.8294        0.3275  8.2709
     71      0.8636        0.3462       0.6917      0.8294        0.3267  8.2540
     72      0.8672        0.3467       0.6908      0.8297        0.3269  8.2575
     73      0.8749        0.3466       0.6917      0.8299        0.3264  8.2407
     74      0.8749        0.3450       0.6908      0.8298        0.3266  8.3001
     75      0.8704        0.3453       0.6899      0.8296        0.3264  8.2709
     76      0.8717        0.3445       0.6911      0.8305        0.3257  8.3126
     77      0.8751        0.3448       0.6913      0.8303        0.3258  8.2880
     78      0.8679        0.3424       0.6917      0.8303        0.3251  8.2870
     79      0.8763        0.3434       0.6917      0.8305        0.3247  8.2653
     80      0.8744        0.3444       0.6913      0.8302        0.3249  8.2756
     81      0.8688        0.3441       0.6911      0.8300        0.3247  8.2658
     82      0.8739        0.3403       0.6918      0.8305        0.3242  8.2687
     83      0.8714        0.3419       0.6913      0.8303        0.3243  8.2860
     84      0.8653        0.3414       0.6922      0.8308        0.3237  8.2673
     85      0.8850        0.3397       0.6920      0.8304        0.3232  8.3053
     86      0.8737        0.3399       0.6920      0.8308        0.3235  8.2637
     87      0.8726        0.3407       0.6924      0.8312        0.3227  8.2110
     88      0.8699        0.3389       0.6922      0.8314        0.3226  8.2271
     89      0.8747        0.3397       0.6922      0.8314        0.3222  8.2145
     90      0.8673        0.3375       0.6924      0.8316        0.3221  8.2458
     91      0.8801        0.3376       0.6920      0.8316        0.3217  8.2851
     92      0.8632        0.3399       0.6931      0.8319        0.3217  8.2653
     93      0.8744        0.3388       0.6922      0.8316        0.3212  8.2678
     94      0.8693        0.3366       0.6936      0.8327        0.3215  8.2609
     95      0.8737        0.3355       0.6932      0.8320        0.3206  8.2752
     96      0.8734        0.3350       0.6934      0.8326        0.3209  8.2765
     97      0.8775        0.3367       0.6931      0.8326        0.3212  8.2490
     98      0.8746        0.3343       0.6924      0.8321        0.3200  8.2551
     99      0.8755        0.3344       0.6934      0.8333        0.3202  8.2565
    100      0.8717        0.3346       0.6934      0.8334        0.3201  8.2535
Accuracy after query 5: 0.7361111111111112
F1 Score after query 5: 0.8557153552741861
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_4.pt

Query 6: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 6 is 320
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.8690        0.3392       0.6955      0.8334        0.3198  9.0711
      2      0.8714        0.3402       0.6995      0.8361        0.3181  9.0960
      3      0.8745        0.3378       0.7014      0.8370        0.3170  9.0979
      4      0.8746        0.3375       0.7007      0.8361        0.3177  9.1061
      5      0.8705        0.3368       0.7016      0.8370        0.3165  9.1231
      6      0.8732        0.3370       0.7038      0.8391        0.3149  9.1336
      7      0.8714        0.3348       0.7019      0.8370        0.3155  9.1225
      8      0.8760        0.3350       0.7028      0.8374        0.3150  9.1377
      9      0.8740        0.3335       0.7030      0.8384        0.3140  9.1262
     10      0.8818        0.3330       0.7045      0.8379        0.3144  9.0968
     11      0.8745        0.3322       0.7040      0.8382        0.3134  9.1002
     12      0.8765        0.3311       0.7036      0.8387        0.3126  9.1205
     13      0.8787        0.3302       0.7052      0.8382        0.3136  9.1094
     14      0.8800        0.3294       0.7047      0.8395        0.3118  9.0943
     15      0.8783        0.3291       0.7047      0.8387        0.3119  9.1025
     16      0.8873        0.3282       0.7075      0.8414        0.3102  9.1046
     17      0.8832        0.3261       0.7069      0.8391        0.3115  9.1165
     18      0.8797        0.3252       0.7068      0.8411        0.3098  9.1101
     19      0.8740        0.3256       0.7063      0.8394        0.3103  9.1314
     20      0.8776        0.3259       0.7063      0.8401        0.3097  9.1243
     21      0.8844        0.3250       0.7069      0.8411        0.3087  9.1325
     22      0.8781        0.3248       0.7063      0.8395        0.3101  9.1778
     23      0.8772        0.3235       0.7068      0.8395        0.3095  9.1210
     24      0.8790        0.3226       0.7068      0.8410        0.3081  9.1369
     25      0.8855        0.3223       0.7054      0.8386        0.3096  9.1046
     26      0.8812        0.3201       0.7085      0.8409        0.3081  9.1188
     27      0.8789        0.3201       0.7073      0.8402        0.3083  9.0942
     28      0.8836        0.3206       0.7075      0.8408        0.3073  9.1198
     29      0.8810        0.3197       0.7056      0.8380        0.3094  9.0958
     30      0.8820        0.3186       0.7071      0.8399        0.3077  9.1438
     31      0.8760        0.3180       0.7078      0.8418        0.3055  9.1786
     32      0.8783        0.3177       0.7090      0.8415        0.3058  9.3691
     33      0.8845        0.3173       0.7089      0.8412        0.3064  9.1370
     34      0.8823        0.3174       0.7092      0.8416        0.3054  9.2117
     35      0.8831        0.3161       0.7089      0.8411        0.3056  9.0841
     36      0.8832        0.3158       0.7090      0.8403        0.3060  9.0876
     37      0.8867        0.3152       0.7092      0.8414        0.3050  9.0910
     38      0.8848        0.3149       0.7095      0.8419        0.3045  9.1244
     39      0.8867        0.3128       0.7099      0.8420        0.3041  9.1437
     40      0.8844        0.3122       0.7092      0.8415        0.3042  9.0986
     41      0.8841        0.3125       0.7089      0.8428        0.3027  9.1053
     42      0.8876        0.3105       0.7089      0.8418        0.3028  9.1100
     43      0.8865        0.3099       0.7099      0.8409        0.3039  9.1361
     44      0.8847        0.3113       0.7087      0.8402        0.3044  9.1690
     45      0.8907        0.3111       0.7108      0.8415        0.3032  9.2317
     46      0.8877        0.3104       0.7102      0.8412        0.3033  9.1641
     47      0.8915        0.3080       0.7092      0.8409        0.3037  9.1439
     48      0.8875        0.3078       0.7102      0.8413        0.3030  9.1527
     49      0.8940        0.3069       0.7106      0.8415        0.3024  9.1295
     50      0.8893        0.3071       0.7108      0.8427        0.3011  9.1154
     51      0.8853        0.3075       0.7109      0.8420        0.3012  9.0981
     52      0.8855        0.3068       0.7102      0.8441        0.2999  9.1411
     53      0.8927        0.3050       0.7118      0.8423        0.3014  9.0951
     54      0.8846        0.3057       0.7101      0.8434        0.2997  9.1057
     55      0.8888        0.3048       0.7115      0.8439        0.2994  9.0939
     56      0.8886        0.3038       0.7109      0.8426        0.2997  9.1462
     57      0.8869        0.3030       0.7127      0.8435        0.2995  9.1248
     58      0.8894        0.3028       0.7115      0.8425        0.3001  9.1265
     59      0.8880        0.3031       0.7116      0.8459        0.2972  9.1263
     60      0.8859        0.3022       0.7116      0.8442        0.2978  9.1179
     61      0.8939        0.3006       0.7127      0.8430        0.2987  9.0990
     62      0.8898        0.3014       0.7115      0.8443        0.2976  9.1327
     63      0.8918        0.2999       0.7111      0.8415        0.2998  9.1154
     64      0.8896        0.3002       0.7125      0.8437        0.2978  9.1270
     65      0.8950        0.2984       0.7083      0.8453        0.2952  9.1213
     66      0.8920        0.2986       0.7127      0.8431        0.2981  9.0937
     67      0.8882        0.2984       0.7118      0.8437        0.2967  9.1030
     68      0.8938        0.2977       0.7128      0.8437        0.2968  9.0932
     69      0.8897        0.2969       0.7127      0.8443        0.2963  9.1175
     70      0.8949        0.2967       0.7127      0.8438        0.2963  9.1368
     71      0.8921        0.2954       0.7125      0.8431        0.2975  9.1315
     72      0.8973        0.2966       0.7118      0.8451        0.2949  9.1250
     73      0.8915        0.2961       0.7116      0.8429        0.2968  9.1376
     74      0.8984        0.2938       0.7123      0.8448        0.2945  9.1196
     75      0.8933        0.2946       0.7130      0.8441        0.2954  9.1224
     76      0.8936        0.2948       0.7125      0.8456        0.2940  9.1190
     77      0.8905        0.2931       0.7104      0.8462        0.2929  9.1133
     78      0.8984        0.2919       0.7106      0.8470        0.2925  9.1859
     79      0.8940        0.2921       0.7134      0.8456        0.2933  9.1352
     80      0.8902        0.2927       0.7104      0.8476        0.2915  9.1976
     81      0.8898        0.2918       0.7109      0.8473        0.2916  9.1103
     82      0.8949        0.2912       0.7113      0.8477        0.2912  9.1102
     83      0.8949        0.2892       0.7104      0.8477        0.2909  9.1019
     84      0.8937        0.2897       0.7128      0.8447        0.2939  9.1338
     85      0.8985        0.2903       0.7130      0.8462        0.2920  9.1232
     86      0.8965        0.2897       0.7128      0.8471        0.2910  9.1442
     87      0.8973        0.2897       0.7120      0.8481        0.2902  9.1226
     88      0.8906        0.2889       0.7118      0.8480        0.2900  9.1395
     89      0.8936        0.2886       0.7148      0.8480        0.2912  9.1527
     90      0.8959        0.2877       0.7134      0.8471        0.2903  9.1229
     91      0.8976        0.2870       0.7122      0.8482        0.2893  9.1288
     92      0.8943        0.2855       0.7134      0.8478        0.2896  9.1114
     93      0.8953        0.2858       0.7134      0.8495        0.2883  9.0985
     94      0.8996        0.2845       0.7128      0.8479        0.2887  9.1561
     95      0.8936        0.2871       0.7142      0.8469        0.2895  9.1374
     96      0.9010        0.2865       0.7134      0.8480        0.2887  9.1126
     97      0.8979        0.2841       0.7118      0.8494        0.2872  9.2379
     98      0.8958        0.2841       0.7141      0.8485        0.2882  9.1421
     99      0.9018        0.2847       0.7141      0.8493        0.2873  9.1661
    100      0.8973        0.2827       0.7141      0.8495        0.2870  9.1563
Accuracy after query 6: 0.7394097222222222
F1 Score after query 6: 0.8523398218861521
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_5.pt

Query 7: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 7 is 560
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.8886        0.2867       0.7137      0.8523        0.2840  10.7157
      2      0.8898        0.2851       0.7172      0.8519        0.2850  10.6479
      3      0.8940        0.2852       0.7141      0.8527        0.2826  10.5897
      4      0.8904        0.2849       0.7172      0.8543        0.2824  10.5907
      5      0.8915        0.2831       0.7191      0.8547        0.2822  10.5901
      6      0.8940        0.2827       0.7163      0.8543        0.2807  10.6092
      7      0.8934        0.2825       0.7170      0.8549        0.2799  10.6718
      8      0.8895        0.2813       0.7167      0.8545        0.2790  10.5834
      9      0.8950        0.2798       0.7168      0.8547        0.2784  10.5862
     10      0.8929        0.2808       0.7167      0.8551        0.2776  10.5871
     11      0.8967        0.2791       0.7179      0.8553        0.2772  10.6082
     12      0.8948        0.2791       0.7203      0.8558        0.2772  10.5884
     13      0.8994        0.2771       0.7207      0.8565        0.2763  10.5779
     14      0.8972        0.2774       0.7214      0.8567        0.2763  10.5947
     15      0.8946        0.2760       0.7205      0.8563        0.2753  10.5840
     16      0.8981        0.2756       0.7184      0.8558        0.2741  10.5732
     17      0.8965        0.2750       0.7182      0.8563        0.2735  10.6875
     18      0.8967        0.2741       0.7189      0.8566        0.2728  10.6890
     19      0.9033        0.2734       0.7196      0.8569        0.2723  10.6088
     20      0.8973        0.2736       0.7188      0.8564        0.2717  10.6117
     21      0.8951        0.2726       0.7194      0.8574        0.2716  10.6142
     22      0.8961        0.2711       0.7196      0.8569        0.2710  10.6406
     23      0.8976        0.2719       0.7198      0.8574        0.2705  10.5936
     24      0.9063        0.2689       0.7191      0.8568        0.2704  10.6189
     25      0.9034        0.2696       0.7207      0.8574        0.2698  10.6013
     26      0.9037        0.2688       0.7191      0.8560        0.2691  10.5780
     27      0.9001        0.2683       0.7198      0.8564        0.2687  10.5691
     28      0.9024        0.2677       0.7184      0.8556        0.2684  10.5937
     29      0.9011        0.2683       0.7172      0.8548        0.2677  10.5843
     30      0.9079        0.2657       0.7212      0.8578        0.2675  10.5939
     31      0.9046        0.2655       0.7188      0.8562        0.2672  10.5936
     32      0.9022        0.2654       0.7226      0.8584        0.2669  10.5939
     33      0.9050        0.2644       0.7186      0.8563        0.2666  10.5958
     34      0.9068        0.2634       0.7127      0.8511        0.2664  10.5866
     35      0.9107        0.2618       0.7200      0.8564        0.2658  10.5943
     36      0.9083        0.2618       0.7189      0.8561        0.2658  10.5887
     37      0.9069        0.2616       0.7153      0.8529        0.2651  10.5942
     38      0.9073        0.2608       0.7165      0.8542        0.2646  10.5680
     39      0.9083        0.2604       0.7175      0.8547        0.2643  10.5700
     40      0.9159        0.2594       0.7135      0.8519        0.2644  10.5878
     41      0.9094        0.2589       0.7161      0.8536        0.2638  10.6306
     42      0.9102        0.2579       0.7125      0.8507        0.2634  10.6016
     43      0.9121        0.2574       0.7153      0.8522        0.2629  10.6230
     44      0.9156        0.2572       0.7179      0.8541        0.2627  10.5903
     45      0.9105        0.2569       0.7189      0.8545        0.2623  10.6077
     46      0.9087        0.2557       0.7148      0.8520        0.2623  10.5914
     47      0.9108        0.2554       0.7144      0.8516        0.2617  10.5881
     48      0.9112        0.2555       0.7179      0.8537        0.2608  10.6094
     49      0.9184        0.2538       0.7128      0.8505        0.2613  10.6015
     50      0.9127        0.2542       0.7179      0.8541        0.2607  10.5940
     51      0.9135        0.2521       0.7198      0.8547        0.2600  10.5872
     52      0.9166        0.2527       0.7175      0.8531        0.2598  10.6919
     53      0.9129        0.2512       0.7198      0.8540        0.2596  10.6482
     54      0.9125        0.2510       0.7207      0.8545        0.2592  10.6010
     55      0.9165        0.2505       0.7108      0.8486        0.2603  10.6237
     56      0.9163        0.2506       0.7182      0.8535        0.2590  10.6033
     57      0.9148        0.2498       0.7193      0.8538        0.2583  10.6002
     58      0.9207        0.2485       0.7139      0.8507        0.2587  10.6042
     59      0.9174        0.2488       0.7233      0.8560        0.2583  10.6089
     60      0.9169        0.2480       0.7167      0.8521        0.2580  10.6645
     61      0.9230        0.2471       0.7215      0.8548        0.2570  10.5911
     62      0.9194        0.2468       0.7160      0.8520        0.2577  10.5890
     63      0.9218        0.2461       0.7142      0.8492        0.2577  10.6081
     64      0.9190        0.2453       0.7080      0.8439        0.2615  10.6039
     65      0.9196        0.2455       0.7189      0.8537        0.2563  10.6163
     66      0.9209        0.2445       0.7158      0.8514        0.2562  10.5969
     67      0.9204        0.2435       0.7148      0.8505        0.2563  10.6029
     68      0.9208        0.2424       0.7174      0.8516        0.2553  10.6260
     69      0.9225        0.2428       0.7212      0.8545        0.2548  10.5675
     70      0.9203        0.2423       0.7175      0.8519        0.2549  10.5385
     71      0.9233        0.2418       0.7148      0.8498        0.2555  10.5324
     72      0.9242        0.2416       0.7175      0.8515        0.2543  10.5090
     73      0.9214        0.2414       0.7194      0.8525        0.2537  10.5369
     74      0.9252        0.2399       0.7234      0.8545        0.2536  10.5458
     75      0.9192        0.2405       0.7017      0.8382        0.2627  10.5751
     76      0.9207        0.2406       0.7172      0.8507        0.2546  10.5637
     77      0.9232        0.2384       0.7128      0.8462        0.2567  10.5465
     78      0.9247        0.2388       0.7168      0.8507        0.2530  10.5531
     79      0.9276        0.2377       0.7207      0.8526        0.2525  10.5364
     80      0.9251        0.2376       0.7151      0.8476        0.2556  10.5484
     81      0.9269        0.2366       0.7181      0.8497        0.2540  10.5446
     82      0.9269        0.2355       0.7144      0.8468        0.2555  10.5679
     83      0.9238        0.2359       0.7184      0.8507        0.2526  10.5425
     84      0.9308        0.2342       0.7207      0.8521        0.2506  10.5255
     85      0.9280        0.2357       0.7104      0.8441        0.2558  10.5404
     86      0.9298        0.2344       0.7128      0.8453        0.2546  10.5455
     87      0.9297        0.2341       0.7215      0.8523        0.2501  10.5927
     88      0.9289        0.2317       0.7191      0.8506        0.2516  10.5949
     89      0.9288        0.2327       0.7163      0.8479        0.2528  10.5742
     90      0.9314        0.2317       0.7203      0.8519        0.2497  10.5557
     91      0.9287        0.2312       0.7186      0.8498        0.2520  10.5334
     92      0.9299        0.2311       0.7203      0.8512        0.2505  10.5541
     93      0.9321        0.2301       0.7186      0.8502        0.2511  10.5923
     94      0.9304        0.2289       0.7238      0.8534        0.2490  10.5268
     95      0.9256        0.2304       0.7349      0.8583        0.2488  10.5374
     96      0.9299        0.2287       0.7153      0.8475        0.2518  10.5174
     97      0.9315        0.2285       0.7181      0.8493        0.2510  10.5347
     98      0.9329        0.2288       0.7049      0.8377        0.2585  10.5552
     99      0.9345        0.2279       0.7123      0.8441        0.2543  10.5586
    100      0.9294        0.2269       0.7188      0.8494        0.2500  10.5524
Accuracy after query 7: 0.7730902777777777
F1 Score after query 7: 0.8799443978835974
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_6.pt

Query 8: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 8 is 1000
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9186        0.2356       0.7247      0.8509        0.2480  13.3519
      2      0.9218        0.2340       0.7101      0.8388        0.2538  13.1731
      3      0.9231        0.2329       0.7311      0.8554        0.2438  13.1649
      4      0.9258        0.2312       0.7302      0.8533        0.2444  13.1573
      5      0.9219        0.2315       0.7043      0.8329        0.2569  13.1396
      6      0.9238        0.2305       0.7285      0.8520        0.2438  13.2250
      7      0.9251        0.2289       0.7307      0.8531        0.2424  13.2389
      8      0.9242        0.2283       0.7274      0.8489        0.2443  13.2202
      9      0.9263        0.2265       0.7269      0.8502        0.2438  13.2117
     10      0.9269        0.2255       0.7436      0.8617        0.2370  13.2433
     11      0.9256        0.2248       0.7009      0.8264        0.2597  13.2154
     12      0.9289        0.2253       0.7337      0.8536        0.2398  13.2173
     13      0.9288        0.2232       0.7247      0.8460        0.2447  13.1887
     14      0.9267        0.2235       0.7311      0.8496        0.2421  13.3757
     15      0.9304        0.2224       0.7372      0.8552        0.2380  13.3033
     16      0.9310        0.2209       0.7227      0.8420        0.2470  13.2687
     17      0.9307        0.2206       0.7505      0.8650        0.2334  13.2250
     18      0.9332        0.2179       0.7424      0.8580        0.2361  13.2605
     19      0.9306        0.2187       0.7344      0.8521        0.2386  13.2591
     20      0.9320        0.2168       0.7401      0.8572        0.2363  13.2213
     21      0.9334        0.2161       0.7406      0.8577        0.2353  13.2983
     22      0.9331        0.2162       0.7453      0.8592        0.2341  13.1644
     23      0.9348        0.2151       0.7601      0.8691        0.2307  13.1497
     24      0.9349        0.2134       0.7625      0.8699        0.2301  13.2030
     25      0.9357        0.2125       0.7458      0.8586        0.2329  13.1975
     26      0.9386        0.2118       0.7531      0.8643        0.2309  13.2071
     27      0.9358        0.2120       0.7403      0.8544        0.2348  13.1910
     28      0.9327        0.2108       0.7479      0.8592        0.2320  13.2098
     29      0.9351        0.2108       0.7408      0.8557        0.2341  13.1872
     30      0.9378        0.2081       0.7438      0.8559        0.2335  13.1888
     31      0.9388        0.2078       0.7432      0.8560        0.2336  13.1603
     32      0.9396        0.2077       0.7627      0.8692        0.2260  13.1539
     33      0.9399        0.2068       0.7552      0.8621        0.2283  13.1936
     34      0.9402        0.2053       0.7500      0.8595        0.2298  13.2149
     35      0.9417        0.2038       0.7436      0.8559        0.2326  13.2092
     36      0.9440        0.2023       0.7444      0.8576        0.2305  13.2029
     37      0.9440        0.2029       0.7175      0.8361        0.2478  13.2070
     38      0.9381        0.2037       0.7590      0.8656        0.2254  13.1990
     39      0.9392        0.2026       0.7545      0.8623        0.2265  13.2154
     40      0.9446        0.2002       0.7215      0.8382        0.2450  13.1975
     41      0.9465        0.1993       0.7439      0.8559        0.2309  13.1942
     42      0.9426        0.2006       0.7099      0.8277        0.2557  13.2091
     43      0.9404        0.2004       0.7585      0.8648        0.2237  13.2179
     44      0.9476        0.1963       0.7467      0.8588        0.2285  13.2186
     45      0.9416        0.1977       0.7481      0.8597        0.2261  13.2209
     46      0.9478        0.1960       0.7693      0.8732        0.2210  13.2033
     47      0.9456        0.1966       0.7625      0.8667        0.2223  13.2071
     48      0.9413        0.1959       0.7337      0.8470        0.2360  13.2292
     49      0.9458        0.1945       0.7524      0.8615        0.2244  13.1693
     50      0.9479        0.1924       0.7700      0.8711        0.2197  13.1699
     51      0.9442        0.1950       0.7559      0.8619        0.2247  13.2023
     52      0.9475        0.1925       0.7720      0.8723        0.2180  13.1991
     53      0.9515        0.1899       0.7458      0.8544        0.2287  13.2108
     54      0.9454        0.1931       0.7576      0.8620        0.2227  13.1910
     55      0.9522        0.1890       0.7731      0.8725        0.2171  13.2109
     56      0.9431        0.1931       0.7743      0.8712        0.2164  13.2049
     57      0.9490        0.1888       0.7528      0.8585        0.2267  13.2196
     58      0.9524        0.1863       0.7707      0.8719        0.2170  13.1693
     59      0.9531        0.1857       0.7740      0.8748        0.2161  13.1710
     60      0.9444        0.1901       0.7543      0.8592        0.2241  13.1934
     61      0.9538        0.1851       0.7630      0.8648        0.2201  13.2200
     62      0.9495        0.1860       0.7573      0.8615        0.2223  13.2221
     63      0.9506        0.1846       0.7762      0.8757        0.2151  13.1994
     64      0.9541        0.1824       0.7476      0.8523        0.2308  13.2068
     65      0.9510        0.1835       0.7786      0.8767        0.2147  13.1876
     66      0.9535        0.1825       0.7450      0.8506        0.2326  13.2092
     67      0.9533        0.1809       0.7554      0.8591        0.2253  13.1916
     68      0.9552        0.1799       0.7696      0.8686        0.2158  13.1888
     69      0.9494        0.1839       0.7519      0.8546        0.2287  13.1930
     70      0.9529        0.1814       0.7781      0.8754        0.2158  13.2192
     71      0.9527        0.1786       0.7571      0.8616        0.2222  13.2222
     72      0.9513        0.1816       0.7538      0.8540        0.2285  13.2161
     73      0.9576        0.1764       0.7731      0.8702        0.2141  13.2191
     74      0.9581        0.1754       0.7771      0.8747        0.2118  13.2092
     75      0.9545        0.1766       0.7736      0.8724        0.2127  13.1846
     76      0.9568        0.1760       0.7470      0.8498        0.2347  13.1832
     77      0.9536        0.1779       0.7807      0.8768        0.2129  13.1992
     78      0.9565        0.1739       0.7694      0.8674        0.2170  13.1921
     79      0.9579        0.1728       0.7747      0.8719        0.2120  13.1830
     80      0.9563        0.1747       0.7556      0.8542        0.2297  13.1957
     81      0.9615        0.1714       0.7755      0.8737        0.2108  13.1914
     82      0.9565        0.1720       0.7797      0.8759        0.2132  13.1868
     83      0.9580        0.1712       0.7679      0.8663        0.2175  13.2357
     84      0.9615        0.1693       0.7703      0.8682        0.2161  13.2070
     85      0.9585        0.1706       0.7516      0.8520        0.2318  13.1806
     86      0.9586        0.1716       0.7767      0.8744        0.2100  13.1568
     87      0.9603        0.1686       0.7622      0.8591        0.2241  13.1751
     88      0.9622        0.1669       0.7689      0.8651        0.2167  13.2327
     89      0.9601        0.1690       0.7535      0.8517        0.2316  13.2238
     90      0.9600        0.1671       0.7660      0.8639        0.2181  13.1866
     91      0.9607        0.1663       0.7778      0.8738        0.2092  13.1902
     92      0.9609        0.1660       0.7774      0.8724        0.2102  13.2250
     93      0.9553        0.1687       0.7774      0.8743        0.2094  13.2105
     94      0.9611        0.1650       0.7623      0.8605        0.2226  13.2022
     95      0.9597        0.1658       0.7778      0.8728        0.2087  13.1715
     96      0.9632        0.1626       0.7753      0.8709        0.2114  13.1865
     97      0.9632        0.1624       0.7670      0.8647        0.2176  13.2495
     98      0.9621        0.1624       0.7790      0.8759        0.2100  13.2175
     99      0.9527        0.1684       0.7646      0.8581        0.2248  13.2097
    100      0.9625        0.1626       0.7578      0.8522        0.2298  13.2206
Accuracy after query 8: 0.8060763888888889
F1 Score after query 8: 0.894337856345501
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_7.pt

Query 9: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 9 is 1776
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9538        0.1670       0.7911      0.8777        0.2009  18.0951
      2      0.9556        0.1645       0.7927      0.8779        0.2004  17.7710
      3      0.9567        0.1642       0.7957      0.8783        0.1989  17.8297
      4      0.9562        0.1637       0.8003      0.8803        0.1984  17.9057
      5      0.9598        0.1621       0.7970      0.8818        0.1992  17.8878
      6      0.9598        0.1605       0.7957      0.8778        0.1984  17.9129
      7      0.9573        0.1605       0.7944      0.8783        0.1995  17.9392
      8      0.9601        0.1585       0.7960      0.8781        0.1989  17.8801
      9      0.9583        0.1594       0.7941      0.8758        0.1986  17.8723
     10      0.9622        0.1565       0.7995      0.8793        0.1971  17.9039
     11      0.9595        0.1564       0.7960      0.8773        0.1979  17.9271
     12      0.9609        0.1553       0.7944      0.8774        0.1991  17.9155
     13      0.9607        0.1547       0.7918      0.8758        0.1987  17.8803
     14      0.9614        0.1549       0.7957      0.8778        0.1986  17.8880
     15      0.9640        0.1523       0.7932      0.8783        0.1989  17.8871
     16      0.9623        0.1525       0.7957      0.8765        0.1965  17.8533
     17      0.9623        0.1512       0.7977      0.8783        0.1958  17.9038
     18      0.9640        0.1508       0.7946      0.8777        0.1976  17.9051
     19      0.9626        0.1494       0.7962      0.8780        0.1962  17.8999
     20      0.9642        0.1482       0.7920      0.8772        0.1985  17.8996
     21      0.9657        0.1474       0.7918      0.8738        0.1967  17.9204
     22      0.9668        0.1462       0.7946      0.8764        0.1974  17.8666
     23      0.9646        0.1465       0.7934      0.8755        0.1966  17.8665
     24      0.9670        0.1451       0.7887      0.8767        0.1999  17.8931
     25      0.9659        0.1456       0.7922      0.8735        0.1980  17.9141
     26      0.9647        0.1449       0.7979      0.8749        0.1939  17.8919
     27      0.9665        0.1437       0.7993      0.8731        0.1959  17.8897
     28      0.9653        0.1434       0.7990      0.8743        0.1952  17.8864
     29      0.9672        0.1420       0.7898      0.8745        0.1968  17.8482
     30      0.9669        0.1408       0.7948      0.8735        0.1949  17.8996
     31      0.9667        0.1409       0.7924      0.8747        0.1961  17.9285
     32      0.9682        0.1401       0.7884      0.8744        0.1976  17.9228
     33      0.9684        0.1398       0.7995      0.8721        0.1963  17.9044
     34      0.9659        0.1403       0.7960      0.8762        0.1936  17.9193
     35      0.9681        0.1378       0.7995      0.8749        0.1934  17.9008
     36      0.9685        0.1377       0.7962      0.8742        0.1943  17.8790
     37      0.9688        0.1368       0.7976      0.8746        0.1941  17.9156
     38      0.9683        0.1358       0.7943      0.8707        0.1963  17.9027
     39      0.9692        0.1350       0.7920      0.8723        0.1971  17.9021
     40      0.9674        0.1355       0.7979      0.8711        0.1961  17.9294
     41      0.9711        0.1339       0.7937      0.8747        0.1941  17.8984
     42      0.9703        0.1336       0.7924      0.8740        0.1946  17.8568
     43      0.9690        0.1339       0.7915      0.8732        0.1953  17.8736
     44      0.9711        0.1322       0.7915      0.8763        0.1966  17.9464
     45      0.9688        0.1320       0.8042      0.8726        0.1955  17.9293
     46      0.9700        0.1309       0.7946      0.8721        0.1959  17.9100
     47      0.9716        0.1300       0.8026      0.8709        0.1986  17.8801
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 9: 0.8199652777777777
F1 Score after query 9: 0.9031269610009546
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_8.pt

Query 10: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 10 is 3160
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9563        0.1484       0.8113      0.8768        0.1857  26.2599
      2      0.9496        0.1532       0.8104      0.8736        0.1785  26.2098
      3      0.9526        0.1473       0.8064      0.8731        0.1842  26.2819
      4      0.9615        0.1397       0.8069      0.8761        0.1857  26.3101
      5      0.9572        0.1433       0.8087      0.8754        0.1803  26.3194
      6      0.9639        0.1360       0.8090      0.8723        0.1893  26.3190
      7      0.9638        0.1355       0.8030      0.8736        0.1904  26.3175
      8      0.9642        0.1332       0.8089      0.8747        0.1881  26.2695
      9      0.9648        0.1333       0.8106      0.8769        0.1838  26.2454
     10      0.9659        0.1314       0.8123      0.8766        0.1872  26.2373
     11      0.9644        0.1328       0.8061      0.8751        0.1853  26.2241
     12      0.9659        0.1295       0.8003      0.8747        0.1885  26.2288
     13      0.9675        0.1283       0.8030      0.8756        0.1871  26.2002
     14      0.9666        0.1277       0.8075      0.8758        0.1871  26.1905
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 10: 0.8201388888888889
F1 Score after query 10: 0.9005995157852881
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_9.pt

Query 11: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 11 is 5624
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9630        0.1361       0.7984      0.8728        0.1869  41.0192
      2      0.9631        0.1344       0.7936      0.8756        0.1859  41.0607
      3      0.9653        0.1315       0.8000      0.8747        0.1856  41.1658
      4      0.9644        0.1302       0.7964      0.8751        0.1885  41.1585
      5      0.9655        0.1284       0.8118      0.8776        0.1832  41.1246
      6      0.9659        0.1273       0.7979      0.8766        0.1862  41.1222
      7      0.9665        0.1254       0.8087      0.8776        0.1850  41.0921
      8      0.9668        0.1239       0.8099      0.8777        0.1854  41.1135
      9      0.9673        0.1222       0.8063      0.8801        0.1819  41.1655
     10      0.9681        0.1210       0.7964      0.8762        0.1870  41.0820
     11      0.9686        0.1191       0.8127      0.8802        0.1812  41.1047
     12      0.9690        0.1178       0.8109      0.8807        0.1817  41.1059
     13      0.9692        0.1162       0.8024      0.8786        0.1853  41.0517
     14      0.9692        0.1147       0.8071      0.8802        0.1824  41.1100
     15      0.9699        0.1142       0.8151      0.8810        0.1784  41.0950
     16      0.9710        0.1126       0.7981      0.8789        0.1873  41.0248
     17      0.9713        0.1112       0.8113      0.8823        0.1808  41.1130
     18      0.9713        0.1102       0.8078      0.8812        0.1815  41.1193
     19      0.9718        0.1086       0.8118      0.8819        0.1814  41.1184
     20      0.9720        0.1077       0.8054      0.8807        0.1844  41.1127
     21      0.9721        0.1064       0.8144      0.8833        0.1789  41.0977
     22      0.9725        0.1053       0.8161      0.8800        0.1784  41.1156
     23      0.9737        0.1041       0.8080      0.8796        0.1836  41.1100
     24      0.9737        0.1028       0.8120      0.8777        0.1840  41.0800
     25      0.9745        0.1019       0.8102      0.8804        0.1812  41.0552
     26      0.9740        0.1008       0.8068      0.8800        0.1823  41.1202
     27      0.9747        0.0998       0.8061      0.8805        0.1847  41.1689
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 11: 0.8159722222222222
F1 Score after query 11: 0.9036671190235387
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_10.pt

Query 12: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 12 is 10000
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9685        0.1072       0.8106      0.8826        0.1781  67.1275
      2      0.9743        0.0991       0.8177      0.8834        0.1755  67.5472
      3      0.9731        0.0995       0.8146      0.8857        0.1816  67.5359
      4      0.9759        0.0956       0.8120      0.8822        0.1845  67.5414
      5      0.9767        0.0934       0.8161      0.8812        0.1804  67.5393
      6      0.9770        0.0916       0.8193      0.8843        0.1780  67.5021
      7      0.9784        0.0896       0.8191      0.8838        0.1772  67.4427
      8      0.9784        0.0889       0.8189      0.8886        0.1809  67.4756
      9      0.9794        0.0872       0.8118      0.8845        0.1844  67.5918
     10      0.9800        0.0857       0.8135      0.8820        0.1842  67.5697
     11      0.9807        0.0845       0.8144      0.8819        0.1825  67.5320
     12      0.9798        0.0840       0.8130      0.8826        0.1841  67.4772
     13      0.9817        0.0818       0.8148      0.8783        0.1809  67.5063
     14      0.9816        0.0808       0.8160      0.8837        0.1829  67.5454
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 12: 0.8387152777777778
F1 Score after query 12: 0.9162988146583406
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_11.pt

Query 13: Using the exact images and labels from the CSV for this iteration.
Number of new samples used for training in Query 13 is 4056
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9806        0.0816       0.8274      0.8746        0.1810  77.9255
      2      0.9772        0.0871       0.7505      0.8564        0.3058  78.5262
      3      0.9803        0.0801       0.8063      0.8894        0.2004  79.0623
      4      0.9823        0.0768       0.8054      0.8652        0.1956  78.3614
      5      0.9826        0.0759       0.8118      0.8688        0.1809  78.2508
      6      0.9832        0.0746       0.7656      0.8417        0.2609  78.2743
      7      0.9833        0.0731       0.8099      0.8686        0.1867  78.4629
      8      0.9844        0.0720       0.8094      0.8690        0.1852  78.3195
      9      0.9837        0.0715       0.8095      0.8741        0.1911  78.3384
     10      0.9847        0.0701       0.8078      0.8735        0.1895  78.3419
     11      0.9849        0.0690       0.8108      0.8703        0.1887  78.4757
     12      0.9851        0.0677       0.8106      0.8766        0.1928  78.2630
     13      0.9846        0.0672       0.8080      0.8698        0.1942  78.4337
     14      0.9818        0.0730       0.7288      0.8416        0.3856  80.2454
     15      0.9828        0.0693       0.8063      0.8713        0.1953  82.2715
     16      0.9864        0.0640       0.8003      0.8613        0.1987  81.9600
     17      0.9860        0.0637       0.8047      0.8729        0.1958  81.7310
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 13: 0.8493055555555555
F1 Score after query 13: 0.9207330380056593
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\model_checkpoint_iteration_12.pt
Best F1 score across iterations: 0.9207330380056593
Performance results saved to D:\Shubham\results\multilabel01\DinoSmall\ActiveLearning\imagesFroMullticlass\avg_confidence\performance_results.npy
PS C:\Users\localuserSG\ActiveLearning\Multilabel01\DinoSmall\imagesFromMulticlass\Avg_confidence>