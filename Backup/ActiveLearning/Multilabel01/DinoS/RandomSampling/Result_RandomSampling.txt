45      0.2787        0.5884       0.3760      0.1825        0.5931  7.3796
     46      0.3743        0.5801       0.3781      0.1823        0.5934  7.4336
     47      0.3778        0.5826       0.3783      0.1846        0.5930  7.3687
     48      0.2945        0.5770       0.3788      0.1857        0.5927  7.4566
     49      0.4084        0.5618       0.3797      0.1864        0.5924  7.3958
     50      0.3326        0.5824       0.3793      0.1873        0.5918  7.3491
     51      0.3837        0.5728       0.3814      0.1883        0.5917  7.3366
     52      0.3789        0.5771       0.3835      0.1883        0.5918  7.3990
     53      0.2868        0.5772       0.3856      0.1886        0.5913  7.3651
     54      0.3871        0.5762       0.3837      0.1891        0.5907  7.4084
     55      0.3819        0.5813       0.3844      0.1899        0.5901  7.4156
     56      0.3691        0.5649       0.3854      0.1913        0.5899  7.4059
     57      0.4244        0.5639       0.3839      0.1927        0.5892  7.4061
     58      0.4244        0.5681       0.3845      0.1948        0.5887  7.3765
     59      0.3567        0.5803       0.3905      0.1965        0.5888  7.3882
     60      0.3952        0.5680       0.3924      0.1977        0.5883  7.3950
     61      0.4212        0.5685       0.3950      0.1994        0.5877  7.3949
     62      0.3919        0.5702       0.3943      0.1998        0.5871  7.3989
     63      0.3821        0.5699       0.3906      0.2005        0.5860  7.3928
     64      0.3812        0.5621       0.3931      0.2008        0.5859  7.3835
     65      0.4004        0.5656       0.3941      0.2015        0.5853  7.3864
     66      0.3419        0.5635       0.3983      0.2016        0.5854  7.3786
     67      0.3949        0.5643       0.4057      0.2021        0.5856  7.3793
     68      0.3884        0.5718       0.4099      0.2028        0.5852  7.3754
     69      0.3831        0.5722       0.4076      0.2051        0.5840  7.3866
     70      0.4005        0.5635       0.4127      0.2074        0.5836  7.3965
     71      0.4333        0.5721       0.4205      0.2082        0.5840  7.3932
     72      0.4577        0.5647       0.4226      0.2078        0.5838  7.4044
     73      0.4125        0.5629       0.4238      0.2086        0.5829  7.3853
     74      0.4142        0.5625       0.4224      0.2108        0.5819  7.3973
     75      0.4450        0.5621       0.4233      0.2131        0.5812  7.4061
     76      0.4259        0.5576       0.4276      0.2144        0.5810  7.4024
     77      0.4370        0.5531       0.4314      0.2154        0.5808  7.4076
     78      0.4012        0.5650       0.4340      0.2159        0.5804  7.3972
     79      0.4250        0.5603       0.4365      0.2164        0.5801  7.3880
     80      0.3974        0.5583       0.4365      0.2184        0.5789  7.3906
     81      0.4329        0.5575       0.4418      0.2192        0.5786  7.3881
     82      0.4585        0.5519       0.4425      0.2217        0.5778  7.3863
     83      0.4612        0.5530       0.4446      0.2246        0.5773  7.3712
     84      0.4633        0.5612       0.4500      0.2269        0.5770  7.3813
     85      0.4295        0.5584       0.4601      0.2287        0.5774  7.3948
     86      0.3906        0.5618       0.4616      0.2297        0.5776  7.3846
     87      0.4530        0.5565       0.4649      0.2293        0.5777  7.3811
     88      0.3995        0.5541       0.4691      0.2311        0.5770  7.3815
     89      0.4432        0.5557       0.4693      0.2325        0.5764  7.3976
     90      0.4308        0.5537       0.4733      0.2320        0.5768  7.3921
     91      0.4773        0.5577       0.4733      0.2346        0.5755  7.3721
     92      0.4267        0.5586       0.4715      0.2387        0.5741  7.3792
     93      0.5245        0.5504       0.4689      0.2407        0.5731  7.3901
     94      0.3464        0.5504       0.4641      0.2432        0.5719  7.3849
     95      0.4308        0.5545       0.4668      0.2441        0.5718  7.3889
     96      0.5093        0.5567       0.4694      0.2448        0.5719  7.3969
     97      0.4308        0.5400       0.4655      0.2498        0.5705  7.3932
     98      0.4843        0.5519       0.4677      0.2520        0.5704  7.3942
     99      0.4531        0.5550       0.4679      0.2534        0.5698  7.3486
    100      0.4815        0.5476       0.4701      0.2537        0.5694  7.3750
Accuracy after query 3: 0.4769097222222222
F1 Score after query 3: 0.28435795975030453
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\RandomSampling\imagesFroMullticlass\model_checkpoint_iteration_2.pt

Query 4: Using the exact images and labels from the CSV for this iteration.
Number of samples used for training in Query 4 is 112
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.4128        0.5554       0.4655      0.2557        0.5681  7.5236
      2      0.4895        0.5541       0.4644      0.2574        0.5671  7.5289
      3      0.4325        0.5560       0.4660      0.2580        0.5663  7.5497
      4      0.4270        0.5512       0.4632      0.2612        0.5650  7.5433
      5      0.4291        0.5571       0.4679      0.2608        0.5650  7.5453
      6      0.4213        0.5562       0.4715      0.2610        0.5648  7.5318
      7      0.4600        0.5495       0.4707      0.2632        0.5639  7.5278
      8      0.4085        0.5539       0.4734      0.2637        0.5634  7.5458
      9      0.4161        0.5489       0.4731      0.2678        0.5624  7.5570
     10      0.4162        0.5457       0.4726      0.2726        0.5611  7.5512
     11      0.4024        0.5480       0.4757      0.2742        0.5607  7.5400
     12      0.4224        0.5479       0.4776      0.2733        0.5604  7.5431
     13      0.4133        0.5498       0.4804      0.2763        0.5594  7.5203
     14      0.4643        0.5411       0.4825      0.2787        0.5585  7.5214
     15      0.4600        0.5445       0.4814      0.2834        0.5572  7.5113
     16      0.3925        0.5477       0.4840      0.2841        0.5567  7.5107
     17      0.4252        0.5460       0.4804      0.2925        0.5549  7.5179
     18      0.5107        0.5386       0.4828      0.2935        0.5544  7.5278
     19      0.4470        0.5371       0.4825      0.2967        0.5534  7.5489
     20      0.4168        0.5415       0.4828      0.2991        0.5527  7.5365
     21      0.4175        0.5468       0.4818      0.3042        0.5512  7.5403
     22      0.4323        0.5373       0.4854      0.3036        0.5511  7.5486
     23      0.4480        0.5414       0.4835      0.3132        0.5490  7.5636
     24      0.4300        0.5439       0.4861      0.3156        0.5481  7.5306
     25      0.4285        0.5448       0.4910      0.3132        0.5482  7.5220
     26      0.4738        0.5373       0.4931      0.3140        0.5476  7.5336
     27      0.4859        0.5391       0.4932      0.3204        0.5463  7.5206
     28      0.4682        0.5359       0.4955      0.3227        0.5458  7.5459
     29      0.4650        0.5391       0.4962      0.3254        0.5450  7.5234
     30      0.3858        0.5427       0.4936      0.3354        0.5430  7.5098
     31      0.4564        0.5381       0.4944      0.3382        0.5421  7.5083
     32      0.4660        0.5351       0.4988      0.3379        0.5418  7.5107
     33      0.4716        0.5337       0.5017      0.3399        0.5413  7.5282
     34      0.5021        0.5292       0.5038      0.3428        0.5403  7.5314
     35      0.4853        0.5332       0.5062      0.3458        0.5395  7.5261
     36      0.4669        0.5353       0.5036      0.3525        0.5376  7.5318
     37      0.4927        0.5323       0.5054      0.3562        0.5367  7.5352
     38      0.4929        0.5314       0.5062      0.3602        0.5355  7.5404
     39      0.4909        0.5324       0.5094      0.3625        0.5348  7.5300
     40      0.4753        0.5300       0.5111      0.3658        0.5339  7.5154
     41      0.5112        0.5235       0.5089      0.3729        0.5320  7.5228
     42      0.5030        0.5274       0.5116      0.3771        0.5308  7.5370
     43      0.4787        0.5319       0.5163      0.3750        0.5310  7.5351
     44      0.4692        0.5281       0.5151      0.3791        0.5296  7.5339
     45      0.5099        0.5224       0.5163      0.3822        0.5282  7.5088
     46      0.5051        0.5220       0.5186      0.3827        0.5277  7.4927
     47      0.5071        0.5263       0.5181      0.3891        0.5258  7.4954
     48      0.4776        0.5210       0.5158      0.3968        0.5238  7.4942
     49      0.4811        0.5230       0.5220      0.3933        0.5240  7.5356
     50      0.5216        0.5240       0.5247      0.3940        0.5233  7.5324
     51      0.5086        0.5241       0.5253      0.3990        0.5219  7.5264
     52      0.8171        0.5231       0.5253      0.4053        0.5200  7.5575
     53      0.5192        0.5251       0.5255      0.4076        0.5189  7.5116
     54      0.4901        0.5202       0.5267      0.4136        0.5173  7.5287
     55      0.5089        0.5202       0.5283      0.4086        0.5179  7.5418
     56      0.5415        0.5197       0.5290      0.4078        0.5174  7.5147
     57      0.5164        0.5216       0.5293      0.4163        0.5150  7.5279
     58      0.5071        0.5180       0.5300      0.4223        0.5133  7.5349
     59      0.4957        0.5183       0.5312      0.4247        0.5123  7.5462
     60      0.5086        0.5159       0.5321      0.4276        0.5111  7.5348
     61      0.5259        0.5147       0.5316      0.4318        0.5095  7.5238
     62      0.5739        0.5136       0.5326      0.4358        0.5082  7.5280
     63      0.5534        0.5155       0.5333      0.4381        0.5072  7.5282
     64      0.5090        0.5124       0.5332      0.4338        0.5073  7.5106
     65      0.4976        0.5105       0.5326      0.4301        0.5074  7.5403
     66      0.5203        0.5149       0.5352      0.4462        0.5042  7.5349
     67      0.5599        0.5084       0.5352      0.4538        0.5022  7.5613
     68      0.5795        0.5064       0.5351      0.4540        0.5015  7.5534
     69      0.5279        0.5098       0.5366      0.4596        0.5001  7.5540
     70      0.5027        0.5075       0.5354      0.4577        0.4996  7.5290
     71      0.5608        0.5021       0.5344      0.4524        0.5002  7.5421
     72      0.5026        0.5089       0.5359      0.4604        0.4979  7.5318
     73      0.5284        0.5013       0.5394      0.4724        0.4950  7.5386
     74      0.5505        0.5071       0.5385      0.4720        0.4944  7.5320
     75      0.5630        0.5070       0.5361      0.4654        0.4949  7.5169
     76      0.5619        0.5088       0.5354      0.4616        0.4952  7.5234
     77      0.8727        0.5029       0.5351      0.4622        0.4947  7.5681
     78      0.4867        0.5040       0.5354      0.4661        0.4929  7.4823
     79      0.5349        0.5006       0.5375      0.4738        0.4904  7.5137
     80      0.5342        0.5015       0.5370      0.4713        0.4904  7.5283
     81      0.5588        0.4989       0.5373      0.4743        0.4893  7.5058
     82      0.6209        0.4955       0.5387      0.4792        0.4877  7.5172
     83      0.4927        0.5047       0.5455      0.4921        0.4849  7.5028
     84      0.6025        0.4975       0.5370      0.4775        0.4867  7.5002
     85      0.5165        0.5007       0.5408      0.4855        0.4847  7.5052
     86      0.6170        0.4950       0.5434      0.4912        0.4831  7.4992
     87      0.5980        0.4952       0.5465      0.4973        0.4814  7.5106
     88      0.5783        0.4943       0.5446      0.4941        0.4813  7.4909
     89      0.5370        0.4966       0.5411      0.4903        0.4814  7.5104
     90      0.5581        0.4946       0.5418      0.4917        0.4805  7.4944
     91      0.6490        0.4889       0.5502      0.5040        0.4780  7.4923
     92      0.5845        0.4921       0.5549      0.5108        0.4762  7.5011
     93      0.5840        0.4920       0.5524      0.5088        0.4761  7.4920
     94      0.6423        0.4895       0.5497      0.5064        0.4759  7.4881
     95      0.6008        0.4885       0.5571      0.5167        0.4735  7.4867
     96      0.6068        0.4896       0.5644      0.5262        0.4712  7.4974
     97      0.5398        0.4916       0.5604      0.5222        0.4714  7.5006
     98      0.6072        0.4843       0.5651      0.5276        0.4697  7.5063
     99      0.5956        0.4873       0.5622      0.5241        0.4700  7.5123
    100      0.6096        0.4850       0.5670      0.5304        0.4685  7.5069
Accuracy after query 4: 0.6065972222222222
F1 Score after query 4: 0.5944635896702124
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\RandomSampling\imagesFroMullticlass\model_checkpoint_iteration_3.pt

Query 5: Using the exact images and labels from the CSV for this iteration.
Number of samples used for training in Query 5 is 208
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.5697        0.4976       0.5813      0.5513        0.4630  7.7754
      2      0.6011        0.4910       0.5830      0.5547        0.4604  7.7900
      3      0.5701        0.4933       0.5806      0.5523        0.4596  7.7770
      4      0.6225        0.4888       0.5877      0.5624        0.4559  7.7910
      5      0.6248        0.4834       0.5894      0.5672        0.4535  7.7766
      6      0.6031        0.4873       0.5873      0.5653        0.4528  7.7687
      7      0.5973        0.4857       0.5865      0.5664        0.4509  7.7659
      8      0.6256        0.4788       0.5931      0.5804        0.4468  7.7606
      9      0.6034        0.4821       0.5941      0.5881        0.4453  7.7679
     10      0.6268        0.4827       0.5962      0.5936        0.4435  7.7705
     11      0.6431        0.4766       0.5981      0.5995        0.4428  7.7672
     12      0.6678        0.4740       0.6009      0.6072        0.4412  7.7702
     13      0.6677        0.4746       0.6069      0.6202        0.4378  7.7885
     14      0.6594        0.4730       0.6057      0.6192        0.4380  7.7718
     15      0.6721        0.4690       0.6083      0.6272        0.4366  7.7717
     16      0.6698        0.4753       0.6106      0.6342        0.4347  7.7759
     17      0.6801        0.4679       0.6137      0.6420        0.4322  7.7764
     18      0.6997        0.4653       0.6125      0.6412        0.4326  7.7736
     19      0.6965        0.4666       0.6200      0.6591        0.4290  7.7795
     20      0.6498        0.4647       0.6212      0.6630        0.4276  7.7785
     21      0.7290        0.4597       0.6293      0.6774        0.4252  7.7827
     22      0.7258        0.4629       0.6269      0.6765        0.4255  7.7647
     23      0.7147        0.4604       0.6306      0.6840        0.4237  7.7773
     24      0.7650        0.4563       0.6368      0.6937        0.4217  7.7752
     25      0.7140        0.4575       0.6337      0.6918        0.4231  7.7495
     26      0.6974        0.4646       0.6469      0.7152        0.4195  7.7631
     27      0.7401        0.4594       0.6462      0.7160        0.4187  7.7730
     28      0.7602        0.4555       0.6503      0.7227        0.4176  7.7714
     29      0.6882        0.4581       0.6630      0.7420        0.4142  7.7817
     30      0.7476        0.4529       0.6590      0.7364        0.4146  7.7816
     31      0.7389        0.4519       0.6672      0.7486        0.4120  7.7592
     32      0.7339        0.4512       0.6620      0.7421        0.4127  7.7838
     33      0.7370        0.4516       0.6550      0.7334        0.4139  7.7463
     34      0.7568        0.4502       0.6694      0.7543        0.4092  7.7568
     35      0.7632        0.4496       0.6726      0.7598        0.4083  7.7784
     36      0.7323        0.4483       0.6778      0.7671        0.4063  7.7593
     37      0.7648        0.4464       0.6760      0.7660        0.4062  7.7598
     38      0.7749        0.4422       0.6771      0.7674        0.4056  7.7626
     39      0.7919        0.4422       0.6780      0.7690        0.4047  7.7541
     40      0.7785        0.4413       0.6792      0.7712        0.4038  7.7549
     41      0.8033        0.4408       0.6807      0.7739        0.4024  7.7513
     42      0.7987        0.4397       0.6773      0.7702        0.4033  7.7604
     43      0.8000        0.4404       0.6778      0.7719        0.4024  7.7751
     44      0.8013        0.4367       0.6840      0.7805        0.3996  7.7752
     45      0.7974        0.4353       0.6873      0.7866        0.3979  7.7885
     46      0.7864        0.4368       0.6818      0.7795        0.3991  7.7699
     47      0.7976        0.4357       0.6818      0.7789        0.3987  7.7585
     48      0.7987        0.4353       0.6814      0.7800        0.3978  7.7722
     49      0.8161        0.4321       0.6872      0.7891        0.3954  7.7747
     50      0.8162        0.4306       0.6839      0.7854        0.3957  7.7670
     51      0.8022        0.4325       0.6807      0.7802        0.3970  7.7779
     52      0.8413        0.4324       0.6922      0.8004        0.3908  7.7649
     53      0.8291        0.4307       0.6892      0.7954        0.3919  7.7856
     54      0.8343        0.4294       0.6908      0.7981        0.3905  7.7964
     55      0.8369        0.4277       0.6870      0.7949        0.3906  7.7680
     56      0.8276        0.4295       0.6887      0.7974        0.3895  7.7657
     57      0.8289        0.4281       0.6868      0.7960        0.3893  7.7439
     58      0.7906        0.4307       0.6892      0.8011        0.3877  7.7877
     59      0.8268        0.4285       0.6885      0.8003        0.3874  7.7813
     60      0.8196        0.4254       0.6875      0.7993        0.3871  7.7879
     61      0.8229        0.4234       0.6924      0.8058        0.3851  7.7900
     62      0.8410        0.4202       0.6925      0.8054        0.3848  7.7906
     63      0.8265        0.4239       0.6925      0.8064        0.3837  7.7799
     64      0.8336        0.4233       0.6859      0.7975        0.3857  7.7718
     65      0.8350        0.4190       0.6922      0.8079        0.3823  7.7765
     66      0.8024        0.4256       0.6924      0.8096        0.3813  7.7620
     67      0.8415        0.4191       0.6915      0.8077        0.3814  7.7608
     68      0.8222        0.4182       0.6854      0.7980        0.3841  7.7803
     69      0.8170        0.4196       0.6866      0.8028        0.3816  7.7645
     70      0.8498        0.4217       0.6847      0.8004        0.3819  7.7366
     71      0.8271        0.4190       0.6906      0.8094        0.3788  7.7687
     72      0.8339        0.4175       0.6911      0.8109        0.3780  7.7397
     73      0.8330        0.4166       0.6851      0.8011        0.3804  7.7782
     74      0.8594        0.4145       0.6884      0.8080        0.3781  7.7687
     75      0.8432        0.4137       0.6903      0.8100        0.3773  7.7702
     76      0.8559        0.4153       0.6882      0.8075        0.3775  7.7773
     77      0.8451        0.4138       0.6924      0.8125        0.3757  7.7462
     78      0.8476        0.4133       0.6891      0.8087        0.3764  7.7763
     79      0.8665        0.4126       0.6960      0.8169        0.3732  7.7742
     80      0.8468        0.4132       0.6852      0.8038        0.3770  7.7435
     81      0.8804        0.4088       0.6922      0.8146        0.3730  7.7570
     82      0.8692        0.4109       0.6917      0.8131        0.3733  7.7773
     83      0.8740        0.4099       0.6852      0.8043        0.3759  7.7413
     84      0.8178        0.4141       0.6948      0.8162        0.3712  7.7483
     85      0.8476        0.4064       0.6885      0.8090        0.3735  7.7450
     86      0.8670        0.4072       0.6851      0.8051        0.3741  7.7441
     87      0.8512        0.4104       0.6924      0.8144        0.3711  7.7597
     88      0.8557        0.4080       0.6917      0.8137        0.3710  7.7508
     89      0.8494        0.4078       0.6865      0.8065        0.3730  7.7574
     90      0.8842        0.4035       0.6918      0.8145        0.3699  7.7630
     91      0.8617        0.4045       0.6884      0.8108        0.3706  7.7771
     92      0.8431        0.4082       0.6917      0.8145        0.3692  7.7573
     93      0.8671        0.4044       0.6872      0.8079        0.3710  7.7641
     94      0.8592        0.4022       0.7010      0.8233        0.3655  7.7737
     95      0.8614        0.4050       0.6937      0.8171        0.3672  7.7816
     96      0.8549        0.4059       0.6960      0.8191        0.3661  7.7742
     97      0.8597        0.4061       0.6988      0.8218        0.3650  7.7761
     98      0.8520        0.4040       0.6955      0.8191        0.3657  7.7791
     99      0.8549        0.4025       0.6906      0.8133        0.3671  7.7842
    100      0.8690        0.4010       0.6943      0.8176        0.3654  7.7773
Accuracy after query 5: 0.7282986111111112
F1 Score after query 5: 0.8319845599629438
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\RandomSampling\imagesFroMullticlass\model_checkpoint_iteration_4.pt

Query 6: Using the exact images and labels from the CSV for this iteration.
Number of samples used for training in Query 6 is 384
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.8689        0.4024       0.7068      0.8306        0.3613  8.2003
      2      0.8729        0.3999       0.7095      0.8336        0.3599  8.2380
      3      0.8714        0.3994       0.7078      0.8329        0.3590  8.2366
      4      0.8662        0.3991       0.7043      0.8311        0.3588  8.2437
      5      0.8693        0.3982       0.7063      0.8326        0.3576  8.2226
      6      0.8652        0.3980       0.7049      0.8326        0.3568  8.2183
      7      0.8699        0.3972       0.7043      0.8327        0.3563  8.2463
      8      0.8763        0.3937       0.7028      0.8304        0.3565  8.2300
      9      0.8787        0.3920       0.7021      0.8315        0.3554  8.2578
     10      0.8750        0.3938       0.7019      0.8312        0.3550  8.2371
     11      0.8755        0.3930       0.7038      0.8330        0.3542  8.2346
     12      0.8753        0.3900       0.7049      0.8336        0.3532  8.2189
     13      0.8823        0.3908       0.7043      0.8335        0.3530  8.2700
     14      0.8681        0.3911       0.7052      0.8341        0.3525  8.2215
     15      0.8725        0.3913       0.7030      0.8318        0.3525  8.2300
     16      0.8764        0.3888       0.7045      0.8337        0.3512  8.2301
     17      0.8909        0.3870       0.7050      0.8338        0.3512  8.2183
     18      0.8884        0.3868       0.7050      0.8340        0.3506  8.2360
     19      0.8727        0.3861       0.7056      0.8343        0.3499  8.2314
     20      0.8769        0.3882       0.7040      0.8340        0.3492  8.2259
     21      0.8745        0.3849       0.7047      0.8339        0.3493  8.2587
     22      0.8763        0.3849       0.7040      0.8333        0.3490  8.2270
     23      0.8680        0.3842       0.7061      0.8354        0.3478  8.2496
     24      0.8776        0.3862       0.7061      0.8355        0.3473  8.2148
     25      0.8743        0.3814       0.7040      0.8340        0.3473  8.2439
     26      0.8769        0.3805       0.7056      0.8353        0.3463  8.2295
     27      0.8803        0.3811       0.7069      0.8369        0.3456  8.2383
     28      0.8857        0.3786       0.7052      0.8348        0.3459  8.2098
     29      0.8830        0.3803       0.7056      0.8354        0.3450  8.2023
     30      0.8776        0.3791       0.7056      0.8353        0.3447  8.2108
     31      0.8749        0.3791       0.7068      0.8362        0.3441  8.2116
     32      0.8850        0.3774       0.7061      0.8353        0.3443  8.2261
     33      0.8735        0.3755       0.7075      0.8367        0.3430  8.2347
     34      0.8774        0.3762       0.7064      0.8358        0.3433  8.2323
     35      0.8818        0.3742       0.7080      0.8373        0.3421  8.2380
     36      0.8817        0.3776       0.7071      0.8363        0.3427  8.2278
     37      0.8818        0.3738       0.7075      0.8372        0.3415  8.2330
     38      0.8803        0.3737       0.7071      0.8364        0.3418  8.2483
     39      0.8860        0.3739       0.7068      0.8370        0.3410  8.2420
     40      0.8750        0.3743       0.7087      0.8386        0.3401  8.2256
     41      0.8816        0.3734       0.7092      0.8388        0.3398  8.2429
     42      0.8871        0.3740       0.7082      0.8388        0.3394  8.2322
     43      0.8831        0.3723       0.7075      0.8372        0.3395  8.2133
     44      0.8813        0.3709       0.7059      0.8354        0.3399  8.2303
     45      0.8763        0.3702       0.7057      0.8348        0.3400  8.2237
     46      0.8843        0.3700       0.7080      0.8376        0.3384  8.2396
     47      0.8768        0.3695       0.7080      0.8379        0.3382  8.2294
     48      0.8889        0.3689       0.7063      0.8353        0.3386  8.2431
     49      0.8855        0.3677       0.7080      0.8378        0.3373  8.2446
     50      0.8814        0.3676       0.7080      0.8372        0.3374  8.2465
     51      0.8814        0.3669       0.7076      0.8370        0.3371  8.2357
     52      0.8811        0.3661       0.7095      0.8389        0.3360  8.2289
     53      0.8806        0.3665       0.7069      0.8363        0.3366  8.2445
     54      0.8831        0.3668       0.7090      0.8390        0.3353  8.2367
     55      0.8830        0.3666       0.7097      0.8394        0.3351  8.2187
     56      0.8837        0.3646       0.7082      0.8377        0.3352  8.2308
     57      0.8872        0.3632       0.7076      0.8362        0.3354  8.2211
     58      0.8875        0.3646       0.7085      0.8381        0.3344  8.2001
     59      0.8822        0.3610       0.7102      0.8397        0.3336  8.2242
     60      0.8934        0.3635       0.7104      0.8403        0.3331  8.1932
     61      0.8778        0.3628       0.7099      0.8396        0.3331  8.2288
     62      0.8852        0.3612       0.7099      0.8397        0.3328  8.2285
     63      0.8885        0.3605       0.7080      0.8374        0.3330  8.2322
     64      0.8781        0.3616       0.7109      0.8403        0.3322  8.2206
     65      0.8878        0.3584       0.7118      0.8411        0.3316  8.2195
     66      0.8823        0.3613       0.7102      0.8398        0.3315  8.2220
     67      0.8843        0.3604       0.7116      0.8403        0.3313  8.2154
     68      0.8891        0.3616       0.7113      0.8400        0.3311  8.2308
     69      0.8952        0.3573       0.7090      0.8375        0.3312  8.2226
     70      0.8833        0.3586       0.7109      0.8408        0.3300  8.2384
     71      0.8811        0.3587       0.7109      0.8396        0.3304  8.2413
     72      0.8854        0.3562       0.7085      0.8373        0.3303  8.2001
     73      0.8881        0.3557       0.7092      0.8368        0.3304  8.2140
     74      0.8893        0.3553       0.7115      0.8403        0.3292  8.2100
     75      0.8869        0.3568       0.7108      0.8391        0.3292  8.2271
     76      0.8897        0.3550       0.7050      0.8337        0.3308  8.2464
     77      0.8851        0.3552       0.7123      0.8415        0.3279  8.2535
     78      0.8839        0.3562       0.7122      0.8411        0.3278  8.2268
     79      0.8874        0.3543       0.7137      0.8427        0.3271  8.2655
     80      0.8858        0.3531       0.7095      0.8380        0.3280  8.2459
     81      0.8853        0.3529       0.7127      0.8420        0.3267  8.2203
     82      0.8867        0.3527       0.7130      0.8417        0.3266  8.2397
     83      0.8920        0.3522       0.7111      0.8397        0.3266  8.2504
     84      0.8853        0.3505       0.7130      0.8421        0.3259  8.2331
     85      0.8838        0.3522       0.7116      0.8398        0.3262  8.2487
     86      0.8873        0.3506       0.7128      0.8414        0.3256  8.2203
     87      0.8867        0.3509       0.7123      0.8406        0.3254  8.2238
     88      0.8829        0.3506       0.7087      0.8378        0.3259  8.2149
     89      0.8814        0.3506       0.7141      0.8434        0.3243  8.2353
     90      0.8859        0.3488       0.7132      0.8421        0.3241  8.2326
     91      0.8830        0.3472       0.7135      0.8422        0.3239  8.2325
     92      0.8886        0.3453       0.7122      0.8394        0.3243  8.2492
     93      0.8903        0.3480       0.7125      0.8397        0.3240  8.2426
     94      0.8865        0.3475       0.7137      0.8424        0.3230  8.2445
     95      0.8907        0.3472       0.7158      0.8447        0.3225  8.2590
     96      0.8917        0.3467       0.7135      0.8422        0.3225  8.2559
     97      0.8957        0.3440       0.7153      0.8441        0.3220  8.2282
     98      0.8954        0.3483       0.7128      0.8403        0.3226  8.2290
     99      0.8877        0.3465       0.7167      0.8459        0.3214  8.2307
    100      0.8884        0.3439       0.7149      0.8428        0.3216  8.2342
Accuracy after query 6: 0.7418402777777777
F1 Score after query 6: 0.8543541270269405
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\RandomSampling\imagesFroMullticlass\model_checkpoint_iteration_5.pt

Query 7: Using the exact images and labels from the CSV for this iteration.
Number of samples used for training in Query 7 is 704
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss     dur
-------  ----------  ------------  -----------  ----------  ------------  ------
      1      0.8802        0.3470       0.7139      0.8453        0.3229  9.0634
      2      0.8818        0.3463       0.7078      0.8427        0.3246  9.0645
      3      0.8816        0.3454       0.7158      0.8465        0.3210  9.0813
      4      0.8823        0.3443       0.7132      0.8454        0.3215  9.0950
      5      0.8776        0.3459       0.7222      0.8507        0.3176  9.0668
      6      0.8826        0.3407       0.7226      0.8511        0.3169  9.0969
      7      0.8766        0.3417       0.7231      0.8517        0.3162  9.0747
      8      0.8810        0.3417       0.7224      0.8517        0.3151  9.0815
      9      0.8871        0.3393       0.7217      0.8518        0.3144  9.0918
     10      0.8796        0.3390       0.7219      0.8518        0.3137  9.0897
     11      0.8806        0.3381       0.7247      0.8532        0.3133  9.0900
     12      0.8873        0.3359       0.7227      0.8522        0.3125  9.0881
     13      0.8877        0.3366       0.7224      0.8519        0.3128  9.0541
     14      0.8893        0.3355       0.7248      0.8537        0.3117  9.0757
     15      0.8886        0.3372       0.7233      0.8527        0.3113  9.0877
     16      0.8840        0.3355       0.7243      0.8538        0.3102  9.0848
     17      0.8863        0.3361       0.7163      0.8481        0.3099  9.0922
     18      0.8891        0.3327       0.7240      0.8536        0.3096  9.0916
     19      0.8826        0.3314       0.7214      0.8521        0.3083  9.1125
     20      0.8871        0.3319       0.7248      0.8547        0.3078  9.1049
     21      0.8911        0.3315       0.7240      0.8544        0.3073  9.0762
     22      0.8868        0.3304       0.7219      0.8532        0.3066  9.0844
     23      0.8863        0.3282       0.7188      0.8513        0.3063  9.0758
     24      0.8914        0.3282       0.7234      0.8547        0.3057  9.0876
     25      0.8945        0.3281       0.7231      0.8547        0.3054  9.0988
     26      0.8927        0.3288       0.7238      0.8548        0.3050  9.0860
     27      0.8927        0.3268       0.7240      0.8547        0.3047  9.0733
     28      0.8920        0.3267       0.7229      0.8543        0.3043  9.0885
     29      0.8882        0.3277       0.7170      0.8509        0.3041  9.0688
     30      0.8901        0.3244       0.7233      0.8555        0.3033  9.1114
     31      0.8870        0.3260       0.7227      0.8546        0.3036  9.1031
     32      0.8988        0.3257       0.7236      0.8555        0.3023  9.0893
     33      0.8886        0.3246       0.7241      0.8557        0.3021  9.0923
     34      0.8884        0.3237       0.7085      0.8460        0.3025  9.0884
     35      0.8925        0.3229       0.7231      0.8557        0.3012  9.0764
     36      0.8931        0.3217       0.7106      0.8482        0.3013  9.0879
     37      0.8897        0.3208       0.7240      0.8560        0.3004  9.0781
     38      0.8922        0.3224       0.7233      0.8558        0.3000  9.1081
     39      0.8938        0.3201       0.7207      0.8544        0.2996  9.0803
     40      0.8925        0.3203       0.7236      0.8561        0.2996  9.0797
     41      0.8919        0.3180       0.7205      0.8544        0.2987  9.0636
     42      0.8955        0.3172       0.7134      0.8500        0.2987  9.0690
     43      0.8942        0.3173       0.7222      0.8558        0.2979  9.0897
     44      0.8977        0.3152       0.7222      0.8557        0.2974  9.0700
     45      0.8979        0.3148       0.7214      0.8552        0.2971  9.0639
     46      0.8924        0.3145       0.7214      0.8551        0.2966  9.0766
     47      0.8956        0.3153       0.7168      0.8516        0.2963  9.0761
     48      0.8914        0.3145       0.7219      0.8555        0.2957  9.0885
     49      0.9023        0.3152       0.7201      0.8540        0.2953  9.0780
     50      0.8961        0.3129       0.7184      0.8526        0.2950  9.1011
     51      0.8948        0.3126       0.7248      0.8573        0.2947  9.0720
     52      0.8990        0.3108       0.7194      0.8537        0.2942  9.0704
     53      0.8979        0.3109       0.7241      0.8566        0.2942  9.0547
     54      0.8953        0.3122       0.7241      0.8564        0.2944  9.0687
     55      0.8962        0.3095       0.7170      0.8514        0.2932  9.0459
     56      0.8949        0.3085       0.7250      0.8568        0.2923  9.0907
     57      0.8987        0.3111       0.7189      0.8537        0.2923  9.0709
     58      0.8964        0.3055       0.7252      0.8569        0.2918  9.0845
     59      0.8962        0.3066       0.7222      0.8552        0.2912  9.0721
     60      0.9003        0.3071       0.7222      0.8549        0.2908  9.1013
     61      0.8997        0.3062       0.7252      0.8574        0.2910  9.0667
     62      0.9011        0.3044       0.7247      0.8564        0.2900  9.0950
     63      0.9018        0.3043       0.7253      0.8576        0.2901  9.0761
     64      0.8984        0.3032       0.7215      0.8545        0.2894  9.1057
     65      0.8958        0.3049       0.7262      0.8569        0.2889  9.0713
     66      0.8996        0.3065       0.7295      0.8585        0.2926  9.0925
     67      0.8915        0.3049       0.7224      0.8551        0.2884  9.0637
     68      0.9001        0.3020       0.7281      0.8584        0.2890  9.0677
     69      0.8991        0.3035       0.7241      0.8563        0.2879  9.0864
     70      0.8983        0.3015       0.7260      0.8575        0.2883  9.0937
     71      0.8990        0.3002       0.7257      0.8567        0.2870  9.0849
     72      0.9030        0.3017       0.7267      0.8579        0.2867  9.1019
     73      0.8971        0.2986       0.7191      0.8521        0.2864  9.0722
     74      0.8993        0.3000       0.7300      0.8592        0.2863  9.0930
     75      0.9009        0.3003       0.7273      0.8582        0.2861  9.0947
     76      0.9005        0.2998       0.7248      0.8568        0.2857  9.0861
     77      0.9044        0.2966       0.7252      0.8570        0.2852  9.0822
     78      0.9022        0.2956       0.7245      0.8567        0.2845  9.0832
     79      0.9014        0.2949       0.7262      0.8573        0.2842  9.0468
     80      0.9029        0.2958       0.7253      0.8568        0.2836  9.0687
     81      0.9032        0.2956       0.7248      0.8555        0.2832  9.0496
     82      0.9009        0.2925       0.7316      0.8592        0.2831  9.0825
     83      0.9014        0.2969       0.7288      0.8587        0.2828  9.0730
     84      0.9004        0.2947       0.7273      0.8578        0.2822  9.0917
     85      0.9024        0.2924       0.7259      0.8569        0.2818  9.1046
     86      0.9046        0.2924       0.7302      0.8592        0.2821  9.0945
     87      0.9026        0.2955       0.7312      0.8599        0.2830  9.0624
     88      0.9026        0.2961       0.7306      0.8598        0.2826  9.0926
     89      0.8989        0.2932       0.7280      0.8584        0.2810  9.0804
     90      0.9070        0.2901       0.7283      0.8583        0.2807  9.0938
     91      0.9038        0.2906       0.7255      0.8566        0.2804  9.0493
     92      0.8958        0.2946       0.7236      0.8555        0.2802  9.0719
     93      0.9026        0.2893       0.7288      0.8584        0.2795  9.0483
     94      0.9091        0.2893       0.7247      0.8555        0.2794  9.0679
     95      0.8953        0.2949       0.7076      0.8369        0.2898  9.0977
     96      0.8024        0.3687       0.6415      0.7487        0.3675  9.0904
     97      0.8830        0.2984       0.7201      0.8548        0.2813  9.0850
     98      0.8974        0.2899       0.7267      0.8585        0.2787  9.1153
     99      0.9029        0.2887       0.7316      0.8603        0.2772  9.0809
    100      0.9034        0.2874       0.7335      0.8609        0.2763  9.1037
Accuracy after query 7: 0.7765625
F1 Score after query 7: 0.8712314139659778
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\RandomSampling\imagesFroMullticlass\model_checkpoint_iteration_6.pt

Query 8: Using the exact images and labels from the CSV for this iteration.
Number of samples used for training in Query 8 is 1264
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.8886        0.2968       0.7231      0.8493        0.3023  10.7168
      2      0.8809        0.2983       0.7399      0.8615        0.2860  10.5581
      3      0.8871        0.2930       0.7408      0.8642        0.2824  10.5431
      4      0.8889        0.2925       0.7368      0.8608        0.2870  10.5601
      5      0.8917        0.2927       0.7405      0.8646        0.2813  10.5455
      6      0.8929        0.2902       0.7434      0.8676        0.2780  10.5454
      7      0.8930        0.2879       0.7441      0.8681        0.2773  10.5694
      8      0.8972        0.2881       0.7450      0.8692        0.2750  10.5529
      9      0.8971        0.2857       0.7455      0.8696        0.2742  10.5561
     10      0.8941        0.2862       0.7429      0.8679        0.2776  10.5900
     11      0.8985        0.2836       0.7460      0.8710        0.2699  10.5532
     12      0.8983        0.2838       0.7486      0.8716        0.2713  10.5781
     13      0.9021        0.2826       0.7432      0.8700        0.2684  10.5666
     14      0.8994        0.2809       0.7458      0.8716        0.2683  10.5664
     15      0.8993        0.2821       0.7491      0.8723        0.2689  10.5569
     16      0.9013        0.2808       0.7457      0.8702        0.2727  10.5403
     17      0.9015        0.2800       0.7406      0.8685        0.2651  10.5590
     18      0.9013        0.2787       0.7465      0.8721        0.2659  10.5697
     19      0.8975        0.2785       0.7472      0.8715        0.2686  10.5494
     20      0.8997        0.2779       0.7453      0.8712        0.2642  10.5498
     21      0.8997        0.2763       0.7507      0.8738        0.2655  10.5737
     22      0.9041        0.2765       0.7431      0.8698        0.2625  10.5657
     23      0.9044        0.2737       0.7457      0.8715        0.2622  10.5440
     24      0.8995        0.2755       0.7411      0.8675        0.2721  10.5360
     25      0.9011        0.2758       0.7491      0.8739        0.2632  10.5717
     26      0.9015        0.2728       0.7451      0.8708        0.2605  10.5534
     27      0.9017        0.2732       0.7490      0.8738        0.2615  10.5546
     28      0.9042        0.2711       0.7495      0.8739        0.2613  10.5523
     29      0.9031        0.2712       0.7514      0.8752        0.2617  10.5680
     30      0.9075        0.2703       0.7505      0.8748        0.2624  10.5778
     31      0.9038        0.2707       0.7427      0.8695        0.2581  10.5398
     32      0.9039        0.2690       0.7450      0.8712        0.2578  10.5868
     33      0.9067        0.2682       0.7498      0.8736        0.2576  10.5995
     34      0.9091        0.2667       0.7493      0.8735        0.2570  10.5670
     35      0.9108        0.2658       0.7530      0.8757        0.2590  10.5709
     36      0.9023        0.2678       0.7439      0.8698        0.2662  10.5841
     37      0.9082        0.2666       0.7497      0.8739        0.2555  10.5897
     38      0.9081        0.2645       0.7516      0.8749        0.2599  10.5682
     39      0.9090        0.2644       0.7531      0.8764        0.2565  10.5619
     40      0.9094        0.2644       0.7535      0.8766        0.2565  10.5554
     41      0.9099        0.2628       0.7526      0.8760        0.2557  10.5428
     42      0.9108        0.2629       0.7486      0.8728        0.2606  10.5749
     43      0.9107        0.2607       0.7531      0.8767        0.2550  10.5503
     44      0.9096        0.2613       0.7552      0.8774        0.2548  10.5688
     45      0.9137        0.2616       0.7535      0.8761        0.2562  10.5670
     46      0.9097        0.2623       0.7498      0.8739        0.2573  10.5797
     47      0.9132        0.2593       0.7547      0.8772        0.2516  10.6387
     48      0.9126        0.2587       0.7536      0.8768        0.2530  10.5470
     49      0.9142        0.2575       0.7545      0.8771        0.2526  10.5352
     50      0.9097        0.2593       0.7467      0.8714        0.2584  10.5184
     51      0.9137        0.2578       0.7561      0.8777        0.2509  10.5122
     52      0.9160        0.2567       0.7554      0.8773        0.2500  10.5557
     53      0.9193        0.2545       0.7540      0.8768        0.2488  10.5393
     54      0.9140        0.2546       0.7568      0.8782        0.2527  10.5446
     55      0.9138        0.2551       0.7469      0.8717        0.2572  10.5332
     56      0.9090        0.2574       0.7418      0.8683        0.2619  10.5370
     57      0.9113        0.2555       0.7561      0.8779        0.2472  10.5307
     58      0.9125        0.2539       0.7446      0.8699        0.2586  10.5372
     59      0.9145        0.2538       0.7568      0.8782        0.2479  10.5406
     60      0.9136        0.2521       0.7578      0.8787        0.2475  10.5397
     61      0.9170        0.2504       0.7595      0.8790        0.2474  10.4899
     62      0.9190        0.2504       0.7568      0.8782        0.2457  10.5557
     63      0.9187        0.2497       0.7569      0.8778        0.2485  10.5497
     64      0.9166        0.2506       0.7557      0.8770        0.2496  10.5868
     65      0.9160        0.2490       0.7592      0.8798        0.2440  10.5702
     66      0.9191        0.2471       0.7606      0.8799        0.2456  10.5569
     67      0.9177        0.2478       0.7491      0.8727        0.2529  10.5819
     68      0.9129        0.2494       0.7417      0.8685        0.2585  10.5348
     69      0.9133        0.2493       0.7615      0.8806        0.2439  10.5550
     70      0.9172        0.2473       0.7470      0.8715        0.2530  10.5740
     71      0.9121        0.2475       0.7606      0.8791        0.2456  10.5709
     72      0.9179        0.2450       0.7604      0.8782        0.2413  10.5465
     73      0.9194        0.2451       0.7606      0.8802        0.2412  10.5610
     74      0.9198        0.2435       0.7618      0.8807        0.2407  10.5623
     75      0.9200        0.2439       0.7613      0.8807        0.2406  10.5562
     76      0.9223        0.2425       0.7632      0.8811        0.2403  10.5658
     77      0.9212        0.2424       0.7568      0.8771        0.2456  10.5617
     78      0.9169        0.2426       0.7549      0.8759        0.2469  10.5716
     79      0.9160        0.2420       0.7646      0.8821        0.2397  10.5705
     80      0.9211        0.2407       0.7649      0.8821        0.2391  10.5905
     81      0.9177        0.2430       0.7635      0.8818        0.2394  10.5856
     82      0.9251        0.2396       0.7649      0.8801        0.2378  10.5615
     83      0.9146        0.2423       0.7608      0.8778        0.2382  10.5353
     84      0.9230        0.2391       0.7648      0.8817        0.2372  10.5413
     85      0.9251        0.2370       0.7672      0.8829        0.2369  10.5470
     86      0.9232        0.2379       0.7661      0.8826        0.2387  10.5559
     87      0.9219        0.2385       0.7616      0.8804        0.2407  10.5716
     88      0.9223        0.2368       0.7675      0.8831        0.2364  10.5679
     89      0.9255        0.2357       0.7656      0.8824        0.2371  10.5540
     90      0.9244        0.2356       0.7681      0.8835        0.2371  10.5755
     91      0.9280        0.2340       0.7656      0.8822        0.2367  10.5880
     92      0.9259        0.2344       0.7684      0.8836        0.2354  10.5480
     93      0.9287        0.2331       0.7688      0.8836        0.2356  10.5585
     94      0.9262        0.2324       0.7562      0.8736        0.2407  10.5606
     95      0.8943        0.2578       0.7417      0.8700        0.2411  10.5325
     96      0.9246        0.2335       0.7637      0.8792        0.2337  10.5266
     97      0.9253        0.2331       0.7635      0.8778        0.2339  10.5554
     98      0.9250        0.2323       0.7582      0.8755        0.2357  10.5735
     99      0.9266        0.2331       0.7628      0.8777        0.2336  10.5750
    100      0.9288        0.2314       0.7602      0.8766        0.2343  10.5712
Accuracy after query 8: 0.7901041666666667
F1 Score after query 8: 0.879599580040897
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\RandomSampling\imagesFroMullticlass\model_checkpoint_iteration_7.pt

Query 9: Using the exact images and labels from the CSV for this iteration.
Number of samples used for training in Query 9 is 2264
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9115        0.2412       0.7677      0.8786        0.2312  13.3688
      2      0.9181        0.2390       0.7644      0.8757        0.2321  13.1466
      3      0.9140        0.2390       0.7543      0.8698        0.2383  13.1660
      4      0.9130        0.2369       0.7727      0.8815        0.2286  13.1989
      5      0.9142        0.2366       0.7580      0.8724        0.2340  13.1825
      6      0.9184        0.2345       0.7752      0.8831        0.2266  13.2105
      7      0.9180        0.2336       0.7792      0.8851        0.2256  13.2352
      8      0.9114        0.2373       0.7415      0.8620        0.2486  13.2285
      9      0.9144        0.2338       0.7818      0.8859        0.2237  13.2576
     10      0.9214        0.2298       0.7799      0.8866        0.2238  13.2277
     11      0.9188        0.2319       0.7733      0.8805        0.2266  13.2198
     12      0.9194        0.2291       0.7807      0.8873        0.2232  13.2297
     13      0.9202        0.2285       0.7819      0.8860        0.2224  13.1933
     14      0.9171        0.2308       0.7748      0.8813        0.2251  13.1930
     15      0.9206        0.2275       0.7816      0.8878        0.2220  13.1906
     16      0.9201        0.2256       0.7826      0.8861        0.2209  13.2004
     17      0.9187        0.2270       0.7844      0.8889        0.2204  13.1929
     18      0.9240        0.2231       0.7859      0.8896        0.2197  13.2141
     19      0.9259        0.2213       0.7856      0.8896        0.2194  13.1947
     20      0.9246        0.2211       0.7884      0.8888        0.2183  13.2114
     21      0.9259        0.2214       0.7905      0.8914        0.2176  13.2167
     22      0.9242        0.2203       0.7839      0.8853        0.2198  13.1582
     23      0.9236        0.2220       0.7358      0.8548        0.2526  13.1617
     24      0.9118        0.2279       0.7877      0.8896        0.2166  13.2024
     25      0.9246        0.2192       0.7896      0.8911        0.2156  13.1885
     26      0.9280        0.2194       0.7854      0.8891        0.2170  13.2148
     27      0.9290        0.2163       0.7873      0.8903        0.2164  13.1990
     28      0.9256        0.2170       0.7908      0.8899        0.2144  13.2179
     29      0.9253        0.2168       0.7922      0.8925        0.2141  13.2097
     30      0.9283        0.2138       0.7929      0.8925        0.2135  13.2195
     31      0.9276        0.2157       0.7906      0.8898        0.2142  13.2224
     32      0.9289        0.2148       0.7917      0.8913        0.2138  13.2078
     33      0.9284        0.2130       0.7898      0.8913        0.2137  13.2075
     34      0.9306        0.2107       0.7944      0.8934        0.2123  13.2217
     35      0.9293        0.2121       0.7944      0.8937        0.2115  13.2023
     36      0.9281        0.2118       0.7889      0.8869        0.2140  13.1805
     37      0.9266        0.2115       0.7943      0.8925        0.2107  13.2375
     38      0.9225        0.2142       0.7880      0.8883        0.2135  13.1994
     39      0.9260        0.2102       0.7849      0.8888        0.2176  13.1905
     40      0.9328        0.2072       0.7964      0.8942        0.2118  13.1805
     41      0.9338        0.2055       0.7995      0.8954        0.2103  13.1860
     42      0.9346        0.2074       0.7960      0.8940        0.2100  13.1892
     43      0.9321        0.2064       0.7948      0.8904        0.2097  13.1863
     44      0.9317        0.2044       0.7995      0.8954        0.2094  13.2103
     45      0.9272        0.2070       0.7903      0.8888        0.2106  13.2209
     46      0.9360        0.2027       0.8009      0.8959        0.2079  13.2018
     47      0.9319        0.2039       0.7981      0.8950        0.2080  13.2091
     48      0.9348        0.2023       0.8017      0.8965        0.2065  13.2108
     49      0.9350        0.2012       0.7972      0.8945        0.2089  13.1842
     50      0.9313        0.2042       0.7915      0.8922        0.2086  13.1678
     51      0.9300        0.2041       0.7854      0.8882        0.2146  13.1860
     52      0.9358        0.1996       0.7917      0.8913        0.2124  13.2218
     53      0.9369        0.1997       0.8003      0.8954        0.2067  13.2150
     54      0.9372        0.1976       0.7997      0.8948        0.2084  13.2088
     55      0.9343        0.2000       0.8016      0.8959        0.2056  13.1962
     56      0.9370        0.1956       0.7861      0.8883        0.2178  13.2092
     57      0.9322        0.2004       0.7972      0.8937        0.2094  13.2165
     58      0.9351        0.1984       0.8014      0.8958        0.2046  13.2008
     59      0.9393        0.1943       0.7979      0.8941        0.2089  13.1683
     60      0.9387        0.1942       0.8061      0.8978        0.2035  13.1868
     61      0.9415        0.1920       0.7998      0.8944        0.2076  13.2128
     62      0.9347        0.1976       0.7891      0.8870        0.2118  13.2250
     63      0.9340        0.1960       0.7708      0.8826        0.2264  13.2247
     64      0.9344        0.1969       0.7986      0.8940        0.2076  13.2057
     65      0.9385        0.1911       0.8061      0.8972        0.2041  13.1898
     66      0.9381        0.1931       0.8035      0.8962        0.2020  13.1992
     67      0.9392        0.1893       0.7800      0.8856        0.2197  13.2148
     68      0.9383        0.1911       0.8071      0.8974        0.2006  13.1748
     69      0.9428        0.1880       0.8063      0.8973        0.2025  13.1754
     70      0.9436        0.1874       0.8056      0.8971        0.2035  13.2061
     71      0.9256        0.2017       0.6915      0.8108        0.3225  13.1817
     72      0.9353        0.1942       0.8021      0.8956        0.2007  13.2193
     73      0.9424        0.1864       0.7948      0.8919        0.2071  13.1851
     74      0.9446        0.1855       0.8003      0.8945        0.2040  13.1910
     75      0.9461        0.1848       0.8056      0.8965        0.2020  13.1823
     76      0.9442        0.1840       0.8078      0.8972        0.1976  13.1934
     77      0.9431        0.1838       0.8078      0.8973        0.1983  13.1702
     78      0.9444        0.1833       0.8082      0.8969        0.1967  13.1689
     79      0.9457        0.1823       0.7880      0.8891        0.2114  13.2075
     80      0.9456        0.1824       0.8090      0.8975        0.1973  13.2181
     81      0.9476        0.1808       0.7993      0.8935        0.2057  13.1897
     82      0.9429        0.1821       0.8094      0.8951        0.1964  13.2087
     83      0.9415        0.1831       0.7804      0.8856        0.2175  13.2059
     84      0.9506        0.1793       0.8087      0.8972        0.1970  13.2135
     85      0.9488        0.1780       0.8127      0.8984        0.1948  13.2146
     86      0.9462        0.1796       0.8066      0.8963        0.1965  13.1835
     87      0.9487        0.1780       0.8122      0.8981        0.1945  13.1885
     88      0.9480        0.1765       0.8017      0.8946        0.2016  13.2083
     89      0.9500        0.1764       0.8120      0.8978        0.1946  13.2097
     90      0.9514        0.1753       0.8003      0.8943        0.2024  13.2171
     91      0.9529        0.1742       0.8061      0.8960        0.1982  13.2263
     92      0.9504        0.1748       0.8116      0.8955        0.1988  13.2138
     93      0.9307        0.1913       0.8017      0.8942        0.1968  13.2028
     94      0.9511        0.1736       0.8049      0.8959        0.1965  13.2089
     95      0.9541        0.1721       0.8075      0.8964        0.1956  13.1697
     96      0.9531        0.1721       0.8118      0.8978        0.1935  13.1752
     97      0.9292        0.1908       0.7149      0.8382        0.2836  13.2364
     98      0.9443        0.1781       0.8054      0.8959        0.1938  13.2175
     99      0.9527        0.1708       0.8069      0.8958        0.1931  13.1918
    100      0.9538        0.1699       0.8092      0.8969        0.1936  13.1901
Accuracy after query 9: 0.8121527777777777
F1 Score after query 9: 0.8952619016269266
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\RandomSampling\imagesFroMullticlass\model_checkpoint_iteration_8.pt

Query 10: Using the exact images and labels from the CSV for this iteration.
Number of samples used for training in Query 10 is 4040
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9377        0.1846       0.7757      0.8806        0.2146  17.8755
      2      0.9448        0.1789       0.8127      0.8957        0.1907  17.7106
      3      0.9458        0.1762       0.8045      0.8889        0.2024  17.8178
      4      0.9484        0.1750       0.8158      0.8981        0.1870  17.9169
      5      0.9474        0.1756       0.8071      0.8904        0.1986  17.9123
      6      0.9483        0.1738       0.8106      0.8942        0.1945  17.9436
      7      0.9490        0.1723       0.8155      0.8988        0.1871  17.9186
      8      0.9520        0.1697       0.8165      0.8975        0.1886  17.9215
      9      0.9540        0.1689       0.8179      0.8991        0.1868  17.8864
     10      0.9527        0.1686       0.8132      0.8966        0.1908  17.9096
     11      0.9538        0.1674       0.8203      0.9016        0.1852  17.9217
     12      0.9470        0.1728       0.7901      0.8875        0.2025  17.9047
     13      0.9537        0.1661       0.8181      0.8981        0.1865  17.8879
     14      0.9569        0.1641       0.8148      0.8967        0.1902  17.9058
     15      0.9543        0.1647       0.8167      0.8980        0.1857  17.8659
     16      0.9575        0.1621       0.8149      0.8959        0.1908  17.8761
     17      0.9548        0.1630       0.8191      0.8988        0.1879  17.8899
     18      0.9572        0.1604       0.8184      0.8977        0.1886  17.8823
     19      0.9577        0.1601       0.8194      0.8984        0.1872  17.8770
     20      0.9551        0.1608       0.8057      0.8889        0.2007  17.8785
     21      0.9539        0.1620       0.8189      0.8981        0.1877  17.8787
     22      0.9567        0.1579       0.8224      0.9001        0.1838  17.8597
     23      0.9594        0.1559       0.8200      0.8988        0.1859  17.8550
     24      0.9581        0.1556       0.8181      0.8979        0.1872  17.8646
     25      0.9598        0.1546       0.8135      0.8944        0.1898  17.8701
     26      0.9580        0.1557       0.8194      0.8976        0.1866  17.8890
     27      0.9586        0.1543       0.8224      0.8996        0.1841  17.8922
     28      0.9584        0.1543       0.8120      0.8902        0.1942  17.8878
     29      0.9605        0.1522       0.8238      0.9006        0.1834  17.8743
     30      0.9610        0.1509       0.8196      0.8975        0.1861  17.8909
     31      0.9602        0.1506       0.8132      0.8927        0.1940  17.8933
     32      0.9622        0.1504       0.8231      0.9002        0.1827  17.9147
     33      0.9621        0.1487       0.8210      0.8987        0.1847  17.9162
     34      0.9631        0.1484       0.8137      0.8914        0.1937  17.9112
     35      0.9543        0.1551       0.8148      0.8902        0.1931  17.8850
     36      0.9619        0.1476       0.8207      0.8966        0.1852  17.8450
     37      0.9631        0.1460       0.8205      0.8965        0.1850  17.8869
     38      0.9634        0.1454       0.8205      0.8982        0.1849  17.8810
     39      0.9635        0.1445       0.8266      0.9017        0.1809  17.8707
     40      0.9637        0.1434       0.8191      0.8957        0.1854  17.8995
     41      0.9614        0.1438       0.8167      0.8931        0.1890  17.8861
     42      0.9638        0.1430       0.8189      0.8952        0.1850  17.8466
     43      0.9640        0.1422       0.8259      0.8996        0.1801  17.8327
     44      0.9653        0.1411       0.8151      0.8924        0.1887  17.9036
     45      0.9612        0.1414       0.8248      0.8996        0.1809  17.8783
     46      0.9634        0.1404       0.8194      0.8952        0.1836  17.8872
     47      0.9642        0.1393       0.8212      0.8951        0.1843  17.8773
     48      0.9661        0.1383       0.8179      0.8947        0.1847  17.9145
     49      0.9508        0.1506       0.8245      0.8944        0.1822  17.8403
     50      0.9653        0.1378       0.8203      0.8924        0.1854  17.8746
     51      0.9640        0.1374       0.8231      0.8954        0.1819  17.9008
     52      0.9652        0.1359       0.8135      0.8870        0.1879  17.9052
     53      0.9653        0.1362       0.8184      0.8924        0.1855  17.8762
     54      0.9649        0.1351       0.8200      0.8926        0.1849  17.8664
     55      0.9655        0.1336       0.8194      0.8923        0.1834  17.8774
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 10: 0.8197916666666667
F1 Score after query 10: 0.8899471707916029
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\RandomSampling\imagesFroMullticlass\model_checkpoint_iteration_9.pt

Query 11: Using the exact images and labels from the CSV for this iteration.
Number of samples used for training in Query 11 is 7200
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9597        0.1374       0.6967      0.8603        0.2653  26.2236
      2      0.9531        0.1444       0.7861      0.8908        0.2065  26.0354
      3      0.9629        0.1353       0.7168      0.8686        0.2505  26.1495
      4      0.9572        0.1401       0.7455      0.8770        0.2342  26.2176
      5      0.9610        0.1356       0.7479      0.8761        0.2369  26.2905
      6      0.9613        0.1335       0.7957      0.8934        0.2001  26.2789
      7      0.9632        0.1300       0.8175      0.9006        0.1857  26.2514
      8      0.9643        0.1290       0.7925      0.8909        0.2062  26.2141
      9      0.9642        0.1281       0.8122      0.8994        0.1902  26.2697
     10      0.9645        0.1266       0.8198      0.9019        0.1832  26.2794
     11      0.9648        0.1259       0.8056      0.8971        0.1954  26.2354
     12      0.9646        0.1247       0.7979      0.8944        0.2024  26.1982
     13      0.9662        0.1239       0.8227      0.9030        0.1835  26.2016
     14      0.9667        0.1229       0.8135      0.9001        0.1906  26.2664
     15      0.9662        0.1222       0.8212      0.9023        0.1839  26.2050
     16      0.9660        0.1211       0.8187      0.9018        0.1860  26.2183
     17      0.9664        0.1201       0.8253      0.9029        0.1797  26.1974
     18      0.9671        0.1189       0.8189      0.9008        0.1822  26.2359
     19      0.9670        0.1183       0.8217      0.9027        0.1862  26.2775
     20      0.9684        0.1178       0.7849      0.8896        0.2141  26.2590
     21      0.9676        0.1169       0.8241      0.9029        0.1805  26.2391
     22      0.9684        0.1157       0.8299      0.9041        0.1760  26.1858
     23      0.9696        0.1145       0.8304      0.9050        0.1781  26.2332
     24      0.9691        0.1139       0.8250      0.9029        0.1796  26.2665
     25      0.9692        0.1135       0.8286      0.9035        0.1760  26.2541
     26      0.9695        0.1122       0.8280      0.9044        0.1795  26.2400
     27      0.9699        0.1116       0.8238      0.9029        0.1824  26.2260
     28      0.9694        0.1115       0.8148      0.8998        0.1865  26.2568
     29      0.9703        0.1104       0.8252      0.9032        0.1806  26.2362
     30      0.9699        0.1095       0.8264      0.9035        0.1799  26.2750
     31      0.9706        0.1090       0.8257      0.9030        0.1790  26.1888
     32      0.9713        0.1080       0.8259      0.9029        0.1787  26.2609
     33      0.9716        0.1074       0.8273      0.9040        0.1804  26.2513
     34      0.9708        0.1068       0.8323      0.9049        0.1763  26.2283
     35      0.9708        0.1065       0.8314      0.9047        0.1749  26.2422
     36      0.9717        0.1058       0.8299      0.9036        0.1747  26.2557
     37      0.9713        0.1054       0.7960      0.8927        0.2018  26.2957
     38      0.9716        0.1046       0.8264      0.9027        0.1782  26.2556
     39      0.9713        0.1038       0.8316      0.9051        0.1800  26.2856
     40      0.9716        0.1032       0.8257      0.9017        0.1777  26.2090
     41      0.9728        0.1026       0.8226      0.9018        0.1833  26.2236
     42      0.9729        0.1019       0.8337      0.9047        0.1766  26.2120
     43      0.9736        0.1008       0.8292      0.9039        0.1804  26.2523
     44      0.9736        0.1003       0.8325      0.9046        0.1769  26.2235
     45      0.9726        0.1004       0.7986      0.8927        0.2040  26.2152
     46      0.9739        0.0993       0.8250      0.9022        0.1827  26.2655
     47      0.9739        0.0987       0.8339      0.9049        0.1736  26.2634
     48      0.9739        0.0982       0.8259      0.9020        0.1800  26.2565
     49      0.9742        0.0977       0.8370      0.9047        0.1703  26.2230
     50      0.9746        0.0969       0.8344      0.9054        0.1761  26.2524
     51      0.9755        0.0963       0.8328      0.9029        0.1742  26.2679
     52      0.9756        0.0958       0.8332      0.9049        0.1747  26.2452
     53      0.9758        0.0950       0.8339      0.9051        0.1767  26.2414
     54      0.9748        0.0950       0.8333      0.9047        0.1742  26.1999
     55      0.9753        0.0944       0.8293      0.9009        0.1765  26.2437
     56      0.9762        0.0938       0.8363      0.9044        0.1716  26.2849
     57      0.9771        0.0929       0.8335      0.9039        0.1756  26.2405
     58      0.9759        0.0930       0.8354      0.9055        0.1757  26.2050
     59      0.9770        0.0918       0.8339      0.9050        0.1791  26.2363
     60      0.9770        0.0913       0.8313      0.9035        0.1799  26.2652
     61      0.9661        0.1068       0.8293      0.8995        0.1720  26.2469
     62      0.9768        0.0909       0.8354      0.9016        0.1689  26.2406
     63      0.9776        0.0902       0.8354      0.9055        0.1775  26.2001
     64      0.9779        0.0898       0.8340      0.9022        0.1724  26.2476
     65      0.9778        0.0892       0.8333      0.9045        0.1791  26.2366
     66      0.9786        0.0886       0.8387      0.9069        0.1756  26.2871
     67      0.9781        0.0880       0.8328      0.9035        0.1773  26.2218
     68      0.9795        0.0876       0.8342      0.9017        0.1727  26.2313
     69      0.9778        0.0878       0.8328      0.9023        0.1759  26.2477
     70      0.9801        0.0866       0.8363      0.9026        0.1724  26.2429
     71      0.9783        0.0866       0.8321      0.9042        0.1818  26.2535
     72      0.9789        0.0856       0.8365      0.9056        0.1774  26.1941
     73      0.9803        0.0851       0.8356      0.9041        0.1759  26.2333
     74      0.9793        0.0852       0.8332      0.9011        0.1748  26.2410
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 11: 0.8258680555555555
F1 Score after query 11: 0.9033850263823987
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\RandomSampling\imagesFroMullticlass\model_checkpoint_iteration_10.pt

Query 12: Using the exact images and labels from the CSV for this iteration.
Number of samples used for training in Query 12 is 12824
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9688        0.0980       0.8361      0.9025        0.1663  40.7536
      2      0.9734        0.0901       0.8340      0.9026        0.1709  40.8767
      3      0.9756        0.0875       0.8375      0.9008        0.1639  41.0059
      4      0.9762        0.0869       0.8342      0.8991        0.1679  41.1733
      5      0.9769        0.0861       0.8375      0.8971        0.1644  41.1394
      6      0.9768        0.0846       0.8366      0.8979        0.1637  41.0532
      7      0.9768        0.0842       0.8380      0.8798        0.1696  41.1010
      8      0.9774        0.0833       0.8226      0.8587        0.1829  41.1003
      9      0.9775        0.0827       0.7946      0.8157        0.2040  41.0621
     10      0.9782        0.0818       0.7955      0.8234        0.2044  41.1029
     11      0.9780        0.0812       0.7896      0.8106        0.2129  41.1497
     12      0.9786        0.0803       0.8366      0.8824        0.1730  41.0772
     13      0.9792        0.0796       0.8153      0.8493        0.1896  41.0622
     14      0.9791        0.0791       0.8389      0.8844        0.1721  41.0897
     15      0.9795        0.0781       0.8359      0.8822        0.1743  41.0584
     16      0.9800        0.0774       0.8460      0.9042        0.1627  41.1073
     17      0.9808        0.0767       0.8516      0.9031        0.1637  41.0958
     18      0.9803        0.0762       0.8479      0.9051        0.1636  41.0728
     19      0.9813        0.0753       0.8483      0.9003        0.1669  41.1222
     20      0.9815        0.0747       0.8417      0.9049        0.1671  41.0775
     21      0.9810        0.0755       0.8516      0.9046        0.1612  41.0475
     22      0.9819        0.0737       0.8552      0.9080        0.1612  41.0936
     23      0.9817        0.0728       0.8516      0.9068        0.1620  41.1084
     24      0.9822        0.0727       0.8519      0.9059        0.1623  41.1028
     25      0.9821        0.0721       0.8488      0.9071        0.1625  41.1076
     26      0.9827        0.0712       0.8519      0.9065        0.1628  41.0912
     27      0.9824        0.0710       0.8503      0.9077        0.1628  41.1251
     28      0.9832        0.0703       0.8535      0.9077        0.1622  41.1217
     29      0.9824        0.0700       0.8502      0.9076        0.1636  41.0749
     30      0.9829        0.0693       0.8418      0.9057        0.1673  41.1032
     31      0.9831        0.0688       0.8498      0.9083        0.1627  41.0814
     32      0.9829        0.0682       0.8521      0.9089        0.1627  41.0308
     33      0.9838        0.0677       0.8542      0.9106        0.1611  41.0939
     34      0.9836        0.0674       0.8556      0.9063        0.1617  41.1368
     35      0.9837        0.0671       0.8507      0.9085        0.1652  41.1000
     36      0.9849        0.0661       0.8403      0.9050        0.1703  41.1286
     37      0.9849        0.0657       0.8483      0.9063        0.1663  41.0875
     38      0.9850        0.0650       0.8418      0.9057        0.1706  41.0580
     39      0.9849        0.0645       0.8465      0.9072        0.1669  41.0717
     40      0.9850        0.0643       0.8425      0.9059        0.1707  41.0634
     41      0.9853        0.0640       0.8443      0.9062        0.1706  41.0333
     42      0.9848        0.0639       0.8422      0.9056        0.1717  41.0577
     43      0.9856        0.0629       0.8439      0.9061        0.1714  41.1051
     44      0.9866        0.0621       0.8427      0.9057        0.1715  41.0568
     45      0.9864        0.0618       0.8438      0.9061        0.1718  41.1046
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 12: 0.8375
F1 Score after query 12: 0.9120704906053098
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\RandomSampling\imagesFroMullticlass\model_checkpoint_iteration_11.pt

Query 13: Using the exact images and labels from the CSV for this iteration.
Number of samples used for training in Query 13 is 22824
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9763        0.0753       0.8375      0.9047        0.1659  66.9773
      2      0.9797        0.0692       0.8352      0.9031        0.1716  67.3930
      3      0.9808        0.0674       0.8372      0.9044        0.1720  67.5926
      4      0.9778        0.0719       0.8389      0.9049        0.1670  67.5592
      5      0.9819        0.0655       0.8311      0.9007        0.1775  67.4676
      6      0.9817        0.0651       0.8389      0.9041        0.1733  67.5268
      7      0.9823        0.0640       0.8377      0.9038        0.1726  67.4875
      8      0.9827        0.0632       0.8380      0.9049        0.1710  67.5327
      9      0.9830        0.0626       0.8311      0.9015        0.1789  67.5930
     10      0.9833        0.0620       0.8366      0.9040        0.1734  67.5410
     11      0.9831        0.0612       0.8391      0.9047        0.1734  67.5460
     12      0.9835        0.0606       0.8318      0.9022        0.1777  67.4890
     13      0.9844        0.0599       0.8413      0.9064        0.1721  67.6038
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 13: 0.8392361111111111
F1 Score after query 13: 0.9142295045092058
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\RandomSampling\imagesFroMullticlass\model_checkpoint_iteration_12.pt

Query 14: Using the exact images and labels from the CSV for this iteration.
Number of samples used for training in Query 14 is 26880
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss      dur
-------  ----------  ------------  -----------  ----------  ------------  -------
      1      0.9816        0.0625       0.8207      0.8929        0.2102  77.7125
      2      0.9816        0.0618       0.8109      0.8890        0.2317  78.3227
      3      0.9826        0.0597       0.8210      0.8938        0.2167  78.3148
      4      0.9828        0.0589       0.8241      0.8949        0.2037  78.2527
      5      0.9836        0.0580       0.8340      0.8988        0.1966  78.2242
      6      0.9835        0.0574       0.8299      0.8974        0.2027  78.2090
      7      0.9834        0.0567       0.8309      0.8984        0.2036  78.1995
      8      0.9836        0.0561       0.8274      0.8974        0.2093  78.3126
      9      0.9842        0.0557       0.8394      0.9023        0.1888  78.2538
     10      0.9845        0.0548       0.8370      0.9015        0.1922  78.2161
     11      0.9843        0.0542       0.8420      0.9039        0.1868  78.1674
     12      0.9843        0.0536       0.8326      0.8995        0.1980  78.2381
     13      0.9852        0.0530       0.8318      0.8992        0.2027  78.2305
     14      0.9848        0.0528       0.8389      0.9019        0.1940  78.2502
     15      0.9854        0.0519       0.8349      0.9010        0.1954  78.1631
     16      0.9856        0.0513       0.8377      0.9027        0.1943  78.1787
     17      0.9864        0.0504       0.8297      0.8985        0.2007  78.2450
     18      0.9859        0.0505       0.8342      0.9004        0.1966  78.2004
     19      0.9865        0.0497       0.8427      0.9049        0.1912  78.1984
     20      0.9860        0.0494       0.8335      0.8996        0.1982  78.2262
     21      0.9868        0.0489       0.8422      0.9047        0.1938  78.2618
     22      0.9870        0.0482       0.8300      0.8982        0.2086  78.2860
     23      0.9869        0.0476       0.8366      0.9015        0.1966  78.2110
Stopping since valid_loss has not improved in the last 13 epochs.
Accuracy after query 14: 0.8381944444444445
F1 Score after query 14: 0.9116946189568372
Model checkpoint saved to D:\Shubham\results\multilabel01\DinoSmall\RandomSampling\imagesFroMullticlass\model_checkpoint_iteration_13.pt
Best F1 score across iterations: 0.9142295045092058
PS C:\Users\localuserSG\ActiveLearning\Multilabel01\DinoSmall\imagesFromMulticlass\RandomSampling>
