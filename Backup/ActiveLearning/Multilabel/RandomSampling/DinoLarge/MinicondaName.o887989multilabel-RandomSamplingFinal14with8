### Starting TaskPrologue of job 887989 on tg092 at Fri 06 Sep 2024 07:16:51 PM CEST
Running on cores 64-95 with governor ondemand
Fri Sep  6 19:16:51 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   34C    P0             54W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

/home/hpc/iwfa/iwfa044h/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).
  from pandas.core.computation.check import NUMEXPR_INSTALLED
X_train_initial shape: torch.Size([26880, 3, 224, 224])
y_train_initial shape: torch.Size([26880, 3])
Adjusted shapes after loading:
X_train_np: (10, 224, 224, 3) y_train_np: (10, 3)
X_pool: (26870, 224, 224, 3) y_pool: (26870, 3)
X_test_np: (5760, 224, 224, 3) y_test_np: (5760, 3)
X_val_np: (5760, 224, 224, 3) y_val_np: (5760, 3)

Iteration: 1
Number of samples in training set: 8
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.3000[0m        [32m0.6667[0m       [35m0.2502[0m      [31m0.7521[0m        [94m0.6755[0m     +  61.9595
      2      [36m0.7222[0m        [32m0.6390[0m       [35m0.3347[0m      0.7433        [94m0.6664[0m     +  58.9232
      3      [36m0.7778[0m        [32m0.6140[0m       [35m0.4071[0m      0.6865        [94m0.6581[0m     +  58.9631
      4      0.7778        [32m0.5895[0m       [35m0.4415[0m      0.5602        [94m0.6507[0m     +  59.0615
      5      [36m0.8889[0m        [32m0.5657[0m       [35m0.4455[0m      0.1264        [94m0.6441[0m     +  59.1199
      6      0.8889        [32m0.5443[0m       [35m0.4467[0m      0.0700        [94m0.6384[0m     +  59.0465
      7      0.5556        [32m0.5245[0m       0.4458      0.0331        [94m0.6335[0m     +  59.1909
      8      0.5556        [32m0.5035[0m       0.4448      0.0176        [94m0.6294[0m     +  59.0562
      9      0.5556        [32m0.4880[0m       [35m0.4474[0m      0.0080        [94m0.6262[0m     +  59.0871
     10      0.5556        [32m0.4688[0m       [35m0.4479[0m      0.0020        [94m0.6237[0m     +  59.0667
     11      0.5556        [32m0.4533[0m       0.4479      0.0004        [94m0.6219[0m     +  59.0868
     12      0.5556        [32m0.4411[0m       0.4479      0.0002        [94m0.6206[0m     +  59.0365
     13      0.3333        [32m0.4263[0m       0.4479      0.0000        [94m0.6197[0m     +  59.1009
     14      0.3333        [32m0.4140[0m       0.4479      0.0000        [94m0.6190[0m     +  59.0719
     15      0.3333        [32m0.4019[0m       0.4479      0.0000        [94m0.6185[0m     +  59.0787
     16      0.3333        [32m0.3901[0m       0.4479      0.0000        [94m0.6180[0m     +  59.0407
     17      0.3333        [32m0.3806[0m       0.4479      0.0000        [94m0.6176[0m     +  59.0714
     18      0.3333        [32m0.3718[0m       0.4479      0.0000        [94m0.6172[0m     +  59.1245
     19      0.3333        [32m0.3618[0m       0.4479      0.0000        [94m0.6169[0m     +  59.0927
     20      0.3333        [32m0.3533[0m       0.4479      0.0000        [94m0.6166[0m     +  59.0014
     21      0.3333        [32m0.3466[0m       0.4479      0.0000        [94m0.6166[0m     +  59.0343
     22      0.3333        [32m0.3391[0m       0.4479      0.0000        0.6167        59.0436
     23      0.3333        [32m0.3327[0m       0.4479      0.0000        0.6170        58.9920
     24      0.5556        [32m0.3260[0m       0.4479      0.0000        0.6174        58.9735
     25      0.5556        [32m0.3190[0m       0.4479      0.0000        0.6179        58.9837
     26      0.5556        [32m0.3133[0m       0.4479      0.0000        0.6183        58.9635
     27      0.8889        [32m0.3080[0m       0.4479      0.0000        0.6187        59.0007
     28      0.8889        [32m0.3024[0m       0.4479      0.0000        0.6188        59.0014
     29      0.8889        [32m0.2966[0m       0.4479      0.0000        0.6188        58.9813
     30      0.8889        [32m0.2903[0m       0.4479      0.0000        0.6184        58.9749
     31      0.8889        [32m0.2862[0m       0.4479      0.0000        0.6180        59.0116
     32      0.8889        [32m0.2806[0m       0.4479      0.0000        0.6172        59.0041
     33      0.8889        [32m0.2744[0m       0.4479      0.0000        [94m0.6163[0m     +  59.0032
     34      0.8889        [32m0.2703[0m       0.4479      0.0000        [94m0.6152[0m     +  59.0288
     35      0.8889        [32m0.2659[0m       0.4479      0.0000        [94m0.6143[0m     +  59.0552
     36      0.8889        [32m0.2604[0m       0.4479      0.0002        [94m0.6133[0m     +  58.9908
     37      0.8889        [32m0.2569[0m       0.4479      0.0007        [94m0.6125[0m     +  59.0460
     38      0.8889        [32m0.2510[0m       0.4479      0.0024        [94m0.6119[0m     +  58.9990
     39      0.8889        [32m0.2471[0m       0.4479      0.0041        [94m0.6112[0m     +  59.0313
     40      0.8889        [32m0.2437[0m       0.4479      0.0052        [94m0.6106[0m     +  59.0251
     41      0.8889        [32m0.2391[0m       0.4479      0.0069        [94m0.6099[0m     +  59.0201
     42      0.8889        [32m0.2354[0m       [35m0.4481[0m      0.0099        [94m0.6089[0m     +  58.9849
     43      0.8889        [32m0.2314[0m       0.4481      0.0118        [94m0.6078[0m     +  58.9441
     44      0.8889        [32m0.2269[0m       0.4481      0.0146        [94m0.6066[0m     +  58.9742
     45      0.8889        [32m0.2232[0m       [35m0.4483[0m      0.0178        [94m0.6053[0m     +  58.9374
     46      0.8889        [32m0.2200[0m       0.4472      0.0257        [94m0.6039[0m     +  58.9362
     47      0.8889        [32m0.2165[0m       0.4483      0.0328        [94m0.6025[0m     +  58.9101
     48      0.8889        [32m0.2130[0m       [35m0.4491[0m      0.0407        [94m0.6011[0m     +  58.8572
     49      0.8889        [32m0.2102[0m       [35m0.4502[0m      0.0507        [94m0.5999[0m     +  58.8686
     50      0.8889        [32m0.2048[0m       [35m0.4505[0m      0.0600        [94m0.5987[0m     +  58.8241
     51      0.8889        [32m0.2028[0m       0.4505      0.0700        [94m0.5975[0m     +  58.8424
     52      0.8889        [32m0.1992[0m       [35m0.4509[0m      0.0829        [94m0.5962[0m     +  58.8281
     53      0.8889        [32m0.1958[0m       [35m0.4517[0m      0.0915        [94m0.5948[0m     +  58.8539
     54      0.8889        [32m0.1929[0m       0.4517      0.0982        [94m0.5934[0m     +  58.7863
     55      0.8889        [32m0.1905[0m       [35m0.4526[0m      0.1060        [94m0.5922[0m     +  58.7972
     56      0.8889        [32m0.1867[0m       [35m0.4530[0m      0.1137        [94m0.5908[0m     +  58.8236
     57      0.8889        [32m0.1840[0m       [35m0.4533[0m      0.1188        [94m0.5899[0m     +  58.8174
     58      0.8889        [32m0.1816[0m       [35m0.4540[0m      0.1236        [94m0.5889[0m     +  58.8150
     59      0.8889        [32m0.1786[0m       [35m0.4542[0m      0.1295        [94m0.5880[0m     +  58.7517
     60      [36m1.0000[0m        [32m0.1756[0m       [35m0.4549[0m      0.1356        [94m0.5873[0m     +  58.7571
     61      1.0000        [32m0.1736[0m       0.4549      0.1410        [94m0.5864[0m     +  58.7756
     62      1.0000        [32m0.1708[0m       [35m0.4554[0m      0.1498        [94m0.5858[0m     +  58.7615
     63      1.0000        [32m0.1694[0m       0.4554      0.1535        [94m0.5854[0m     +  58.7659
     64      1.0000        [32m0.1666[0m       0.4554      0.1590        [94m0.5853[0m     +  58.7316
     65      1.0000        [32m0.1643[0m       [35m0.4561[0m      0.1640        0.5854        58.7172
     66      1.0000        [32m0.1622[0m       0.4561      0.1693        [94m0.5851[0m     +  58.7234
     67      1.0000        [32m0.1598[0m       0.4559      0.1737        [94m0.5846[0m     +  58.7513
     68      1.0000        [32m0.1573[0m       0.4561      0.1782        [94m0.5840[0m     +  58.7249
     69      1.0000        [32m0.1558[0m       [35m0.4569[0m      0.1838        [94m0.5831[0m     +  58.7335
     70      1.0000        [32m0.1542[0m       0.4568      0.1870        [94m0.5825[0m     +  58.7033
     71      1.0000        [32m0.1515[0m       0.4568      0.1900        [94m0.5819[0m     +  58.7045
     72      1.0000        [32m0.1502[0m       0.4568      0.1929        [94m0.5813[0m     +  58.7120
     73      1.0000        [32m0.1477[0m       [35m0.4571[0m      0.1951        [94m0.5808[0m     +  58.7114
     74      1.0000        [32m0.1465[0m       [35m0.4576[0m      0.1990        [94m0.5802[0m     +  58.7257
     75      1.0000        [32m0.1451[0m       [35m0.4578[0m      0.2022        [94m0.5801[0m     +  58.7023
     76      1.0000        [32m0.1430[0m       [35m0.4582[0m      0.2043        [94m0.5799[0m     +  58.7474
     77      1.0000        [32m0.1417[0m       [35m0.4585[0m      0.2063        [94m0.5797[0m     +  58.7672
     78      1.0000        [32m0.1401[0m       [35m0.4589[0m      0.2098        [94m0.5793[0m     +  58.7736
     79      1.0000        [32m0.1383[0m       [35m0.4602[0m      0.2120        [94m0.5790[0m     +  58.7196
     80      1.0000        [32m0.1377[0m       [35m0.4616[0m      0.2148        0.5790        58.7103
     81      1.0000        [32m0.1363[0m       [35m0.4625[0m      0.2179        0.5794        58.6891
     82      1.0000        [32m0.1343[0m       [35m0.4635[0m      0.2198        0.5797        58.7168
     83      1.0000        [32m0.1327[0m       [35m0.4639[0m      0.2222        0.5800        58.7434
     84      1.0000        [32m0.1326[0m       [35m0.4649[0m      0.2248        0.5803        58.7600
     85      1.0000        [32m0.1312[0m       [35m0.4653[0m      0.2260        0.5808        58.7573
     86      1.0000        [32m0.1303[0m       [35m0.4660[0m      0.2280        0.5808        58.6879
     87      1.0000        [32m0.1285[0m       [35m0.4667[0m      0.2305        0.5806        58.7078
     88      1.0000        [32m0.1274[0m       [35m0.4674[0m      0.2332        0.5801        58.6857
     89      1.0000        [32m0.1265[0m       [35m0.4679[0m      0.2355        0.5797        58.7100
     90      1.0000        [32m0.1262[0m       [35m0.4688[0m      0.2382        0.5794        58.7665
     91      1.0000        [32m0.1245[0m       0.4688      0.2396        0.5795        58.6811
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.2871330217139178
Test Accuracy: 0.5036458333333333
Number of samples added in this iteration: 16

Iteration: 2
Number of samples in training set: 24
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.4010[0m        [32m0.4429[0m       [35m0.5057[0m      [31m0.3520[0m        [94m0.5339[0m     +  59.4495
      2      [36m0.4444[0m        [32m0.4139[0m       [35m0.5092[0m      [31m0.4232[0m        [94m0.5118[0m     +  59.0270
      3      [36m0.4872[0m        [32m0.3968[0m       0.4905      [31m0.4663[0m        [94m0.5013[0m     +  59.0377
      4      [36m0.5833[0m        [32m0.3848[0m       0.4748      [31m0.4873[0m        [94m0.4949[0m     +  59.0710
      5      [36m0.6296[0m        [32m0.3719[0m       0.4731      [31m0.5001[0m        [94m0.4899[0m     +  59.0920
      6      0.6296        [32m0.3595[0m       0.4776      [31m0.5008[0m        [94m0.4861[0m     +  59.0801
      7      0.6296        [32m0.3471[0m       0.4842      0.4962        [94m0.4836[0m     +  59.0715
      8      0.6296        [32m0.3352[0m       0.4950      0.4877        [94m0.4831[0m     +  59.1036
      9      0.6296        [32m0.3268[0m       0.5059      0.4809        0.4842        59.0794
     10      0.6296        [32m0.3168[0m       [35m0.5135[0m      0.4703        0.4864        59.0587
     11      0.6296        [32m0.3094[0m       0.5132      0.4613        0.4885        59.0623
     12      0.6296        [32m0.3028[0m       [35m0.5149[0m      0.4570        0.4892        59.0595
     13      0.6296        [32m0.2960[0m       [35m0.5158[0m      0.4541        0.4877        59.0419
     14      [36m0.6471[0m        [32m0.2888[0m       [35m0.5165[0m      0.4560        0.4840        59.0061
     15      [36m0.6667[0m        [32m0.2831[0m       0.5160      0.4597        [94m0.4787[0m     +  59.0143
     16      0.6667        [32m0.2772[0m       0.5148      0.4647        [94m0.4724[0m     +  59.0492
     17      0.6667        [32m0.2701[0m       0.5108      0.4703        [94m0.4660[0m     +  59.0569
     18      0.6667        [32m0.2651[0m       0.5068      0.4747        [94m0.4601[0m     +  59.0626
     19      0.6667        [32m0.2600[0m       0.5028      0.4774        [94m0.4553[0m     +  59.0579
     20      0.6667        [32m0.2546[0m       0.5014      0.4792        [94m0.4519[0m     +  59.0457
     21      0.6667        [32m0.2507[0m       0.5017      0.4806        [94m0.4495[0m     +  59.0614
     22      0.6667        [32m0.2455[0m       0.5036      0.4807        [94m0.4481[0m     +  59.0586
     23      0.6667        [32m0.2422[0m       0.5047      0.4807        [94m0.4469[0m     +  59.0873
     24      0.6667        [32m0.2390[0m       0.5050      0.4803        [94m0.4455[0m     +  59.0756
     25      0.6667        [32m0.2339[0m       0.5054      0.4817        [94m0.4436[0m     +  59.0729
     26      0.6667        [32m0.2308[0m       0.5054      0.4837        [94m0.4406[0m     +  59.0680
     27      0.6667        [32m0.2276[0m       0.5036      0.4867        [94m0.4370[0m     +  59.0493
     28      [36m0.7778[0m        [32m0.2245[0m       0.5023      0.4895        [94m0.4333[0m     +  59.0690
     29      [36m0.9167[0m        [32m0.2213[0m       0.5026      0.4933        [94m0.4301[0m     +  59.0485
     30      [36m0.9630[0m        [32m0.2184[0m       0.5031      0.4992        [94m0.4277[0m     +  59.0768
     31      [36m1.0000[0m        [32m0.2150[0m       0.5071      [31m0.5085[0m        [94m0.4261[0m     +  59.0564
     32      1.0000        [32m0.2128[0m       0.5104      [31m0.5139[0m        [94m0.4254[0m     +  59.0858
     33      1.0000        [32m0.2092[0m       0.5130      [31m0.5181[0m        0.4255        59.0543
     34      1.0000        [32m0.2068[0m       0.5156      [31m0.5206[0m        0.4260        59.0009
     35      1.0000        [32m0.2043[0m       [35m0.5212[0m      [31m0.5300[0m        0.4265        59.0300
     36      1.0000        [32m0.2027[0m       [35m0.5238[0m      [31m0.5348[0m        0.4267        59.0122
     37      1.0000        [32m0.1988[0m       [35m0.5276[0m      [31m0.5407[0m        0.4267        59.0107
     38      1.0000        [32m0.1967[0m       [35m0.5304[0m      [31m0.5463[0m        0.4265        59.0250
     39      1.0000        [32m0.1945[0m       [35m0.5345[0m      [31m0.5541[0m        0.4264        59.0129
     40      1.0000        [32m0.1914[0m       [35m0.5363[0m      [31m0.5557[0m        0.4268        59.0104
     41      1.0000        [32m0.1893[0m       [35m0.5401[0m      [31m0.5566[0m        0.4278        59.0208
     42      1.0000        [32m0.1871[0m       [35m0.5418[0m      0.5555        0.4291        59.0209
     43      1.0000        [32m0.1853[0m       [35m0.5431[0m      0.5542        0.4308        59.0124
     44      1.0000        [32m0.1827[0m       [35m0.5446[0m      0.5538        0.4323        59.0141
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.5920159735157964
Test Accuracy: 0.5383680555555556
Number of samples added in this iteration: 32

Iteration: 3
Number of samples in training set: 56
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.8227[0m        [32m0.2696[0m       [35m0.5470[0m      [31m0.6392[0m        [94m0.3948[0m     +  59.9028
      2      [36m0.8636[0m        [32m0.2551[0m       [35m0.5590[0m      [31m0.6725[0m        [94m0.3822[0m     +  59.6983
      3      [36m0.9078[0m        [32m0.2469[0m       [35m0.5740[0m      [31m0.6826[0m        [94m0.3748[0m     +  59.6837
      4      [36m0.9400[0m        [32m0.2367[0m       [35m0.5905[0m      0.6813        [94m0.3725[0m     +  59.6748
      5      0.9221        [32m0.2288[0m       [35m0.5977[0m      0.6780        0.3728        59.7205
      6      0.9396        [32m0.2228[0m       [35m0.6036[0m      0.6803        [94m0.3703[0m     +  59.6752
      7      0.9221        [32m0.2178[0m       [35m0.6120[0m      [31m0.6923[0m        [94m0.3635[0m     +  59.7758
      8      [36m0.9469[0m        [32m0.2120[0m       [35m0.6220[0m      [31m0.7120[0m        [94m0.3546[0m     +  59.7659
      9      [36m0.9633[0m        [32m0.2065[0m       [35m0.6292[0m      [31m0.7269[0m        [94m0.3459[0m     +  59.7636
     10      [36m0.9718[0m        [32m0.2023[0m       [35m0.6359[0m      [31m0.7393[0m        [94m0.3394[0m     +  59.6986
     11      0.9718        [32m0.1981[0m       [35m0.6438[0m      [31m0.7468[0m        [94m0.3352[0m     +  59.6910
     12      [36m0.9862[0m        [32m0.1943[0m       [35m0.6476[0m      [31m0.7483[0m        [94m0.3326[0m     +  59.6930
     13      0.9862        [32m0.1905[0m       [35m0.6514[0m      [31m0.7484[0m        [94m0.3315[0m     +  59.7327
     14      0.9858        [32m0.1871[0m       [35m0.6523[0m      0.7462        [94m0.3307[0m     +  59.7232
     15      0.9858        [32m0.1843[0m       [35m0.6536[0m      0.7459        [94m0.3294[0m     +  59.7584
     16      [36m0.9919[0m        [32m0.1808[0m       [35m0.6557[0m      [31m0.7492[0m        [94m0.3268[0m     +  59.7221
     17      0.9919        [32m0.1781[0m       [35m0.6597[0m      [31m0.7553[0m        [94m0.3232[0m     +  59.7306
     18      0.9919        [32m0.1755[0m       [35m0.6615[0m      [31m0.7605[0m        [94m0.3193[0m     +  59.7066
     19      [36m1.0000[0m        [32m0.1730[0m       [35m0.6623[0m      [31m0.7642[0m        [94m0.3160[0m     +  59.6999
     20      1.0000        [32m0.1708[0m       [35m0.6634[0m      [31m0.7665[0m        [94m0.3139[0m     +  59.7526
     21      1.0000        [32m0.1684[0m       0.6634      0.7655        [94m0.3131[0m     +  59.7147
     22      1.0000        [32m0.1668[0m       [35m0.6639[0m      0.7631        0.3132        59.7427
     23      1.0000        [32m0.1645[0m       [35m0.6670[0m      0.7622        0.3136        59.6833
     24      1.0000        [32m0.1622[0m       [35m0.6689[0m      0.7627        0.3139        59.6589
     25      1.0000        [32m0.1606[0m       [35m0.6705[0m      0.7639        0.3133        59.6529
     26      0.9919        [32m0.1589[0m       [35m0.6715[0m      0.7652        [94m0.3115[0m     +  59.7453
     27      1.0000        [32m0.1566[0m       [35m0.6731[0m      [31m0.7688[0m        [94m0.3091[0m     +  59.6907
     28      1.0000        [32m0.1548[0m       [35m0.6738[0m      [31m0.7719[0m        [94m0.3067[0m     +  59.6994
     29      1.0000        [32m0.1532[0m       [35m0.6760[0m      [31m0.7763[0m        [94m0.3046[0m     +  59.7947
     30      1.0000        [32m0.1514[0m       [35m0.6767[0m      [31m0.7782[0m        [94m0.3035[0m     +  59.6758
     31      1.0000        [32m0.1499[0m       [35m0.6774[0m      [31m0.7794[0m        [94m0.3032[0m     +  59.7000
     32      1.0000        [32m0.1483[0m       [35m0.6783[0m      [31m0.7796[0m        0.3037        59.6610
     33      1.0000        [32m0.1466[0m       0.6783      [31m0.7797[0m        0.3041        59.6477
     34      1.0000        [32m0.1453[0m       0.6778      [31m0.7797[0m        0.3041        59.6123
     35      1.0000        [32m0.1441[0m       [35m0.6790[0m      [31m0.7819[0m        [94m0.3032[0m     +  59.6520
     36      1.0000        [32m0.1426[0m       [35m0.6802[0m      [31m0.7847[0m        [94m0.3017[0m     +  59.6211
     37      1.0000        [32m0.1411[0m       0.6802      [31m0.7861[0m        [94m0.3002[0m     +  59.6942
     38      1.0000        [32m0.1398[0m       [35m0.6832[0m      [31m0.7900[0m        [94m0.2989[0m     +  59.6946
     39      1.0000        [32m0.1383[0m       [35m0.6844[0m      [31m0.7915[0m        [94m0.2983[0m     +  59.6536
     40      1.0000        [32m0.1374[0m       0.6839      0.7907        [94m0.2980[0m     +  59.7004
     41      1.0000        [32m0.1359[0m       0.6830      0.7896        0.2981        59.6843
     42      1.0000        [32m0.1347[0m       0.6813      0.7881        0.2981        59.6430
     43      1.0000        [32m0.1341[0m       0.6811      0.7884        [94m0.2976[0m     +  59.6373
     44      1.0000        [32m0.1328[0m       0.6833      0.7907        [94m0.2966[0m     +  59.7133
     45      1.0000        [32m0.1315[0m       [35m0.6852[0m      [31m0.7925[0m        [94m0.2953[0m     +  59.6654
     46      1.0000        [32m0.1305[0m       0.6852      [31m0.7938[0m        [94m0.2940[0m     +  59.6925
     47      1.0000        [32m0.1298[0m       [35m0.6861[0m      [31m0.7947[0m        [94m0.2933[0m     +  59.6758
     48      1.0000        [32m0.1288[0m       0.6856      0.7941        [94m0.2931[0m     +  59.7330
     49      1.0000        [32m0.1279[0m       0.6851      0.7933        [94m0.2929[0m     +  59.6850
     50      1.0000        [32m0.1268[0m       0.6852      0.7934        [94m0.2927[0m     +  59.6956
     51      1.0000        [32m0.1256[0m       0.6847      0.7933        [94m0.2922[0m     +  59.7225
     52      1.0000        [32m0.1248[0m       0.6851      0.7941        [94m0.2915[0m     +  59.7037
     53      1.0000        [32m0.1241[0m       0.6856      [31m0.7956[0m        [94m0.2906[0m     +  59.7111
     54      1.0000        [32m0.1232[0m       0.6859      [31m0.7968[0m        [94m0.2898[0m     +  59.7017
     55      1.0000        [32m0.1224[0m       0.6861      [31m0.7971[0m        [94m0.2894[0m     +  59.7061
     56      1.0000        [32m0.1215[0m       0.6861      0.7969        [94m0.2893[0m     +  59.6966
     57      1.0000        [32m0.1208[0m       0.6859      0.7966        [94m0.2890[0m     +  59.7026
     58      1.0000        [32m0.1200[0m       0.6852      0.7960        [94m0.2888[0m     +  59.6870
     59      1.0000        [32m0.1193[0m       0.6859      0.7968        [94m0.2881[0m     +  59.7119
     60      1.0000        [32m0.1183[0m       [35m0.6868[0m      [31m0.7979[0m        [94m0.2874[0m     +  59.7089
     61      1.0000        [32m0.1177[0m       [35m0.6875[0m      [31m0.7989[0m        [94m0.2868[0m     +  59.7060
     62      1.0000        [32m0.1169[0m       0.6875      [31m0.7993[0m        [94m0.2864[0m     +  59.6927
     63      1.0000        [32m0.1165[0m       0.6875      [31m0.7996[0m        [94m0.2858[0m     +  59.7034
     64      1.0000        [32m0.1156[0m       0.6865      0.7991        [94m0.2855[0m     +  59.6994
     65      1.0000        [32m0.1150[0m       0.6873      [31m0.7998[0m        [94m0.2853[0m     +  59.6974
     66      1.0000        [32m0.1147[0m       0.6868      0.7997        [94m0.2850[0m     +  59.7182
     67      1.0000        [32m0.1136[0m       0.6870      [31m0.8002[0m        [94m0.2844[0m     +  59.6994
     68      1.0000        [32m0.1130[0m       0.6868      [31m0.8007[0m        [94m0.2837[0m     +  59.6752
     69      1.0000        [32m0.1127[0m       [35m0.6880[0m      [31m0.8018[0m        [94m0.2830[0m     +  59.6917
     70      1.0000        [32m0.1118[0m       0.6878      0.8018        [94m0.2826[0m     +  59.6648
     71      1.0000        [32m0.1114[0m       [35m0.6882[0m      0.8016        [94m0.2824[0m     +  59.6817
     72      1.0000        [32m0.1107[0m       0.6880      0.8017        [94m0.2821[0m     +  59.6788
     73      1.0000        [32m0.1103[0m       0.6882      [31m0.8021[0m        [94m0.2817[0m     +  59.6971
     74      1.0000        [32m0.1096[0m       [35m0.6887[0m      [31m0.8028[0m        [94m0.2812[0m     +  59.7198
     75      1.0000        [32m0.1091[0m       [35m0.6898[0m      [31m0.8045[0m        [94m0.2807[0m     +  59.6662
     76      1.0000        [32m0.1084[0m       0.6898      [31m0.8047[0m        [94m0.2804[0m     +  59.6971
     77      1.0000        [32m0.1079[0m       0.6896      0.8047        [94m0.2801[0m     +  59.6735
     78      1.0000        [32m0.1075[0m       0.6898      [31m0.8048[0m        [94m0.2799[0m     +  59.6690
     79      1.0000        [32m0.1069[0m       0.6898      0.8043        [94m0.2798[0m     +  59.7173
     80      1.0000        [32m0.1062[0m       [35m0.6899[0m      0.8044        [94m0.2794[0m     +  59.6998
     81      1.0000        [32m0.1060[0m       [35m0.6905[0m      [31m0.8051[0m        [94m0.2787[0m     +  59.6927
     82      1.0000        [32m0.1053[0m       0.6899      [31m0.8051[0m        [94m0.2780[0m     +  59.6990
     83      1.0000        [32m0.1049[0m       0.6901      [31m0.8054[0m        [94m0.2775[0m     +  59.6967
     84      1.0000        [32m0.1044[0m       0.6898      0.8051        [94m0.2773[0m     +  59.6911
     85      1.0000        [32m0.1039[0m       0.6896      0.8046        0.2774        59.6777
     86      1.0000        [32m0.1034[0m       0.6889      0.8042        0.2774        59.6224
     87      1.0000        [32m0.1030[0m       0.6884      0.8042        [94m0.2772[0m     +  59.6554
     88      1.0000        [32m0.1029[0m       0.6884      0.8050        [94m0.2767[0m     +  59.6802
     89      1.0000        [32m0.1023[0m       0.6872      0.8049        [94m0.2763[0m     +  59.7125
     90      1.0000        [32m0.1017[0m       0.6866      0.8045        [94m0.2761[0m     +  59.6769
     91      1.0000        [32m0.1013[0m       0.6866      0.8044        [94m0.2759[0m     +  59.7074
     92      1.0000        [32m0.1009[0m       0.6873      0.8047        0.2759        59.7346
     93      1.0000        [32m0.1004[0m       0.6873      0.8046        [94m0.2758[0m     +  59.7134
     94      1.0000        [32m0.1000[0m       0.6877      0.8049        [94m0.2752[0m     +  59.6834
     95      1.0000        [32m0.0996[0m       0.6880      [31m0.8058[0m        [94m0.2743[0m     +  59.7837
     96      1.0000        [32m0.0993[0m       0.6887      [31m0.8071[0m        [94m0.2737[0m     +  59.6744
     97      1.0000        [32m0.0989[0m       0.6889      [31m0.8074[0m        [94m0.2734[0m     +  59.7036
     98      1.0000        [32m0.0985[0m       0.6885      0.8065        0.2737        59.7564
     99      1.0000        [32m0.0980[0m       0.6872      0.8048        0.2743        59.7392
    100      1.0000        [32m0.0979[0m       0.6873      0.8053        0.2741        59.7027
Test F1 Score: 0.8132118085569262
Test Accuracy: 0.7149305555555555
Number of samples added in this iteration: 56

Iteration: 4
Number of samples in training set: 112
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9424[0m        [32m0.1515[0m       [35m0.7149[0m      [31m0.8475[0m        [94m0.2564[0m     +  61.1650
      2      [36m0.9521[0m        0.1546       [35m0.7174[0m      0.8416        [94m0.2542[0m     +  61.0899
      3      [36m0.9636[0m        [32m0.1445[0m       0.7118      0.8239        0.2638        60.8970
      4      0.9512        [32m0.1407[0m       0.6997      0.8084        0.2738        60.8971
      5      0.9509        0.1412       0.7038      0.8150        0.2708        60.8728
      6      [36m0.9659[0m        [32m0.1383[0m       0.7168      0.8311        0.2607        60.8656
      7      [36m0.9802[0m        [32m0.1341[0m       [35m0.7222[0m      0.8387        [94m0.2523[0m     +  60.8768
      8      0.9721        [32m0.1321[0m       [35m0.7271[0m      0.8456        [94m0.2482[0m     +  60.8611
      9      0.9721        [32m0.1317[0m       [35m0.7292[0m      0.8465        [94m0.2468[0m     +  60.8426
     10      0.9721        [32m0.1306[0m       [35m0.7302[0m      0.8433        0.2472        60.8652
     11      0.9768        [32m0.1279[0m       [35m0.7311[0m      0.8396        0.2497        60.9322
     12      0.9768        [32m0.1260[0m       0.7299      0.8334        0.2533        60.9095
     13      0.9802        [32m0.1247[0m       0.7295      0.8309        0.2561        60.9232
     14      0.9802        [32m0.1240[0m       0.7297      0.8310        0.2560        60.9118
     15      0.9802        [32m0.1227[0m       [35m0.7325[0m      0.8339        0.2531        60.8370
     16      0.9802        [32m0.1214[0m       [35m0.7356[0m      0.8400        0.2490        60.8299
     17      0.9802        [32m0.1192[0m       0.7352      0.8430        [94m0.2456[0m     +  60.8333
     18      [36m0.9839[0m        [32m0.1182[0m       [35m0.7377[0m      0.8471        [94m0.2436[0m     +  60.8600
     19      [36m0.9875[0m        [32m0.1173[0m       0.7370      0.8470        [94m0.2430[0m     +  60.8450
     20      0.9875        [32m0.1165[0m       [35m0.7378[0m      0.8470        0.2435        60.8777
     21      0.9875        [32m0.1150[0m       0.7372      0.8441        0.2451        60.8234
     22      0.9875        [32m0.1139[0m       0.7349      0.8400        0.2475        60.8207
     23      0.9875        [32m0.1129[0m       0.7323      0.8369        0.2498        60.8243
     24      0.9875        [32m0.1122[0m       0.7316      0.8359        0.2508        60.8143
     25      0.9875        [32m0.1116[0m       0.7323      0.8365        0.2501        60.8246
     26      0.9875        [32m0.1107[0m       0.7332      0.8381        0.2481        60.8140
     27      0.9875        [32m0.1098[0m       0.7339      0.8404        0.2460        60.8151
     28      0.9875        [32m0.1090[0m       0.7339      0.8424        0.2444        60.8217
     29      0.9875        [32m0.1083[0m       0.7352      0.8447        0.2438        60.8238
     30      0.9875        [32m0.1078[0m       0.7339      0.8431        0.2441        60.8142
     31      0.9875        [32m0.1069[0m       0.7339      0.8420        0.2451        60.8103
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8290843572781457
Test Accuracy: 0.7293402777777778
Number of samples added in this iteration: 100

Iteration: 5
Number of samples in training set: 212
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9050[0m        [32m0.1826[0m       [35m0.7590[0m      [31m0.8476[0m        [94m0.2364[0m     +  63.0119
      2      [36m0.9158[0m        [32m0.1699[0m       0.7493      [31m0.8591[0m        [94m0.2328[0m     +  62.9505
      3      0.9098        [32m0.1692[0m       0.7488      0.8576        [94m0.2293[0m     +  62.9396
      4      [36m0.9196[0m        [32m0.1632[0m       0.7523      0.8549        0.2308        62.9421
      5      [36m0.9228[0m        [32m0.1588[0m       0.7554      0.8538        0.2306        62.9019
      6      [36m0.9330[0m        [32m0.1544[0m       [35m0.7597[0m      0.8551        [94m0.2279[0m     +  62.9119
      7      [36m0.9433[0m        [32m0.1515[0m       0.7592      0.8551        [94m0.2270[0m     +  62.9810
      8      [36m0.9496[0m        [32m0.1506[0m       [35m0.7599[0m      0.8563        [94m0.2264[0m     +  62.9259
      9      [36m0.9531[0m        [32m0.1483[0m       0.7595      0.8559        [94m0.2263[0m     +  62.9588
     10      0.9531        [32m0.1458[0m       0.7585      0.8549        0.2273        62.9371
     11      0.9531        [32m0.1437[0m       [35m0.7606[0m      0.8566        0.2275        62.9012
     12      0.9496        [32m0.1418[0m       [35m0.7625[0m      0.8586        [94m0.2262[0m     +  62.8941
     13      [36m0.9582[0m        [32m0.1393[0m       [35m0.7641[0m      [31m0.8599[0m        [94m0.2250[0m     +  62.9588
     14      [36m0.9618[0m        [32m0.1378[0m       [35m0.7660[0m      [31m0.8607[0m        [94m0.2249[0m     +  62.9411
     15      0.9618        [32m0.1361[0m       0.7660      0.8600        0.2254        62.9322
     16      [36m0.9654[0m        [32m0.1342[0m       0.7658      0.8605        0.2263        62.9066
     17      0.9654        [32m0.1320[0m       0.7637      0.8603        0.2270        62.8977
     18      0.9654        [32m0.1303[0m       0.7609      0.8600        0.2269        62.9028
     19      0.9654        [32m0.1288[0m       0.7627      [31m0.8613[0m        0.2266        62.9140
     20      [36m0.9672[0m        [32m0.1272[0m       0.7641      [31m0.8618[0m        0.2265        62.9972
     21      [36m0.9714[0m        [32m0.1255[0m       0.7656      0.8618        0.2270        62.9019
     22      0.9714        [32m0.1239[0m       0.7649      0.8612        0.2278        62.9291
     23      0.9714        [32m0.1222[0m       0.7635      0.8614        0.2283        62.9062
     24      0.9714        [32m0.1207[0m       0.7615      [31m0.8621[0m        0.2286        62.9060
     25      0.9714        [32m0.1191[0m       0.7608      0.8618        0.2287        62.9022
     26      [36m0.9731[0m        [32m0.1177[0m       0.7618      0.8619        0.2289        62.9009
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8577328972184676
Test Accuracy: 0.7680555555555556
Number of samples added in this iteration: 176

Iteration: 6
Number of samples in training set: 388
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9156[0m        [32m0.1773[0m       [35m0.7618[0m      [31m0.8767[0m        [94m0.2332[0m     +  66.6414
      2      [36m0.9267[0m        [32m0.1737[0m       [35m0.7771[0m      0.8689        [94m0.2211[0m     +  66.6267
      3      [36m0.9324[0m        [32m0.1637[0m       0.7639      0.8493        0.2304        66.6079
      4      [36m0.9333[0m        [32m0.1619[0m       [35m0.7825[0m      0.8726        [94m0.2180[0m     +  66.5369
      5      [36m0.9397[0m        [32m0.1579[0m       [35m0.7849[0m      0.8738        [94m0.2166[0m     +  66.6177
      6      0.9368        [32m0.1548[0m       0.7767      0.8614        0.2192        66.6025
      7      0.9353        [32m0.1534[0m       0.7842      0.8686        [94m0.2154[0m     +  66.5268
      8      [36m0.9427[0m        [32m0.1495[0m       [35m0.7905[0m      0.8755        [94m0.2121[0m     +  66.5830
      9      [36m0.9448[0m        [32m0.1473[0m       [35m0.7929[0m      0.8737        [94m0.2116[0m     +  66.5530
     10      [36m0.9469[0m        [32m0.1449[0m       0.7918      0.8702        0.2119        66.5415
     11      0.9425        [32m0.1426[0m       [35m0.7944[0m      0.8752        [94m0.2103[0m     +  66.6031
     12      [36m0.9526[0m        [32m0.1403[0m       [35m0.7967[0m      0.8762        [94m0.2094[0m     +  66.5167
     13      [36m0.9573[0m        [32m0.1383[0m       0.7960      0.8741        0.2096        66.5371
     14      [36m0.9596[0m        [32m0.1362[0m       [35m0.7976[0m      0.8761        [94m0.2089[0m     +  66.4952
     15      [36m0.9629[0m        [32m0.1341[0m       [35m0.7990[0m      [31m0.8777[0m        [94m0.2080[0m     +  66.5377
     16      [36m0.9641[0m        [32m0.1322[0m       [35m0.7995[0m      0.8769        [94m0.2076[0m     +  66.5375
     17      0.9629        [32m0.1303[0m       [35m0.8007[0m      [31m0.8777[0m        [94m0.2067[0m     +  66.5481
     18      0.9637        [32m0.1285[0m       [35m0.8028[0m      [31m0.8791[0m        [94m0.2058[0m     +  66.5149
     19      [36m0.9658[0m        [32m0.1266[0m       [35m0.8033[0m      0.8784        [94m0.2053[0m     +  66.5411
     20      0.9655        [32m0.1249[0m       [35m0.8045[0m      [31m0.8796[0m        [94m0.2045[0m     +  66.5542
     21      [36m0.9667[0m        [32m0.1233[0m       [35m0.8061[0m      [31m0.8806[0m        [94m0.2039[0m     +  66.5231
     22      [36m0.9702[0m        [32m0.1215[0m       [35m0.8073[0m      [31m0.8806[0m        [94m0.2033[0m     +  66.5852
     23      [36m0.9720[0m        [32m0.1200[0m       [35m0.8080[0m      [31m0.8810[0m        [94m0.2026[0m     +  66.5296
     24      [36m0.9752[0m        [32m0.1184[0m       [35m0.8095[0m      [31m0.8823[0m        [94m0.2021[0m     +  66.5628
     25      0.9752        [32m0.1169[0m       [35m0.8111[0m      [31m0.8827[0m        [94m0.2017[0m     +  66.5608
     26      0.9752        [32m0.1154[0m       0.8111      [31m0.8827[0m        [94m0.2011[0m     +  66.5259
     27      [36m0.9761[0m        [32m0.1140[0m       [35m0.8134[0m      [31m0.8837[0m        [94m0.2006[0m     +  66.5476
     28      0.9761        [32m0.1127[0m       [35m0.8141[0m      [31m0.8838[0m        [94m0.2002[0m     +  66.5575
     29      [36m0.9773[0m        [32m0.1112[0m       [35m0.8156[0m      [31m0.8848[0m        [94m0.1996[0m     +  66.5599
     30      0.9773        [32m0.1098[0m       [35m0.8165[0m      [31m0.8850[0m        [94m0.1992[0m     +  66.5474
     31      [36m0.9793[0m        [32m0.1085[0m       0.8165      0.8848        [94m0.1987[0m     +  66.5582
     32      [36m0.9811[0m        [32m0.1073[0m       [35m0.8170[0m      0.8849        [94m0.1983[0m     +  66.5696
     33      0.9811        [32m0.1063[0m       [35m0.8184[0m      [31m0.8857[0m        [94m0.1981[0m     +  66.5628
     34      0.9811        [32m0.1050[0m       [35m0.8191[0m      [31m0.8860[0m        [94m0.1979[0m     +  66.5829
     35      0.9811        [32m0.1039[0m       [35m0.8194[0m      [31m0.8863[0m        [94m0.1976[0m     +  66.5492
     36      0.9811        [32m0.1028[0m       [35m0.8196[0m      [31m0.8863[0m        [94m0.1975[0m     +  66.5763
     37      0.9811        [32m0.1019[0m       [35m0.8203[0m      [31m0.8870[0m        [94m0.1973[0m     +  66.5630
     38      0.9811        [32m0.1009[0m       0.8201      [31m0.8871[0m        [94m0.1971[0m     +  66.5658
     39      [36m0.9829[0m        [32m0.1001[0m       0.8200      [31m0.8874[0m        [94m0.1970[0m     +  66.5823
     40      [36m0.9837[0m        [32m0.0991[0m       0.8196      0.8870        [94m0.1970[0m     +  66.5890
     41      [36m0.9857[0m        [32m0.0983[0m       0.8198      [31m0.8876[0m        [94m0.1969[0m     +  66.5585
     42      0.9857        [32m0.0974[0m       0.8193      0.8872        [94m0.1968[0m     +  66.5307
     43      0.9857        [32m0.0965[0m       0.8196      [31m0.8881[0m        [94m0.1966[0m     +  66.5227
     44      0.9857        [32m0.0956[0m       0.8193      0.8875        0.1967        66.5631
     45      0.9857        [32m0.0948[0m       0.8198      [31m0.8883[0m        [94m0.1966[0m     +  66.4997
     46      0.9837        [32m0.0940[0m       0.8187      0.8869        0.1967        66.5517
     47      0.9857        [32m0.0932[0m       0.8194      [31m0.8888[0m        [94m0.1966[0m     +  66.4999
     48      0.9857        [32m0.0923[0m       0.8191      0.8870        0.1967        66.5623
     49      0.9857        [32m0.0916[0m       0.8187      [31m0.8889[0m        [94m0.1965[0m     +  66.4964
     50      [36m0.9866[0m        [32m0.0907[0m       0.8198      0.8872        0.1969        66.5600
     51      0.9866        [32m0.0900[0m       0.8184      [31m0.8893[0m        [94m0.1964[0m     +  66.5144
     52      0.9866        [32m0.0895[0m       0.8198      0.8868        0.1968        66.5531
     53      0.9866        [32m0.0888[0m       0.8179      [31m0.8900[0m        [94m0.1963[0m     +  66.5147
     54      0.9866        [32m0.0881[0m       0.8201      0.8855        0.1969        66.6251
     55      0.9866        [32m0.0877[0m       0.8175      [31m0.8906[0m        [94m0.1962[0m     +  66.6845
     56      0.9866        [32m0.0871[0m       0.8201      0.8837        0.1972        66.5761
     57      [36m0.9878[0m        [32m0.0869[0m       0.8139      0.8902        0.1965        66.5975
     58      0.9866        [32m0.0864[0m       0.8177      0.8802        0.1985        66.5697
     59      0.9858        0.0866       0.8099      0.8903        0.1977        66.5756
     60      0.9866        [32m0.0863[0m       0.8132      0.8726        0.2020        66.5057
     61      0.9858        0.0872       0.8059      0.8896        0.1992        66.5385
     62      0.9867        0.0868       0.8087      0.8667        0.2039        66.4946
     63      0.9858        0.0881       0.8052      0.8877        0.1984        66.4964
     64      0.9867        0.0868       0.8194      0.8809        0.1985        66.4987
     65      0.9858        [32m0.0857[0m       0.8170      0.8873        [94m0.1951[0m     +  66.5072
     66      [36m0.9885[0m        [32m0.0826[0m       0.8097      0.8882        0.1966        66.5533
     67      [36m0.9897[0m        [32m0.0815[0m       [35m0.8210[0m      0.8835        0.1964        66.5092
     68      0.9897        0.0818       0.8160      0.8881        [94m0.1947[0m     +  66.5194
     69      [36m0.9904[0m        [32m0.0806[0m       0.8155      0.8884        0.1956        66.5515
     70      [36m0.9917[0m        [32m0.0798[0m       0.8198      0.8867        0.1951        66.5182
     71      0.9917        [32m0.0795[0m       0.8170      0.8885        0.1949        66.5099
     72      0.9917        [32m0.0790[0m       0.8177      0.8887        0.1954        66.4970
     73      0.9917        [32m0.0783[0m       0.8191      0.8870        0.1950        66.5054
     74      0.9917        [32m0.0780[0m       0.8165      0.8876        0.1950        66.5159
     75      0.9917        [32m0.0774[0m       0.8187      0.8881        0.1952        66.4941
     76      0.9917        [32m0.0770[0m       0.8198      0.8881        0.1950        66.5106
     77      0.9917        [32m0.0765[0m       0.8167      0.8872        0.1953        66.7166
     78      0.9917        [32m0.0761[0m       0.8194      0.8878        0.1953        66.4935
     79      0.9917        [32m0.0757[0m       0.8184      0.8873        0.1954        66.5326
     80      0.9917        [32m0.0753[0m       0.8167      0.8866        0.1956        66.5178
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8928665723016079
Test Accuracy: 0.8170138888888889
Number of samples added in this iteration: 316

Iteration: 7
Number of samples in training set: 704
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9592[0m        [32m0.1119[0m       [35m0.8049[0m      [31m0.8572[0m        [94m0.2047[0m     +  73.2194
      2      [36m0.9614[0m        [32m0.1087[0m       [35m0.8104[0m      [31m0.8843[0m        [94m0.1954[0m     +  73.0891
      3      [36m0.9659[0m        [32m0.0995[0m       [35m0.8146[0m      0.8711        0.1996        73.0655
      4      [36m0.9719[0m        [32m0.0968[0m       [35m0.8255[0m      [31m0.8877[0m        [94m0.1905[0m     +  73.0342
      5      [36m0.9768[0m        [32m0.0921[0m       0.8205      0.8794        0.1966        73.2189
      6      [36m0.9805[0m        [32m0.0901[0m       [35m0.8260[0m      0.8864        0.1922        73.3609
      7      0.9803        [32m0.0875[0m       0.8236      0.8802        0.1949        73.2831
      8      [36m0.9852[0m        [32m0.0856[0m       [35m0.8262[0m      0.8861        0.1921        73.1635
      9      0.9851        [32m0.0834[0m       0.8203      0.8746        0.1958        73.1840
     10      [36m0.9873[0m        [32m0.0821[0m       [35m0.8269[0m      0.8865        0.1924        73.1636
     11      0.9873        [32m0.0802[0m       0.8186      0.8718        0.1967        73.1600
     12      [36m0.9877[0m        [32m0.0792[0m       [35m0.8271[0m      0.8872        0.1921        73.1631
     13      [36m0.9882[0m        [32m0.0774[0m       0.8207      0.8723        0.1966        73.1627
     14      [36m0.9888[0m        [32m0.0765[0m       0.8262      [31m0.8879[0m        0.1915        73.1665
     15      0.9886        [32m0.0753[0m       0.8210      0.8723        0.1957        73.1336
     16      [36m0.9925[0m        [32m0.0745[0m       0.8264      [31m0.8894[0m        0.1907        73.2761
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8833929987359107
Test Accuracy: 0.8142361111111112
Number of samples added in this iteration: 564

Iteration: 8
Number of samples in training set: 1268
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.9656[0m        [32m0.1058[0m       [35m0.8132[0m      [31m0.8927[0m        [94m0.1933[0m     +  84.7843
      2      0.9630        [32m0.1010[0m       [35m0.8297[0m      [31m0.8961[0m        [94m0.1818[0m     +  84.6694
      3      0.9620        [32m0.1007[0m       0.8181      0.8687        0.1904        84.7269
      4      [36m0.9769[0m        [32m0.0910[0m       [35m0.8309[0m      0.8897        [94m0.1803[0m     +  84.6736
      5      [36m0.9792[0m        [32m0.0886[0m       [35m0.8321[0m      0.8928        [94m0.1789[0m     +  84.7134
      6      [36m0.9799[0m        [32m0.0872[0m       0.8273      0.8857        0.1804        84.7762
      7      [36m0.9818[0m        [32m0.0844[0m       0.8293      0.8886        0.1806        84.7639
      8      0.9818        [32m0.0821[0m       0.8313      0.8922        0.1793        84.7504
      9      [36m0.9837[0m        [32m0.0803[0m       [35m0.8335[0m      0.8931        [94m0.1788[0m     +  84.7426
     10      [36m0.9851[0m        [32m0.0787[0m       [35m0.8340[0m      0.8929        [94m0.1787[0m     +  84.7887
     11      [36m0.9853[0m        [32m0.0773[0m       0.8333      0.8918        [94m0.1784[0m     +  84.7689
     12      [36m0.9864[0m        [32m0.0757[0m       [35m0.8347[0m      0.8914        [94m0.1779[0m     +  84.7853
     13      [36m0.9869[0m        [32m0.0741[0m       [35m0.8366[0m      0.8914        [94m0.1770[0m     +  84.8029
     14      0.9869        [32m0.0724[0m       0.8365      0.8905        [94m0.1766[0m     +  84.8254
     15      [36m0.9880[0m        [32m0.0708[0m       [35m0.8380[0m      0.8911        [94m0.1765[0m     +  84.8150
     16      [36m0.9889[0m        [32m0.0696[0m       [35m0.8394[0m      0.8917        [94m0.1760[0m     +  84.7943
     17      [36m0.9906[0m        [32m0.0685[0m       [35m0.8408[0m      0.8935        [94m0.1757[0m     +  84.7829
     18      [36m0.9911[0m        [32m0.0676[0m       [35m0.8410[0m      0.8946        [94m0.1756[0m     +  84.7872
     19      0.9911        [32m0.0665[0m       0.8385      0.8958        0.1770        84.7840
     20      [36m0.9922[0m        [32m0.0652[0m       0.8351      [31m0.8980[0m        0.1807        84.7583
     21      [36m0.9930[0m        [32m0.0646[0m       0.8316      [31m0.8983[0m        0.1838        84.7574
     22      [36m0.9930[0m        0.0648       0.8366      [31m0.9006[0m        0.1826        84.7613
     23      0.9925        0.0657       0.8408      [31m0.9024[0m        0.1818        84.7587
     24      0.9916        0.0663       [35m0.8425[0m      [31m0.9034[0m        0.1825        84.7509
     25      0.9871        0.0689       0.8408      0.9027        0.1826        84.7854
     26      0.9782        0.0744       0.8177      0.8912        0.1918        84.9176
     27      0.9715        0.0811       0.7672      0.8069        0.2311        84.7423
     28      0.9752        0.0787       0.8351      0.8986        0.1802        84.7404
     29      0.9899        [32m0.0636[0m       0.8342      0.8907        0.1787        84.7397
     30      0.9888        [32m0.0622[0m       0.8337      0.8932        0.1793        84.7458
     31      [36m0.9943[0m        [32m0.0592[0m       0.8396      0.8935        [94m0.1745[0m     +  84.7340
     32      0.9941        [32m0.0578[0m       0.8363      0.8943        0.1789        84.7682
     33      [36m0.9945[0m        [32m0.0565[0m       0.8351      0.8915        0.1787        84.7391
     34      [36m0.9961[0m        [32m0.0556[0m       0.8352      0.8935        0.1792        84.7415
     35      0.9961        [32m0.0548[0m       0.8339      0.8920        0.1791        84.7367
     36      0.9961        [32m0.0541[0m       0.8363      0.8941        0.1789        84.7466
     37      0.9961        [32m0.0534[0m       0.8333      0.8918        0.1793        84.7281
     38      0.9961        [32m0.0528[0m       0.8335      0.8922        0.1796        84.7299
     39      [36m0.9964[0m        [32m0.0521[0m       0.8332      0.8924        0.1798        84.7355
     40      0.9964        [32m0.0515[0m       0.8330      0.8923        0.1801        84.7323
     41      0.9964        [32m0.0509[0m       0.8337      0.8926        0.1799        84.6942
     42      0.9964        [32m0.0503[0m       0.8339      0.8928        0.1803        84.6661
     43      [36m0.9969[0m        [32m0.0498[0m       0.8339      0.8928        0.1804        84.6875
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8901697003264184
Test Accuracy: 0.8140625
Number of samples added in this iteration: 1000

Iteration: 9
Number of samples in training set: 2268
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9781[0m        [32m0.0751[0m       [35m0.8309[0m      [31m0.8987[0m        [94m0.1787[0m     +  105.3972
      2      [36m0.9802[0m        [32m0.0719[0m       0.8252      0.8733        0.1821        105.4234
      3      [36m0.9834[0m        [32m0.0675[0m       [35m0.8405[0m      0.8919        [94m0.1707[0m     +  105.3734
      4      [36m0.9887[0m        [32m0.0640[0m       0.8378      0.8925        0.1730        105.4145
      5      0.9886        [32m0.0609[0m       0.8328      0.8892        0.1757        105.3683
      6      0.9883        [32m0.0604[0m       0.8384      0.8978        0.1714        105.4774
      7      [36m0.9902[0m        [32m0.0588[0m       0.8392      [31m0.9024[0m        0.1718        105.2891
      8      0.9895        [32m0.0588[0m       0.8321      0.8993        0.1751        105.2819
      9      0.9901        [32m0.0583[0m       0.8168      0.8903        0.1866        105.2813
     10      [36m0.9907[0m        0.0592       0.8021      0.8744        0.2036        105.2678
     11      0.9861        0.0619       0.8399      0.8988        0.1734        105.2847
     12      0.9841        0.0650       [35m0.8455[0m      [31m0.9072[0m        0.1735        105.2883
     13      0.9760        0.0741       0.8429      0.8968        [94m0.1655[0m     +  105.5919
     14      0.9787        0.0696       0.8182      0.8764        0.1917        105.3090
     15      0.9893        [32m0.0583[0m       [35m0.8477[0m      0.8956        0.1722        105.2927
     16      [36m0.9912[0m        [32m0.0520[0m       0.8293      0.8709        0.1840        105.2701
     17      0.9902        0.0521       0.8366      0.8817        0.1758        105.5226
     18      [36m0.9928[0m        [32m0.0484[0m       0.8220      0.8570        0.1912        105.3547
     19      0.9911        0.0492       0.8155      0.8464        0.1948        105.3674
     20      0.9919        0.0488       0.8071      0.8346        0.2017        105.3637
     21      0.9915        0.0487       0.8036      0.8294        0.2049        105.3771
     22      0.9916        0.0491       0.7937      0.8116        0.2143        105.3752
     23      0.9904        0.0501       0.8002      0.8259        0.2042        105.3664
     24      0.9854        0.0528       0.8217      0.8577        0.1904        105.3731
     25      0.9842        0.0534       0.8125      0.8533        0.1971        105.3668
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.9054730693397035
Test Accuracy: 0.8314236111111111
Number of samples added in this iteration: 1780

Iteration: 10
Number of samples in training set: 4048
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9721[0m        [32m0.0733[0m       [35m0.8267[0m      [31m0.8992[0m        [94m0.1774[0m     +  142.0712
      2      [36m0.9773[0m        [32m0.0657[0m       0.8257      0.8811        [94m0.1715[0m     +  142.1627
      3      [36m0.9824[0m        [32m0.0581[0m       [35m0.8269[0m      0.8867        [94m0.1710[0m     +  142.1410
      4      [36m0.9855[0m        [32m0.0549[0m       [35m0.8302[0m      0.8909        [94m0.1687[0m     +  142.1569
      5      [36m0.9870[0m        [32m0.0527[0m       [35m0.8365[0m      0.8975        [94m0.1657[0m     +  142.1466
      6      [36m0.9877[0m        [32m0.0508[0m       [35m0.8384[0m      [31m0.9012[0m        [94m0.1644[0m     +  142.1815
      7      [36m0.9891[0m        [32m0.0493[0m       0.8366      [31m0.9020[0m        [94m0.1637[0m     +  142.3123
      8      [36m0.9896[0m        [32m0.0477[0m       0.8316      0.8985        0.1689        142.1235
      9      0.9891        [32m0.0468[0m       [35m0.8385[0m      0.9003        0.1638        142.0890
     10      0.9880        [32m0.0456[0m       0.8333      0.8863        0.1734        142.1027
     11      0.9888        0.0462       0.8215      0.8693        0.1803        142.0930
     12      0.9881        [32m0.0455[0m       0.8141      0.8636        0.1889        142.0913
     13      0.9881        0.0461       [35m0.8394[0m      0.8967        [94m0.1618[0m     +  142.0675
     14      [36m0.9911[0m        [32m0.0431[0m       [35m0.8431[0m      [31m0.9035[0m        [94m0.1600[0m     +  141.9800
     15      [36m0.9931[0m        [32m0.0392[0m       [35m0.8438[0m      0.8937        0.1629        142.1248
     16      [36m0.9941[0m        [32m0.0378[0m       0.8427      0.8992        0.1602        142.0894
     17      [36m0.9943[0m        [32m0.0369[0m       0.8378      0.8995        0.1627        142.0131
     18      [36m0.9949[0m        [32m0.0355[0m       0.8417      0.8919        0.1643        141.9832
     19      [36m0.9954[0m        [32m0.0344[0m       0.8411      0.8951        0.1647        141.9836
     20      0.9952        [32m0.0338[0m       [35m0.8469[0m      0.9018        [94m0.1588[0m     +  141.9957
     21      [36m0.9955[0m        [32m0.0326[0m       0.8457      0.9011        0.1596        142.0367
     22      [36m0.9962[0m        [32m0.0315[0m       0.8469      0.9006        0.1635        142.0612
     23      [36m0.9964[0m        [32m0.0309[0m       0.8458      0.8998        0.1654        142.1091
     24      [36m0.9965[0m        [32m0.0303[0m       0.8410      0.8965        0.1693        142.0822
     25      [36m0.9968[0m        [32m0.0298[0m       0.8420      0.8954        0.1707        142.0875
     26      [36m0.9973[0m        [32m0.0290[0m       0.8347      0.8843        0.1789        142.0353
     27      0.9973        [32m0.0284[0m       0.8403      0.8889        0.1769        142.0058
     28      [36m0.9974[0m        [32m0.0282[0m       0.8422      0.8901        0.1800        142.2049
     29      0.9969        0.0285       0.8365      0.8771        0.1881        142.1812
     30      0.9959        0.0286       0.8398      0.8869        0.1782        141.9893
     31      0.9956        0.0288       0.8425      0.8941        0.1761        141.9894
     32      0.9952        0.0294       0.8378      0.8920        0.1762        142.1996
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.9048409830332668
Test Accuracy: 0.8260416666666667
Number of samples added in this iteration: 3164

Iteration: 11
Number of samples in training set: 7212
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9827[0m        [32m0.0484[0m       [35m0.8408[0m      [31m0.8973[0m        [94m0.1628[0m     +  207.6357
      2      [36m0.9833[0m        [32m0.0461[0m       0.8337      [31m0.8982[0m        0.1721        207.6408
      3      [36m0.9864[0m        [32m0.0419[0m       0.8273      0.8968        0.1828        207.4163
      4      [36m0.9881[0m        [32m0.0387[0m       0.8224      0.8947        0.1871        207.3339
      5      [36m0.9902[0m        [32m0.0356[0m       0.8281      [31m0.8985[0m        0.1853        207.4037
      6      0.9902        [32m0.0345[0m       0.8299      0.8965        0.1811        207.4167
      7      [36m0.9906[0m        [32m0.0333[0m       0.8182      0.8901        0.1899        207.3147
      8      [36m0.9919[0m        [32m0.0315[0m       0.8174      0.8926        0.1913        207.2988
      9      [36m0.9923[0m        [32m0.0300[0m       0.8224      0.8883        0.1836        207.2870
     10      [36m0.9925[0m        [32m0.0291[0m       0.8255      0.8973        0.1866        207.3605
     11      [36m0.9926[0m        [32m0.0285[0m       0.8377      [31m0.9052[0m        0.1848        207.4043
     12      0.9919        0.0290       0.8267      0.8883        0.1773        207.3597
     13      [36m0.9927[0m        [32m0.0274[0m       0.8299      0.8738        0.1851        207.5297
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.8950883324875344
Test Accuracy: 0.8411458333333334
Number of samples added in this iteration: 5624

Iteration: 12
Number of samples in training set: 12836
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9830[0m        [32m0.0415[0m       [35m0.8380[0m      [31m0.8962[0m        [94m0.1657[0m     +  323.7957
      2      [36m0.9845[0m        [32m0.0391[0m       0.8333      0.8870        0.1716        323.5776
      3      [36m0.9877[0m        [32m0.0345[0m       [35m0.8434[0m      0.8929        0.1702        323.5383
      4      [36m0.9891[0m        [32m0.0315[0m       0.8399      0.8941        0.1683        323.5056
      5      [36m0.9903[0m        [32m0.0293[0m       0.8340      [31m0.8980[0m        0.1782        323.4642
      6      [36m0.9912[0m        [32m0.0278[0m       0.8299      [31m0.8983[0m        0.1886        323.5088
      7      0.9908        [32m0.0267[0m       0.8295      0.8941        0.1897        323.6694
      8      0.9912        [32m0.0260[0m       0.8200      0.8859        0.1994        323.4304
      9      [36m0.9917[0m        [32m0.0250[0m       0.8175      0.8871        0.2033        323.4286
     10      0.9914        [32m0.0244[0m       0.8233      0.8976        0.1941        323.6513
     11      0.9905        0.0246       0.8234      0.8895        0.1985        323.4333
     12      [36m0.9925[0m        [32m0.0217[0m       0.8415      0.8982        0.1823        323.4295
     13      [36m0.9942[0m        [32m0.0196[0m       [35m0.8460[0m      [31m0.9041[0m        0.1822        323.6410
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.9291154295674343
Test Accuracy: 0.8736111111111111
Number of samples added in this iteration: 10000

Iteration: 13
Number of samples in training set: 22836
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp       dur
-------  ----------  ------------  -----------  ----------  ------------  ----  --------
      1      [36m0.9854[0m        [32m0.0329[0m       [35m0.8368[0m      [31m0.8885[0m        [94m0.1748[0m     +  529.9199
      2      [36m0.9870[0m        [32m0.0293[0m       [35m0.8417[0m      [31m0.8925[0m        0.1770        530.0602
      3      [36m0.9886[0m        [32m0.0264[0m       0.8349      0.8879        0.1871        529.7889
      4      [36m0.9891[0m        [32m0.0243[0m       0.8198      0.8684        0.2102        529.7895
      5      [36m0.9906[0m        [32m0.0223[0m       0.8212      0.8692        0.2116        529.7995
      6      [36m0.9914[0m        [32m0.0205[0m       0.8102      0.8454        0.2392        529.7762
      7      [36m0.9916[0m        [32m0.0202[0m       0.8155      0.8572        0.2211        529.7675
      8      [36m0.9922[0m        [32m0.0194[0m       0.8115      0.8625        0.2207        530.0063
      9      [36m0.9931[0m        [32m0.0181[0m       0.8043      0.8644        0.2363        529.7780
     10      0.9927        [32m0.0176[0m       0.8170      0.8896        0.2318        529.7718
     11      0.9930        [32m0.0169[0m       0.8191      [31m0.8977[0m        0.2447        529.7689
     12      [36m0.9940[0m        [32m0.0148[0m       0.8200      [31m0.8981[0m        0.2443        529.7580
     13      [36m0.9951[0m        [32m0.0133[0m       0.8224      0.8978        0.2309        529.7668
Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Stopping since valid_loss has not improved in the last 13 epochs.
Test F1 Score: 0.9232302474246522
Test Accuracy: 0.8602430555555556
Traceback (most recent call last):
  File "/home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/multilabel-RandomSamplingFinal14with8.py", line 314, in <module>
    X_add, X_pool, y_add, y_pool = train_test_split(X_pool, y_pool, train_size=train_size)
  File "/home/hpc/iwfa/iwfa044h/.local/lib/python3.8/site-packages/sklearn/utils/_param_validation.py", line 214, in wrapper
    return func(*args, **kwargs)
  File "/home/hpc/iwfa/iwfa044h/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py", line 2649, in train_test_split
    n_train, n_test = _validate_shuffle_split(
  File "/home/hpc/iwfa/iwfa044h/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py", line 2262, in _validate_shuffle_split
    raise ValueError(
ValueError: train_size=4042 should be either positive and smaller than the number of samples 4042 or a float in the (0, 1) range
=== JOB_STATISTICS ===
=== current date     : Sat 07 Sep 2024 10:04:37 AM CEST
= Job-ID             : 887989 on tinygpu
= Job-Name           : MinicondaName
= Job-Command        : /home/woody/iwfa/iwfa044h/runner3.sh
= Initial workdir    : /home/woody/iwfa/iwfa044h
= Queue/Partition    : a100
= Slurm account      : iwfa with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 14:47:54
= Total RAM usage    : 51.7 GiB of requested  GiB (%)   
= Node list          : tg092
= Subm/Elig/Start/End: 2024-09-06T19:15:18 / 2024-09-06T19:15:18 / 2024-09-06T19:16:43 / 2024-09-07T10:04:37
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           56.5G   104.9G   209.7G        N/A     141K     500K   1,000K        N/A    
    /home/woody        813.2G  1000.0G  1500.0G        N/A   1,810K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:81:00.0, 564887, 95 %, 16 %, 39936 MiB, 53171387 ms
