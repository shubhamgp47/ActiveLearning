 7.2083
     66      0.8750        1.3099       0.2127      0.2127        2.0462        7.2301
     67      0.8750        1.3039       0.2139      0.2139        2.0570        7.2132
     68      1.0000        1.2628       0.2141      0.2141        2.0555        7.2116
     69      1.0000        1.2194       0.2181      0.2181        2.0495        7.2403
     70      1.0000        1.2400       0.2184      0.2184        2.0452        7.2275
     71      1.0000        1.2003       0.2102      0.2102        2.0554        7.2104
     72      0.8750        1.2398       0.2123      0.2123        2.0682        7.2354
     73      1.0000        1.2198       0.2158      0.2158        2.0561        7.2298
     74      1.0000        1.1865       0.2168      0.2168        2.0445        7.2128
     75      1.0000        1.1925       0.2186      0.2186        2.0565        7.2145
Stopping since valid_loss has not improved in the last 15 epochs.
Pre F1 score = 0.238

Iteration:  1
Selecting 16 informative samples:

Training started with 24 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      0.3333        1.8290       0.2340      0.2340        1.9814     +  7.2603
      2      0.4167        1.7394       0.2422      0.2422        1.9266     +  7.2575
      3      0.5417        1.7449       0.2451      0.2451        1.8978     +  7.2599
      4      0.4167        1.6912       0.2464      0.2464        1.8705     +  7.2551
      5      0.5000        1.6177       0.2519      0.2519        1.8490     +  7.2779
      6      0.5417        1.6102       0.2540      0.2540        1.8304     +  7.2678
      7      0.5000        1.5760       0.2559      0.2559        1.8167     +  7.2493
      8      0.5417        1.5840       0.2632      0.2632        1.8076     +  7.2759
      9      0.5417        1.5412       0.2637      0.2637        1.7957     +  7.2384
     10      0.5417        1.5549       0.2630      0.2630        1.7950     +  7.2379
     11      0.4583        1.5728       0.2823      0.2823        1.7902     +  7.2385
     12      0.5000        1.4943       0.2884      0.2884        1.7889     +  7.2574
     13      0.5833        1.5045       0.2901      0.2901        1.7827     +  7.2644
     14      0.5417        1.4930       0.2905      0.2905        1.7767     +  7.2683
     15      0.5000        1.5123       0.3134      0.3134        1.7752     +  7.2704
     16      0.5833        1.4867       0.3038      0.3038        1.7650     +  8.4067
     17      0.5000        1.4886       0.2835      0.2835        1.7555     +  12.0742
     18      0.5000        1.4489       0.2946      0.2946        1.7579        12.0881
     19      0.5417        1.4660       0.2995      0.2995        1.7571        12.0682
     20      0.5833        1.4044       0.2920      0.2920        1.7588        12.0593
     21      0.5833        1.4184       0.3040      0.3040        1.7516     +  12.0345
     22      0.5833        1.4185       0.2863      0.2863        1.7493     +  11.9937
     23      0.5417        1.3830       0.2943      0.2943        1.7538        11.9902
     24      0.5833        1.4317       0.3326      0.3326        1.7609        12.0096
     25      0.6250        1.3835       0.3217      0.3217        1.7514        12.0857
     26      0.5833        1.4001       0.3132      0.3132        1.7552        12.0206
     27      0.7083        1.3776       0.2944      0.2944        1.7493        12.0538
     28      0.6250        1.3898       0.3134      0.3134        1.7439     +  12.0429
     29      0.5833        1.3735       0.3196      0.3196        1.7501        12.0420
     30      0.6667        1.3421       0.3208      0.3208        1.7436     +  12.0588
     31      0.5833        1.3768       0.2894      0.2894        1.7436     +  12.0607
     32      0.7083        1.3099       0.3281      0.3281        1.7431     +  12.0083
     33      0.7917        1.3122       0.3231      0.3231        1.7504        12.0115
     34      0.7083        1.2537       0.3118      0.3118        1.7453        11.9957
     35      0.7083        1.2855       0.3219      0.3219        1.7426     +  12.0275
     36      0.8333        1.3006       0.3271      0.3271        1.7426     +  12.0105
     37      0.8333        1.2741       0.3196      0.3196        1.7432        12.0451
     38      0.7917        1.2693       0.3243      0.3243        1.7480        12.0288
     39      0.7917        1.2700       0.3378      0.3378        1.7421     +  12.0571
     40      0.7917        1.2723       0.3137      0.3137        1.7456        12.0421
     41      0.8750        1.2233       0.3438      0.3438        1.7479        12.0797
     42      0.7500        1.2727       0.3087      0.3087        1.7478        12.0123
     43      0.8333        1.2637       0.3128      0.3128        1.7545        12.0412
     44      0.7917        1.2505       0.3135      0.3135        1.7582        12.0399
     45      0.9167        1.2271       0.3370      0.3370        1.7548        12.0926
     46      0.8333        1.2487       0.3165      0.3165        1.7555        12.0749
     47      0.8333        1.1759       0.3352      0.3352        1.7464        12.1659
     48      0.8750        1.1894       0.3278      0.3278        1.7457        12.2229
     49      0.9583        1.1976       0.3377      0.3377        1.7443        12.2387
     50      0.9167        1.1783       0.3181      0.3181        1.7461        12.1585
     51      0.9583        1.1643       0.3309      0.3309        1.7426        12.1633
     52      0.9583        1.1608       0.3135      0.3135        1.7532        12.2101
     53      0.8750        1.1580       0.3201      0.3201        1.7490        12.0843
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 1: 0.36493055555555554
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_0.pt  

Iteration:  2
Selecting 32 informative samples:

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.4286        1.6541       0.3535      0.3535        1.6921     +  12.1862
      2      0.5179        1.5668       0.3507      0.3507        1.6449     +  12.1304
      3      0.5893        1.5131       0.3964      0.3964        1.6071     +  12.2189
      4      0.5536        1.4662       0.4451      0.4451        1.5813     +  12.2572
      5      0.6429        1.4391       0.4887      0.4887        1.5616     +  12.2520
      6      0.5714        1.4308       0.4967      0.4967        1.5521     +  12.2416
      7      0.7500        1.4093       0.4939      0.4939        1.5307     +  12.2779
      8      0.6786        1.4046       0.4507      0.4507        1.5501        12.2561
      9      0.6607        1.4068       0.5144      0.5144        1.5167     +  12.2018
     10      0.6607        1.3952       0.4743      0.4743        1.5261        12.2305
     11      0.7857        1.3746       0.5568      0.5568        1.4856     +  12.1540
     12      0.7143        1.3484       0.5358      0.5358        1.4886        12.1860
     13      0.8036        1.3117       0.5776      0.5776        1.4659     +  12.2244
     14      0.7500        1.3084       0.5236      0.5236        1.4848        12.2792
     15      0.7857        1.3051       0.5884      0.5884        1.4488     +  12.2460
     16      0.7857        1.2930       0.4936      0.4936        1.5007        12.2638
     17      0.7679        1.3447       0.5729      0.5729        1.4730        12.1933
     18      0.7321        1.3371       0.5330      0.5330        1.4717        12.2260
     19      0.8393        1.2841       0.5990      0.5990        1.4321     +  12.2628
     20      0.7679        1.2640       0.5580      0.5580        1.4504        12.2133
     21      0.7857        1.2414       0.6062      0.6062        1.4169     +  12.1261
     22      0.8214        1.2543       0.5226      0.5226        1.4651        12.1689
     23      0.8036        1.2498       0.6153      0.6153        1.4286        12.1702
     24      0.7857        1.2691       0.5130      0.5130        1.4660        12.1641
     25      0.6964        1.2499       0.6101      0.6101        1.4279        12.1786
     26      0.8036        1.2761       0.5156      0.5156        1.4608        12.1841
     27      0.8036        1.2050       0.6172      0.6172        1.3981     +  12.1828
     28      0.7857        1.2094       0.5576      0.5576        1.4287        12.2302
     29      0.8393        1.1953       0.6125      0.6125        1.3886     +  12.1675
     30      0.8214        1.1650       0.6047      0.6047        1.3911        12.1654
     31      0.8214        1.1557       0.5887      0.5887        1.4042        12.1170
     32      0.8214        1.1614       0.6042      0.6042        1.3904        12.1536
     33      0.8214        1.1574       0.5938      0.5938        1.3984        12.1540
     34      0.8214        1.1580       0.6191      0.6191        1.3752     +  12.1576
     35      0.8393        1.1570       0.5842      0.5842        1.4003        12.1791
     36      0.8393        1.1300       0.6210      0.6210        1.3682     +  12.1482
     37      0.8393        1.1332       0.5411      0.5411        1.4284        12.2003
     38      0.7857        1.1376       0.6207      0.6207        1.3884        12.1904
     39      0.8393        1.1558       0.5231      0.5231        1.4397        12.1966
     40      0.7857        1.1683       0.6220      0.6220        1.3920        12.1433
     41      0.8036        1.1882       0.5312      0.5312        1.4354        12.1469
     42      0.8214        1.1414       0.6262      0.6262        1.3659     +  12.1548
     43      0.8393        1.1148       0.5734      0.5734        1.4008        12.2140
     44      0.8750        1.1067       0.6226      0.6226        1.3573     +  12.2029
     45      0.8571        1.0952       0.5731      0.5731        1.3975        12.2372
     46      0.8750        1.0981       0.6243      0.6243        1.3517     +  12.2449
     47      0.8393        1.0878       0.5766      0.5766        1.3930        12.2156
     48      0.8571        1.0698       0.6233      0.6233        1.3489     +  12.1912
     49      0.8571        1.0752       0.5795      0.5795        1.3848        12.1248
     50      0.8571        1.0499       0.6234      0.6234        1.3463     +  12.1493
     51      0.8571        1.0585       0.5925      0.5925        1.3692        12.1425
     52      0.8036        1.0513       0.6219      0.6219        1.3397     +  12.1610
     53      0.8571        1.0420       0.5910      0.5910        1.3718        12.2062
     54      0.8750        1.0314       0.6174      0.6174        1.3404        12.1815
     55      0.8750        1.0175       0.5849      0.5849        1.3764        12.1657
     56      0.8393        1.0407       0.6267      0.6267        1.3370     +  12.1509
     57      0.8750        1.0500       0.5793      0.5793        1.3766        12.1094
     58      0.8571        1.0534       0.6252      0.6252        1.3350     +  12.1555
     59      0.8750        1.0377       0.5469      0.5469        1.4097        12.1628
     60      0.8750        1.0534       0.6104      0.6104        1.3795        12.1213
     61      0.8571        1.1411       0.4833      0.4833        1.4667        12.1236
     62      0.8036        1.1192       0.5922      0.5922        1.4231        12.1946
     63      0.8214        1.1568       0.5658      0.5658        1.3887        12.1908
     64      0.9107        1.0132       0.6139      0.6139        1.3361        12.1786
     65      0.8214        1.0042       0.6057      0.6057        1.3405        12.1764
     66      0.8929        0.9920       0.6156      0.6156        1.3310     +  12.1865
     67      0.8929        0.9637       0.6099      0.6099        1.3369        12.1859
     68      0.8929        0.9590       0.6137      0.6137        1.3306     +  12.2277
     69      0.9107        0.9708       0.6076      0.6076        1.3362        12.1688
     70      0.8750        0.9761       0.6193      0.6193        1.3255     +  12.1320
     71      0.8929        0.9555       0.6158      0.6158        1.3296        12.1620
     72      0.8929        0.9787       0.6156      0.6156        1.3290        12.2017
     73      0.8929        0.9676       0.6182      0.6182        1.3253     +  12.2090
     74      0.8929        0.9565       0.6073      0.6073        1.3365        12.1942
     75      0.9107        0.9452       0.6101      0.6101        1.3299        12.2281
     76      0.9107        0.9362       0.6089      0.6089        1.3301        12.1844
     77      0.8929        0.9370       0.6177      0.6177        1.3198     +  12.1713
     78      0.8929        0.9452       0.6024      0.6024        1.3338        12.1818
     79      0.9107        0.9170       0.6181      0.6181        1.3147     +  12.1153
     80      0.9286        0.9279       0.6089      0.6089        1.3266        12.1494
     81      0.9107        0.9205       0.6043      0.6043        1.3300        12.1456
     82      0.9107        0.9105       0.6153      0.6153        1.3176        12.1699
     83      0.8929        0.9254       0.6111      0.6111        1.3228        12.1823
     84      0.9107        0.9165       0.6062      0.6062        1.3231        12.1710
     85      0.9107        0.9069       0.6189      0.6189        1.3150        12.1577
     86      0.9107        0.9063       0.6085      0.6085        1.3206        12.1366
     87      0.9286        0.9112       0.6170      0.6170        1.3116     +  12.1796
     88      0.9107        0.9097       0.5927      0.5927        1.3401        12.1938
     89      0.9107        0.8924       0.6201      0.6201        1.3076     +  12.1453
     90      0.9107        0.8888       0.5856      0.5856        1.3510        12.1364
     91      0.9286        0.8954       0.6255      0.6255        1.3045     +  12.1603
     92      0.9286        0.9179       0.5517      0.5517        1.4044        12.2293
     93      0.9107        0.9553       0.5920      0.5920        1.3864        12.1769
     94      0.8750        1.0468       0.4839      0.4839        1.4909        12.2083
     95      0.8214        1.0322       0.5394      0.5394        1.4786        12.2012
     96      0.7857        1.1300       0.5648      0.5648        1.3835        12.2339
     97      0.9286        0.9225       0.6033      0.6033        1.3267        12.2253
     98      0.9464        0.8676       0.6125      0.6125        1.3144        12.2276
     99      0.9286        0.8644       0.6083      0.6083        1.3170        12.1564
    100      0.9286        0.8614       0.6040      0.6040        1.3174        12.1606
F1 Score after query 2: 0.671875
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_1.pt  

Iteration:  3
Selecting 56 informative samples:

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.6429        1.2589       0.5497      0.5497        1.3330     +  12.4011
      2      0.7411        1.1864       0.5818      0.5818        1.2663     +  12.3759
      3      0.7411        1.1782       0.5901      0.5901        1.2386     +  12.3567
      4      0.7589        1.1392       0.5986      0.5986        1.2152     +  12.3690
      5      0.7411        1.1263       0.5915      0.5915        1.2186        12.3581
      6      0.7679        1.1056       0.6302      0.6302        1.1756     +  12.3424
      7      0.7679        1.0936       0.5884      0.5884        1.2150        12.4154
      8      0.7411        1.0900       0.6394      0.6394        1.1539     +  12.3894
      9      0.7946        1.0745       0.5917      0.5917        1.2030        12.3692
     10      0.7679        1.0755       0.6627      0.6627        1.1289     +  12.3609
     11      0.7679        1.0706       0.5675      0.5675        1.2313        12.3707
     12      0.7589        1.0633       0.6825      0.6825        1.1269     +  12.3722
     13      0.7946        1.0731       0.5132      0.5132        1.3243        12.3592
     14      0.7054        1.0946       0.6832      0.6832        1.1367        12.3806
     15      0.7411        1.1170       0.5325      0.5325        1.2803        12.3721
     16      0.7500        1.0774       0.6722      0.6722        1.1165     +  12.3551
     17      0.7768        1.0350       0.5976      0.5976        1.1809        12.3864
     18      0.7768        1.0305       0.6608      0.6608        1.1084     +  12.4112
     19      0.8125        0.9921       0.6196      0.6196        1.1510        12.4373
     20      0.7857        1.0021       0.6667      0.6667        1.0991     +  12.4734
     21      0.8036        0.9929       0.6151      0.6151        1.1527        12.4147
     22      0.7768        0.9905       0.6729      0.6729        1.0934     +  12.4066
     23      0.7946        1.0023       0.6031      0.6031        1.1630        12.4316
     24      0.8125        0.9845       0.6745      0.6745        1.0901     +  12.4506
     25      0.8125        0.9863       0.5872      0.5872        1.1862        12.4303
     26      0.7768        0.9819       0.6825      0.6825        1.0926        12.4063
     27      0.8304        0.9888       0.5628      0.5628        1.2354        12.4021
     28      0.7589        0.9900       0.6844      0.6844        1.0926        12.3949
     29      0.8304        0.9800       0.5714      0.5714        1.2178        12.3898
     30      0.7768        0.9778       0.6839      0.6839        1.0934        12.3753
     31      0.8393        0.9794       0.5913      0.5913        1.1859        12.3670
     32      0.7679        0.9809       0.6821      0.6821        1.0795     +  12.3840
     33      0.8482        0.9396       0.6208      0.6208        1.1387        12.4054
     34      0.8214        0.9288       0.6750      0.6750        1.0757     +  12.4049
     35      0.8304        0.9329       0.6276      0.6276        1.1270        12.4666
     36      0.8125        0.9329       0.6783      0.6783        1.0746     +  12.4314
     37      0.8571        0.9161       0.6269      0.6269        1.1333        12.5122
     38      0.8125        0.9126       0.6825      0.6825        1.0689     +  12.3874
     39      0.8661        0.9286       0.6148      0.6148        1.1442        12.3959
     40      0.8125        0.9072       0.6840      0.6840        1.0683     +  12.3862
     41      0.8661        0.9066       0.6175      0.6175        1.1397        12.3479
     42      0.8214        0.9010       0.6842      0.6842        1.0634     +  12.3839
     43      0.8839        0.9049       0.6111      0.6111        1.1472        12.3952
     44      0.8214        0.9028       0.6880      0.6880        1.0660        12.4216
     45      0.8929        0.8977       0.6181      0.6181        1.1409        12.4418
     46      0.8571        0.8862       0.6811      0.6811        1.0640        12.4543
     47      0.8750        0.8875       0.6264      0.6264        1.1332        12.4638
     48      0.8393        0.8886       0.6842      0.6842        1.0626     +  12.4084
     49      0.8929        0.8745       0.6106      0.6106        1.1468        12.4411
     50      0.8571        0.8782       0.6887      0.6887        1.0662        12.3956
     51      0.8929        0.8907       0.5955      0.5955        1.1750        12.3957
     52      0.8304        0.8819       0.6892      0.6892        1.0677        12.4682
     53      0.9107        0.8801       0.6061      0.6061        1.1562        12.5148
     54      0.8839        0.8643       0.6880      0.6880        1.0645        12.4890
     55      0.9018        0.8680       0.6092      0.6092        1.1510        12.4389
     56      0.8571        0.8647       0.6863      0.6863        1.0556     +  12.4147
     57      0.9286        0.8561       0.6271      0.6271        1.1236        12.4206
     58      0.8750        0.8466       0.6819      0.6819        1.0527     +  12.4356
     59      0.9464        0.8288       0.6436      0.6436        1.0972        12.3914
     60      0.9107        0.8330       0.6795      0.6795        1.0550        12.3664
     61      0.9554        0.8234       0.6443      0.6443        1.0964        12.3713
     62      0.9018        0.8291       0.6760      0.6760        1.0567        12.3744
     63      0.9375        0.8146       0.6469      0.6469        1.0951        12.3526
     64      0.9375        0.8221       0.6786      0.6786        1.0584        12.3474
     65      0.9375        0.8160       0.6462      0.6462        1.0951        12.3784
     66      0.9375        0.7992       0.6832      0.6832        1.0541        12.3869
     67      0.9375        0.8051       0.6429      0.6429        1.1043        12.3689
     68      0.9464        0.7990       0.6804      0.6804        1.0532        12.3818
     69      0.9375        0.8035       0.6403      0.6403        1.1083        12.3582
     70      0.9286        0.8117       0.6819      0.6819        1.0520     +  12.3754
     71      0.9375        0.7967       0.6365      0.6365        1.1126        12.3568
     72      0.9196        0.8047       0.6851      0.6851        1.0511     +  12.3909
     73      0.9554        0.8035       0.6406      0.6406        1.1060        12.3833
     74      0.9554        0.7849       0.6866      0.6866        1.0480     +  12.4083
     75      0.9643        0.7818       0.6286      0.6286        1.1223        12.3860
     76      0.9375        0.7969       0.6856      0.6856        1.0474     +  12.3891
     77      0.9554        0.7975       0.6372      0.6372        1.1117        12.4413
     78      0.9554        0.7731       0.6854      0.6854        1.0468     +  12.4215
     79      0.9554        0.7867       0.6201      0.6201        1.1461        12.4117
     80      0.9554        0.7833       0.6911      0.6911        1.0496        12.4036
     81      0.9554        0.7900       0.5991      0.5991        1.1791        12.4215
     82      0.9107        0.7926       0.6908      0.6908        1.0601        12.4138
     83      0.9464        0.7963       0.5823      0.5823        1.2055        12.4065
     84      0.9018        0.8008       0.6845      0.6845        1.0861        12.3770
     85      0.9464        0.8274       0.5694      0.5694        1.2285        12.3886
     86      0.9018        0.8086       0.6892      0.6892        1.0671        12.3793
     87      0.9464        0.8011       0.6271      0.6271        1.1234        12.3450
     88      0.9286        0.7656       0.6861      0.6861        1.0457     +  12.3488
     89      0.9643        0.7645       0.6552      0.6552        1.0786        12.3967
     90      0.9375        0.7588       0.6727      0.6727        1.0536        12.4012
     91      0.9554        0.7520       0.6675      0.6675        1.0607        12.3580
     92      0.9643        0.7444       0.6714      0.6714        1.0557        12.3864
     93      0.9554        0.7478       0.6668      0.6668        1.0611        12.3819
     94      0.9554        0.7463       0.6700      0.6700        1.0584        12.3990
     95      0.9643        0.7297       0.6715      0.6715        1.0587        12.3939
     96      0.9643        0.7288       0.6665      0.6665        1.0632        12.3962
     97      0.9643        0.7330       0.6771      0.6771        1.0491        12.3768
     98      0.9554        0.7168       0.6707      0.6707        1.0560        12.3638
     99      0.9732        0.7205       0.6634      0.6634        1.0675        12.3687
    100      0.9554        0.7332       0.6757      0.6757        1.0520        12.3369
F1 Score after query 3: 0.7255208333333333
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_2.pt  

Iteration:  4
Selecting 96 informative samples:

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.6923        1.0839       0.6658      0.6658        1.0569     +  12.8239
      2      0.7548        1.0859       0.5252      0.5252        1.4662        12.8091
      3      0.4760        1.4728       0.5795      0.5795        1.3511        12.8018
      4      0.6538        1.2956       0.6953      0.6953        1.0254     +  12.8179
      5      0.7788        1.0526       0.6823      0.6823        1.0200     +  12.8728
      6      0.8125        1.0161       0.6767      0.6767        1.0250        12.8624
      7      0.8221        1.0019       0.6510      0.6510        1.0521        12.8569
      8      0.7740        1.0182       0.6510      0.6510        1.0408        12.8587
      9      0.8077        1.0009       0.6792      0.6792        1.0086     +  12.8036
     10      0.8462        0.9628       0.6922      0.6922        1.0001     +  12.8323
     11      0.8702        0.9432       0.6872      0.6872        1.0059        12.8492
     12      0.8462        0.9356       0.6707      0.6707        1.0090        12.7931
     13      0.8413        0.9421       0.6102      0.6102        1.1345        12.7603
     14      0.7163        1.0535       0.6698      0.6698        1.0211        12.7585
     15      0.8221        0.9665       0.6997      0.6997        1.0533        12.8343
     16      0.8413        0.9757       0.6134      0.6134        1.2847        12.7691
     17      0.7404        1.1422       0.6691      0.6691        1.1014        12.7640
     18      0.8654        0.9641       0.6991      0.6991        1.0013        12.7909
     19      0.8990        0.8892       0.6642      0.6642        1.0262        12.7938
     20      0.8702        0.9059       0.6587      0.6587        1.0272        12.7722
     21      0.8654        0.9003       0.6679      0.6679        1.0095        12.7975
     22      0.8846        0.8765       0.6872      0.6872        0.9983     +  12.7870
     23      0.8798        0.8588       0.7042      0.7042        0.9983     +  12.8734
     24      0.8990        0.8480       0.6950      0.6950        1.0541        12.9196
     25      0.8990        0.8893       0.6967      0.6967        1.0560        12.9220
     26      0.8702        0.8879       0.6950      0.6950        1.0453        12.8711
     27      0.8990        0.8408       0.6766      0.6766        1.0040        12.8111
     28      0.8894        0.8452       0.6212      0.6212        1.1047        12.8046
     29      0.8077        0.9434       0.6479      0.6479        1.0325        12.8323
     30      0.8654        0.8772       0.6934      0.6934        1.0135        12.8036
     31      0.9135        0.8157       0.7050      0.7050        1.0050        12.8197
     32      0.9087        0.8005       0.7092      0.7092        0.9948     +  12.8935
     33      0.9231        0.8047       0.7043      0.7043        0.9971        12.8317
     34      0.9183        0.7839       0.7023      0.7023        0.9940     +  12.7980
     35      0.9279        0.7870       0.6979      0.6979        1.0041        12.8324
     36      0.9231        0.7687       0.6983      0.6983        1.0029        12.8590
     37      0.9375        0.7707       0.6986      0.6986        0.9962        12.8570
     38      0.9327        0.7639       0.6894      0.6894        1.0034        12.7599
     39      0.9423        0.7636       0.6778      0.6778        1.0031        12.7634
     40      0.9279        0.7600       0.6776      0.6776        1.0010        12.7522
     41      0.9327        0.7615       0.6677      0.6677        1.0136        12.7899
     42      0.9279        0.7676       0.6663      0.6663        1.0156        12.8056
     43      0.9327        0.7635       0.6653      0.6653        1.0132        12.7708
     44      0.9423        0.7571       0.6642      0.6642        1.0195        12.7777
     45      0.9423        0.7501       0.6901      0.6901        0.9934     +  12.8292
     46      0.9423        0.7333       0.6767      0.6767        1.1004        12.8336
     47      0.9231        0.8036       0.6804      0.6804        1.1096        12.8398
     48      0.8125        0.9076       0.6170      0.6170        1.2264        12.7600
     49      0.8798        0.8781       0.6979      0.6979        1.0205        12.7445
     50      0.9231        0.7456       0.6774      0.6774        1.0149        12.7722
     51      0.9327        0.7472       0.6785      0.6785        1.0031        12.7889
     52      0.9375        0.7175       0.6840      0.6840        1.0092        12.7833
     53      0.9423        0.7072       0.6993      0.6993        0.9890     +  12.7854
     54      0.9471        0.6974       0.7090      0.7090        0.9876     +  12.8279
     55      0.9375        0.6940       0.7127      0.7127        0.9931        12.8525
     56      0.9327        0.7061       0.7113      0.7113        0.9975        12.8095
     57      0.9423        0.6986       0.7130      0.7130        0.9835     +  12.7650
     58      0.9327        0.6864       0.7068      0.7068        0.9846        12.8234
     59      0.9423        0.6802       0.7049      0.7049        0.9882        12.7999
     60      0.9615        0.6825       0.6970      0.6970        0.9919        12.8930
     61      0.9567        0.6655       0.6948      0.6948        1.0129        12.9215
     62      0.9519        0.6742       0.7030      0.7030        0.9994        12.8873
     63      0.9519        0.6643       0.7023      0.7023        0.9829     +  12.8989
     64      0.9567        0.6572       0.6944      0.6944        1.0000        12.8492
     65      0.9471        0.6590       0.6800      0.6800        1.0186        12.8216
     66      0.9663        0.6641       0.6677      0.6677        1.0260        12.7827
     67      0.9183        0.7043       0.6099      0.6099        1.1950        12.7612
     68      0.8029        0.9229       0.6217      0.6217        1.2501        12.7789
     69      0.7885        0.9503       0.4411      0.4411        1.5992        12.7273
     70      0.7212        1.1231       0.6656      0.6656        1.0592        12.7727
     71      0.9087        0.7281       0.6927      0.6927        0.9906        12.7801
     72      0.9423        0.6717       0.7069      0.7069        0.9828     +  12.7863
     73      0.9471        0.6554       0.7073      0.7073        0.9841        12.7979
     74      0.9519        0.6475       0.7104      0.7104        0.9848        12.8237
     75      0.9423        0.6476       0.7028      0.7028        0.9825     +  12.8173
     76      0.9471        0.6359       0.7064      0.7064        0.9800     +  12.7948
     77      0.9519        0.6339       0.6993      0.6993        0.9814        12.7487
     78      0.9663        0.6196       0.6988      0.6988        0.9795     +  12.7872
     79      0.9567        0.6209       0.7021      0.7021        0.9820        12.8418
     80      0.9567        0.6244       0.7083      0.7083        0.9848        12.7763
     81      0.9567        0.6121       0.7092      0.7092        0.9882        12.8395
     82      0.9567        0.6174       0.7016      0.7016        0.9838        12.7932
     83      0.9567        0.6068       0.7069      0.7069        0.9751     +  12.8415
     84      0.9663        0.6014       0.7014      0.7014        0.9848        12.8575
     85      0.9663        0.5942       0.7047      0.7047        0.9878        12.8085
     86      0.9663        0.5960       0.7012      0.7012        0.9874        12.8173
     87      0.9615        0.5959       0.7101      0.7101        0.9808        12.8225
     88      0.9712        0.5889       0.7113      0.7113        0.9838        12.8290
     89      0.9760        0.5800       0.7052      0.7052        0.9820        12.8961
     90      0.9663        0.5869       0.7078      0.7078        0.9830        12.8421
     91      0.9663        0.5812       0.6997      0.6997        0.9849        12.8796
     92      0.9760        0.5754       0.6984      0.6984        0.9823        12.7846
     93      0.9808        0.5676       0.6972      0.6972        0.9883        12.7843
     94      0.9663        0.5787       0.6974      0.6974        0.9843        12.7803
     95      0.9663        0.5674       0.6918      0.6918        0.9934        12.7847
     96      0.9808        0.5659       0.7064      0.7064        0.9819        12.7694
     97      0.9712        0.5709       0.7104      0.7104        0.9927        12.7915
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 4: 0.7171875
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_3.pt  

Iteration:  5
Selecting 176 informative samples:

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7266        0.9932       0.5168      0.5168        1.3502     +  13.3273
      2      0.6172        1.1843       0.7196      0.7196        0.9375     +  13.4504
      3      0.7578        1.0152       0.5960      0.5960        1.1486        13.4643
      4      0.7422        1.0344       0.6771      0.6771        0.9561        13.5292
      5      0.6901        1.0710       0.5830      0.5830        1.1771        13.5433
      6      0.7370        1.0515       0.6828      0.6828        0.9791        13.5400
      7      0.8385        0.8937       0.7385      0.7385        0.8836     +  13.5569
      8      0.8594        0.8223       0.6594      0.6594        1.0258        13.5590
      9      0.8646        0.8117       0.7542      0.7542        0.8582     +  13.4421
     10      0.8984        0.7769       0.6561      0.6561        1.1066        13.4687
     11      0.8594        0.8257       0.7476      0.7476        0.8862        13.6009
     12      0.9062        0.7807       0.6589      0.6589        1.1088        13.5306
     13      0.8594        0.7939       0.7479      0.7479        0.8780        13.5567
     14      0.9271        0.7256       0.7283      0.7283        0.8825        13.4800
     15      0.9349        0.7037       0.7500      0.7500        0.8779        13.5040
     16      0.9401        0.6861       0.7446      0.7446        0.8732        13.4236
     17      0.9401        0.6798       0.7516      0.7516        0.8800        13.4445
     18      0.9453        0.6735       0.7318      0.7318        0.8997        13.4664
     19      0.9375        0.6725       0.7148      0.7148        0.9077        13.4460
     20      0.9479        0.6571       0.7481      0.7481        0.8887        13.5628
     21      0.9479        0.6491       0.7556      0.7556        0.8668        13.5378
     22      0.9505        0.6459       0.7130      0.7130        0.9315        13.4467
     23      0.9453        0.6485       0.7552      0.7552        0.8511     +  13.5975
     24      0.9375        0.6550       0.6993      0.6993        0.9593        13.4599
     25      0.9427        0.6565       0.7510      0.7510        0.8577        13.5043
     26      0.9479        0.6248       0.7179      0.7179        0.9275        13.4130
     27      0.9531        0.6205       0.7450      0.7450        0.8933        13.4159
     28      0.9531        0.6041       0.7438      0.7438        0.8961        13.4622
     29      0.9557        0.5990       0.7401      0.7401        0.9060        13.4733
     30      0.9531        0.6011       0.7441      0.7441        0.8987        13.4898
     31      0.9557        0.5872       0.7349      0.7349        0.9079        13.5370
     32      0.9531        0.5927       0.7436      0.7436        0.8875        13.4960
     33      0.9557        0.5798       0.7502      0.7502        0.8828        13.5166
     34      0.9557        0.5803       0.7434      0.7434        0.8855        13.5568
     35      0.9609        0.5846       0.7247      0.7247        0.9246        13.5071
     36      0.9583        0.5770       0.7599      0.7599        0.8679        13.4778
     37      0.9609        0.5734       0.7441      0.7441        0.8929        13.4716
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 5: 0.7453125
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_4.pt  

Iteration:  6
Selecting 320 informative samples:

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.6932        1.0354       0.4569      0.4569        1.3545     +  14.4432
      2      0.7045        1.0588       0.6672      0.6672        0.9218     +  14.7243
      3      0.8054        0.8889       0.7698      0.7698        0.7933     +  14.5220
      4      0.8224        0.8585       0.5972      0.5972        1.2123        14.7571
      5      0.7685        0.9406       0.6660      0.6660        0.9064        14.7486
      6      0.8011        0.8543       0.7378      0.7378        0.8730        14.5234
      7      0.8693        0.7734       0.6976      0.6976        0.9919        14.6555
      8      0.8722        0.7716       0.6684      0.6684        0.9321        14.4744
      9      0.8196        0.8249       0.7583      0.7583        0.8255        14.5148
     10      0.9077        0.6874       0.6797      0.6797        0.9000        14.4493
     11      0.8040        0.8411       0.6318      0.6318        1.0213        14.5808
     12      0.8750        0.7699       0.7398      0.7398        0.8729        14.5470
     13      0.8594        0.7786       0.7498      0.7498        0.8493        14.6110
     14      0.9034        0.6859       0.6674      0.6674        0.9360        14.4525
     15      0.9134        0.6706       0.7021      0.7021        0.8686        14.5807
     16      0.9205        0.6484       0.6797      0.6797        0.9128        14.5241
     17      0.9176        0.6457       0.6958      0.6958        0.8886        14.5828
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 6: 0.7711805555555555
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_5.pt  

Iteration:  7
Selecting 560 informative samples:

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7318        0.9441       0.7306      0.7306        0.9086     +  16.0598
      2      0.7666        0.9119       0.7323      0.7323        0.8373     +  16.1784
      3      0.7674        0.8725       0.8113      0.8113        0.6994     +  16.2598
      4      0.7832        0.8520       0.7641      0.7641        0.7458        16.2731
      5      0.7975        0.8331       0.7899      0.7899        0.7177        16.0197
      6      0.8505        0.7428       0.7205      0.7205        0.8201        16.0064
      7      0.8188        0.7734       0.7392      0.7392        0.8004        15.9403
      8      0.8339        0.7421       0.7653      0.7653        0.7516        16.0050
      9      0.8441        0.7118       0.7651      0.7651        0.7646        15.9177
     10      0.8552        0.6908       0.7642      0.7642        0.7729        15.9899
     11      0.8647        0.6729       0.7510      0.7510        0.8373        16.1250
     12      0.8718        0.6625       0.7783      0.7783        0.7647        16.1806
     13      0.8908        0.6408       0.7049      0.7049        0.9527        16.3390
     14      0.8616        0.6903       0.7840      0.7840        0.6942     +  16.0706
     15      0.9035        0.6116       0.7493      0.7493        0.7807        16.3320
     16      0.8726        0.6329       0.7674      0.7674        0.7604        16.0497
     17      0.9217        0.5897       0.7446      0.7446        0.8813        16.1205
     18      0.8900        0.6152       0.7762      0.7762        0.7403        16.0628
     23      0.9138        0.5568       0.7849      0.7849        0.7193        16.2489
     24      0.9185        0.5608       0.7024      0.7024        1.0111        16.1133
     25      0.9272        0.5541       0.7755      0.7755        0.7407        16.1996
     26      0.9407        0.5126       0.7701      0.7701        0.7450        16.2845
     27      0.9343        0.5209       0.7722      0.7722        0.7418        10.9297
     28      0.9470        0.5001       0.7405      0.7405        0.8865        10.5895
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 7: 0.7763888888888889
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_6.pt

Iteration:  8
Selecting 1000 informative samples:

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7951        0.8578       0.5724      0.5724        1.2945     +  13.1714
      2      0.7469        0.9124       0.7780      0.7780        0.7270     +  13.2169
      3      0.8852        0.6220       0.8023      0.8023        0.6596     +  13.2195
      4      0.9156        0.5589       0.7826      0.7826        0.6828        13.2079
      5      0.8993        0.5667       0.8023      0.8023        0.6360     +  13.2449
      6      0.9214        0.5301       0.7759      0.7759        0.7004        13.2673
      7      0.9196        0.5149       0.8033      0.8033        0.6414        13.3012
      8      0.9276        0.5016       0.7958      0.7958        0.6774        13.2093
      9      0.9289        0.4867       0.7937      0.7937        0.6778        13.2307
     10      0.9311        0.4853       0.7866      0.7866        0.6686        13.2399
     11      0.9377        0.4586       0.7972      0.7972        0.6704        13.2574
     12      0.9377        0.4559       0.7951      0.7951        0.6732        13.2577
     13      0.9408        0.4455       0.7917      0.7917        0.6875        13.2671
     14      0.9368        0.4476       0.7944      0.7944        0.6650        13.2630
     15      0.9382        0.4395       0.7899      0.7899        0.6642        13.2627
     16      0.9461        0.4244       0.7965      0.7965        0.6662        13.2232
     17      0.9505        0.4049       0.7964      0.7964        0.6682        13.2063
     18      0.9519        0.3983       0.7910      0.7910        0.6759        13.1854
     19      0.9523        0.3962       0.7748      0.7748        0.7069        13.2066
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 8: 0.7883680555555556
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_7.pt

Iteration:  9
Selecting 1776 informative samples:

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.8337        0.6737       0.7870      0.7870        0.6738     +  17.8522
      2      0.8458        0.6314       0.8064      0.8064        0.6211     +  17.8163
      3      0.8770        0.5544       0.8359      0.8359        0.5534     +  17.8254
      4      0.8775        0.5313       0.8187      0.8187        0.5812        17.9373
      5      0.8948        0.5104       0.8182      0.8182        0.5823        17.9441
      6      0.9027        0.4793       0.8318      0.8318        0.5588        17.9057
      7      0.8903        0.4988       0.8187      0.8187        0.5927        17.9336
      8      0.9037        0.4699       0.8318      0.8318        0.5574        17.9276
      9      0.9104        0.4496       0.8359      0.8359        0.5538        17.8945
     10      0.9012        0.4734       0.8224      0.8224        0.5902        17.9155
     11      0.9324        0.4118       0.8382      0.8382        0.5585        17.9368
     12      0.9144        0.4326       0.8413      0.8413        0.5433     +  17.9308
     13      0.9243        0.4050       0.8274      0.8274        0.5820        17.8920
     14      0.9304        0.3891       0.8271      0.8271        0.5802        17.9302
     15      0.9295        0.3926       0.8274      0.8274        0.5906        17.9077
     16      0.9339        0.3748       0.8266      0.8266        0.5942        17.8902
     17      0.9433        0.3579       0.8194      0.8194        0.6160        17.9627
     18      0.9413        0.3538       0.8177      0.8177        0.6396        17.9282
     19      0.9455        0.3446       0.8222      0.8222        0.6284        17.9329
     20      0.9411        0.3526       0.8203      0.8203        0.6296        17.9522
     21      0.9473        0.3354       0.8116      0.8116        0.6433        17.9349
     22      0.9527        0.3194       0.8146      0.8146        0.6257        17.9115
     23      0.9537        0.3136       0.8155      0.8155        0.6487        17.8798
     24      0.9280        0.3735       0.8010      0.8010        0.7005        17.9541
     25      0.9567        0.3105       0.8231      0.8231        0.6114        17.9267
     26      0.8975        0.4666       0.8139      0.8139        0.6339        17.9224
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 9: 0.8315972222222223
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_8.pt

Iteration:  10
Selecting 3160 informative samples:

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.8946        0.4553       0.8250      0.8250        0.5634     +  26.2796
      2      0.9085        0.4091       0.8132      0.8132        0.6087        26.2512
      3      0.9072        0.4036       0.8174      0.8174        0.6026        26.3037
      4      0.9271        0.3520       0.8182      0.8182        0.5989        26.2295
      5      0.9289        0.3442       0.7924      0.7924        0.6692        26.2260
      6      0.9331        0.3331       0.7835      0.7835        0.6961        26.2658
      7      0.9378        0.3192       0.7920      0.7920        0.6876        26.2583
      8      0.9415        0.3037       0.7915      0.7915        0.7000        26.2748
      9      0.9392        0.3031       0.7903      0.7903        0.7123        26.1478
     10      0.9482        0.2837       0.7932      0.7932        0.7182        26.2861
     11      0.9521        0.2737       0.7932      0.7932        0.7096        26.2904
     12      0.9507        0.2743       0.7955      0.7955        0.7235        26.3299
     13      0.9583        0.2538       0.7922      0.7922        0.7176        26.2283
     14      0.9614        0.2423       0.8002      0.8002        0.7264        26.2390
     15      0.9618        0.2387       0.7984      0.7984        0.7193        26.2927
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 10: 0.8071180555555556
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_9.pt

Iteration:  11
Selecting 5624 informative samples:

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.9283        0.3226       0.7752      0.7752        0.7741     +  41.1389
      2      0.9652        0.2146       0.7932      0.7932        0.7371     +  41.1579
      3      0.9677        0.2000       0.7778      0.7778        0.7972        41.0898
      4      0.9722        0.1820       0.7906      0.7906        0.7549        41.1925
      5      0.9743        0.1741       0.7878      0.7878        0.7474        41.1964
      6      0.9735        0.1712       0.7925      0.7925        0.7688        41.1493
      7      0.9743        0.1706       0.8052      0.8052        0.7553        41.1836
      8      0.9773        0.1564       0.7977      0.7977        0.7647        41.1581
      9      0.9788        0.1526       0.7934      0.7934        0.7879        41.0955
     10      0.9712        0.1653       0.8042      0.8042        0.7509        41.1447
     11      0.9808        0.1412       0.7964      0.7964        0.7700        41.1360
     12      0.9810        0.1389       0.7960      0.7960        0.7856        41.0493
     13      0.9817        0.1310       0.7899      0.7899        0.8097        41.1954
     14      0.9851        0.1240       0.7910      0.7910        0.8198        41.2111
     15      0.9846        0.1233       0.7977      0.7977        0.7893        41.1110
     16      0.9800        0.1312       0.7936      0.7936        0.8192        41.2270
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 11: 0.8321180555555554
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_10.pt

Iteration:  12
Selecting 10000 informative samples:

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.9659        0.1759       0.7976      0.7976        0.8282     +  67.5250
      2      0.9807        0.1177       0.8087      0.8087        0.7284     +  67.8441
      3      0.9878        0.0983       0.7974      0.7974        0.7964        67.7902
      4      0.9891        0.0915       0.7809      0.7809        0.8430        67.7829
      5      0.9897        0.0869       0.7957      0.7957        0.7996        67.9975
      6      0.9899        0.0836       0.7936      0.7936        0.8377        67.9869
      7      0.9919        0.0777       0.7778      0.7778        0.9165        67.6714
      8      0.9911        0.0767       0.7785      0.7785        0.8980        67.7516
      9      0.9917        0.0744       0.7885      0.7885        0.8773        67.6502
     10      0.9930        0.0686       0.7828      0.7828        0.8852        67.7221
     11      0.9915        0.0703       0.7774      0.7774        0.9408        67.5667
     12      0.9819        0.0985       0.7911      0.7911        0.8584        67.8015
     13      0.9832        0.0924       0.7839      0.7839        0.8790        67.6556
     14      0.9920        0.0672       0.7825      0.7825        0.9214        67.6752
     15      0.9925        0.0641       0.7873      0.7873        0.8922        67.7550
     16      0.9897        0.0700       0.7927      0.7927        0.8911        67.5993
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 12: 0.8255208333333334
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_11.pt

Iteration:  13
Selecting 4056 informative samples:

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.9949        0.0506       0.7868      0.7868        0.9243     +  78.4559
      2      0.9957        0.0475       0.7898      0.7898        0.9385        78.4768
      3      0.9958        0.0456       0.7861      0.7861        0.9674        78.3923
      4      0.9962        0.0443       0.7918      0.7918        0.9423        78.3696
      5      0.9964        0.0432       0.7757      0.7757        1.0334        78.3466
      6      0.9962        0.0430       0.7898      0.7898        0.9775        78.4863
      7      0.9872        0.0700       0.7750      0.7750        1.0191        78.3863
      8      0.9939        0.0482       0.7924      0.7924        0.9337        78.3179
      9      0.9944        0.0459       0.7870      0.7870        0.9781        78.4302
     10      0.9735        0.1170       0.8016      0.8016        0.8838     +  78.3113
     11      0.9904        0.0572       0.8045      0.8045        0.8649     +  78.3580
     12      0.9911        0.0548       0.7951      0.7951        0.9367        78.4583
     13      0.9948        0.0430       0.8024      0.8024        0.9247        78.3574
     14      0.9955        0.0409       0.8158      0.8158        0.8698        78.3367
     15      0.9965        0.0378       0.8024      0.8024        0.9319        78.3840
     16      0.9965        0.0369       0.7967      0.7967        0.9701        78.3654
     17      0.9972        0.0344       0.8030      0.8030        0.9229        78.4243
     18      0.9974        0.0334       0.8002      0.8002        0.9738        78.4229
     19      0.9975        0.0327       0.7962      0.7962        0.9821        78.3147
     20      0.9976        0.0317       0.7924      0.7924        1.0292        78.4587
     21      0.9977        0.0310       0.8023      0.8023        0.9830        78.5209
     22      0.9978        0.0302       0.8005      0.8005        1.0009        78.3100
     23      0.9978        0.0299       0.7977      0.7977        1.0169        78.3452
     24      0.9976        0.0293       0.8003      0.8003        1.0113        78.3600
     25      0.9979        0.0287       0.7995      0.7995        1.0247        78.4720
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 13: 0.8303819444444445
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\model_checkpoint_iteration_12.pt
Performance results saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\margin_sampling\Run2\performance_results.npy
PS C:\Users\localuserSG\ActiveLearning\Multiclass01\DinoSmall\MS>