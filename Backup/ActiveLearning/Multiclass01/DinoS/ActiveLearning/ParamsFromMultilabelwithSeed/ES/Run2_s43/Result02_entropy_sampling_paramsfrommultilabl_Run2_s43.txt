     14      0.8750        1.2075       0.3365      0.3365        1.8015        12.0848
     15      0.8750        1.1622       0.3380      0.3380        1.8028        12.0437
     16      0.8750        1.1526       0.3330      0.3330        1.8045        12.0168
     17      0.8750        1.1561       0.3378      0.3378        1.7912        12.0315
     18      0.8750        1.1576       0.3345      0.3345        1.7877        12.0131
     19      0.8750        1.1015       0.3352      0.3352        1.7971        12.0350
     20      0.9167        1.0952       0.3352      0.3352        1.7898        12.0342
     21      0.8750        1.1133       0.3373      0.3373        1.7965        11.9960
     22      0.9167        1.0907       0.3318      0.3318        1.8020        11.9715
     23      0.9583        1.0549       0.3290      0.3290        1.8061        12.0067
     24      0.9583        1.0718       0.3281      0.3281        1.8130        12.0771
     25      0.9167        1.0718       0.3299      0.3299        1.7996        12.0485
     26      0.9583        1.0542       0.3241      0.3241        1.8204        11.9878
     27      1.0000        1.0193       0.3274      0.3274        1.8109        12.0103
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 1: 0.3223958333333333
Number of samples used for retraining: 24
Number of samples in pool after training and deleting samples: 26856
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_0.pt 

Iteration:  2
Selecting 32 informative samples:

Training started with 56 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.4286        1.6593       0.4250      0.4250        1.7148     +  12.2058
      2      0.8929        1.3864       0.4720      0.4720        1.6687     +  12.1593
      3      0.9286        1.2470       0.4816      0.4816        1.6481     +  12.2127
      4      0.8929        1.1714       0.4651      0.4651        1.6360     +  12.2026
      5      0.8750        1.0725       0.4540      0.4540        1.6300     +  12.1856
      6      0.8929        1.0182       0.4510      0.4510        1.6261     +  12.1521
      7      0.8750        0.9976       0.4507      0.4507        1.6228     +  12.1479
      8      0.8393        0.9621       0.4509      0.4509        1.6193     +  12.1794
      9      0.8750        0.9301       0.4510      0.4510        1.6191     +  12.1880
     10      0.8750        0.9045       0.4510      0.4510        1.6182     +  12.1500
     11      0.9107        0.8760       0.4510      0.4510        1.6178     +  12.1820
     12      0.8929        0.8694       0.4507      0.4507        1.6142     +  12.1819
     13      0.8750        0.8379       0.4512      0.4512        1.6104     +  12.2138
     14      0.8929        0.8248       0.4510      0.4510        1.6136        12.1826
     15      0.8929        0.8309       0.4507      0.4507        1.6144        12.1596
     16      0.9107        0.8255       0.4512      0.4512        1.6137        12.1364
     17      0.8929        0.7937       0.4507      0.4507        1.6152        12.1535
     18      0.9286        0.8085       0.4510      0.4510        1.6133        12.1769
     19      0.9286        0.7739       0.4512      0.4512        1.6119        12.1932
     20      0.8929        0.7794       0.4507      0.4507        1.6103     +  12.2008
     21      0.9107        0.7698       0.4509      0.4509        1.6096     +  12.1780
     22      0.9286        0.7617       0.4512      0.4512        1.6095     +  12.1829
     23      0.9107        0.7472       0.4512      0.4512        1.6073     +  12.2062
     24      0.9643        0.7447       0.4514      0.4514        1.6062     +  12.2449
     25      0.9286        0.7432       0.4516      0.4516        1.6050     +  12.1754
     26      0.9107        0.7329       0.4514      0.4514        1.6110        12.1449
     27      0.9286        0.7361       0.4517      0.4517        1.6117        12.1008
     28      0.9464        0.7270       0.4523      0.4523        1.6105        12.1542
     29      0.9464        0.7111       0.4523      0.4523        1.6110        12.2268
     30      0.9464        0.7043       0.4524      0.4524        1.6124        12.1910
     31      0.9643        0.7001       0.4526      0.4526        1.6131        12.1715
     32      0.9464        0.6932       0.4531      0.4531        1.6110        12.1703
     33      0.9464        0.7085       0.4531      0.4531        1.6099        12.2155
     34      0.9464        0.6926       0.4538      0.4538        1.6099        12.1848
     35      0.9643        0.6671       0.4545      0.4545        1.6084        12.1701
     36      0.9464        0.6863       0.4552      0.4552        1.6080        12.1335
     37      0.9643        0.6629       0.4573      0.4573        1.6071        12.1355
     38      0.9821        0.6740       0.4608      0.4608        1.6068        12.1977
     39      0.9821        0.6696       0.4601      0.4601        1.6090        12.1855
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 2: 0.49618055555555557
Number of samples used for retraining: 56
Number of samples in pool after training and deleting samples: 26824
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_1.pt 

Iteration:  3
Selecting 56 informative samples:

Training started with 112 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.5714        1.2759       0.4635      0.4635        1.5893     +  12.4053
      2      0.7054        1.2681       0.4651      0.4651        1.5732     +  12.3703
      3      0.6964        1.2394       0.4686      0.4686        1.5614     +  12.4071
      4      0.6964        1.2017       0.4741      0.4741        1.5508     +  12.3853
      5      0.7054        1.1889       0.4807      0.4807        1.5407     +  12.4295
      6      0.7411        1.1638       0.4821      0.4821        1.5351     +  12.3598
      7      0.7143        1.1589       0.4807      0.4807        1.5311     +  12.3735
      8      0.7321        1.1339       0.4878      0.4878        1.5234     +  12.3875
      9      0.7500        1.1315       0.4818      0.4818        1.5237        12.3416
     10      0.7679        1.1343       0.4998      0.4998        1.5161     +  12.3665
     11      0.7768        1.0962       0.4847      0.4847        1.5190        12.4380
     12      0.7679        1.1014       0.4826      0.4826        1.5167        12.4364
     13      0.7768        1.0944       0.4925      0.4925        1.5118     +  12.4122
     14      0.8036        1.0792       0.4816      0.4816        1.5180        12.3982
     15      0.7946        1.0735       0.5205      0.5205        1.5080     +  12.4487
     16      0.7946        1.0901       0.4844      0.4844        1.5122        12.4197
     17      0.8214        1.0497       0.5227      0.5227        1.5003     +  12.4058
     18      0.8214        1.0550       0.4833      0.4833        1.5145        12.3881
     19      0.8214        1.0478       0.5151      0.5151        1.4999     +  12.3920
     20      0.7946        1.0800       0.4792      0.4792        1.5141        12.4266
     21      0.7679        1.0515       0.5118      0.5118        1.5003        12.4457
     22      0.7857        1.0619       0.4988      0.4988        1.4965     +  12.4135
     23      0.8214        1.0217       0.5123      0.5123        1.4937     +  12.4428
     24      0.8661        1.0027       0.4984      0.4984        1.4990        12.4018
     25      0.8661        0.9988       0.5212      0.5212        1.4911     +  12.3879
     26      0.8482        0.9884       0.4898      0.4898        1.5053        12.3542
     27      0.8304        1.0088       0.4972      0.4972        1.4999        12.3682
     28      0.8393        1.0481       0.4938      0.4938        1.4985        12.3571
     29      0.8482        0.9760       0.5285      0.5285        1.4859     +  12.3940
     30      0.8661        0.9744       0.4931      0.4931        1.5007        12.3984
     31      0.8571        0.9663       0.5125      0.5125        1.4896        12.4362
     32      0.8571        0.9801       0.4835      0.4835        1.5107        12.4096
     33      0.8304        0.9871       0.4910      0.4910        1.5002        12.4074
     34      0.8482        1.0054       0.5021      0.5021        1.4854     +  12.3855
     35      0.8482        0.9558       0.5283      0.5283        1.4769     +  12.4337
     36      0.8661        0.9437       0.5224      0.5224        1.4799        12.3995
     37      0.8571        0.9378       0.5245      0.5245        1.4818        12.3647
     38      0.8661        0.9255       0.5181      0.5181        1.4854        12.3388
     39      0.8661        0.9077       0.5227      0.5227        1.4859        12.3606
     40      0.8661        0.9145       0.5198      0.5198        1.4874        12.3466
     41      0.8661        0.9209       0.5212      0.5212        1.4858        12.3909
     42      0.8750        0.9003       0.5222      0.5222        1.4852        12.4064
     43      0.8750        0.9083       0.5196      0.5196        1.4850        12.3906
     44      0.8661        0.9032       0.5208      0.5208        1.4839        12.4499
     45      0.8750        0.8924       0.5217      0.5217        1.4835        12.4939
     46      0.8750        0.9059       0.5200      0.5200        1.4817        12.4231
     47      0.8750        0.8999       0.5210      0.5210        1.4830        12.3444
     48      0.8750        0.8830       0.5219      0.5219        1.4832        12.3757
     49      0.8661        0.8723       0.5200      0.5200        1.4801        12.4686
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 3: 0.5597222222222222
Number of samples used for retraining: 112
Number of samples in pool after training and deleting samples: 26768
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_2.pt 

Iteration:  4
Selecting 96 informative samples:

Training started with 208 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.5240        1.5350       0.5116      0.5116        1.4530     +  12.8033
      2      0.5385        1.4715       0.4993      0.4993        1.4665        12.7534
      3      0.5673        1.4188       0.4837      0.4837        1.4442     +  12.7930
      4      0.5385        1.4264       0.5380      0.5380        1.5073        12.8072
      5      0.5769        1.4312       0.5783      0.5783        1.4051     +  12.8061
      6      0.6779        1.3350       0.5590      0.5590        1.3714     +  12.8203
      7      0.6490        1.2972       0.5696      0.5696        1.3607     +  12.8514
      8      0.6683        1.2447       0.5785      0.5785        1.3508     +  12.8513
      9      0.7067        1.2246       0.6141      0.6141        1.3271     +  12.7657
     10      0.7115        1.2113       0.6023      0.6023        1.3501        12.7636
     11      0.7644        1.1960       0.5227      0.5227        1.4037        12.8117
     12      0.6538        1.2417       0.5056      0.5056        1.4605        12.8298
     13      0.6827        1.3124       0.5710      0.5710        1.3557        12.7891
     14      0.6971        1.2158       0.6441      0.6441        1.3417        12.7618
     15      0.7885        1.1691       0.4809      0.4809        1.4446        12.8118
     16      0.5865        1.3169       0.5545      0.5545        1.3879        12.8056
     17      0.7067        1.2534       0.6165      0.6165        1.3751        12.7545
     18      0.7740        1.1907       0.5410      0.5410        1.3526        12.7314
     19      0.7644        1.1319       0.6453      0.6453        1.3041     +  12.7532
     20      0.8221        1.1187       0.5655      0.5655        1.3146        12.8152
     21      0.8077        1.0900       0.6451      0.6451        1.2915     +  12.9144
     22      0.8221        1.0859       0.5701      0.5701        1.3149        12.8345
     23      0.7692        1.0793       0.6495      0.6495        1.3090        12.8786
     24      0.8221        1.1030       0.5819      0.5819        1.2987        12.8697
     25      0.7740        1.1096       0.6510      0.6510        1.3064        12.8944
     26      0.8077        1.0734       0.5293      0.5293        1.3563        12.8786
     27      0.7981        1.0704       0.5467      0.5467        1.3611        12.8158
     28      0.8125        1.0456       0.6349      0.6349        1.3095        12.7481
     29      0.8173        1.0653       0.5290      0.5290        1.3487        12.8022
     30      0.8269        1.0252       0.6010      0.6010        1.2815     +  12.7703
     31      0.8269        1.0173       0.6125      0.6125        1.3395        12.8678
     32      0.8365        1.0388       0.5097      0.5097        1.3819        12.8227
     33      0.8221        1.0377       0.5759      0.5759        1.3122        12.7642
     34      0.8462        0.9997       0.6259      0.6259        1.3038        12.8060
     35      0.8125        1.0370       0.5410      0.5410        1.3222        12.7702
     36      0.8462        0.9836       0.6184      0.6184        1.2504     +  12.8651
     37      0.8317        0.9651       0.6568      0.6568        1.2470     +  12.7421
     38      0.8510        0.9599       0.5528      0.5528        1.3032        12.7751
     39      0.8365        0.9624       0.6403      0.6403        1.2444     +  12.7944
     40      0.8462        0.9459       0.6134      0.6134        1.2460        12.8073
     41      0.8269        0.9494       0.6602      0.6602        1.2559        12.8063
     42      0.8558        0.9542       0.5620      0.5620        1.2880        12.7784
     43      0.8269        0.9479       0.6490      0.6490        1.2486        12.7716
     44      0.8606        0.9414       0.6200      0.6200        1.2430     +  12.7604
     45      0.8269        0.9400       0.6628      0.6628        1.2435        12.8077
     46      0.8558        0.9252       0.5533      0.5533        1.2957        12.7937
     47      0.8462        0.9231       0.6130      0.6130        1.2621        12.7673
     48      0.8846        0.9158       0.6615      0.6615        1.2314     +  12.8071
     49      0.8558        0.9335       0.5667      0.5667        1.2827        12.8807
     50      0.8750        0.9052       0.6345      0.6345        1.2229     +  12.8023
     51      0.8510        0.9052       0.6694      0.6694        1.2502        12.8009
     52      0.8702        0.9182       0.5611      0.5611        1.2713        12.8192
     53      0.8558        0.8950       0.5936      0.5936        1.2644        12.7783
     54      0.8702        0.8921       0.6682      0.6682        1.2165     +  12.8148
     55      0.8654        0.8928       0.6102      0.6102        1.2283        12.7935
     56      0.9087        0.8665       0.6460      0.6460        1.2016     +  12.8328
     57      0.8654        0.8705       0.6580      0.6580        1.2033        12.8074
     58      0.8750        0.8614       0.5901      0.5901        1.2493        12.7995
     59      0.8846        0.8586       0.6344      0.6344        1.2120        12.7581
     60      0.8942        0.8493       0.6686      0.6686        1.2054        12.7878
     61      0.8942        0.8524       0.5658      0.5658        1.2759        12.7445
     62      0.9087        0.8489       0.6455      0.6455        1.2067        12.7602
     63      0.8942        0.8403       0.6731      0.6731        1.1972     +  12.8172
     64      0.8942        0.8353       0.6002      0.6002        1.2362        12.8301
     65      0.9038        0.8226       0.6601      0.6601        1.1901     +  12.8384
     66      0.9038        0.8177       0.6271      0.6271        1.2091        12.8229
     67      0.9038        0.8184       0.6720      0.6720        1.1980        12.8532
     68      0.8990        0.8297       0.5917      0.5917        1.2381        12.7979
     69      0.8942        0.8178       0.5858      0.5858        1.2555        12.8622
     70      0.9135        0.8138       0.6701      0.6701        1.1863     +  12.7797
     71      0.8942        0.8299       0.6731      0.6731        1.1897        12.8185
     72      0.9087        0.8118       0.5729      0.5729        1.2567        12.8577
     73      0.9038        0.8170       0.5852      0.5852        1.2593        12.8346
     74      0.9135        0.8086       0.6693      0.6693        1.1983        12.8372
     75      0.8942        0.8152       0.6342      0.6342        1.1925        12.7476
     76      0.9231        0.7977       0.6415      0.6415        1.1964        12.8211
     77      0.9231        0.7841       0.6694      0.6694        1.1778     +  12.8567
     78      0.9183        0.7852       0.5925      0.5925        1.2372        12.9013
     79      0.9183        0.7785       0.6403      0.6403        1.1997        12.9151
     80      0.9327        0.7724       0.6762      0.6762        1.1935        12.8786
     81      0.9038        0.7859       0.5981      0.5981        1.2236        12.8416
     82      0.9135        0.7691       0.6292      0.6292        1.1997        12.7879
     83      0.9327        0.7619       0.6795      0.6795        1.1769     +  12.8071
     84      0.9038        0.7792       0.6000      0.6000        1.2226        12.7539
     85      0.9279        0.7587       0.6491      0.6491        1.1842        12.8140
     86      0.9279        0.7487       0.6774      0.6774        1.1686     +  12.7894
     87      0.9087        0.7511       0.6220      0.6220        1.1982        12.8405
     88      0.9327        0.7373       0.6217      0.6217        1.2045        12.8371
     89      0.9375        0.7417       0.6778      0.6778        1.1667     +  12.7688
     90      0.9087        0.7494       0.6691      0.6691        1.1649     +  12.8295
     91      0.9279        0.7392       0.5696      0.5696        1.2677        12.8478
     92      0.9327        0.7559       0.5903      0.5903        1.2569        12.8087
     93      0.9279        0.7405       0.6750      0.6750        1.1993        12.7283
     94      0.9087        0.7553       0.6457      0.6457        1.1800        12.7590
     95      0.9279        0.7269       0.6477      0.6477        1.1792        12.7784
     96      0.9231        0.7225       0.6559      0.6559        1.1682        12.8815
     97      0.9279        0.7181       0.6748      0.6748        1.1603     +  12.8392
     98      0.9375        0.7122       0.6293      0.6293        1.1869        12.8875
     99      0.9423        0.7132       0.6434      0.6434        1.1816        12.8711
    100      0.9327        0.7062       0.6842      0.6842        1.1607        12.8328
F1 Score after query 4: 0.6859375
Number of samples used for retraining: 208
Number of samples in pool after training and deleting samples: 26672
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_3.pt 

Iteration:  5
Selecting 176 informative samples:

Training started with 384 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7917        1.0067       0.4852      0.4852        1.3889     +  13.5569
      2      0.6615        1.1882       0.4988      0.4988        1.4204        13.5750
      3      0.6667        1.1978       0.5323      0.5323        1.2653     +  13.6073
      4      0.6406        1.1958       0.6639      0.6639        1.1324     +  13.6115
      5      0.7240        1.0289       0.6806      0.6806        1.0982     +  13.5061
      6      0.7839        0.9106       0.6802      0.6802        1.1048        13.4847
      7      0.8151        0.8986       0.6431      0.6431        1.1350        13.5175
      8      0.8203        0.8797       0.6490      0.6490        1.1300        13.5157
      9      0.8203        0.8646       0.6760      0.6760        1.1034        13.5625
     10      0.8359        0.8358       0.6825      0.6825        1.0982        13.5311
     11      0.8281        0.8239       0.6771      0.6771        1.1058        13.4951
     12      0.8620        0.8134       0.6207      0.6207        1.1692        13.5057
     13      0.8385        0.8137       0.6590      0.6590        1.1235        13.4553
     14      0.8490        0.8108       0.6108      0.6108        1.1808        13.4193
     15      0.8359        0.8030       0.6648      0.6648        1.1181        13.3907
     16      0.8568        0.7866       0.6786      0.6786        1.1108        13.5249
     17      0.8854        0.7616       0.6950      0.6950        1.0884     +  13.5868
     18      0.8828        0.7473       0.7203      0.7203        1.0857     +  13.4612
     19      0.8594        0.7662       0.7161      0.7161        1.1520        13.5354
     20      0.8490        0.8220       0.6741      0.6741        1.1080        13.5761
     21      0.9115        0.7411       0.6988      0.6988        1.0771     +  13.5579
     22      0.9115        0.7203       0.6790      0.6790        1.0926        13.6072
     23      0.9141        0.7085       0.6917      0.6917        1.0794        13.4972
     24      0.9115        0.7042       0.6797      0.6797        1.0911        13.4633
     25      0.9219        0.6886       0.6837      0.6837        1.0842        13.6382
     26      0.9271        0.6823       0.6641      0.6641        1.0992        13.6400
     27      0.9219        0.6700       0.6922      0.6922        1.0783        13.4460
     28      0.9349        0.6690       0.6849      0.6849        1.0806        13.4681
     29      0.9323        0.6690       0.6901      0.6901        1.0798        13.5275
     30      0.9375        0.6557       0.6939      0.6939        1.0754     +  13.5639
     31      0.9297        0.6550       0.6936      0.6936        1.0754        13.4518
     32      0.9375        0.6578       0.7078      0.7078        1.0693     +  13.4695
     33      0.9193        0.6695       0.7181      0.7181        1.0982        13.5117
     34      0.8854        0.7261       0.7163      0.7163        1.0816        13.4692
     35      0.9297        0.6966       0.6394      0.6394        1.1317        13.5573
     36      0.9375        0.6596       0.6403      0.6403        1.1209        13.5644
     37      0.9375        0.6393       0.6073      0.6073        1.1671        13.4360
     38      0.9349        0.6382       0.6304      0.6304        1.1419        13.6118
     39      0.9375        0.6504       0.6274      0.6274        1.1547        13.5610
     40      0.9323        0.6450       0.6649      0.6649        1.1065        13.5304
     41      0.9375        0.6354       0.6608      0.6608        1.1059        13.4223
     42      0.9297        0.6330       0.6665      0.6665        1.0964        13.5611
     43      0.9323        0.6203       0.6840      0.6840        1.0739        13.5277
     44      0.9375        0.6100       0.6863      0.6863        1.0671     +  13.5275
     45      0.9453        0.5957       0.6878      0.6878        1.0692        13.5139
     46      0.9505        0.5909       0.6924      0.6924        1.0624     +  13.4305
     47      0.9479        0.5833       0.6957      0.6957        1.0589     +  13.5054
     48      0.9531        0.5891       0.6842      0.6842        1.0705        13.5532
     49      0.9505        0.5821       0.6905      0.6905        1.0706        13.4471
     50      0.9505        0.5757       0.6825      0.6825        1.0689        13.4650
     51      0.9479        0.5807       0.6681      0.6681        1.0926        13.5086
     52      0.9453        0.5679       0.6927      0.6927        1.0599        13.6502
     53      0.9479        0.5668       0.6984      0.6984        1.0532     +  13.6119
     54      0.9505        0.5654       0.6797      0.6797        1.0691        13.6070
     55      0.9531        0.5596       0.7009      0.7009        1.0486     +  13.5898
     56      0.9583        0.5590       0.6976      0.6976        1.0513        13.6478
     57      0.9479        0.5602       0.7073      0.7073        1.0384     +  13.5738
     58      0.9479        0.5626       0.7161      0.7161        1.0440        13.4955
     59      0.9089        0.6275       0.6111      0.6111        1.2390        13.4471
     60      0.8646        0.7524       0.6168      0.6168        1.1485        13.5208
     61      0.9193        0.6423       0.6984      0.6984        1.0535        13.5431
     62      0.9453        0.5725       0.6668      0.6668        1.0776        13.5805
     63      0.9479        0.5588       0.6818      0.6818        1.0624        13.5732
     64      0.9479        0.5428       0.6856      0.6856        1.0510        13.5294
     65      0.9609        0.5408       0.6934      0.6934        1.0401        13.4977
     66      0.9531        0.5370       0.6927      0.6927        1.0414        13.4593
     67      0.9479        0.5338       0.6932      0.6932        1.0406        13.3682
     68      0.9583        0.5315       0.6917      0.6917        1.0457        13.4384
     69      0.9557        0.5311       0.6960      0.6960        1.0409        13.5293
     70      0.9583        0.5270       0.6944      0.6944        1.0453        13.5373
     71      0.9557        0.5270       0.6962      0.6962        1.0432        13.4323
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 5: 0.6949652777777777
Number of samples used for retraining: 384
Number of samples in pool after training and deleting samples: 26496
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_4.pt 

Iteration:  6
Selecting 320 informative samples:

Training started with 704 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.6875        1.0438       0.6024      0.6024        1.3034     +  14.5969
      2      0.7216        1.0400       0.6899      0.6899        1.0900     +  14.4557
      3      0.7898        0.9754       0.6703      0.6703        1.1830        14.5110
      4      0.8153        0.9385       0.5616      0.5616        1.2532        14.5982
      5      0.8509        0.8754       0.6392      0.6392        1.1523        14.4450
      6      0.8750        0.7982       0.6970      0.6970        1.0460     +  14.4810
      7      0.8935        0.7599       0.7014      0.7014        1.0415     +  14.4936
      8      0.8977        0.7413       0.7087      0.7087        1.0405     +  14.5105
      9      0.8949        0.7256       0.6335      0.6335        1.1468        14.5990
     10      0.8963        0.7416       0.6873      0.6873        1.0425        14.6252
     11      0.9062        0.7115       0.6983      0.6983        1.0505        14.6244
     12      0.9034        0.6909       0.7099      0.7099        1.0513        14.3910
     13      0.9148        0.6775       0.6962      0.6962        1.0378     +  14.3545
     14      0.9091        0.6731       0.6998      0.6998        1.0619        14.5932
     15      0.9219        0.6576       0.6931      0.6931        1.0232     +  14.5008
     16      0.9105        0.6583       0.7005      0.7005        1.0499        14.5386
     17      0.9233        0.6440       0.6958      0.6958        1.0355        14.6034
     18      0.9205        0.6349       0.6950      0.6950        1.0246        14.5392
     19      0.9190        0.6320       0.7016      0.7016        1.0294        14.6509
     20      0.9233        0.6235       0.6894      0.6894        1.0309        14.4111
     21      0.9190        0.6151       0.6964      0.6964        1.0227     +  14.5910
     22      0.9261        0.6123       0.6891      0.6891        1.0626        14.6177
     23      0.9247        0.6090       0.6559      0.6559        1.0884        14.4649
     24      0.9134        0.6241       0.6908      0.6908        1.0436        14.3821
     25      0.9276        0.5910       0.6837      0.6837        1.0428        14.4405
     26      0.9318        0.5857       0.6884      0.6884        1.0381        14.5310
     27      0.9276        0.5823       0.6922      0.6922        1.0316        14.5973
     28      0.9332        0.5726       0.6774      0.6774        1.0452        14.6616
     29      0.9318        0.5729       0.6802      0.6802        1.0538        14.6064
     30      0.9261        0.5709       0.6733      0.6733        1.0444        14.4434
     31      0.9347        0.5667       0.6950      0.6950        1.0299        14.3826
     32      0.9418        0.5609       0.6837      0.6837        1.0516        14.2855
     33      0.9389        0.5585       0.6741      0.6741        1.0692        14.6198
     34      0.9290        0.5705       0.7040      0.7040        1.0224     +  14.4203
     35      0.9361        0.5466       0.7042      0.7042        1.0265        14.5365
     36      0.9432        0.5411       0.6913      0.6913        1.0378        14.5546
     37      0.9432        0.5407       0.6929      0.6929        1.0327        14.3425
     38      0.9432        0.5337       0.6826      0.6826        1.0549        14.4580
     39      0.9375        0.5316       0.6870      0.6870        1.0482        14.3870
     40      0.9474        0.5249       0.6863      0.6863        1.0589        14.4792
     41      0.9474        0.5190       0.6885      0.6885        1.0540        14.3383
     42      0.9446        0.5172       0.6814      0.6814        1.0743        14.6290
     43      0.9474        0.5167       0.7010      0.7010        1.0433        14.4650
     44      0.9219        0.5757       0.6427      0.6427        1.1190        14.7949
     45      0.9403        0.5256       0.6844      0.6844        1.0422        14.6343
     46      0.9403        0.5123       0.6780      0.6780        1.0692        14.6357
     47      0.9446        0.5071       0.6887      0.6887        1.0709        14.5252
     48      0.9304        0.5358       0.6988      0.6988        1.0122     +  14.6450
     49      0.9489        0.4964       0.7144      0.7144        1.0117     +  14.5566
     50      0.9418        0.4937       0.7075      0.7075        1.0090     +  14.6059
     51      0.9474        0.4905       0.7085      0.7085        1.0057     +  14.5300
     52      0.9503        0.4822       0.6983      0.6983        1.0376        14.6828
     53      0.9545        0.4765       0.6964      0.6964        1.0222        14.5587
     54      0.9517        0.4752       0.6957      0.6957        1.0341        14.5485
     55      0.9517        0.4700       0.6934      0.6934        1.0202        14.6901
     56      0.9432        0.4781       0.7066      0.7066        1.0493        14.4149
     57      0.9403        0.5040       0.5854      0.5854        1.2673        14.3522
     58      0.8494        0.6797       0.6420      0.6420        1.1398        14.3484
     59      0.9247        0.5542       0.5474      0.5474        1.2588        14.4822
     60      0.8963        0.5902       0.6155      0.6155        1.1545        14.3702
     61      0.9290        0.5371       0.7073      0.7073        1.0262        14.5038
     62      0.9403        0.4979       0.7102      0.7102        0.9852     +  14.3898
     63      0.9474        0.4697       0.7158      0.7158        0.9784     +  14.5585
     64      0.9545        0.4596       0.7094      0.7094        0.9886        14.4192
     65      0.9517        0.4535       0.7090      0.7090        0.9887        14.5903
     66      0.9503        0.4460       0.7083      0.7083        0.9854        14.6339
     67      0.9531        0.4478       0.7130      0.7130        0.9858        14.7326
     68      0.9560        0.4415       0.7141      0.7141        0.9844        14.5353
     69      0.9574        0.4385       0.7038      0.7038        1.0031        14.6583
     70      0.9602        0.4359       0.7009      0.7009        1.0020        14.5826
     71      0.9560        0.4325       0.7021      0.7021        1.0196        14.6323
     72      0.9616        0.4296       0.6828      0.6828        1.0535        14.4582
     73      0.9659        0.4294       0.6965      0.6965        1.0252        14.3819
     74      0.9588        0.4290       0.7054      0.7054        1.0430        14.4269
     75      0.9403        0.4626       0.6988      0.6988        1.0104        14.6215
     76      0.9616        0.4208       0.6990      0.6990        1.0127        14.4371
     77      0.9588        0.4223       0.7090      0.7090        1.0292        14.4493
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 6: 0.6876736111111111
Number of samples used for retraining: 704
Number of samples in pool after training and deleting samples: 26176
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_5.pt 

Iteration:  7
Selecting 560 informative samples:

Training started with 1264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7492        0.8531       0.6991      0.6991        0.9953     +  16.1745
      2      0.7120        0.9811       0.7642      0.7642        0.8870     +  16.3190
      3      0.7927        0.7954       0.7786      0.7786        0.7333     +  16.2679
      4      0.8853        0.6222       0.7778      0.7778        0.7335        16.1940
      5      0.8956        0.5838       0.7630      0.7630        0.8026        15.9746
      6      0.9074        0.5560       0.7806      0.7806        0.7333     +  16.0996
      7      0.8829        0.6063       0.7875      0.7875        0.6999     +  16.3412
      8      0.9043        0.5708       0.7780      0.7780        0.7403        16.3665
      9      0.9122        0.5448       0.8017      0.8017        0.7152        16.1169
     10      0.9248        0.5168       0.7802      0.7802        0.7337        16.5020
     11      0.9264        0.5023       0.7939      0.7939        0.7141        16.4607
     12      0.9248        0.5003       0.7847      0.7847        0.7329        16.2848
     13      0.9296        0.4859       0.7868      0.7868        0.7227        15.9732
     14      0.9320        0.4818       0.7814      0.7814        0.7450        16.1336
     15      0.9351        0.4685       0.7858      0.7858        0.7262        16.2556
     16      0.9383        0.4579       0.7783      0.7783        0.7366        16.2907
     17      0.9320        0.4614       0.7863      0.7863        0.7267        16.0596
     18      0.9343        0.4466       0.7858      0.7858        0.7289        16.2403
     19      0.8979        0.5364       0.7745      0.7745        0.7460        16.1150
     20      0.9367        0.4577       0.7866      0.7866        0.7265        16.0964
     21      0.9359        0.4363       0.7882      0.7882        0.7296        16.1186
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 7: 0.7772569444444445
Number of samples used for retraining: 1264
Number of samples in pool after training and deleting samples: 25616
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_6.pt 

Iteration:  8
Selecting 1000 informative samples:

Training started with 2264 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.7345        0.9061       0.7167      0.7167        0.8276     +  18.6603
      2      0.7743        0.8508       0.7811      0.7811        0.6605     +  19.2674
      3      0.8379        0.6873       0.7148      0.7148        0.8065        19.3363
      4      0.7977        0.7769       0.4281      0.4281        1.2204        19.1942
      5      0.8317        0.6799       0.7847      0.7847        0.6226     +  19.0671
      6      0.8812        0.5788       0.7781      0.7781        0.6376        19.1476
      7      0.8520        0.6372       0.4201      0.4201        1.4287        18.9538
      8      0.8754        0.5924       0.7646      0.7646        0.6654        18.8313
      9      0.9046        0.5277       0.7833      0.7833        0.6404        19.0271
     10      0.9002        0.5206       0.8068      0.8068        0.6354        18.8165
     11      0.9090        0.5021       0.7408      0.7408        0.7268        19.1258
     12      0.9130        0.4918       0.7476      0.7476        0.7113        18.9845
     13      0.9218        0.4748       0.7361      0.7361        0.7514        19.0712
     14      0.9134        0.4779       0.8045      0.8045        0.6714        18.8984
     15      0.9156        0.4714       0.7531      0.7531        0.7017        19.0337
     16      0.9267        0.4493       0.7450      0.7450        0.7384        18.8079
     17      0.9262        0.4511       0.7915      0.7915        0.6474        19.1201
     18      0.9015        0.4907       0.6345      0.6345        1.0837        18.6903
     19      0.9024        0.4849       0.8024      0.8024        0.6302        18.7197
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 8: 0.7776041666666667
Number of samples used for retraining: 2264
Number of samples in pool after training and deleting samples: 24616
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_7.pt 

Iteration:  9
Selecting 1776 informative samples:

Training started with 4040 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.8718        0.5847       0.7741      0.7741        0.6976     +  23.8191
      2      0.8800        0.5444       0.7392      0.7392        0.8281        24.0703
      3      0.8973        0.4968       0.6976      0.6976        1.0663        24.2003
      4      0.9000        0.4817       0.7882      0.7882        0.6907     +  23.8919
      5      0.8985        0.4859       0.8094      0.8094        0.5949     +  24.5435
      6      0.9248        0.4301       0.7839      0.7839        0.7059        24.0120
      7      0.9230        0.4206       0.7833      0.7833        0.7211        24.3498
      8      0.9302        0.3973       0.7837      0.7837        0.7436        24.0879
      9      0.9302        0.3984       0.8097      0.8097        0.6376        24.3921
     10      0.9317        0.3790       0.8148      0.8148        0.6333        24.0159
     11      0.9386        0.3638       0.8012      0.8012        0.6763        23.4736
     12      0.9379        0.3555       0.8142      0.8142        0.6436        23.7770
     13      0.9347        0.3663       0.8238      0.8238        0.6045        23.9137
     14      0.9295        0.3706       0.7814      0.7814        0.7243        23.3639
     15      0.9347        0.3650       0.8227      0.8227        0.6215        24.4304
     16      0.9428        0.3302       0.8175      0.8175        0.6297        23.7608
     17      0.9512        0.3178       0.8189      0.8189        0.6375        24.2281
     18      0.9520        0.3114       0.8234      0.8234        0.6254        23.7903
     19      0.9347        0.3468       0.8123      0.8123        0.6321        23.7603
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 9: 0.8430555555555556
Number of samples used for retraining: 4040
Number of samples in pool after training and deleting samples: 22840
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_8.pt 

Iteration:  10
Selecting 3160 informative samples:

Training started with 7200 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.8604        0.5063       0.8016      0.8016        0.6383     +  32.8802
      2      0.8850        0.4470       0.8215      0.8215        0.5635     +  32.7353
      7      0.9290        0.3306       0.8191      0.8191        0.5886        32.7420
      8      0.9336        0.3210       0.8172      0.8172        0.6067        32.4446
      9      0.9358        0.3114       0.8125      0.8125        0.6253        32.5320
     10      0.9403        0.3001       0.8122      0.8122        0.6426        32.4854
     11      0.9413        0.2911       0.8085      0.8085        0.6678        32.1528
     12      0.9436        0.2853       0.8080      0.8080        0.6697        26.3680
     13      0.9393        0.2859       0.8024      0.8024        0.6844        26.1893
     14      0.9433        0.2788       0.8033      0.8033        0.6962        26.1637
     15      0.9468        0.2706       0.7990      0.7990        0.7178        26.3432
     16      0.9503        0.2585       0.7950      0.7950        0.7436        26.3639
     17      0.9468        0.2575       0.7998      0.7998        0.7327        26.3171
     18      0.8669        0.5130       0.7872      0.7872        0.7525        26.2773
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 10: 0.8513888888888889
Number of samples used for retraining: 7200
Number of samples in pool after training and deleting samples: 19680
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_9.pt

Iteration:  11
Selecting 5624 informative samples:

Training started with 12824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.9369        0.2934       0.8068      0.8068        0.6571     +  41.1643
      2      0.9201        0.3347       0.8106      0.8106        0.6306     +  41.0915
      3      0.9475        0.2520       0.8115      0.8115        0.6537        41.1909
      4      0.9519        0.2385       0.8042      0.8042        0.6661        41.2217
      5      0.9563        0.2168       0.8068      0.8068        0.6839        41.4492
      6      0.9576        0.2106       0.8010      0.8010        0.7027        41.2755
      7      0.9641        0.1937       0.8036      0.8036        0.6920        41.2496
      8      0.9641        0.1907       0.7958      0.7958        0.7259        41.3136
      9      0.9641        0.1855       0.8000      0.8000        0.7126        41.2569
     10      0.9689        0.1741       0.7920      0.7920        0.7378        41.1906
     11      0.9669        0.1727       0.7939      0.7939        0.7516        41.2671
     12      0.9697        0.1678       0.7917      0.7917        0.7734        41.2696
     13      0.9720        0.1597       0.7905      0.7905        0.7885        41.1833
     14      0.9725        0.1563       0.7892      0.7892        0.8053        41.2818
     15      0.9698        0.1569       0.7983      0.7983        0.7816        41.2326
     16      0.9739        0.1458       0.7854      0.7854        0.8127        41.2134
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 11: 0.8350694444444444
Number of samples used for retraining: 12824
Number of samples in pool after training and deleting samples: 14056
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_10.pt

Iteration:  12
Selecting 10000 informative samples:

Training started with 22824 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.9784        0.1265       0.7955      0.7955        0.7998     +  67.8315
      2      0.9837        0.1090       0.7997      0.7997        0.7811     +  67.7708
      3      0.9740        0.1317       0.7958      0.7958        0.7974        68.0327
      4      0.9844        0.0978       0.7960      0.7960        0.8258        67.9664
      5      0.9857        0.0946       0.7934      0.7934        0.8349        67.6404
      6      0.9876        0.0883       0.7941      0.7941        0.8362        67.9449
      7      0.9859        0.0899       0.7882      0.7882        0.8594        67.8294
      8      0.9865        0.0862       0.7858      0.7858        0.8527        67.5527
      9      0.9874        0.0819       0.7950      0.7950        0.8850        67.7112
     10      0.9809        0.1004       0.7896      0.7896        0.8675        68.3131
     11      0.9887        0.0774       0.7878      0.7878        0.8789        67.8884
     12      0.9905        0.0715       0.7892      0.7892        0.8525        67.8842
     13      0.9924        0.0666       0.7910      0.7910        0.8572        67.8220
     14      0.9912        0.0678       0.7905      0.7905        0.9136        68.0365
     15      0.9903        0.0673       0.7939      0.7939        0.8803        67.9771
     16      0.9919        0.0638       0.7977      0.7977        0.8988        67.9131
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 12: 0.8298611111111112
Number of samples used for retraining: 22824
Number of samples in pool after training and deleting samples: 4056
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_11.pt

Iteration:  13
Selecting 4056 informative samples:

Training started with 26880 samples:

Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      0.9936        0.0542       0.7806      0.7806        0.9304     +  78.6436
      2      0.9936        0.0527       0.7786      0.7786        0.9852        78.7066
      3      0.9940        0.0506       0.7882      0.7882        0.9530        78.6793
      4      0.9931        0.0526       0.7837      0.7837        0.9629        78.7438
      5      0.9869        0.0718       0.7936      0.7936        0.9226     +  78.6667
      6      0.9946        0.0465       0.8031      0.8031        0.8984     +  78.5386
      7      0.9940        0.0477       0.7991      0.7991        0.9002        78.6721
      8      0.9952        0.0440       0.7936      0.7936        0.9299        78.6660
      9      0.9959        0.0413       0.7922      0.7922        0.9476        78.6416
     10      0.9967        0.0389       0.7873      0.7873        0.9632        78.6180
     11      0.9957        0.0408       0.7915      0.7915        0.9701        78.7613
     12      0.9965        0.0375       0.7885      0.7885        0.9703        78.7441
     13      0.9971        0.0352       0.7896      0.7896        0.9997        78.7164
     14      0.9971        0.0350       0.7990      0.7990        0.9616        78.7058
     15      0.9973        0.0343       0.7990      0.7990        0.9685        78.5951
     16      0.9975        0.0325       0.7964      0.7964        0.9819        78.6283
     17      0.9977        0.0312       0.7976      0.7976        0.9775        78.7035
     18      0.9978        0.0308       0.7964      0.7964        1.0079        78.6467
     19      0.9968        0.0328       0.7943      0.7943        0.9808        78.6297
     20      0.9974        0.0317       0.7974      0.7974        1.0077        78.8398
Stopping since valid_loss has not improved in the last 15 epochs.
F1 Score after query 13: 0.8258680555555556
Number of samples used for retraining: 26880
Number of samples in pool after training and deleting samples: 0
Model checkpoint saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\model_checkpoint_iteration_12.pt
Performance results saved to D:\Shubham\results\Multiclass01\DinoSmall\ActiveLearning\ParamsFromMultilabel\entropy_sampling\Run2\performance_results.npy