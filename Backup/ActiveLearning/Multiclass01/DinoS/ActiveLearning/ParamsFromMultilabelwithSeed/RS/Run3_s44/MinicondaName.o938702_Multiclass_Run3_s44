### Starting TaskPrologue of job 938702 on tg097 at Tue 19 Nov 2024 01:52:16 PM CET
Running on cores 64-95 with governor ondemand
Tue Nov 19 13:52:16 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   36C    P0             54W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
### Finished TaskPrologue

/home/hpc/iwfa/iwfa044h/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).
  from pandas.core.computation.check import NUMEXPR_INSTALLED

  0%|          | 0/26880 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 6796/26880 [00:00<00:00, 67953.75it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 15140/26880 [00:00<00:00, 71958.47it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 23450/26880 [00:00<00:00, 74972.41it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26880/26880 [00:00<00:00, 78871.13it/s]

  0%|          | 0/5760 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5760/5760 [00:00<00:00, 75375.69it/s]

  0%|          | 0/5760 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5760/5760 [00:00<00:00, 87434.78it/s](26880, 5)
(5760, 5)
(5760, 5)
3
multiclass_train_df shape: (26880, 2)
multiclass_test_df shape: (5760, 2)
multiclass_val_df shape: (5760, 2)
Calculated train_size: 8
Adjusted shapes after loading:
X_initial_np: (8, 224, 224, 3) y_initial_np: (8,)
X_train_np: (26880, 224, 224, 3) y_train_np: (26880,)
X_pool_np: (26872, 224, 224, 3) y_pool_np: (26872,)
X_test_np: (5760, 224, 224, 3) y_test_np: (5760,)
X_val_np: (5760, 224, 224, 3) y_val_np: (5760,)

Iteration 1: Using initial samples.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      [36m0.2500[0m        [32m2.1073[0m       [35m0.0016[0m      [31m0.1260[0m        [94m2.1430[0m     +  8.6293
      2      0.0278        2.1667       0.0016      0.1260        [94m2.1381[0m     +  6.4240
      3      [36m0.4167[0m        [32m2.0689[0m       [35m0.0030[0m      [31m0.1267[0m        [94m2.1301[0m     +  6.1788
      4      0.2857        2.0835       [35m0.0057[0m      [31m0.3779[0m        [94m2.1240[0m     +  6.1715
      5      0.1562        [32m2.0020[0m       [35m0.0292[0m      0.2631        [94m2.1112[0m     +  6.1786
      6      0.2857        2.0299       0.0226      0.1362        2.1190        6.1751
      7      0.2500        2.0327       [35m0.0312[0m      0.1405        2.1158        6.1869
      8      0.2917        [32m1.9428[0m       [35m0.0439[0m      0.1454        [94m2.1079[0m     +  6.3313
      9      0.1562        2.0188       [35m0.0655[0m      0.2771        [94m2.0987[0m     +  6.2435
     10      0.1607        1.9652       [35m0.0783[0m      0.2807        [94m2.0907[0m     +  6.1724
     11      [36m0.5625[0m        1.9781       [35m0.0814[0m      0.2819        2.0927        6.1771
     12      0.5000        2.1032       0.0776      [31m0.4083[0m        2.0912        6.1705
     13      0.4167        1.9642       [35m0.1040[0m      0.2930        [94m2.0802[0m     +  6.1740
     14      0.5417        [32m1.8956[0m       [35m0.1245[0m      0.0490        [94m2.0696[0m     +  6.1696
     15      0.3750        1.9006       [35m0.1535[0m      0.0916        [94m2.0603[0m     +  6.3870
     16      0.4250        1.9314       [35m0.1608[0m      0.1142        2.0649        6.1674
     17      [36m0.6667[0m        [32m1.8575[0m       [35m0.1736[0m      0.1228        [94m2.0583[0m     +  6.1707
     18      0.5625        1.9702       [35m0.1818[0m      0.1222        [94m2.0542[0m     +  6.1741
     19      0.5000        [32m1.8196[0m       [35m0.1944[0m      0.1281        [94m2.0417[0m     +  6.1704
     20      0.5417        1.8680       0.1835      0.1226        2.0453        6.1708
     21      0.6667        1.8234       0.1872      0.1229        [94m2.0383[0m     +  6.1718
     22      [36m0.6875[0m        1.8273       0.1896      0.1276        [94m2.0372[0m     +  6.1926
     23      0.6667        [32m1.7829[0m       0.1833      0.1294        2.0505        6.3658
     24      0.5000        1.8008       0.1913      0.1412        2.0421        6.1873
     25      0.5000        1.8221       0.1911      0.1389        2.0472        6.1707
     26      [36m1.0000[0m        1.8199       0.1825      0.1364        2.0517        6.1747
     27      0.5208        [32m1.7339[0m       0.1573      0.1153        2.0664        6.1780
     28      0.6667        [32m1.6173[0m       0.1457      0.1073        2.0839        6.1730
     29      0.5500        1.6762       0.1639      0.1179        2.0724        6.1733
     30      0.6667        1.7072       0.1807      0.1254        2.0600        6.1686
     31      0.5208        1.6970       0.1845      0.1337        2.0588        6.1679
     32      0.8333        1.7027       0.1861      0.1337        2.0565        6.3465
     33      0.8333        [32m1.6153[0m       0.1719      0.1262        2.0630        6.2302
     34      0.6667        [32m1.6149[0m       0.1821      0.1370        2.0663        6.1722
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 1: Test F1 Score: 0.18177083333333333
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_0.pt

Iteration 2: Requesting 16 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      [36m0.6220[0m        [32m1.9161[0m       [35m0.2184[0m      [31m0.2742[0m        [94m2.0246[0m     +  6.2215
      2      0.5065        [32m1.8535[0m       [35m0.2311[0m      [31m0.2787[0m        [94m1.9914[0m     +  6.2021
      3      0.4434        1.8588       [35m0.2594[0m      [31m0.2914[0m        [94m1.9609[0m     +  6.2078
      4      0.4696        [32m1.8346[0m       [35m0.2924[0m      [31m0.3064[0m        [94m1.9368[0m     +  6.2066
      5      [36m0.6548[0m        [32m1.8050[0m       [35m0.3389[0m      [31m0.3252[0m        [94m1.9128[0m     +  6.2130
      6      0.4226        [32m1.7943[0m       [35m0.3595[0m      [31m0.3289[0m        [94m1.8979[0m     +  6.2092
      7      0.6086        1.7959       [35m0.3773[0m      [31m0.3340[0m        [94m1.8834[0m     +  6.2529
      8      0.6027        [32m1.7756[0m       [35m0.3802[0m      [31m0.3371[0m        [94m1.8695[0m     +  6.3642
      9      0.6399        [32m1.7636[0m       [35m0.3826[0m      [31m0.3385[0m        [94m1.8565[0m     +  6.2233
     10      0.5473        [32m1.7432[0m       [35m0.3889[0m      [31m0.3406[0m        [94m1.8453[0m     +  6.1978
     11      [36m0.6949[0m        [32m1.7413[0m       [35m0.3903[0m      [31m0.3419[0m        [94m1.8354[0m     +  6.2014
     12      0.4653        [32m1.7108[0m       0.3898      0.3412        [94m1.8225[0m     +  6.1983
     13      0.5629        [32m1.6897[0m       0.3807      0.3362        [94m1.8145[0m     +  6.2023
     14      0.5875        [32m1.6825[0m       0.3828      0.3379        [94m1.8050[0m     +  6.2051
     15      [36m0.7042[0m        1.7165       0.3882      0.3372        [94m1.7934[0m     +  6.3058
     16      0.4706        [32m1.6692[0m       [35m0.3911[0m      0.3322        [94m1.7840[0m     +  6.3182
     17      0.5952        1.6804       [35m0.3925[0m      0.3383        [94m1.7813[0m     +  6.3114
     18      0.4727        [32m1.6379[0m       [35m0.4087[0m      0.3413        [94m1.7692[0m     +  6.3358
     19      [36m0.7407[0m        [32m1.6073[0m       0.4010      0.3345        [94m1.7640[0m     +  6.2738
     20      0.6042        1.6252       0.3957      0.3315        [94m1.7631[0m     +  6.2117
     21      0.6782        1.6110       0.3762      0.2951        [94m1.7594[0m     +  6.2062
     22      0.7111        1.6219       0.3696      0.2556        [94m1.7504[0m     +  6.2028
     23      0.3652        1.6099       0.3842      0.2926        [94m1.7494[0m     +  6.2037
     24      [36m0.8083[0m        [32m1.5972[0m       0.3892      0.3033        [94m1.7407[0m     +  6.2054
     25      0.6158        [32m1.5696[0m       0.4017      0.3069        [94m1.7320[0m     +  6.2069
     26      0.6786        1.5735       0.3965      0.2864        [94m1.7293[0m     +  6.2094
     27      0.5582        [32m1.5626[0m       0.4003      0.2889        [94m1.7248[0m     +  6.1966
     28      0.7250        [32m1.5560[0m       [35m0.4208[0m      [31m0.4186[0m        [94m1.7092[0m     +  6.2119
     29      0.3515        [32m1.5402[0m       0.3799      0.2652        [94m1.7049[0m     +  6.2230
     30      0.5742        1.5697       0.4043      0.4145        [94m1.6986[0m     +  6.3494
     31      0.6604        [32m1.5018[0m       [35m0.4391[0m      [31m0.5476[0m        [94m1.6890[0m     +  6.2507
     32      0.6023        [32m1.4971[0m       0.4250      0.4069        [94m1.6818[0m     +  6.2072
     33      0.5271        [32m1.4824[0m       0.4243      0.5333        1.6830        6.2220
     34      0.5815        1.5205       0.4193      0.3055        [94m1.6810[0m     +  6.2082
     35      0.4841        1.4839       0.4172      0.4313        1.6844        6.2086
     36      0.6624        1.5068       0.4174      0.4317        1.6845        6.2116
     37      0.7155        [32m1.4397[0m       0.4288      0.4417        [94m1.6799[0m     +  6.2089
     38      0.4361        1.4829       [35m0.4432[0m      0.4596        [94m1.6745[0m     +  6.2057
     39      0.5825        1.4648       0.4297      0.4533        1.6777        6.2186
     40      0.5740        1.4551       [35m0.4571[0m      0.4742        1.6758        6.2215
     41      0.6007        1.4673       0.4198      0.3048        [94m1.6645[0m     +  6.2018
     42      0.6694        [32m1.4239[0m       0.4519      0.4654        [94m1.6600[0m     +  6.2078
     43      0.5735        [32m1.4045[0m       0.4434      0.4658        1.6632        6.3647
     44      0.6389        [32m1.3952[0m       0.4481      0.4629        [94m1.6560[0m     +  6.2572
     45      0.8083        [32m1.3660[0m       0.4408      0.3287        [94m1.6517[0m     +  6.2129
     46      0.7806        1.3783       [35m0.4635[0m      0.4769        [94m1.6444[0m     +  6.2305
     47      0.7288        1.3855       0.4530      0.3406        1.6449        6.2008
     48      0.4819        1.3831       [35m0.4658[0m      0.4756        [94m1.6419[0m     +  6.2186
     49      [36m0.8194[0m        [32m1.3501[0m       [35m0.4707[0m      0.4769        [94m1.6336[0m     +  6.2045
     50      0.5944        1.3992       [35m0.4743[0m      0.4760        [94m1.6318[0m     +  6.2045
     51      0.7109        [32m1.3318[0m       0.4642      0.4740        1.6403        6.2026
     52      0.6315        [32m1.3153[0m       0.4726      0.4790        1.6356        6.2032
     53      0.8056        1.3249       0.4688      0.4750        1.6329        6.2415
     54      [36m0.9194[0m        1.3286       0.4693      0.4767        [94m1.6310[0m     +  6.3980
     55      0.7806        1.3168       0.4710      0.4693        [94m1.6305[0m     +  6.2175
     56      0.6347        [32m1.2931[0m       0.4641      0.3496        [94m1.6247[0m     +  6.2104
     57      0.7778        [32m1.2796[0m       0.4658      0.4731        [94m1.6217[0m     +  6.2034
     58      0.7109        1.3346       [35m0.4818[0m      0.4857        [94m1.6098[0m     +  6.2085
     59      0.7806        1.3097       0.4726      0.4761        1.6130        6.3922
     60      0.6419        [32m1.2788[0m       [35m0.4854[0m      0.3696        1.6155        6.1986
     61      0.4954        1.2934       0.4733      0.4605        1.6142        6.2012
     62      0.7806        [32m1.2622[0m       0.4764      0.3709        1.6202        6.2022
     63      0.5660        1.2898       0.4804      0.4714        1.6158        6.3912
     64      0.6486        1.2828       0.4729      0.4909        1.6315        6.2028
     65      0.7704        [32m1.2564[0m       0.4807      0.3581        1.6156        6.2055
     66      0.6760        [32m1.2530[0m       [35m0.4858[0m      0.3690        1.6120        6.4174
     67      0.7667        1.2647       [35m0.5132[0m      0.3709        [94m1.5929[0m     +  6.2070
     68      0.7760        [32m1.2409[0m       0.4922      0.3730        1.6007        6.2068
     69      0.7149        1.2514       0.4931      0.4738        1.6034        6.2297
     70      [36m0.9510[0m        [32m1.1835[0m       0.4708      0.3746        1.6192        6.3670
     71      0.9405        1.2052       0.4927      0.4757        1.5971        6.2041
     72      0.9510        1.2066       0.4972      0.3773        [94m1.5895[0m     +  6.2076
     73      0.7667        1.2206       0.4939      0.4810        1.5956        6.3828
     74      0.8081        [32m1.1617[0m       0.4804      0.3769        1.6018        6.2073
     75      0.8433        1.1790       0.4884      0.4668        1.6022        6.2069
     76      0.6694        1.2138       0.4804      0.3862        1.6090        6.3520
     77      0.7760        1.1922       0.4894      0.3658        1.5940        6.2500
     78      0.9121        1.1734       0.5071      0.3844        [94m1.5809[0m     +  6.2101
     79      0.7667        1.1941       0.4972      0.3681        1.5896        6.2010
     80      0.9510        [32m1.1410[0m       0.4807      0.4001        1.6037        6.3836
     81      0.9405        1.1897       0.4703      0.3186        1.6251        6.2144
     82      0.7997        1.2569       0.4877      0.3866        1.6075        6.1991
     83      0.9121        1.1642       0.4757      0.3886        1.6094        6.4066
     84      0.9510        [32m1.1130[0m       0.5075      0.4075        [94m1.5798[0m     +  6.2073
     85      0.8016        1.1444       [35m0.5139[0m      0.3841        [94m1.5783[0m     +  6.2063
     86      0.9121        [32m1.1109[0m       0.4970      0.4120        1.5967        6.2205
     87      0.9121        1.1218       0.4918      0.4075        1.5979        6.3721
     88      0.9510        [32m1.0921[0m       [35m0.5172[0m      0.4043        1.5827        6.1976
     89      0.9121        1.0960       [35m0.5194[0m      0.4115        [94m1.5754[0m     +  6.2173
     90      0.9510        [32m1.0754[0m       0.4960      0.4041        1.5943        6.3827
     91      0.9510        1.0799       0.5009      0.3951        1.5851        6.2042
     92      0.9510        1.1192       0.4955      0.4082        1.5815        6.2002
     93      0.9028        [32m1.0449[0m       0.4969      0.3617        1.5938        6.3429
     94      0.9121        1.1184       0.4786      0.4246        1.6095        6.2607
     95      0.8016        1.0848       0.5106      0.3511        1.5793        6.2115
     96      0.8681        1.1070       0.4828      0.4273        1.5982        6.2140
     97      0.9121        1.0614       0.5175      0.3925        [94m1.5642[0m     +  6.4062
     98      0.9510        1.0626       0.5028      0.4247        1.5824        6.2207
     99      0.9405        1.0589       0.5149      0.3875        1.5800        6.2118
    100      0.9121        1.0708       0.4953      0.4253        1.5939        6.3835
Iteration 2: Test F1 Score: 0.48975694444444445
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_1.pt

Iteration 3: Requesting 32 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      [36m0.6836[0m        [32m1.3759[0m       [35m0.5615[0m      [31m0.2295[0m        [94m1.4961[0m     +  6.3130
      2      [36m0.7123[0m        [32m1.3160[0m       [35m0.5648[0m      [31m0.4667[0m        [94m1.4715[0m     +  6.2902
      3      0.6814        [32m1.2889[0m       [35m0.5780[0m      [31m0.5882[0m        [94m1.4495[0m     +  6.4846
      4      0.6960        [32m1.2639[0m       0.5693      0.4664        1.4522        6.2847
      5      0.6894        [32m1.2590[0m       [35m0.5997[0m      0.3424        [94m1.4274[0m     +  6.2778
      6      [36m0.8019[0m        [32m1.2406[0m       0.5766      0.2192        1.4414        6.2792
      7      0.5508        1.2411       [35m0.6146[0m      0.2212        [94m1.4063[0m     +  6.4217
      8      0.5765        [32m1.2154[0m       0.5905      0.2236        1.4268        6.3339
      9      0.5842        [32m1.1988[0m       0.5892      0.2031        1.4172        6.2785
     10      0.5615        1.2377       0.5957      0.2172        1.4218        6.2984
     11      0.4771        1.2141       [35m0.6231[0m      0.2263        [94m1.3883[0m     +  6.4767
     12      0.5793        [32m1.1538[0m       [35m0.6340[0m      0.2326        [94m1.3785[0m     +  6.2849
     13      0.7294        [32m1.1533[0m       0.6276      0.2220        1.3786        6.2860
     14      0.7105        1.1714       0.6141      0.3486        1.3969        6.4742
     15      0.4935        1.1651       0.6215      0.2200        [94m1.3707[0m     +  6.2832
     16      0.5842        [32m1.1441[0m       0.5832      0.2182        1.4109        6.2795
     17      0.5714        1.1722       0.5566      0.3106        1.4293        6.2809
     18      0.4150        1.2237       0.5726      0.2106        1.4292        6.4680
     19      0.5943        [32m1.1397[0m       [35m0.6405[0m      0.2342        [94m1.3468[0m     +  6.2794
     20      0.4969        [32m1.1029[0m       [35m0.6410[0m      0.2369        1.3518        6.2922
     21      0.5742        1.1178       0.6368      0.2253        1.3479        6.4782
     22      0.6355        1.1190       0.6278      0.2281        1.3648        6.2805
     23      0.6235        [32m1.0988[0m       [35m0.6431[0m      0.2313        [94m1.3338[0m     +  6.2788
     24      0.7402        [32m1.0837[0m       0.6384      0.2317        1.3448        6.4011
     25      0.5258        1.1057       0.6356      0.2260        1.3387        6.3636
     26      0.3871        [32m1.0825[0m       0.6016      0.2238        1.3734        6.2731
     27      0.5693        [32m1.0824[0m       0.6030      0.2108        1.3621        6.2820
     28      0.5265        1.0962       0.5767      0.2128        1.3995        6.4693
     29      0.5827        1.0885       0.6431      0.2329        [94m1.3214[0m     +  6.2911
     30      0.5945        [32m1.0674[0m       0.6099      0.2242        1.3626        6.2938
     31      0.6296        1.0740       0.6347      0.2231        1.3360        6.4731
     32      0.6362        [32m1.0603[0m       0.6220      0.2267        1.3559        6.2814
     33      0.5379        [32m1.0305[0m       [35m0.6448[0m      0.2303        [94m1.3118[0m     +  6.2748
     34      0.3866        1.0381       0.6443      0.2316        1.3206        6.2990
     35      0.6353        [32m1.0112[0m       [35m0.6469[0m      0.2320        [94m1.3109[0m     +  6.4731
     36      0.7512        1.0143       [35m0.6486[0m      0.2334        [94m1.3053[0m     +  6.2757
     37      [36m0.8912[0m        1.0129       0.6469      0.2298        [94m1.3028[0m     +  6.2803
     38      0.7632        [32m0.9816[0m       0.6394      0.2332        1.3165        6.4771
     39      0.6354        1.0136       0.6253      0.2215        1.3191        6.2794
     40      0.7282        1.0222       0.5849      0.3438        1.3761        6.2781
     41      0.5384        1.0664       0.5856      0.3271        1.3690        6.4209
     42      0.6095        1.0875       0.5234      0.1995        1.4433        6.3356
     43      0.5401        1.0932       0.6285      0.2265        [94m1.3022[0m     +  6.2791
     44      0.6318        1.0005       0.6431      0.2360        1.3069        6.2862
     45      0.6299        0.9845       [35m0.6523[0m      0.2341        [94m1.2885[0m     +  6.4762
     46      0.6122        [32m0.9805[0m       0.6498      0.2351        1.2895        6.2849
     47      0.6353        [32m0.9701[0m       [35m0.6526[0m      0.2351        [94m1.2834[0m     +  6.2854
     48      0.7636        [32m0.9454[0m       0.6517      0.2332        1.2854        6.4736
     49      0.7632        0.9645       0.6502      0.2324        1.2848        6.2776
     50      0.6356        0.9647       0.6491      0.2289        [94m1.2807[0m     +  6.2782
     51      0.7402        [32m0.9391[0m       [35m0.6542[0m      0.2355        [94m1.2711[0m     +  6.2960
     52      0.6356        0.9489       0.6507      0.2301        1.2759        6.4690
     53      0.7636        [32m0.9177[0m       0.6509      0.2366        1.2824        6.2796
     54      0.6354        0.9313       0.6443      0.2286        1.2918        6.2808
     55      0.7104        0.9575       0.6464      0.2346        1.2842        6.4852
     56      0.6353        0.9415       0.6472      0.2310        1.2802        6.2799
     57      0.6489        [32m0.9127[0m       0.6505      0.2361        1.2735        6.2792
     58      0.6799        [32m0.9068[0m       0.6521      0.2334        1.2727        6.3831
     59      0.6559        0.9112       0.6451      0.2378        1.2859        6.3716
     60      0.7829        0.9073       0.6424      0.2282        1.2836        6.2789
     61      0.5399        0.9301       0.6422      0.2375        1.2934        6.2800
     62      0.6353        0.9273       0.6229      0.2223        1.2977        6.4708
     63      0.6368        0.9246       0.6085      0.2253        1.3316        6.2866
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 3: Test F1 Score: 0.6473958333333333
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_2.pt

Iteration 4: Requesting 56 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      [36m0.7627[0m        [32m1.0576[0m       [35m0.6559[0m      [31m0.2337[0m        [94m1.2433[0m     +  6.6355
      2      0.6546        1.0597       0.6207      0.2201        1.2882        6.4103
      3      0.5087        [32m1.0347[0m       0.6530      [31m0.2356[0m        1.2548        6.4083
      4      0.5522        1.0397       0.5911      0.1985        1.3138        6.4057
      5      0.3384        1.0436       0.6484      [31m0.2363[0m        1.2521        6.5916
      6      0.5724        1.0372       0.5953      0.1990        1.3012        6.4140
      7      0.3740        [32m1.0170[0m       [35m0.6616[0m      [31m0.2398[0m        [94m1.2317[0m     +  6.4073
      8      0.4789        [32m1.0124[0m       0.6139      0.2111        1.2714        6.4093
      9      0.5108        [32m0.9874[0m       [35m0.6651[0m      [31m0.2415[0m        [94m1.2142[0m     +  6.6156
     10      0.6092        [32m0.9855[0m       0.5960      0.1921        1.2902        6.4100
     11      0.5991        0.9880       0.6625      0.2395        1.2171        6.4073
     12      0.5602        [32m0.9692[0m       0.6089      0.2058        1.2695        6.6034
     13      0.3733        0.9897       [35m0.6675[0m      [31m0.2420[0m        [94m1.2008[0m     +  6.4238
     14      0.5981        0.9789       0.6123      0.2130        1.2634        6.4481
     15      0.3786        [32m0.9628[0m       0.6651      0.2378        [94m1.1947[0m     +  6.4009
     16      0.5775        0.9788       0.6017      0.2035        1.2757        6.5908
     17      0.3625        0.9698       [35m0.6677[0m      0.2411        [94m1.1917[0m     +  6.4110
     18      0.6092        [32m0.9461[0m       0.6214      0.2077        1.2345        6.4151
     19      0.4741        [32m0.9343[0m       [35m0.6681[0m      0.2395        [94m1.1785[0m     +  6.6002
     20      0.4683        [32m0.9303[0m       0.6321      0.2245        1.2256        6.4189
     21      0.4749        [32m0.9085[0m       0.6648      0.2330        [94m1.1724[0m     +  6.4038
     22      0.6894        0.9098       0.6391      0.2271        1.2122        6.5652
     23      0.4011        [32m0.9045[0m       0.6651      0.2342        [94m1.1698[0m     +  6.4554
     24      0.5099        [32m0.8881[0m       0.6417      0.2279        1.2076        6.4058
     25      0.5114        0.8892       0.6637      0.2313        [94m1.1619[0m     +  6.4090
     26      0.6126        0.9036       0.6444      0.2336        1.2102        6.5913
     27      0.6343        0.8916       0.6646      0.2304        [94m1.1554[0m     +  6.4682
     28      0.5250        0.9005       0.6241      0.2221        1.2318        6.4152
     29      0.4773        0.9058       0.6667      0.2368        [94m1.1549[0m     +  6.6143
     30      0.6234        0.9091       0.5990      0.2005        1.2640        6.4129
     31      0.3549        0.9273       [35m0.6717[0m      [31m0.2431[0m        1.1647        6.4099
     32      0.5243        0.9143       0.6099      0.1958        1.2413        6.4445
     33      0.3914        0.8921       [35m0.6760[0m      [31m0.2474[0m        [94m1.1505[0m     +  6.5747
     34      0.6387        [32m0.8826[0m       0.6408      0.2198        1.1892        6.4133
     35      0.4187        [32m0.8745[0m       0.6691      0.2423        1.1506        6.4031
     36      0.5101        [32m0.8583[0m       0.6432      0.2204        1.1839        6.6227
     37      0.5183        [32m0.8529[0m       0.6720      0.2438        [94m1.1414[0m     +  6.4451
     38      0.6361        [32m0.8526[0m       0.6431      0.2267        1.1901        6.4063
     39      0.4348        [32m0.8421[0m       0.6703      0.2382        [94m1.1351[0m     +  6.5984
     40      0.5619        [32m0.8399[0m       0.6359      0.2258        1.2047        6.4185
     41      0.4148        0.8481       0.6719      0.2382        [94m1.1293[0m     +  6.4119
     42      0.5126        0.8418       0.6396      0.2280        1.1941        6.4235
     43      0.6470        [32m0.8250[0m       0.6686      0.2360        1.1307        6.6514
     44      0.7237        [32m0.8194[0m       0.6497      0.2370        1.1811        6.4116
     45      0.4444        0.8275       0.6694      0.2341        [94m1.1247[0m     +  6.3996
     46      0.7135        0.8424       0.6477      0.2384        1.1816        6.6405
     47      0.4232        [32m0.8157[0m       0.6679      0.2308        [94m1.1240[0m     +  6.4246
     48      0.6234        0.8358       0.6451      0.2367        1.1844        6.4073
     49      0.6857        0.8232       0.6670      0.2289        1.1265        6.5276
     50      0.5215        0.8205       0.6444      0.2374        1.1897        6.4893
     51      0.6230        0.8331       0.6682      0.2296        1.1271        6.4036
     52      0.5209        0.8206       0.6472      0.2387        1.1795        6.4084
     53      0.5253        0.8221       0.6679      0.2343        [94m1.1238[0m     +  6.5972
     54      [36m0.7731[0m        [32m0.7897[0m       0.6556      0.2395        1.1579        6.4142
     55      0.5291        0.8032       0.6693      0.2390        1.1249        6.4156
     56      0.6558        0.7965       0.6519      0.2300        1.1579        6.6369
     57      0.5959        [32m0.7728[0m       0.6747      0.2468        [94m1.1163[0m     +  6.4184
     58      0.6431        0.7846       0.6464      0.2262        1.1652        6.4069
     59      0.6590        0.7869       [35m0.6818[0m      [31m0.2525[0m        [94m1.1063[0m     +  6.4091
     60      0.5177        0.7803       0.6349      0.2171        1.1879        6.6019
     61      0.4425        0.8020       [35m0.6839[0m      0.2514        [94m1.1022[0m     +  6.4129
     62      0.6559        0.8145       0.6160      0.2077        1.2209        6.4054
     63      0.4283        0.8119       0.6819      0.2499        1.1098        6.5875
     64      0.5614        0.8241       0.6083      0.1958        1.2327        6.4074
     65      0.4214        0.8165       [35m0.6859[0m      [31m0.2528[0m        [94m1.0958[0m     +  6.4058
     66      0.6355        0.7885       0.6380      0.2240        1.1760        6.6119
     67      0.5737        0.7775       0.6771      0.2470        1.0994        6.4259
     68      [36m0.7814[0m        [32m0.7610[0m       0.6575      0.2319        1.1440        6.4374
     69      0.7425        [32m0.7527[0m       0.6734      0.2442        1.1051        6.4936
     70      0.5304        0.7573       0.6564      0.2317        1.1409        6.6309
     71      0.6077        0.7535       0.6703      0.2389        1.1074        6.4298
     72      0.6156        [32m0.7462[0m       0.6663      0.2434        1.1216        6.4180
     73      0.6646        [32m0.7368[0m       0.6655      0.2306        1.1139        6.6310
     74      0.6148        0.7416       0.6618      0.2462        1.1360        6.4275
     75      0.4411        0.7473       0.6703      0.2326        1.1025        6.4369
     76      0.5220        0.7406       0.6467      0.2390        1.1659        6.6243
     77      0.6632        0.7727       0.6708      0.2263        1.1099        6.4323
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 4: Test F1 Score: 0.6776041666666667
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_3.pt

Iteration 5: Requesting 96 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      [36m0.6619[0m        [32m0.9175[0m       [35m0.6877[0m      [31m0.2524[0m        [94m1.0725[0m     +  6.8973
      2      0.4653        0.9211       0.6750      0.2237        1.0963        6.6674
      3      0.4158        0.9329       0.6764      0.2256        1.0880        6.6413
      4      0.4373        [32m0.8989[0m       0.6712      0.2255        1.0761        6.6523
      5      0.4575        [32m0.8710[0m       0.6729      0.2305        [94m1.0678[0m     +  6.6472
      6      0.3848        [32m0.8501[0m       0.6781      0.2330        [94m1.0539[0m     +  6.6641
      7      0.4771        [32m0.8470[0m       [35m0.6891[0m      0.2428        [94m1.0425[0m     +  6.8647
      8      0.5601        [32m0.8417[0m       [35m0.6924[0m      0.2478        [94m1.0390[0m     +  6.6558
      9      0.4662        0.8425       0.6854      0.2439        1.0404        6.6561
     10      0.3962        [32m0.8222[0m       0.6842      0.2423        1.0393        6.8397
     11      0.4916        0.8262       0.6905      0.2438        [94m1.0338[0m     +  6.6719
     12      0.4676        0.8236       [35m0.6950[0m      0.2493        [94m1.0299[0m     +  6.6587
     13      0.4771        [32m0.8112[0m       [35m0.7038[0m      [31m0.2625[0m        [94m1.0161[0m     +  6.6785
     14      0.5616        [32m0.8013[0m       0.6868      0.2505        1.0341        6.6724
     15      0.4109        [32m0.7882[0m       0.6896      0.2545        1.0276        6.9398
     16      0.4117        [32m0.7858[0m       0.7002      0.2583        [94m1.0110[0m     +  6.6867
     17      0.4094        [32m0.7767[0m       [35m0.7045[0m      [31m0.2663[0m        [94m1.0071[0m     +  6.6824
     18      0.4992        [32m0.7757[0m       [35m0.7111[0m      [31m0.2725[0m        [94m0.9951[0m     +  6.6860
     19      0.5014        [32m0.7723[0m       0.7101      0.2679        0.9997        6.9273
     20      0.4129        0.7843       0.7094      0.2686        0.9962        6.7053
     21      0.5149        [32m0.7588[0m       0.7104      [31m0.2741[0m        0.9986        6.6776
     22      0.5521        [32m0.7445[0m       0.7042      0.2726        0.9975        6.9271
     23      0.5132        [32m0.7415[0m       [35m0.7165[0m      [31m0.2801[0m        [94m0.9882[0m     +  6.6856
     24      0.5106        0.7455       [35m0.7174[0m      [31m0.2802[0m        [94m0.9869[0m     +  6.6922
     25      0.4419        [32m0.7355[0m       0.7111      0.2726        0.9932        6.9464
     26      0.4297        [32m0.7258[0m       0.7109      0.2722        0.9930        6.7197
     27      0.4390        0.7294       0.7155      [31m0.2806[0m        [94m0.9831[0m     +  6.6977
     28      0.5163        [32m0.7208[0m       [35m0.7233[0m      [31m0.2904[0m        [94m0.9745[0m     +  6.8549
     29      0.5048        0.7295       [35m0.7302[0m      [31m0.3017[0m        0.9794        6.7815
     30      0.6041        0.7295       [35m0.7352[0m      [31m0.3077[0m        0.9828        6.6723
     31      0.5269        0.7307       0.7344      0.3056        [94m0.9741[0m     +  6.7062
     32      0.6236        [32m0.7158[0m       0.7247      0.2930        [94m0.9731[0m     +  6.9442
     33      0.4633        [32m0.6946[0m       0.7165      0.2829        0.9867        6.7094
     34      0.5387        0.7010       0.7038      0.2736        0.9952        6.7440
     35      0.4466        0.6981       0.7153      0.2817        0.9823        7.1556
     36      0.5392        [32m0.6859[0m       0.7269      0.2883        [94m0.9664[0m     +  6.8807
     37      0.6316        0.6876       0.7205      0.2775        0.9697        6.8747
     38      0.5861        0.7195       0.7028      0.2574        1.0070        7.0122
     39      0.4905        0.7493       0.7036      0.2541        1.0081        6.7447
     40      0.4926        0.7422       0.7033      0.2552        0.9998        6.7405
     41      0.4405        0.6999       0.7266      0.2897        [94m0.9568[0m     +  7.0188
     42      0.4507        [32m0.6639[0m       [35m0.7439[0m      [31m0.3126[0m        [94m0.9425[0m     +  6.7672
     43      0.5871        [32m0.6633[0m       [35m0.7457[0m      [31m0.3192[0m        0.9473        6.7449
     44      0.4611        0.6846       [35m0.7495[0m      [31m0.3213[0m        0.9484        6.9968
     45      0.5845        0.6665       0.7444      0.3144        0.9449        6.7561
     46      0.5473        [32m0.6486[0m       0.7351      0.3022        0.9506        6.7379
     47      0.4622        [32m0.6427[0m       0.7325      0.2992        0.9492        6.7661
     48      0.5508        [32m0.6400[0m       0.7373      0.3046        0.9439        7.0027
     49      0.5931        0.6413       0.7352      0.3018        0.9443        6.7469
     50      0.5509        [32m0.6357[0m       0.7413      0.3086        [94m0.9399[0m     +  6.7467
     51      0.5407        [32m0.6296[0m       0.7465      0.3144        [94m0.9351[0m     +  7.1515
     52      0.4643        [32m0.6249[0m       0.7495      0.3164        [94m0.9343[0m     +  6.7216
     53      0.5493        0.6354       0.7481      0.3119        [94m0.9309[0m     +  6.7207
     54      [36m0.6701[0m        0.6301       0.7425      0.3073        0.9436        6.9647
     55      0.5578        0.6363       0.7450      0.3109        0.9415        6.7351
     56      0.5566        [32m0.6193[0m       0.7488      0.3165        [94m0.9298[0m     +  6.7144
     57      0.6366        [32m0.6139[0m       0.7410      0.3069        0.9334        6.9413
     58      0.6004        [32m0.5990[0m       0.7264      0.2967        0.9455        6.7306
     59      0.4784        0.6019       0.7128      0.2822        0.9703        6.7104
     60      0.5541        0.6087       0.7151      0.2854        0.9639        6.9480
     61      0.5582        0.6025       0.7193      0.2862        0.9594        6.7563
     62      0.4802        0.5994       0.7255      0.2933        0.9505        6.7002
     63      0.5545        [32m0.5825[0m       0.7337      0.2992        0.9382        6.7097
     64      0.5637        [32m0.5798[0m       0.7493      0.3117        [94m0.9186[0m     +  6.9630
     65      0.4798        0.5877       [35m0.7509[0m      0.3167        0.9354        6.7941
     66      0.5505        0.6268       0.6670      0.2832        1.0759        6.7241
     67      0.3692        0.8519       0.5036      0.2024        1.4153        6.9829
     68      0.3041        1.0111       0.7002      0.2515        0.9764        6.7074
     69      0.5384        0.6428       0.7285      0.2821        0.9453        6.6420
     70      0.5581        0.6014       0.7339      0.2979        0.9296        6.8681
     71      0.5995        0.5799       0.7361      0.3055        0.9276        6.6652
     72      0.5648        [32m0.5664[0m       0.7378      0.3030        0.9225        6.6565
     73      0.5688        [32m0.5625[0m       0.7368      0.3015        0.9245        6.7633
     74      0.5611        0.5633       0.7424      0.3113        [94m0.9181[0m     +  6.7740
     75      0.5623        [32m0.5590[0m       0.7469      0.3124        [94m0.9133[0m     +  6.6521
     76      0.6405        0.5611       0.7392      0.3066        0.9209        6.6489
     77      0.4829        [32m0.5539[0m       0.7352      0.3018        0.9261        6.8601
     78      0.4862        [32m0.5495[0m       0.7431      0.3081        0.9164        6.6582
     79      0.5702        [32m0.5438[0m       0.7444      0.3115        0.9162        6.6571
     80      [36m0.7005[0m        0.5442       0.7319      0.2975        0.9304        6.8587
     81      0.4884        [32m0.5356[0m       0.7483      0.3134        [94m0.9091[0m     +  6.6665
     82      0.6531        [32m0.5354[0m       0.7453      0.3080        0.9117        6.6628
     83      0.6936        0.5359       0.7436      0.3098        0.9150        6.8298
     84      0.5731        [32m0.5337[0m       0.7403      0.3061        0.9171        6.7019
     85      0.6582        [32m0.5230[0m       0.7382      0.3056        0.9207        6.6430
     86      0.5738        0.5279       0.7446      0.3104        0.9121        6.6510
     87      0.6175        [32m0.5207[0m       0.7422      0.3098        0.9145        6.8548
     88      0.7005        [32m0.5094[0m       0.7444      0.3098        0.9118        6.6538
     89      0.6604        0.5110       0.7418      0.3079        0.9151        6.6526
     90      0.6159        0.5129       0.7450      0.3091        0.9094        6.8654
     91      0.5721        0.5172       0.7448      0.3068        [94m0.9068[0m     +  6.6748
     92      0.6575        0.5200       0.7490      0.3134        [94m0.9040[0m     +  6.6605
     93      0.6571        0.5117       0.7474      0.3133        0.9044        6.8738
     94      0.6589        [32m0.5070[0m       0.7474      0.3125        0.9060        6.6641
     95      0.5749        [32m0.5007[0m       0.7484      0.3133        [94m0.9033[0m     +  6.6513
     96      0.6556        0.5017       0.7401      0.3078        0.9146        6.7496
     97      0.5764        [32m0.4910[0m       0.7288      0.2959        0.9331        6.7757
     98      [36m0.7048[0m        0.4928       0.7250      0.2930        0.9388        6.6497
     99      0.5775        0.4974       0.7314      0.2997        0.9279        6.6428
    100      0.6193        [32m0.4902[0m       0.7384      0.3051        0.9185        6.8617
Iteration 5: Test F1 Score: 0.7430555555555556
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_4.pt

Iteration 6: Requesting 176 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      [36m0.4619[0m        [32m0.6963[0m       [35m0.7465[0m      [31m0.3139[0m        [94m0.8886[0m     +  7.1178
      2      0.4615        [32m0.6836[0m       0.7415      0.3092        [94m0.8838[0m     +  7.0658
      3      [36m0.5521[0m        [32m0.6598[0m       [35m0.7549[0m      [31m0.3185[0m        [94m0.8611[0m     +  7.2777
      4      0.4683        [32m0.6562[0m       0.7184      0.2777        0.9188        7.0795
      5      0.5051        [32m0.6561[0m       0.7451      [31m0.3253[0m        0.9082        7.0615
      6      0.4519        0.8229       0.5292      0.1609        1.3556        7.0676
      7      0.3791        0.7892       [35m0.7689[0m      [31m0.3312[0m        [94m0.8261[0m     +  7.0499
      8      [36m0.5536[0m        [32m0.6408[0m       0.7646      0.3228        0.8325        7.0524
      9      [36m0.5619[0m        [32m0.6163[0m       [35m0.7714[0m      0.3310        0.8276        7.2697
     10      0.4696        0.6222       0.7701      0.3308        [94m0.8238[0m     +  7.0751
     11      0.5584        [32m0.6127[0m       0.7642      0.3273        0.8245        7.0563
     12      0.5265        [32m0.6013[0m       0.7655      0.3233        0.8288        7.0413
     13      [36m0.5987[0m        [32m0.5937[0m       0.7689      [31m0.3354[0m        [94m0.8177[0m     +  7.0608
     14      0.4836        0.5983       0.7314      0.2851        0.8953        7.2355
     15      0.5680        0.7071       0.7707      0.3293        [94m0.7911[0m     +  7.0783
     16      0.5209        [32m0.5910[0m       0.7668      0.3244        0.8164        7.0510
     17      0.5709        [32m0.5656[0m       0.7556      0.3153        0.8353        7.0486
     18      0.4887        [32m0.5574[0m       0.7705      0.3294        0.8098        7.0599
     19      0.5335        0.5641       0.7441      0.3065        0.8589        7.2685
     20      0.4828        0.5644       0.7453      0.3186        0.8619        7.0541
     21      0.5763        0.5973       0.6823      0.2407        1.0378        7.0493
     22      0.4143        0.6342       0.7312      0.2934        0.8987        7.2521
     23      0.4991        0.6629       0.7514      0.3333        0.8155        7.0843
     24      0.4469        0.5653       0.7625      0.3224        0.8117        7.0592
     25      0.5731        [32m0.5395[0m       0.7630      0.3204        0.8140        7.2627
     26      0.5429        [32m0.5250[0m       0.7674      0.3269        0.7990        7.0812
     27      0.5843        0.5263       0.7639      0.3240        0.8027        7.0564
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 6: Test F1 Score: 0.7739583333333333
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_5.pt

Iteration 7: Requesting 320 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      [36m0.5201[0m        [32m0.6247[0m       [35m0.7705[0m      [31m0.3418[0m        [94m0.7755[0m     +  7.8736
      2      0.4475        0.6375       0.7524      0.3353        0.8109        7.8333
      3      0.4633        0.6496       0.7405      0.3234        0.7991        8.0298
      4      0.4660        [32m0.6213[0m       0.7405      0.3275        0.8138        7.8903
      5      0.4777        [32m0.5963[0m       0.7365      0.3264        0.8106        7.8391
      6      0.4924        [32m0.5933[0m       0.7521      0.3342        0.7887        7.8339
      7      0.4595        [32m0.5744[0m       0.7618      0.3403        [94m0.7719[0m     +  7.8293
      8      0.4384        [32m0.5625[0m       0.7681      0.3384        [94m0.7614[0m     +  7.8349
      9      [36m0.5223[0m        [32m0.5407[0m       0.7366      0.3294        0.7978        8.0741
     10      [36m0.5630[0m        0.5476       0.7566      0.3379        0.7718        7.8589
     11      0.5241        0.5416       0.7651      0.3314        0.7659        7.8758
     12      0.4801        [32m0.5265[0m       0.7321      0.3140        0.8345        7.8266
     13      0.4099        0.6611       0.6776      0.2830        0.9039        8.0573
     14      0.4488        0.5443       0.7481      0.3336        0.7696        7.8649
     15      0.4829        [32m0.5132[0m       0.7688      [31m0.3429[0m        [94m0.7455[0m     +  7.8294
     16      0.5435        [32m0.4976[0m       0.7505      0.3366        0.7654        7.8286
     17      0.4664        [32m0.4923[0m       0.7420      0.3343        0.7824        8.0261
     18      0.5379        0.4932       0.7352      0.3288        0.7957        7.8977
     19      0.4669        [32m0.4916[0m       [35m0.7802[0m      [31m0.3476[0m        [94m0.7288[0m     +  7.8615
     20      0.4954        [32m0.4756[0m       [35m0.7837[0m      [31m0.3485[0m        [94m0.7213[0m     +  7.8331
     21      0.4965        [32m0.4661[0m       0.7203      0.3200        0.8199        7.8346
     22      0.4652        0.4778       0.7406      0.3326        0.7894        8.0447
     23      0.4976        [32m0.4627[0m       0.7710      [31m0.3489[0m        0.7319        7.8962
     24      0.4994        [32m0.4595[0m       0.7734      0.3416        0.7562        7.8310
     25      0.4742        [32m0.4422[0m       0.7413      0.3321        0.7845        8.0262
     26      0.5502        0.4496       0.6830      0.3185        0.8904        7.8638
     27      0.3771        0.7076       0.7238      0.3213        0.8106        7.8524
     28      0.5468        0.4494       0.7200      0.3195        0.8267        8.3938
     29      0.4757        [32m0.4413[0m       0.7450      0.3337        0.7780        8.0170
     30      0.4801        [32m0.4267[0m       0.7420      0.3309        0.7883        7.9623
     31      0.4821        [32m0.4211[0m       0.7476      0.3361        0.7719        8.4674
     32      0.5588        [32m0.4136[0m       0.7528      0.3375        0.7608        8.0388
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 7: Test F1 Score: 0.7479166666666667
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_6.pt

Iteration 8: Requesting 560 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp     dur
-------  ----------  ------------  -----------  ----------  ------------  ----  ------
      1      [36m0.4634[0m        [32m0.4732[0m       [35m0.7622[0m      [31m0.3435[0m        [94m0.7309[0m     +  9.6781
      2      [36m0.4780[0m        0.4821       0.7438      0.3165        0.8169        9.2887
      3      0.4570        [32m0.4558[0m       0.7394      0.3300        0.7653        9.7111
      4      [36m0.4936[0m        [32m0.4466[0m       [35m0.7891[0m      0.3388        [94m0.7158[0m     +  9.3016
      5      [36m0.5293[0m        [32m0.4333[0m       0.7845      [31m0.3505[0m        [94m0.6923[0m     +  9.2921
      6      0.4969        0.4382       0.7245      0.3024        0.9051        9.3076
      7      0.3994        0.6786       0.7703      0.3289        0.7452        9.3070
      8      0.4860        [32m0.4267[0m       0.7726      0.3342        0.7414        9.7225
      9      0.5126        [32m0.4136[0m       0.7698      0.3430        0.7322        9.3559
     10      0.5159        [32m0.4060[0m       0.7826      0.3344        0.7505        9.2983
     11      0.5245        [32m0.4034[0m       0.7852      0.3314        0.7521        9.4527
     12      0.5161        [32m0.3988[0m       0.7780      0.3286        0.7744        9.2412
     13      [36m0.5499[0m        [32m0.3870[0m       0.7302      0.3260        0.7978        9.2417
     14      0.5307        0.3890       [35m0.7896[0m      0.3412        0.7397        9.2396
     15      [36m0.5593[0m        [32m0.3755[0m       0.7736      0.3432        0.7259        9.3174
     16      0.5551        [32m0.3648[0m       0.7658      0.3393        0.7504        9.3682
     17      0.5446        [32m0.3639[0m       0.7363      0.3298        0.7830        9.2500
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 8: Test F1 Score: 0.7784722222222222
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_7.pt

Iteration 9: Requesting 1000 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.5030[0m        [32m0.4748[0m       [35m0.7882[0m      [31m0.3448[0m        [94m0.7145[0m     +  11.6968
      2      [36m0.5283[0m        [32m0.4324[0m       [35m0.7901[0m      [31m0.3461[0m        [94m0.7038[0m     +  11.9048
      3      [36m0.5392[0m        [32m0.4183[0m       0.7884      0.3432        0.7204        11.7473
      4      [36m0.5490[0m        [32m0.4099[0m       [35m0.7936[0m      [31m0.3513[0m        [94m0.7010[0m     +  11.6535
      5      0.5438        [32m0.3771[0m       0.7837      0.3390        0.7416        12.3255
      6      [36m0.5622[0m        [32m0.3681[0m       0.7792      0.3428        0.7441        12.3680
      7      0.5541        [32m0.3558[0m       0.7911      0.3504        0.7248        12.2997
      8      0.5411        0.3730       0.7922      [31m0.3519[0m        [94m0.6895[0m     +  11.9088
      9      0.5596        [32m0.3505[0m       [35m0.7972[0m      [31m0.3530[0m        [94m0.6864[0m     +  12.2989
     10      [36m0.5695[0m        0.3522       0.7856      0.3479        0.7415        12.3756
     11      0.5515        0.3769       0.7648      0.3404        0.7345        12.0176
     12      [36m0.5781[0m        0.3601       0.7819      0.3476        0.6931        11.9422
     13      0.5752        [32m0.3236[0m       [35m0.7979[0m      [31m0.3576[0m        0.6904        11.9369
     14      0.5719        [32m0.3186[0m       0.7943      0.3437        0.7359        11.9485
     15      0.5763        [32m0.3129[0m       0.7809      0.3481        0.7285        12.0448
     16      0.5735        [32m0.3052[0m       0.7839      0.3508        0.7218        11.7992
     17      0.5680        [32m0.3036[0m       0.7856      0.3535        0.7083        11.9167
     18      0.5742        [32m0.2952[0m       0.7892      0.3544        0.7161        12.0833
     19      0.5702        [32m0.2910[0m       0.7943      0.3568        0.7089        12.0917
     20      0.5692        [32m0.2864[0m       0.7915      [31m0.3580[0m        0.7012        12.0833
     21      [36m0.5835[0m        [32m0.2784[0m       0.7911      [31m0.3591[0m        0.6955        11.9613
     22      [36m0.6058[0m        [32m0.2765[0m       [35m0.8012[0m      [31m0.3633[0m        [94m0.6830[0m     +  11.8278
     23      [36m0.6100[0m        [32m0.2694[0m       0.8007      0.3632        [94m0.6812[0m     +  11.5685
     24      0.6086        [32m0.2663[0m       0.7990      0.3624        0.6956        11.9828
     25      0.6056        [32m0.2624[0m       0.7965      0.3612        0.6917        11.9563
     26      [36m0.6212[0m        [32m0.2574[0m       [35m0.8021[0m      0.3622        0.6874        11.7341
     27      0.5998        [32m0.2550[0m       [35m0.8069[0m      [31m0.3652[0m        0.6886        12.0104
     28      0.6209        [32m0.2511[0m       0.8052      [31m0.3656[0m        0.6818        11.9892
     29      [36m0.6220[0m        [32m0.2484[0m       0.8024      0.3642        0.6834        11.5900
     30      0.6219        [32m0.2432[0m       0.8049      0.3643        0.6889        11.7665
     31      [36m0.6315[0m        [32m0.2391[0m       [35m0.8128[0m      [31m0.3674[0m        [94m0.6733[0m     +  12.1351
     32      0.6264        [32m0.2391[0m       [35m0.8130[0m      0.3653        0.6874        12.0358
     33      [36m0.6346[0m        [32m0.2321[0m       0.8057      0.3660        [94m0.6727[0m     +  11.6932
     34      0.6112        [32m0.2298[0m       0.8056      0.3662        0.6771        11.9553
     35      [36m0.6598[0m        [32m0.2260[0m       0.8128      [31m0.3675[0m        0.6800        12.0010
     36      0.6486        [32m0.2225[0m       [35m0.8132[0m      0.3670        0.6749        11.9279
     37      0.6508        [32m0.2188[0m       0.8035      0.3636        0.6874        11.6745
     38      0.6491        [32m0.2154[0m       0.8083      0.3645        0.6863        11.9375
     39      0.6557        [32m0.2121[0m       0.8094      0.3654        0.6878        11.7836
     40      0.6456        [32m0.2112[0m       0.8082      0.3652        0.6924        12.0696
     41      0.6571        [32m0.2065[0m       0.8111      0.3659        0.6936        12.2390
     42      [36m0.6637[0m        [32m0.2030[0m       0.8085      0.3657        0.6960        12.0720
     43      [36m0.6787[0m        [32m0.2010[0m       0.8092      0.3646        0.7087        12.0864
     44      0.6504        [32m0.1984[0m       0.8087      0.3648        0.7024        12.0674
     45      0.6727        [32m0.1978[0m       0.8106      0.3634        0.7081        11.9249
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 9: Test F1 Score: 0.8185763888888888
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_8.pt

Iteration 10: Requesting 1776 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.5565[0m        [32m0.4173[0m       [35m0.7792[0m      [31m0.3564[0m        [94m0.6442[0m     +  15.7834
      2      [36m0.6193[0m        [32m0.2766[0m       0.7703      0.3542        0.6881        15.7986
      3      [36m0.6444[0m        [32m0.2567[0m       0.7618      0.3488        0.7246        15.7827
      4      0.6418        0.2652       0.7623      0.3494        0.7048        15.9246
      5      [36m0.6608[0m        [32m0.2448[0m       [35m0.7833[0m      [31m0.3606[0m        [94m0.6424[0m     +  15.8417
      6      0.6595        [32m0.2388[0m       0.7569      0.3454        0.7321        15.7796
      7      [36m0.7892[0m        [32m0.2275[0m       [35m0.8181[0m      [31m0.3774[0m        [94m0.6040[0m     +  16.2223
      8      0.6533        0.2310       0.7852      0.3639        0.6525        15.7896
      9      0.6696        [32m0.2202[0m       0.7877      0.3652        0.6625        15.7796
     10      0.6700        [32m0.2141[0m       0.7915      0.3667        0.6675        15.7808
     11      0.6804        [32m0.2060[0m       0.8016      0.3723        0.6426        15.7911
     12      0.6732        [32m0.2029[0m       0.8094      0.3752        0.6270        15.9266
     13      0.6791        [32m0.1964[0m       0.8163      [31m0.3798[0m        0.6162        15.8054
     14      0.6985        [32m0.1962[0m       0.8049      0.3743        0.6335        15.7791
     15      0.7028        [32m0.1934[0m       0.8130      0.3775        0.6240        15.7921
     16      0.6888        [32m0.1868[0m       0.8132      0.3628        0.7032        15.7948
     17      0.6883        0.1879       0.7937      0.3673        0.6678        15.9125
     18      0.7147        [32m0.1791[0m       0.8040      0.3719        0.6477        15.7962
     19      0.6942        [32m0.1770[0m       0.8035      0.3724        0.6476        15.7920
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 10: Test F1 Score: 0.8118055555555556
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_9.pt

Iteration 11: Requesting 3160 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.6460[0m        [32m0.2921[0m       [35m0.8142[0m      [31m0.3711[0m        [94m0.5983[0m     +  23.4869
      2      [36m0.6719[0m        [32m0.2232[0m       [35m0.8403[0m      [31m0.3914[0m        [94m0.5451[0m     +  23.7199
      3      [36m0.6797[0m        [32m0.2105[0m       0.8033      0.3740        0.6087        23.5209
      4      [36m0.6951[0m        [32m0.2017[0m       0.7632      0.3469        0.7442        23.4706
      5      0.6852        [32m0.1917[0m       0.7944      0.3690        0.6570        23.4650
      6      0.6924        [32m0.1845[0m       0.8146      0.3814        0.6159        23.4636
      7      [36m0.7010[0m        [32m0.1828[0m       0.7832      0.3616        0.6943        23.4595
      8      [36m0.7097[0m        [32m0.1764[0m       0.8005      0.3728        0.6544        23.5266
      9      0.7053        [32m0.1739[0m       0.7587      0.3433        0.7968        23.6255
     10      0.7035        [32m0.1677[0m       0.8175      [31m0.5083[0m        0.6102        23.4973
     11      [36m0.7101[0m        [32m0.1594[0m       0.8198      [31m0.5089[0m        0.6129        23.3412
     12      [36m0.7259[0m        [32m0.1585[0m       0.8156      0.3817        0.6193        23.3323
     13      [36m0.7319[0m        [32m0.1529[0m       0.8064      0.3798        0.6481        23.4385
     14      [36m0.7344[0m        [32m0.1479[0m       0.8007      0.3736        0.6646        23.3812
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 11: Test F1 Score: 0.8223958333333333
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_10.pt

Iteration 12: Requesting 5624 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.7210[0m        [32m0.1835[0m       [35m0.8507[0m      [31m0.5252[0m        [94m0.5202[0m     +  37.8335
      2      0.7158        [32m0.1743[0m       0.8425      0.5206        0.5512        38.0229
      3      0.7138        0.1853       0.8354      0.5045        0.5779        37.9611
      4      0.7192        [32m0.1650[0m       0.8406      0.5097        0.5694        37.8225
      5      [36m0.7381[0m        [32m0.1554[0m       0.8271      0.4962        0.6362        37.9869
      6      [36m0.7480[0m        [32m0.1476[0m       0.8359      0.5110        0.5699        37.9887
      7      [36m0.7513[0m        [32m0.1452[0m       0.8328      0.5058        0.5898        38.1037
      8      [36m0.7577[0m        [32m0.1415[0m       0.8323      0.3829        0.6082        38.0980
      9      [36m0.7656[0m        [32m0.1356[0m       0.8370      0.3839        0.5819        38.0316
     10      [36m0.7665[0m        [32m0.1321[0m       0.8425      0.5186        0.5582        37.9998
     11      [36m0.7731[0m        [32m0.1253[0m       0.8460      0.5213        0.5655        37.8738
     12      [36m0.7771[0m        0.1273       0.8505      0.4004        0.5382        38.0945
     13      [36m0.7858[0m        [32m0.1228[0m       0.8399      0.5181        0.5862        38.0620
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 12: Test F1 Score: 0.8484375
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_11.pt

Iteration 13: Requesting 10000 samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.7603[0m        [32m0.1595[0m       [35m0.8115[0m      [31m0.6310[0m        [94m0.6987[0m     +  60.6722
      2      [36m0.7619[0m        [32m0.1487[0m       [35m0.8705[0m      0.4162        [94m0.4589[0m     +  60.6697
      3      [36m0.7722[0m        [32m0.1384[0m       0.8582      0.5408        0.4998        60.5947
      4      0.7721        [32m0.1339[0m       0.8663      0.4141        0.4770        60.7952
      5      [36m0.7788[0m        [32m0.1269[0m       0.8602      [31m0.6654[0m        0.4849        60.6871
      6      [36m0.7834[0m        [32m0.1191[0m       0.8538      0.4145        0.5131        60.7359
      7      [36m0.7872[0m        [32m0.1155[0m       0.8571      0.5426        0.5139        60.6252
      8      [36m0.7910[0m        [32m0.1110[0m       0.8531      0.5363        0.5221        60.6228
      9      [36m0.7917[0m        [32m0.1070[0m       0.8490      0.4122        0.5471        60.7548
     10      [36m0.8041[0m        [32m0.1015[0m       0.8542      0.4153        0.5339        60.7726
     11      0.8038        [32m0.0978[0m       0.8543      0.4144        0.5429        60.7620
     12      [36m0.8060[0m        [32m0.0964[0m       0.8474      0.4164        0.5652        60.7476
     13      [36m0.8106[0m        [32m0.0914[0m       0.8535      0.4141        0.5469        60.7116
     14      [36m0.8121[0m        [32m0.0913[0m       0.8578      0.4270        0.5574        60.9852
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 13: Test F1 Score: 0.8413194444444444
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_12.pt

Iteration 14: Requesting all remaining samples.
Re-initializing module.
Re-initializing criterion.
Re-initializing optimizer.
  epoch    train_f1    train_loss    valid_acc    valid_f1    valid_loss    cp      dur
-------  ----------  ------------  -----------  ----------  ------------  ----  -------
      1      [36m0.8058[0m        [32m0.1021[0m       [35m0.8403[0m      [31m0.5283[0m        [94m0.5952[0m     +  70.4323
      2      0.8030        [32m0.0997[0m       [35m0.8517[0m      [31m0.6556[0m        [94m0.5459[0m     +  70.3683
      3      [36m0.8101[0m        [32m0.0950[0m       [35m0.8543[0m      0.4104        0.5749        70.3555
      4      [36m0.8159[0m        [32m0.0898[0m       0.6127      0.4577        2.1989        71.5663
      5      0.8092        0.0962       0.8292      [31m0.7699[0m        0.6415        70.3071
      6      [36m0.8187[0m        [32m0.0829[0m       0.8503      0.4139        0.5822        70.3451
      7      [36m0.8242[0m        [32m0.0789[0m       0.8502      0.4389        0.5856        70.3866
      8      [36m0.8290[0m        [32m0.0740[0m       0.8516      0.5356        0.5954        70.3916
      9      [36m0.8300[0m        [32m0.0716[0m       0.8467      0.4171        0.6224        70.6986
     10      0.8234        0.0772       0.8505      0.4353        0.5913        70.6791
     11      [36m0.8316[0m        [32m0.0687[0m       0.8488      0.4126        0.6047        70.6930
     12      [36m0.8368[0m        [32m0.0626[0m       0.8458      0.5364        0.6382        70.7766
     13      [36m0.8386[0m        0.0648       0.8347      0.4250        0.6864        70.5128
     14      0.8378        0.0644       0.8474      0.4222        0.6061        70.4255

Using cache found in /home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)
  warnings.warn("xFormers is not available (SwiGLU)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)
  warnings.warn("xFormers is not available (Attention)")
/home/hpc/iwfa/iwfa044h/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)
  warnings.warn("xFormers is not available (Block)")
Stopping since valid_loss has not improved in the last 13 epochs.
Iteration 14: Test F1 Score: 0.8432291666666667
Model checkpoint saved to /home/woody/iwfa/iwfa044h/CleanLab_Test/ActiveLearningApproaches/results/multiclass01/MulticlassRS/MLParameters/DinoSmall/Micro/Run3_s4420241119-135257/model_checkpoint_iteration_13.pt
F1 Test Data: [0.18177083333333333, 0.48975694444444445, 0.6473958333333333, 0.6776041666666667, 0.7430555555555556, 0.7739583333333333, 0.7479166666666667, 0.7784722222222222, 0.8185763888888888, 0.8118055555555556, 0.8223958333333333, 0.8484375, 0.8413194444444444, 0.8432291666666667]
Performance Test Data: [[0.18177083333333333, 0.48975694444444445, 0.6473958333333333, 0.6776041666666667, 0.7430555555555556, 0.7739583333333333, 0.7479166666666667, 0.7784722222222222, 0.8185763888888888, 0.8118055555555556, 0.8223958333333333, 0.8484375, 0.8413194444444444, 0.8432291666666667]]
=== JOB_STATISTICS ===
=== current date     : Tue 19 Nov 2024 03:56:24 PM CET
= Job-ID             : 938702 on tinygpu
= Job-Name           : MinicondaName
= Job-Command        : /home/woody/iwfa/iwfa044h/runner.sh
= Initial workdir    : /home/woody/iwfa/iwfa044h
= Queue/Partition    : a100
= Slurm account      : iwfa with QOS=normal
= Requested resources:  for 1-00:00:00
= Elapsed runtime    : 02:04:15
= Total RAM usage    : 65.6 GiB of requested  GiB (%)   
= Node list          : tg097
= Subm/Elig/Start/End: 2024-11-19T13:51:49 / 2024-11-19T13:51:49 / 2024-11-19T13:52:09 / 2024-11-19T15:56:24
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           57.5G   104.9G   209.7G        N/A     154K     500K   1,000K        N/A    
    /home/woody        871.5G  1000.0G  1500.0G        N/A   1,812K   5,000K   7,500K        N/A    
======================
=== GPU utilization ==
gpu_name, gpu_bus_id, pid, gpu_utilization [%], mem_utilization [%], max_memory_usage [MiB], time [ms]
NVIDIA A100-SXM4-40GB, 00000000:81:00.0, 4123103, 86 %, 25 %, 9738 MiB, 7406924 ms
